import argparse
import concurrent.futures
import json
import os
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor

from tqdm import tqdm
import anthropic
from openai import OpenAI


def load_jsonl(file_path):
    """
    Load data from a JSONL (JSON Lines) file.

    Args:
        file_path (str): Path to the input JSONL file.

    Returns:
        list: A list of dictionaries loaded from the file.
    """
    data = []
    with open(file_path, "r", encoding="utf-8") as file:
        for line in file:
            data.append(json.loads(line))
    return data


def new_directory(path):
    """
    Create a new directory if it does not already exist.

    Args:
        path (str): Directory path to create.
    """
    if not os.path.exists(path):
        os.makedirs(path)


def write_response(results, data_list, output_path):
    """
    Write responses back into the data structure and save as a JSONL file.

    Args:
        results (list): A list of response strings aligned with data_list.
        data_list (list): Original loaded data (each a dict) to which responses are added.
        output_path (str): File path to save the output JSONL file.
    """
    formatted_data = []
    for i, data in enumerate(data_list):
        data["response"] = results[i]
        formatted_data.append(data)

    if output_path:
        directory_path = os.path.dirname(output_path)
        new_directory(directory_path)
        with open(output_path, "w", encoding="utf-8") as f:
            for instance in formatted_data:
                f.write(json.dumps(instance) + "\n")


def llm_infer(
    messages,
    model_name,
    temperature=0.001,
    max_tokens=512,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    timeout=10,
    stop=None,
):
    """
    Query a Large Language Model (LLM) API for a response to a given prompt.

    Args:
        messages (list): A list of message dicts as per OpenAI API format.
        model_name (str): Name of the model to query (e.g., "gpt-3.5-turbo", "deepseek", "claude").
        temperature (float): Sampling temperature.
        max_tokens (int): Maximum number of tokens to generate.
        top_p (float): Nucleus sampling parameter.
        frequency_penalty (float): Penalize new tokens based on their frequency.
        presence_penalty (float): Penalize new tokens if they appear in the text so far.
        timeout (int): Timeout for the request (not directly used here).
        stop (list): Tokens at which generation will stop.

    Returns:
        str: The response content generated by the model.
    """
    # Initialize model-specific client
    if model_name in ["gpt-3.5-turbo", "gpt-44o"]:
        engine = model_name
        client = OpenAI(api_key="YOUR_API_KEY")
    elif model_name == "deepseek":
        engine = "deepseek-chat"
        client = OpenAI(
            api_key="YOUR_API_KEY",
            base_url="https://api.deepseek.com",
        )
    elif model_name == "claude":
        # Claude uses the anthropic API
        engine = "claude-3-5-sonnet-20241022"
        client = anthropic.Anthropic(api_key="YOUR_API_KEY")
        return _retry_claude_inference(
            client, engine, messages, temperature, max_tokens
        )
    else:
        raise ValueError("Invalid model name provided.")

    # For models queried via OpenAI interface:
    return _retry_openai_inference(
        client,
        engine,
        messages,
        temperature,
        max_tokens,
        top_p,
        frequency_penalty,
        presence_penalty,
        stop,
    )


def _retry_claude_inference(client, engine, messages, temperature, max_tokens):
    """
    Retry mechanism for Claude inference requests, given its API format.

    Args:
        client: Anthropic API client instance.
        engine (str): Claude model name.
        messages (list): List of user/system messages.
        temperature (float): Sampling temperature.
        max_tokens (int): Maximum number of tokens to generate.

    Returns:
        str: The response text from Claude.
    """
    while True:
        try:
            message = client.messages.create(
                model=engine,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return message.content[0].text
        except Exception as e:
            print(f"Claude API error: {e}")
            time.sleep(1)


def _retry_openai_inference(
    client,
    engine,
    messages,
    temperature,
    max_tokens,
    top_p,
    frequency_penalty,
    presence_penalty,
    stop,
):
    """
    Retry mechanism for OpenAI inference requests.

    Args:
        client: OpenAI API client instance.
        engine (str): OpenAI model name.
        messages (list): Messages to send to the API.
        temperature (float): Sampling temperature.
        max_tokens (int): Maximum number of tokens.
        top_p (float): Nucleus sampling parameter.
        frequency_penalty (float): Frequency penalty.
        presence_penalty (float): Presence penalty.
        stop (list): Stop tokens.

    Returns:
        str: The content of the model's response.
    """
    while True:
        try:
            completion = client.chat.completions.create(
                model=engine,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=top_p,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                stop=stop,
            )
            return completion.choices[0].message.content
        except Exception as e:
            print(f"OpenAI API error: {e}")
            time.sleep(1)


def post_process(response, language="Python"):
    """
    Post-process the model response by extracting code blocks and cleaning them.

    Steps:
    1. Extract code from fenced code blocks (```).
    2. Remove lines containing ellipses ("...").

    Args:
        response (str): The model-generated text.
        language (str): Expected language of the code block (e.g., "Python", "SQL").

    Returns:
        str: Cleaned code string.
    """
    code_pattern = rf"```(?:{language.lower()}|{language})?\n([\s\S]*?)\n```"
    match = re.search(code_pattern, response, re.IGNORECASE)

    if match:
        code = match.group(1)
    else:
        # If the code block pattern is not found, attempt manual cleanup
        code = response.strip()
        code = (
            code.replace("```", "").replace(language.lower(), "").replace(language, "")
        )

    # Remove lines containing "..."
    cleaned_lines = [line for line in code.split("\n") if "..." not in line]

    return "\n".join(cleaned_lines).strip()


def worker_function(task):
    """
    Worker function for multithreading: queries the model and post-processes the result.

    Args:
        task (tuple): (prompt, index, target_language, model_name)

    Returns:
        tuple: (index, response) for sorting and assembling results later.
    """
    prompt, idx, target_language, model_name = task
    messages = [{"role": "user", "content": prompt}]
    response = llm_infer(messages, model_name)
    response = post_process(response, target_language)
    print(response)
    return idx, response


def collect_response_from_gpt(
    prompt_list, model_name, num_threads=32, target_language=None
):
    """
    Generate responses from GPT models in parallel.

    Args:
        prompt_list (list): A list of prompt strings.
        model_name (str): The LLM model name.
        num_threads (int): Number of threads for parallel processing.
        target_language (str): Language for post-processing (e.g., "Python" or "SQL").

    Returns:
        list: A list of responses in the same order as prompt_list.
    """
    tasks = [
        (prompt_list[i], i, target_language, model_name)
        for i in range(len(prompt_list))
    ]

    responses = []
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        future_to_task = {
            executor.submit(worker_function, task): task for task in tasks
        }
        for future in tqdm(
            concurrent.futures.as_completed(future_to_task), total=len(tasks)
        ):
            responses.append(future.result())

    # Sort responses based on the original order (index)
    responses.sort(key=lambda x: x[0])
    return [r[1] for r in responses]


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--prompt_path",
        type=str,
        required=True,
        help="Path to the input JSONL file containing prompts.",
    )
    parser.add_argument(
        "--output_path", type=str, required=True, help="Path to the output JSONL file."
    )
    parser.add_argument(
        "--model_name", type=str, default="gpt-4o", help="Name of the LLM model."
    )
    args = parser.parse_args()

    # Load the prompts and determine target language
    data_list = load_jsonl(args.prompt_path)
    prompts = [data["prompt"] for data in data_list]

    # Infer target language from filename if specified, otherwise default to Python
    file_basename = os.path.basename(args.prompt_path)
    if file_basename.split(".")[0].split("_")[-1] == "postgresql":
        target_language = "SQL"
    elif "identify" in file_basename:
        target_language = "JSON"
    else:
        target_language = "Python"

    # Generate responses
    results = collect_response_from_gpt(prompts, args.model_name, 32, target_language)

    # Write the output results in JSONL format
    write_response(results, data_list, args.output_path)
