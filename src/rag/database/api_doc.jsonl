{"API_Name": "np.abs", "Docstring": "absolute(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the absolute value element-wise.\n\n``np.abs`` is a shorthand for this function.\n\nParameters\n----------\nx : array_like\n    Input array.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nabsolute : ndarray\n    An ndarray containing the absolute value of\n    each element in `x`.  For complex input, ``a + ib``, the\n    absolute value is :math:`\\sqrt{ a^2 + b^2 }`.\n    This is a scalar if `x` is a scalar.\n\nExamples\n--------\n>>> x = np.array([-1.2, 1.2])\n>>> np.absolute(x)\narray([ 1.2,  1.2])\n>>> np.absolute(1.2 + 1j)\n1.5620499351813308\n\nPlot the function over ``[-10, 10]``:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(start=-10, stop=10, num=101)\n>>> plt.plot(x, np.absolute(x))\n>>> plt.show()\n\nPlot the function over the complex plane:\n\n>>> xx = x + 1j * x[:, np.newaxis]\n>>> plt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\n>>> plt.show()\n\nThe `abs` function can be used as a shorthand for ``np.absolute`` on\nndarrays.\n\n>>> x = np.array([-1.2, 1.2])\n>>> abs(x)\narray([1.2, 1.2])", "Library": "NumPy"}
{"API_Name": "np.add", "Docstring": "add(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nAdd arguments element-wise.\n\nParameters\n----------\nx1, x2 : array_like\n    The arrays to be added.\n    If ``x1.shape != x2.shape``, they must be broadcastable to a common\n    shape (which becomes the shape of the output).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nadd : ndarray or scalar\n    The sum of `x1` and `x2`, element-wise.\n    This is a scalar if both `x1` and `x2` are scalars.\n\nNotes\n-----\nEquivalent to `x1` + `x2` in terms of array broadcasting.\n\nExamples\n--------\n>>> np.add(1.0, 4.0)\n5.0\n>>> x1 = np.arange(9.0).reshape((3, 3))\n>>> x2 = np.arange(3.0)\n>>> np.add(x1, x2)\narray([[  0.,   2.,   4.],\n       [  3.,   5.,   7.],\n       [  6.,   8.,  10.]])\n\nThe ``+`` operator can be used as a shorthand for ``np.add`` on ndarrays.\n\n>>> x1 = np.arange(9.0).reshape((3, 3))\n>>> x2 = np.arange(3.0)\n>>> x1 + x2\narray([[ 0.,  2.,  4.],\n       [ 3.,  5.,  7.],\n       [ 6.,  8., 10.]])", "Library": "NumPy"}
{"API_Name": "np.all", "Docstring": "Test whether all array elements along a given axis evaluate to True.\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a logical AND reduction is performed.\n    The default (``axis=None``) is to perform a logical AND over all\n    the dimensions of the input array. `axis` may be negative, in\n    which case it counts from the last to the first axis.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a reduction is performed on multiple\n    axes, instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternate output array in which to place the result.\n    It must have the same shape as the expected output and its\n    type is preserved (e.g., if ``dtype(out)`` is float, the result\n    will consist of 0.0's and 1.0's). See :ref:`ufuncs-output-type` for more\n    details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `all` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in checking for all `True` values.\n    See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nall : ndarray, bool\n    A new boolean or array is returned unless `out` is specified,\n    in which case a reference to `out` is returned.\n\nSee Also\n--------\nndarray.all : equivalent method\n\nany : Test whether any element along a given axis evaluates to True.\n\nNotes\n-----\nNot a Number (NaN), positive infinity and negative infinity\nevaluate to `True` because these are not equal to zero.\n\nExamples\n--------\n>>> np.all([[True,False],[True,True]])\nFalse\n\n>>> np.all([[True,False],[True,True]], axis=0)\narray([ True, False])\n\n>>> np.all([-1, 4, 5])\nTrue\n\n>>> np.all([1.0, np.nan])\nTrue\n\n>>> np.all([[True, True], [False, True]], where=[[True], [False]])\nTrue\n\n>>> o=np.array(False)\n>>> z=np.all([-1, 4, 5], out=o)\n>>> id(z), id(o), z\n(28293632, 28293632, array(True)) # may vary", "Library": "NumPy"}
{"API_Name": "np.amax", "Docstring": "Return the maximum of an array or maximum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the maximum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amax` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The minimum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namax : ndarray or scalar\n    Maximum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is an int, the result is an array of dimension\n    ``a.ndim - 1``. If `axis` is a tuple, the result is an array of \n    dimension ``a.ndim - len(axis)``.\n\nSee Also\n--------\namin :\n    The minimum value of an array along a given axis, propagating any NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignoring any NaNs.\nmaximum :\n    Element-wise maximum of two arrays, propagating any NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignoring any NaNs.\nargmax :\n    Return the indices of the maximum values.\n\nnanmin, minimum, fmin\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding max value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmax.\n\nDon't use `amax` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n``amax(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amax(a)           # Maximum of the flattened array\n3\n>>> np.amax(a, axis=0)   # Maxima along the first axis\narray([2, 3])\n>>> np.amax(a, axis=1)   # Maxima along the second axis\narray([1, 3])\n>>> np.amax(a, where=[False, True], initial=-1, axis=0)\narray([-1,  3])\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amax(b)\nnan\n>>> np.amax(b, where=~np.isnan(b), initial=-1)\n4.0\n>>> np.nanmax(b)\n4.0\n\nYou can use an initial value to compute the maximum of an empty slice, or\nto initialize it to a different value:\n\n>>> np.amax([[-50], [10]], axis=-1, initial=0)\narray([ 0, 10])\n\nNotice that the initial value is used as one of the elements for which the\nmaximum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\n>>> np.amax([5], initial=6)\n6\n>>> max([5], default=6)\n5", "Library": "NumPy"}
{"API_Name": "np.any", "Docstring": "Test whether any array element along a given axis evaluates to True.\n\nReturns single boolean if `axis` is ``None``\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a logical OR reduction is performed.\n    The default (``axis=None``) is to perform a logical OR over all\n    the dimensions of the input array. `axis` may be negative, in\n    which case it counts from the last to the first axis.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a reduction is performed on multiple\n    axes, instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternate output array in which to place the result.  It must have\n    the same shape as the expected output and its type is preserved\n    (e.g., if it is of type float, then it will remain so, returning\n    1.0 for True and 0.0 for False, regardless of the type of `a`).\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `any` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in checking for any `True` values.\n    See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nany : bool or ndarray\n    A new boolean or `ndarray` is returned unless `out` is specified,\n    in which case a reference to `out` is returned.\n\nSee Also\n--------\nndarray.any : equivalent method\n\nall : Test whether all elements along a given axis evaluate to True.\n\nNotes\n-----\nNot a Number (NaN), positive infinity and negative infinity evaluate\nto `True` because these are not equal to zero.\n\nExamples\n--------\n>>> np.any([[True, False], [True, True]])\nTrue\n\n>>> np.any([[True, False], [False, False]], axis=0)\narray([ True, False])\n\n>>> np.any([-1, 0, 5])\nTrue\n\n>>> np.any(np.nan)\nTrue\n\n>>> np.any([[True, False], [False, False]], where=[[False], [True]])\nFalse\n\n>>> o=np.array(False)\n>>> z=np.any([-1, 4, 5], out=o)\n>>> z, o\n(array(True), array(True))\n>>> # Check now that z is a reference to o\n>>> z is o\nTrue\n>>> id(z), id(o) # identity of z and o              # doctest: +SKIP\n(191614240, 191614240)", "Library": "NumPy"}
{"API_Name": "np.append", "Docstring": "Append values to the end of an array.\n\nParameters\n----------\narr : array_like\n    Values are appended to a copy of this array.\nvalues : array_like\n    These values are appended to a copy of `arr`.  It must be of the\n    correct shape (the same shape as `arr`, excluding `axis`).  If\n    `axis` is not specified, `values` can be any shape and will be\n    flattened before use.\naxis : int, optional\n    The axis along which `values` are appended.  If `axis` is not\n    given, both `arr` and `values` are flattened before use.\n\nReturns\n-------\nappend : ndarray\n    A copy of `arr` with `values` appended to `axis`.  Note that\n    `append` does not occur in-place: a new array is allocated and\n    filled.  If `axis` is None, `out` is a flattened array.\n\nSee Also\n--------\ninsert : Insert elements into an array.\ndelete : Delete elements from an array.\n\nExamples\n--------\n>>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\narray([1, 2, 3, ..., 7, 8, 9])\n\nWhen `axis` is specified, `values` must have the correct shape.\n\n>>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n>>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\nTraceback (most recent call last):\n    ...\nValueError: all the input arrays must have same number of dimensions, but\nthe array at index 0 has 2 dimension(s) and the array at index 1 has 1\ndimension(s)", "Library": "NumPy"}
{"API_Name": "np.apply_along_axis", "Docstring": "Apply a function to 1-D slices along the given axis.\n\nExecute `func1d(a, *args, **kwargs)` where `func1d` operates on 1-D arrays\nand `a` is a 1-D slice of `arr` along `axis`.\n\nThis is equivalent to (but faster than) the following use of `ndindex` and\n`s_`, which sets each of ``ii``, ``jj``, and ``kk`` to a tuple of indices::\n\n    Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n    for ii in ndindex(Ni):\n        for kk in ndindex(Nk):\n            f = func1d(arr[ii + s_[:,] + kk])\n            Nj = f.shape\n            for jj in ndindex(Nj):\n                out[ii + jj + kk] = f[jj]\n\nEquivalently, eliminating the inner loop, this can be expressed as::\n\n    Ni, Nk = a.shape[:axis], a.shape[axis+1:]\n    for ii in ndindex(Ni):\n        for kk in ndindex(Nk):\n            out[ii + s_[...,] + kk] = func1d(arr[ii + s_[:,] + kk])\n\nParameters\n----------\nfunc1d : function (M,) -> (Nj...)\n    This function should accept 1-D arrays. It is applied to 1-D\n    slices of `arr` along the specified axis.\naxis : integer\n    Axis along which `arr` is sliced.\narr : ndarray (Ni..., M, Nk...)\n    Input array.\nargs : any\n    Additional arguments to `func1d`.\nkwargs : any\n    Additional named arguments to `func1d`.\n\n    .. versionadded:: 1.9.0\n\n\nReturns\n-------\nout : ndarray  (Ni..., Nj..., Nk...)\n    The output array. The shape of `out` is identical to the shape of\n    `arr`, except along the `axis` dimension. This axis is removed, and\n    replaced with new dimensions equal to the shape of the return value\n    of `func1d`. So if `func1d` returns a scalar `out` will have one\n    fewer dimensions than `arr`.\n\nSee Also\n--------\napply_over_axes : Apply a function repeatedly over multiple axes.\n\nExamples\n--------\n>>> def my_func(a):\n...     \"\"\"Average first and last element of a 1-D array\"\"\"\n...     return (a[0] + a[-1]) * 0.5\n>>> b = np.array([[1,2,3], [4,5,6], [7,8,9]])\n>>> np.apply_along_axis(my_func, 0, b)\narray([4., 5., 6.])\n>>> np.apply_along_axis(my_func, 1, b)\narray([2.,  5.,  8.])\n\nFor a function that returns a 1D array, the number of dimensions in\n`outarr` is the same as `arr`.\n\n>>> b = np.array([[8,1,7], [4,3,9], [5,2,6]])\n>>> np.apply_along_axis(sorted, 1, b)\narray([[1, 7, 8],\n       [3, 4, 9],\n       [2, 5, 6]])\n\nFor a function that returns a higher dimensional array, those dimensions\nare inserted in place of the `axis` dimension.\n\n>>> b = np.array([[1,2,3], [4,5,6], [7,8,9]])\n>>> np.apply_along_axis(np.diag, -1, b)\narray([[[1, 0, 0],\n        [0, 2, 0],\n        [0, 0, 3]],\n       [[4, 0, 0],\n        [0, 5, 0],\n        [0, 0, 6]],\n       [[7, 0, 0],\n        [0, 8, 0],\n        [0, 0, 9]]])", "Library": "NumPy"}
{"API_Name": "np.arange", "Docstring": "arange([start,] stop[, step,], dtype=None, *, like=None)\n\nReturn evenly spaced values within a given interval.\n\n``arange`` can be called with a varying number of positional arguments:\n\n* ``arange(stop)``: Values are generated within the half-open interval\n  ``[0, stop)`` (in other words, the interval including `start` but\n  excluding `stop`).\n* ``arange(start, stop)``: Values are generated within the half-open\n  interval ``[start, stop)``.\n* ``arange(start, stop, step)`` Values are generated within the half-open\n  interval ``[start, stop)``, with spacing between values given by\n  ``step``.\n\nFor integer arguments the function is roughly equivalent to the Python\nbuilt-in :py:class:`range`, but returns an ndarray rather than a ``range``\ninstance.\n\nWhen using a non-integer step, such as 0.1, it is often better to use\n`numpy.linspace`.\n\nSee the Warning sections below for more information.\n\nParameters\n----------\nstart : integer or real, optional\n    Start of interval.  The interval includes this value.  The default\n    start value is 0.\nstop : integer or real\n    End of interval.  The interval does not include this value, except\n    in some cases where `step` is not an integer and floating point\n    round-off affects the length of `out`.\nstep : integer or real, optional\n    Spacing between values.  For any output `out`, this is the distance\n    between two adjacent values, ``out[i+1] - out[i]``.  The default\n    step size is 1.  If `step` is specified as a position argument,\n    `start` must also be given.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, infer the data\n    type from the other input arguments.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\narange : ndarray\n    Array of evenly spaced values.\n\n    For floating point arguments, the length of the result is\n    ``ceil((stop - start)/step)``.  Because of floating point overflow,\n    this rule may result in the last element of `out` being greater\n    than `stop`.\n\nWarnings\n--------\nThe length of the output might not be numerically stable.\n\nAnother stability issue is due to the internal implementation of\n`numpy.arange`.\nThe actual step value used to populate the array is\n``dtype(start + step) - dtype(start)`` and not `step`. Precision loss\ncan occur here, due to casting or due to using floating points when\n`start` is much larger than `step`. This can lead to unexpected\nbehaviour. For example::\n\n  >>> np.arange(0, 5, 0.5, dtype=int)\n  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n  >>> np.arange(-3, 3, 0.5, dtype=int)\n  array([-3, -2, -1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\nIn such cases, the use of `numpy.linspace` should be preferred.\n\nThe built-in :py:class:`range` generates :std:doc:`Python built-in integers\nthat have arbitrary size <python:c-api/long>`, while `numpy.arange`\nproduces `numpy.int32` or `numpy.int64` numbers. This may result in\nincorrect results for large integer values::\n\n  >>> power = 40\n  >>> modulo = 10000\n  >>> x1 = [(n ** power) % modulo for n in range(8)]\n  >>> x2 = [(n ** power) % modulo for n in np.arange(8)]\n  >>> print(x1)\n  [0, 1, 7776, 8801, 6176, 625, 6576, 4001]  # correct\n  >>> print(x2)\n  [0, 1, 7776, 7185, 0, 5969, 4816, 3361]  # incorrect\n\nSee Also\n--------\nnumpy.linspace : Evenly spaced numbers with careful handling of endpoints.\nnumpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\nnumpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n:ref:`how-to-partition`\n\nExamples\n--------\n>>> np.arange(3)\narray([0, 1, 2])\n>>> np.arange(3.0)\narray([ 0.,  1.,  2.])\n>>> np.arange(3,7)\narray([3, 4, 5, 6])\n>>> np.arange(3,7,2)\narray([3, 5])", "Library": "NumPy"}
{"API_Name": "np.arcsin", "Docstring": "arcsin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nInverse sine, element-wise.\n\nParameters\n----------\nx : array_like\n    `y`-coordinate on the unit circle.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nangle : ndarray\n    The inverse sine of each element in `x`, in radians and in the\n    closed interval ``[-pi/2, pi/2]``.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nsin, cos, arccos, tan, arctan, arctan2, emath.arcsin\n\nNotes\n-----\n`arcsin` is a multivalued function: for each `x` there are infinitely\nmany numbers `z` such that :math:`sin(z) = x`.  The convention is to\nreturn the angle `z` whose real part lies in [-pi/2, pi/2].\n\nFor real-valued input data types, *arcsin* always returns real output.\nFor each value that cannot be expressed as a real number or infinity,\nit yields ``nan`` and sets the `invalid` floating point error flag.\n\nFor complex-valued input, `arcsin` is a complex analytic function that\nhas, by convention, the branch cuts [-inf, -1] and [1, inf]  and is\ncontinuous from above on the former and from below on the latter.\n\nThe inverse sine is also known as `asin` or sin^{-1}.\n\nReferences\n----------\nAbramowitz, M. and Stegun, I. A., *Handbook of Mathematical Functions*,\n10th printing, New York: Dover, 1964, pp. 79ff.\nhttps://personal.math.ubc.ca/~cbm/aands/page_79.htm\n\nExamples\n--------\n>>> np.arcsin(1)     # pi/2\n1.5707963267948966\n>>> np.arcsin(-1)    # -pi/2\n-1.5707963267948966\n>>> np.arcsin(0)\n0.0", "Library": "NumPy"}
{"API_Name": "np.argmax", "Docstring": "Returns the indices of the maximum values along an axis.\n\nParameters\n----------\na : array_like\n    Input array.\naxis : int, optional\n    By default, the index is into the flattened array, otherwise\n    along the specified axis.\nout : array, optional\n    If provided, the result will be inserted into this array. It should\n    be of the appropriate shape and dtype.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the array.\n\n    .. versionadded:: 1.22.0\n\nReturns\n-------\nindex_array : ndarray of ints\n    Array of indices into the array. It has the same shape as `a.shape`\n    with the dimension along `axis` removed. If `keepdims` is set to True,\n    then the size of `axis` will be 1 with the resulting array having same\n    shape as `a.shape`.\n\nSee Also\n--------\nndarray.argmax, argmin\namax : The maximum value along a given axis.\nunravel_index : Convert a flat index into an index tuple.\ntake_along_axis : Apply ``np.expand_dims(index_array, axis)``\n                  from argmax to an array as if by calling max.\n\nNotes\n-----\nIn case of multiple occurrences of the maximum values, the indices\ncorresponding to the first occurrence are returned.\n\nExamples\n--------\n>>> a = np.arange(6).reshape(2,3) + 10\n>>> a\narray([[10, 11, 12],\n       [13, 14, 15]])\n>>> np.argmax(a)\n5\n>>> np.argmax(a, axis=0)\narray([1, 1, 1])\n>>> np.argmax(a, axis=1)\narray([2, 2])\n\nIndexes of the maximal elements of a N-dimensional array:\n\n>>> ind = np.unravel_index(np.argmax(a, axis=None), a.shape)\n>>> ind\n(1, 2)\n>>> a[ind]\n15\n\n>>> b = np.arange(6)\n>>> b[1] = 5\n>>> b\narray([0, 5, 2, 3, 4, 5])\n>>> np.argmax(b)  # Only the first occurrence is returned.\n1\n\n>>> x = np.array([[4,2,3], [1,0,3]])\n>>> index_array = np.argmax(x, axis=-1)\n>>> # Same as np.amax(x, axis=-1, keepdims=True)\n>>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\narray([[4],\n       [3]])\n>>> # Same as np.amax(x, axis=-1)\n>>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\narray([4, 3])\n\nSetting `keepdims` to `True`,\n\n>>> x = np.arange(24).reshape((2, 3, 4))\n>>> res = np.argmax(x, axis=1, keepdims=True)\n>>> res.shape\n(2, 1, 4)", "Library": "NumPy"}
{"API_Name": "np.argmin", "Docstring": "Returns the indices of the minimum values along an axis.\n\nParameters\n----------\na : array_like\n    Input array.\naxis : int, optional\n    By default, the index is into the flattened array, otherwise\n    along the specified axis.\nout : array, optional\n    If provided, the result will be inserted into this array. It should\n    be of the appropriate shape and dtype.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the array.\n\n    .. versionadded:: 1.22.0\n\nReturns\n-------\nindex_array : ndarray of ints\n    Array of indices into the array. It has the same shape as `a.shape`\n    with the dimension along `axis` removed. If `keepdims` is set to True,\n    then the size of `axis` will be 1 with the resulting array having same\n    shape as `a.shape`.\n\nSee Also\n--------\nndarray.argmin, argmax\namin : The minimum value along a given axis.\nunravel_index : Convert a flat index into an index tuple.\ntake_along_axis : Apply ``np.expand_dims(index_array, axis)``\n                  from argmin to an array as if by calling min.\n\nNotes\n-----\nIn case of multiple occurrences of the minimum values, the indices\ncorresponding to the first occurrence are returned.\n\nExamples\n--------\n>>> a = np.arange(6).reshape(2,3) + 10\n>>> a\narray([[10, 11, 12],\n       [13, 14, 15]])\n>>> np.argmin(a)\n0\n>>> np.argmin(a, axis=0)\narray([0, 0, 0])\n>>> np.argmin(a, axis=1)\narray([0, 0])\n\nIndices of the minimum elements of a N-dimensional array:\n\n>>> ind = np.unravel_index(np.argmin(a, axis=None), a.shape)\n>>> ind\n(0, 0)\n>>> a[ind]\n10\n\n>>> b = np.arange(6) + 10\n>>> b[4] = 10\n>>> b\narray([10, 11, 12, 13, 10, 15])\n>>> np.argmin(b)  # Only the first occurrence is returned.\n0\n\n>>> x = np.array([[4,2,3], [1,0,3]])\n>>> index_array = np.argmin(x, axis=-1)\n>>> # Same as np.amin(x, axis=-1, keepdims=True)\n>>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1)\narray([[2],\n       [0]])\n>>> # Same as np.amax(x, axis=-1)\n>>> np.take_along_axis(x, np.expand_dims(index_array, axis=-1), axis=-1).squeeze(axis=-1)\narray([2, 0])\n\nSetting `keepdims` to `True`,\n\n>>> x = np.arange(24).reshape((2, 3, 4))\n>>> res = np.argmin(x, axis=1, keepdims=True)\n>>> res.shape\n(2, 1, 4)", "Library": "NumPy"}
{"API_Name": "np.argsort", "Docstring": "Returns the indices that would sort an array.\n\nPerform an indirect sort along the given axis using the algorithm specified\nby the `kind` keyword. It returns an array of indices of the same shape as\n`a` that index data along the given axis in sorted order.\n\nParameters\n----------\na : array_like\n    Array to sort.\naxis : int or None, optional\n    Axis along which to sort.  The default is -1 (the last axis). If None,\n    the flattened array is used.\nkind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n    Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n    and 'mergesort' use timsort under the covers and, in general, the\n    actual implementation will vary with data type. The 'mergesort' option\n    is retained for backwards compatibility.\n\n    .. versionchanged:: 1.15.0.\n       The 'stable' option was added.\norder : str or list of str, optional\n    When `a` is an array with fields defined, this argument specifies\n    which fields to compare first, second, etc.  A single field can\n    be specified as a string, and not all fields need be specified,\n    but unspecified fields will still be used, in the order in which\n    they come up in the dtype, to break ties.\n\nReturns\n-------\nindex_array : ndarray, int\n    Array of indices that sort `a` along the specified `axis`.\n    If `a` is one-dimensional, ``a[index_array]`` yields a sorted `a`.\n    More generally, ``np.take_along_axis(a, index_array, axis=axis)``\n    always yields the sorted `a`, irrespective of dimensionality.\n\nSee Also\n--------\nsort : Describes sorting algorithms used.\nlexsort : Indirect stable sort with multiple keys.\nndarray.sort : Inplace sort.\nargpartition : Indirect partial sort.\ntake_along_axis : Apply ``index_array`` from argsort\n                  to an array as if by calling sort.\n\nNotes\n-----\nSee `sort` for notes on the different sorting algorithms.\n\nAs of NumPy 1.4.0 `argsort` works with real/complex arrays containing\nnan values. The enhanced sort order is documented in `sort`.\n\nExamples\n--------\nOne dimensional array:\n\n>>> x = np.array([3, 1, 2])\n>>> np.argsort(x)\narray([1, 2, 0])\n\nTwo-dimensional array:\n\n>>> x = np.array([[0, 3], [2, 2]])\n>>> x\narray([[0, 3],\n       [2, 2]])\n\n>>> ind = np.argsort(x, axis=0)  # sorts along first axis (down)\n>>> ind\narray([[0, 1],\n       [1, 0]])\n>>> np.take_along_axis(x, ind, axis=0)  # same as np.sort(x, axis=0)\narray([[0, 2],\n       [2, 3]])\n\n>>> ind = np.argsort(x, axis=1)  # sorts along last axis (across)\n>>> ind\narray([[0, 1],\n       [0, 1]])\n>>> np.take_along_axis(x, ind, axis=1)  # same as np.sort(x, axis=1)\narray([[0, 3],\n       [2, 2]])\n\nIndices of the sorted elements of a N-dimensional array:\n\n>>> ind = np.unravel_index(np.argsort(x, axis=None), x.shape)\n>>> ind\n(array([0, 1, 1, 0]), array([0, 0, 1, 1]))\n>>> x[ind]  # same as np.sort(x, axis=None)\narray([0, 2, 2, 3])\n\nSorting with keys:\n\n>>> x = np.array([(1, 0), (0, 1)], dtype=[('x', '<i4'), ('y', '<i4')])\n>>> x\narray([(1, 0), (0, 1)],\n      dtype=[('x', '<i4'), ('y', '<i4')])\n\n>>> np.argsort(x, order=('x','y'))\narray([1, 0])\n\n>>> np.argsort(x, order=('y','x'))\narray([0, 1])", "Library": "NumPy"}
{"API_Name": "np.argwhere", "Docstring": "Find the indices of array elements that are non-zero, grouped by element.\n\nParameters\n----------\na : array_like\n    Input data.\n\nReturns\n-------\nindex_array : (N, a.ndim) ndarray\n    Indices of elements that are non-zero. Indices are grouped by element.\n    This array will have shape ``(N, a.ndim)`` where ``N`` is the number of\n    non-zero items.\n\nSee Also\n--------\nwhere, nonzero\n\nNotes\n-----\n``np.argwhere(a)`` is almost the same as ``np.transpose(np.nonzero(a))``,\nbut produces a result of the correct shape for a 0D array.\n\nThe output of ``argwhere`` is not suitable for indexing arrays.\nFor this purpose use ``nonzero(a)`` instead.\n\nExamples\n--------\n>>> x = np.arange(6).reshape(2,3)\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.argwhere(x>1)\narray([[0, 2],\n       [1, 0],\n       [1, 1],\n       [1, 2]])", "Library": "NumPy"}
{"API_Name": "np.array", "Docstring": "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0,\n      like=None)\n\nCreate an array.\n\nParameters\n----------\nobject : array_like\n    An array, any object exposing the array interface, an object whose\n    __array__ method returns an array, or any (nested) sequence.\n    If object is a scalar, a 0-dimensional array containing object is\n    returned.\ndtype : data-type, optional\n    The desired data-type for the array.  If not given, then the type will\n    be determined as the minimum type required to hold the objects in the\n    sequence.\ncopy : bool, optional\n    If true (default), then the object is copied.  Otherwise, a copy will\n    only be made if __array__ returns a copy, if obj is a nested sequence,\n    or if a copy is needed to satisfy any of the other requirements\n    (`dtype`, `order`, etc.).\norder : {'K', 'A', 'C', 'F'}, optional\n    Specify the memory layout of the array. If object is not an array, the\n    newly created array will be in C order (row major) unless 'F' is\n    specified, in which case it will be in Fortran order (column major).\n    If object is an array the following holds.\n\n    ===== ========= ===================================================\n    order  no copy                     copy=True\n    ===== ========= ===================================================\n    'K'   unchanged F & C order preserved, otherwise most similar order\n    'A'   unchanged F order if input is F and not C, otherwise C order\n    'C'   C order   C order\n    'F'   F order   F order\n    ===== ========= ===================================================\n\n    When ``copy=False`` and a copy is made for other reasons, the result is\n    the same as if ``copy=True``, with some exceptions for 'A', see the\n    Notes section. The default order is 'K'.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise\n    the returned array will be forced to be a base-class array (default).\nndmin : int, optional\n    Specifies the minimum number of dimensions that the resulting\n    array should have.  Ones will be prepended to the shape as\n    needed to meet this requirement.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    An array object satisfying the specified requirements.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\nWhen order is 'A' and `object` is an array in neither 'C' nor 'F' order,\nand a copy is forced by a change in dtype, then the order of the result is\nnot necessarily 'C' as expected. This is likely a bug.\n\nExamples\n--------\n>>> np.array([1, 2, 3])\narray([1, 2, 3])\n\nUpcasting:\n\n>>> np.array([1, 2, 3.0])\narray([ 1.,  2.,  3.])\n\nMore than one dimension:\n\n>>> np.array([[1, 2], [3, 4]])\narray([[1, 2],\n       [3, 4]])\n\nMinimum dimensions 2:\n\n>>> np.array([1, 2, 3], ndmin=2)\narray([[1, 2, 3]])\n\nType provided:\n\n>>> np.array([1, 2, 3], dtype=complex)\narray([ 1.+0.j,  2.+0.j,  3.+0.j])\n\nData-type consisting of more than one element:\n\n>>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n>>> x['a']\narray([1, 3])\n\nCreating an array from sub-classes:\n\n>>> np.array(np.mat('1 2; 3 4'))\narray([[1, 2],\n       [3, 4]])\n\n>>> np.array(np.mat('1 2; 3 4'), subok=True)\nmatrix([[1, 2],\n        [3, 4]])", "Library": "NumPy"}
{"API_Name": "np.array_equal", "Docstring": "True if two arrays have the same shape and elements, False otherwise.\n\nParameters\n----------\na1, a2 : array_like\n    Input arrays.\nequal_nan : bool\n    Whether to compare NaN's as equal. If the dtype of a1 and a2 is\n    complex, values will be considered equal if either the real or the\n    imaginary component of a given value is ``nan``.\n\n    .. versionadded:: 1.19.0\n\nReturns\n-------\nb : bool\n    Returns True if the arrays are equal.\n\nSee Also\n--------\nallclose: Returns True if two arrays are element-wise equal within a\n          tolerance.\narray_equiv: Returns True if input arrays are shape consistent and all\n             elements equal.\n\nExamples\n--------\n>>> np.array_equal([1, 2], [1, 2])\nTrue\n>>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\nTrue\n>>> np.array_equal([1, 2], [1, 2, 3])\nFalse\n>>> np.array_equal([1, 2], [1, 4])\nFalse\n>>> a = np.array([1, np.nan])\n>>> np.array_equal(a, a)\nFalse\n>>> np.array_equal(a, a, equal_nan=True)\nTrue\n\nWhen ``equal_nan`` is True, complex values with nan components are\nconsidered equal if either the real *or* the imaginary components are nan.\n\n>>> a = np.array([1 + 1j])\n>>> b = a.copy()\n>>> a.real = np.nan\n>>> b.imag = np.nan\n>>> np.array_equal(a, b, equal_nan=True)\nTrue", "Library": "NumPy"}
{"API_Name": "np.asarray", "Docstring": "asarray(a, dtype=None, order=None, *, like=None)\n\nConvert the input to an array.\n\nParameters\n----------\na : array_like\n    Input data, in any form that can be converted to an array.  This\n    includes lists, lists of tuples, tuples, tuples of tuples, tuples\n    of lists and ndarrays.\ndtype : data-type, optional\n    By default, the data-type is inferred from the input data.\norder : {'C', 'F', 'A', 'K'}, optional\n    Memory layout.  'A' and 'K' depend on the order of input array a.\n    'C' row-major (C-style),\n    'F' column-major (Fortran-style) memory representation.\n    'A' (any) means 'F' if `a` is Fortran contiguous, 'C' otherwise\n    'K' (keep) preserve input order\n    Defaults to 'K'.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Array interpretation of `a`.  No copy is performed if the input\n    is already an ndarray with matching dtype and order.  If `a` is a\n    subclass of ndarray, a base class ndarray is returned.\n\nSee Also\n--------\nasanyarray : Similar function which passes through subclasses.\nascontiguousarray : Convert input to a contiguous array.\nasfarray : Convert input to a floating point ndarray.\nasfortranarray : Convert input to an ndarray with column-major\n                 memory order.\nasarray_chkfinite : Similar function which checks input for NaNs and Infs.\nfromiter : Create an array from an iterator.\nfromfunction : Construct an array by executing a function on grid\n               positions.\n\nExamples\n--------\nConvert a list into an array:\n\n>>> a = [1, 2]\n>>> np.asarray(a)\narray([1, 2])\n\nExisting arrays are not copied:\n\n>>> a = np.array([1, 2])\n>>> np.asarray(a) is a\nTrue\n\nIf `dtype` is set, array is copied only if dtype does not match:\n\n>>> a = np.array([1, 2], dtype=np.float32)\n>>> np.asarray(a, dtype=np.float32) is a\nTrue\n>>> np.asarray(a, dtype=np.float64) is a\nFalse\n\nContrary to `asanyarray`, ndarray subclasses are not passed through:\n\n>>> issubclass(np.recarray, np.ndarray)\nTrue\n>>> a = np.array([(1.0, 2), (3.0, 4)], dtype='f4,i4').view(np.recarray)\n>>> np.asarray(a) is a\nFalse\n>>> np.asanyarray(a) is a\nTrue", "Library": "NumPy"}
{"API_Name": "np.bincount", "Docstring": "bincount(x, /, weights=None, minlength=0)\n\nCount number of occurrences of each value in array of non-negative ints.\n\nThe number of bins (of size 1) is one larger than the largest value in\n`x`. If `minlength` is specified, there will be at least this number\nof bins in the output array (though it will be longer if necessary,\ndepending on the contents of `x`).\nEach bin gives the number of occurrences of its index value in `x`.\nIf `weights` is specified the input array is weighted by it, i.e. if a\nvalue ``n`` is found at position ``i``, ``out[n] += weight[i]`` instead\nof ``out[n] += 1``.\n\nParameters\n----------\nx : array_like, 1 dimension, nonnegative ints\n    Input array.\nweights : array_like, optional\n    Weights, array of the same shape as `x`.\nminlength : int, optional\n    A minimum number of bins for the output array.\n\n    .. versionadded:: 1.6.0\n\nReturns\n-------\nout : ndarray of ints\n    The result of binning the input array.\n    The length of `out` is equal to ``np.amax(x)+1``.\n\nRaises\n------\nValueError\n    If the input is not 1-dimensional, or contains elements with negative\n    values, or if `minlength` is negative.\nTypeError\n    If the type of the input is float or complex.\n\nSee Also\n--------\nhistogram, digitize, unique\n\nExamples\n--------\n>>> np.bincount(np.arange(5))\narray([1, 1, 1, 1, 1])\n>>> np.bincount(np.array([0, 1, 1, 3, 2, 1, 7]))\narray([1, 3, 1, 1, 0, 0, 0, 1])\n\n>>> x = np.array([0, 1, 1, 3, 2, 1, 7, 23])\n>>> np.bincount(x).size == np.amax(x)+1\nTrue\n\nThe input array needs to be of integer dtype, otherwise a\nTypeError is raised:\n\n>>> np.bincount(np.arange(5, dtype=float))\nTraceback (most recent call last):\n  ...\nTypeError: Cannot cast array data from dtype('float64') to dtype('int64')\naccording to the rule 'safe'\n\nA possible use of ``bincount`` is to perform sums over\nvariable-size chunks of an array, using the ``weights`` keyword.\n\n>>> w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights\n>>> x = np.array([0, 1, 1, 2, 2, 2])\n>>> np.bincount(x,  weights=w)\narray([ 0.3,  0.7,  1.1])", "Library": "NumPy"}
{"API_Name": "np.char.endswith", "Docstring": "Returns a boolean array which is `True` where the string element\nin `a` ends with `suffix`, otherwise `False`.\n\nCalls `str.endswith` element-wise.\n\nParameters\n----------\na : array_like of str or unicode\n\nsuffix : str\n\nstart, end : int, optional\n    With optional `start`, test beginning at that position. With\n    optional `end`, stop comparing at that position.\n\nReturns\n-------\nout : ndarray\n    Outputs an array of bools.\n\nSee Also\n--------\nstr.endswith\n\nExamples\n--------\n>>> s = np.array(['foo', 'bar'])\n>>> s[0] = 'foo'\n>>> s[1] = 'bar'\n>>> s\narray(['foo', 'bar'], dtype='<U3')\n>>> np.char.endswith(s, 'ar')\narray([False,  True])\n>>> np.char.endswith(s, 'a', start=1, end=2)\narray([False,  True])", "Library": "NumPy"}
{"API_Name": "np.char.replace", "Docstring": "For each element in `a`, return a copy of the string with all\noccurrences of substring `old` replaced by `new`.\n\nCalls `str.replace` element-wise.\n\nParameters\n----------\na : array-like of str or unicode\n\nold, new : str or unicode\n\ncount : int, optional\n    If the optional argument `count` is given, only the first\n    `count` occurrences are replaced.\n\nReturns\n-------\nout : ndarray\n    Output array of str or unicode, depending on input type\n\nSee Also\n--------\nstr.replace\n\nExamples\n--------\n>>> a = np.array([\"That is a mango\", \"Monkeys eat mangos\"])\n>>> np.char.replace(a, 'mango', 'banana')\narray(['That is a banana', 'Monkeys eat bananas'], dtype='<U19')\n\n>>> a = np.array([\"The dish is fresh\", \"This is it\"])\n>>> np.char.replace(a, 'is', 'was')\narray(['The dwash was fresh', 'Thwas was it'], dtype='<U19')", "Library": "NumPy"}
{"API_Name": "np.char.split", "Docstring": "For each element in `a`, return a list of the words in the\nstring, using `sep` as the delimiter string.\n\nCalls `str.split` element-wise.\n\nParameters\n----------\na : array_like of str or unicode\n\nsep : str or unicode, optional\n   If `sep` is not specified or None, any whitespace string is a\n   separator.\n\nmaxsplit : int, optional\n    If `maxsplit` is given, at most `maxsplit` splits are done.\n\nReturns\n-------\nout : ndarray\n    Array of list objects\n\nSee Also\n--------\nstr.split, rsplit", "Library": "NumPy"}
{"API_Name": "np.clip", "Docstring": "Clip (limit) the values in an array.\n\nGiven an interval, values outside the interval are clipped to\nthe interval edges.  For example, if an interval of ``[0, 1]``\nis specified, values smaller than 0 become 0, and values larger\nthan 1 become 1.\n\nEquivalent to but faster than ``np.minimum(a_max, np.maximum(a, a_min))``.\n\nNo check is performed to ensure ``a_min < a_max``.\n\nParameters\n----------\na : array_like\n    Array containing elements to clip.\na_min, a_max : array_like or None\n    Minimum and maximum value. If ``None``, clipping is not performed on\n    the corresponding edge. Only one of `a_min` and `a_max` may be\n    ``None``. Both are broadcast against `a`.\nout : ndarray, optional\n    The results will be placed in this array. It may be the input\n    array for in-place clipping.  `out` must be of the right shape\n    to hold the output.  Its type is preserved.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nclipped_array : ndarray\n    An array with the elements of `a`, but where values\n    < `a_min` are replaced with `a_min`, and those > `a_max`\n    with `a_max`.\n\nSee Also\n--------\n:ref:`ufuncs-output-type`\n\nNotes\n-----\nWhen `a_min` is greater than `a_max`, `clip` returns an\narray in which all values are equal to `a_max`,\nas shown in the second example.\n\nExamples\n--------\n>>> a = np.arange(10)\n>>> a\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.clip(a, 1, 8)\narray([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])\n>>> np.clip(a, 8, 1)\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n>>> np.clip(a, 3, 6, out=a)\narray([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n>>> a\narray([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])\n>>> a = np.arange(10)\n>>> a\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8)\narray([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])", "Library": "NumPy"}
{"API_Name": "np.column_stack", "Docstring": "Stack 1-D arrays as columns into a 2-D array.\n\nTake a sequence of 1-D arrays and stack them as columns\nto make a single 2-D array. 2-D arrays are stacked as-is,\njust like with `hstack`.  1-D arrays are turned into 2-D columns\nfirst.\n\nParameters\n----------\ntup : sequence of 1-D or 2-D arrays.\n    Arrays to stack. All of them must have the same first dimension.\n\nReturns\n-------\nstacked : 2-D array\n    The array formed by stacking the given arrays.\n\nSee Also\n--------\nstack, hstack, vstack, concatenate\n\nExamples\n--------\n>>> a = np.array((1,2,3))\n>>> b = np.array((2,3,4))\n>>> np.column_stack((a,b))\narray([[1, 2],\n       [2, 3],\n       [3, 4]])", "Library": "NumPy"}
{"API_Name": "np.concatenate", "Docstring": "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n\nJoin a sequence of arrays along an existing axis.\n\nParameters\n----------\na1, a2, ... : sequence of array_like\n    The arrays must have the same shape, except in the dimension\n    corresponding to `axis` (the first, by default).\naxis : int, optional\n    The axis along which the arrays will be joined.  If axis is None,\n    arrays are flattened before use.  Default is 0.\nout : ndarray, optional\n    If provided, the destination to place the result. The shape must be\n    correct, matching that of what concatenate would have returned if no\n    out argument were specified.\ndtype : str or dtype\n    If provided, the destination array will have this dtype. Cannot be\n    provided together with `out`.\n\n    .. versionadded:: 1.20.0\n\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nres : ndarray\n    The concatenated array.\n\nSee Also\n--------\nma.concatenate : Concatenate function that preserves input masks.\narray_split : Split an array into multiple sub-arrays of equal or\n              near-equal size.\nsplit : Split array into a list of multiple sub-arrays of equal size.\nhsplit : Split array into multiple sub-arrays horizontally (column wise).\nvsplit : Split array into multiple sub-arrays vertically (row wise).\ndsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\nstack : Stack a sequence of arrays along a new axis.\nblock : Assemble arrays from blocks.\nhstack : Stack arrays in sequence horizontally (column wise).\nvstack : Stack arrays in sequence vertically (row wise).\ndstack : Stack arrays in sequence depth wise (along third dimension).\ncolumn_stack : Stack 1-D arrays as columns into a 2-D array.\n\nNotes\n-----\nWhen one or more of the arrays to be concatenated is a MaskedArray,\nthis function will return a MaskedArray object instead of an ndarray,\nbut the input masks are *not* preserved. In cases where a MaskedArray\nis expected as input, use the ma.concatenate function from the masked\narray module instead.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> b = np.array([[5, 6]])\n>>> np.concatenate((a, b), axis=0)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n>>> np.concatenate((a, b.T), axis=1)\narray([[1, 2, 5],\n       [3, 4, 6]])\n>>> np.concatenate((a, b), axis=None)\narray([1, 2, 3, 4, 5, 6])\n\nThis function will not preserve masking of MaskedArray inputs.\n\n>>> a = np.ma.arange(3)\n>>> a[1] = np.ma.masked\n>>> b = np.arange(2, 5)\n>>> a\nmasked_array(data=[0, --, 2],\n             mask=[False,  True, False],\n       fill_value=999999)\n>>> b\narray([2, 3, 4])\n>>> np.concatenate([a, b])\nmasked_array(data=[0, 1, 2, 2, 3, 4],\n             mask=False,\n       fill_value=999999)\n>>> np.ma.concatenate([a, b])\nmasked_array(data=[0, --, 2, 2, 3, 4],\n             mask=[False,  True, False, False, False, False],\n       fill_value=999999)", "Library": "NumPy"}
{"API_Name": "np.copy", "Docstring": "Return an array copy of the given object.\n\nParameters\n----------\na : array_like\n    Input data.\norder : {'C', 'F', 'A', 'K'}, optional\n    Controls the memory layout of the copy. 'C' means C-order,\n    'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n    'C' otherwise. 'K' means match the layout of `a` as closely\n    as possible. (Note that this function and :meth:`ndarray.copy` are very\n    similar, but have different default values for their order=\n    arguments.)\nsubok : bool, optional\n    If True, then sub-classes will be passed-through, otherwise the\n    returned array will be forced to be a base-class array (defaults to False).\n\n    .. versionadded:: 1.19.0\n\nReturns\n-------\narr : ndarray\n    Array interpretation of `a`.\n\nSee Also\n--------\nndarray.copy : Preferred method for creating an array copy\n\nNotes\n-----\nThis is equivalent to:\n\n>>> np.array(a, copy=True)  #doctest: +SKIP\n\nExamples\n--------\nCreate an array x, with a reference y and a copy z:\n\n>>> x = np.array([1, 2, 3])\n>>> y = x\n>>> z = np.copy(x)\n\nNote that, when we modify x, y changes, but not z:\n\n>>> x[0] = 10\n>>> x[0] == y[0]\nTrue\n>>> x[0] == z[0]\nFalse\n\nNote that, np.copy clears previously set WRITEABLE=False flag.\n\n>>> a = np.array([1, 2, 3])\n>>> a.flags[\"WRITEABLE\"] = False\n>>> b = np.copy(a)\n>>> b.flags[\"WRITEABLE\"]\nTrue\n>>> b[0] = 3\n>>> b\narray([3, 2, 3])\n\nNote that np.copy is a shallow copy and will not copy object\nelements within arrays. This is mainly important for arrays\ncontaining Python objects. The new array will contain the\nsame object which may lead to surprises if that object can\nbe modified (is mutable):\n\n>>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> b = np.copy(a)\n>>> b[2][0] = 10\n>>> a\narray([1, 'm', list([10, 3, 4])], dtype=object)\n\nTo ensure all elements within an ``object`` array are copied,\nuse `copy.deepcopy`:\n\n>>> import copy\n>>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)\n>>> c = copy.deepcopy(a)\n>>> c[2][0] = 10\n>>> c\narray([1, 'm', list([10, 3, 4])], dtype=object)\n>>> a\narray([1, 'm', list([2, 3, 4])], dtype=object)", "Library": "NumPy"}
{"API_Name": "np.core.records.fromarrays", "Docstring": "Create a record array from a (flat) list of arrays\n\nParameters\n----------\narrayList : list or tuple\n    List of array-like objects (such as lists, tuples,\n    and ndarrays).\ndtype : data-type, optional\n    valid dtype for all arrays\nshape : int or tuple of ints, optional\n    Shape of the resulting array. If not provided, inferred from\n    ``arrayList[0]``.\nformats, names, titles, aligned, byteorder :\n    If `dtype` is ``None``, these arguments are passed to\n    `numpy.format_parser` to construct a dtype. See that function for\n    detailed documentation.\n\nReturns\n-------\nnp.recarray\n    Record array consisting of given arrayList columns.\n\nExamples\n--------\n>>> x1=np.array([1,2,3,4])\n>>> x2=np.array(['a','dd','xyz','12'])\n>>> x3=np.array([1.1,2,3,4])\n>>> r = np.core.records.fromarrays([x1,x2,x3],names='a,b,c')\n>>> print(r[1])\n(2, 'dd', 2.0) # may vary\n>>> x1[1]=34\n>>> r.a\narray([1, 2, 3, 4])\n\n>>> x1 = np.array([1, 2, 3, 4])\n>>> x2 = np.array(['a', 'dd', 'xyz', '12'])\n>>> x3 = np.array([1.1, 2, 3,4])\n>>> r = np.core.records.fromarrays(\n...     [x1, x2, x3],\n...     dtype=np.dtype([('a', np.int32), ('b', 'S3'), ('c', np.float32)]))\n>>> r\nrec.array([(1, b'a', 1.1), (2, b'dd', 2. ), (3, b'xyz', 3. ),\n           (4, b'12', 4. )],\n          dtype=[('a', '<i4'), ('b', 'S3'), ('c', '<f4')])", "Library": "NumPy"}
{"API_Name": "np.corrcoef", "Docstring": "Return Pearson product-moment correlation coefficients.\n\nPlease refer to the documentation for `cov` for more detail.  The\nrelationship between the correlation coefficient matrix, `R`, and the\ncovariance matrix, `C`, is\n\n.. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }\n\nThe values of `R` are between -1 and 1, inclusive.\n\nParameters\n----------\nx : array_like\n    A 1-D or 2-D array containing multiple variables and observations.\n    Each row of `x` represents a variable, and each column a single\n    observation of all those variables. Also see `rowvar` below.\ny : array_like, optional\n    An additional set of variables and observations. `y` has the same\n    shape as `x`.\nrowvar : bool, optional\n    If `rowvar` is True (default), then each row represents a\n    variable, with observations in the columns. Otherwise, the relationship\n    is transposed: each column represents a variable, while the rows\n    contain observations.\nbias : _NoValue, optional\n    Has no effect, do not use.\n\n    .. deprecated:: 1.10.0\nddof : _NoValue, optional\n    Has no effect, do not use.\n\n    .. deprecated:: 1.10.0\ndtype : data-type, optional\n    Data-type of the result. By default, the return data-type will have\n    at least `numpy.float64` precision.\n\n    .. versionadded:: 1.20\n\nReturns\n-------\nR : ndarray\n    The correlation coefficient matrix of the variables.\n\nSee Also\n--------\ncov : Covariance matrix\n\nNotes\n-----\nDue to floating point rounding the resulting array may not be Hermitian,\nthe diagonal elements may not be 1, and the elements may not satisfy the\ninequality abs(a) <= 1. The real and imaginary parts are clipped to the\ninterval [-1,  1] in an attempt to improve on that situation but is not\nmuch help in the complex case.\n\nThis function accepts but discards arguments `bias` and `ddof`.  This is\nfor backwards compatibility with previous versions of this function.  These\narguments had no effect on the return values of the function and can be\nsafely ignored in this and previous versions of numpy.\n\nExamples\n--------\nIn this example we generate two random arrays, ``xarr`` and ``yarr``, and\ncompute the row-wise and column-wise Pearson correlation coefficients,\n``R``. Since ``rowvar`` is  true by  default, we first find the row-wise\nPearson correlation coefficients between the variables of ``xarr``.\n\n>>> import numpy as np\n>>> rng = np.random.default_rng(seed=42)\n>>> xarr = rng.random((3, 3))\n>>> xarr\narray([[0.77395605, 0.43887844, 0.85859792],\n       [0.69736803, 0.09417735, 0.97562235],\n       [0.7611397 , 0.78606431, 0.12811363]])\n>>> R1 = np.corrcoef(xarr)\n>>> R1\narray([[ 1.        ,  0.99256089, -0.68080986],\n       [ 0.99256089,  1.        , -0.76492172],\n       [-0.68080986, -0.76492172,  1.        ]])\n\nIf we add another set of variables and observations ``yarr``, we can\ncompute the row-wise Pearson correlation coefficients between the\nvariables in ``xarr`` and ``yarr``.\n\n>>> yarr = rng.random((3, 3))\n>>> yarr\narray([[0.45038594, 0.37079802, 0.92676499],\n       [0.64386512, 0.82276161, 0.4434142 ],\n       [0.22723872, 0.55458479, 0.06381726]])\n>>> R2 = np.corrcoef(xarr, yarr)\n>>> R2\narray([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  ,\n        -0.99004057],\n       [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098,\n        -0.99981569],\n       [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355,\n         0.77714685],\n       [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855,\n        -0.83571711],\n       [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        ,\n         0.97517215],\n       [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215,\n         1.        ]])\n\nFinally if we use the option ``rowvar=False``, the columns are now\nbeing treated as the variables and we will find the column-wise Pearson\ncorrelation coefficients between variables in ``xarr`` and ``yarr``.\n\n>>> R3 = np.corrcoef(xarr, yarr, rowvar=False)\n>>> R3\narray([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 ,\n         0.22423734],\n       [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587,\n        -0.44069024],\n       [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648,\n         0.75137473],\n       [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469,\n         0.47536961],\n       [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        ,\n        -0.46666491],\n       [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491,\n         1.        ]])", "Library": "NumPy"}
{"API_Name": "np.correlate", "Docstring": "Cross-correlation of two 1-dimensional sequences.\n\nThis function computes the correlation as generally defined in signal\nprocessing texts:\n\n.. math:: c_k = \\sum_n a_{n+k} \\cdot \\overline{v}_n\n\nwith a and v sequences being zero-padded where necessary and\n:math:`\\overline x` denoting complex conjugation.\n\nParameters\n----------\na, v : array_like\n    Input sequences.\nmode : {'valid', 'same', 'full'}, optional\n    Refer to the `convolve` docstring.  Note that the default\n    is 'valid', unlike `convolve`, which uses 'full'.\nold_behavior : bool\n    `old_behavior` was removed in NumPy 1.10. If you need the old\n    behavior, use `multiarray.correlate`.\n\nReturns\n-------\nout : ndarray\n    Discrete cross-correlation of `a` and `v`.\n\nSee Also\n--------\nconvolve : Discrete, linear convolution of two one-dimensional sequences.\nmultiarray.correlate : Old, no conjugate, version of correlate.\nscipy.signal.correlate : uses FFT which has superior performance on large arrays. \n\nNotes\n-----\nThe definition of correlation above is not unique and sometimes correlation\nmay be defined differently. Another common definition is:\n\n.. math:: c'_k = \\sum_n a_{n} \\cdot \\overline{v_{n+k}}\n\nwhich is related to :math:`c_k` by :math:`c'_k = c_{-k}`.\n\n`numpy.correlate` may perform slowly in large arrays (i.e. n = 1e5) because it does\nnot use the FFT to compute the convolution; in that case, `scipy.signal.correlate` might\nbe preferable.\n\n\nExamples\n--------\n>>> np.correlate([1, 2, 3], [0, 1, 0.5])\narray([3.5])\n>>> np.correlate([1, 2, 3], [0, 1, 0.5], \"same\")\narray([2. ,  3.5,  3. ])\n>>> np.correlate([1, 2, 3], [0, 1, 0.5], \"full\")\narray([0.5,  2. ,  3.5,  3. ,  0. ])\n\nUsing complex sequences:\n\n>>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full')\narray([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])\n\nNote that you get the time reversed, complex conjugated result\n(:math:`\\overline{c_{-k}}`) when the two input sequences a and v change \nplaces:\n\n>>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')\narray([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])", "Library": "NumPy"}
{"API_Name": "np.cos", "Docstring": "cos(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCosine element-wise.\n\nParameters\n----------\nx : array_like\n    Input array in radians.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    The corresponding cosine values.\n    This is a scalar if `x` is a scalar.\n\nNotes\n-----\nIf `out` is provided, the function writes the result into it,\nand returns a reference to `out`.  (See Examples)\n\nReferences\n----------\nM. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions.\nNew York, NY: Dover, 1972.\n\nExamples\n--------\n>>> np.cos(np.array([0, np.pi/2, np.pi]))\narray([  1.00000000e+00,   6.12303177e-17,  -1.00000000e+00])\n>>>\n>>> # Example of providing the optional output parameter\n>>> out1 = np.array([0], dtype='d')\n>>> out2 = np.cos([0.1], out1)\n>>> out2 is out1\nTrue\n>>>\n>>> # Example of ValueError due to provision of shape mis-matched `out`\n>>> np.cos(np.zeros((3,3)),np.zeros((2,2)))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: operands could not be broadcast together with shapes (3,3) (2,2)", "Library": "NumPy"}
{"API_Name": "np.cumsum", "Docstring": "Return the cumulative sum of the elements along a given axis.\n\nParameters\n----------\na : array_like\n    Input array.\naxis : int, optional\n    Axis along which the cumulative sum is computed. The default\n    (None) is to compute the cumsum over the flattened array.\ndtype : dtype, optional\n    Type of the returned array and of the accumulator in which the\n    elements are summed.  If `dtype` is not specified, it defaults\n    to the dtype of `a`, unless `a` has an integer dtype with a\n    precision less than that of the default platform integer.  In\n    that case, the default platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output\n    but the type will be cast if necessary. See :ref:`ufuncs-output-type` for\n    more details.\n\nReturns\n-------\ncumsum_along_axis : ndarray.\n    A new array holding the result is returned unless `out` is\n    specified, in which case a reference to `out` is returned. The\n    result has the same size as `a`, and the same shape as `a` if\n    `axis` is not None or `a` is a 1-d array.\n\nSee Also\n--------\nsum : Sum array elements.\ntrapz : Integration of array values using the composite trapezoidal rule.\ndiff : Calculate the n-th discrete difference along given axis.\n\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\n\n``cumsum(a)[-1]`` may not be equal to ``sum(a)`` for floating-point\nvalues since ``sum`` may use a pairwise summation routine, reducing\nthe roundoff-error. See `sum` for more information.\n\nExamples\n--------\n>>> a = np.array([[1,2,3], [4,5,6]])\n>>> a\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> np.cumsum(a)\narray([ 1,  3,  6, 10, 15, 21])\n>>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\narray([  1.,   3.,   6.,  10.,  15.,  21.])\n\n>>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\narray([[1, 2, 3],\n       [5, 7, 9]])\n>>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\narray([[ 1,  3,  6],\n       [ 4,  9, 15]])\n\n``cumsum(b)[-1]`` may not be equal to ``sum(b)``\n\n>>> b = np.array([1, 2e-9, 3e-9] * 1000000)\n>>> b.cumsum()[-1]\n1000000.0050045159\n>>> b.sum()\n1000000.0050000029", "Library": "NumPy"}
{"API_Name": "np.deg2rad", "Docstring": "deg2rad(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nConvert angles from degrees to radians.\n\nParameters\n----------\nx : array_like\n    Angles in degrees.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    The corresponding angle in radians.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nrad2deg : Convert angles from radians to degrees.\nunwrap : Remove large jumps in angle by wrapping.\n\nNotes\n-----\n.. versionadded:: 1.3.0\n\n``deg2rad(x)`` is ``x * pi / 180``.\n\nExamples\n--------\n>>> np.deg2rad(180)\n3.1415926535897931", "Library": "NumPy"}
{"API_Name": "np.degrees", "Docstring": "degrees(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nConvert angles from radians to degrees.\n\nParameters\n----------\nx : array_like\n    Input array in radians.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray of floats\n    The corresponding degree values; if `out` was supplied this is a\n    reference to it.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nrad2deg : equivalent function\n\nExamples\n--------\nConvert a radian array to degrees\n\n>>> rad = np.arange(12.)*np.pi/6\n>>> np.degrees(rad)\narray([   0.,   30.,   60.,   90.,  120.,  150.,  180.,  210.,  240.,\n        270.,  300.,  330.])\n\n>>> out = np.zeros((rad.shape))\n>>> r = np.degrees(rad, out)\n>>> np.all(r == out)\nTrue", "Library": "NumPy"}
{"API_Name": "np.delete", "Docstring": "Return a new array with sub-arrays along an axis deleted. For a one\ndimensional array, this returns those entries not returned by\n`arr[obj]`.\n\nParameters\n----------\narr : array_like\n    Input array.\nobj : slice, int or array of ints\n    Indicate indices of sub-arrays to remove along the specified axis.\n\n    .. versionchanged:: 1.19.0\n        Boolean indices are now treated as a mask of elements to remove,\n        rather than being cast to the integers 0 and 1.\n\naxis : int, optional\n    The axis along which to delete the subarray defined by `obj`.\n    If `axis` is None, `obj` is applied to the flattened array.\n\nReturns\n-------\nout : ndarray\n    A copy of `arr` with the elements specified by `obj` removed. Note\n    that `delete` does not occur in-place. If `axis` is None, `out` is\n    a flattened array.\n\nSee Also\n--------\ninsert : Insert elements into an array.\nappend : Append elements at the end of an array.\n\nNotes\n-----\nOften it is preferable to use a boolean mask. For example:\n\n>>> arr = np.arange(12) + 1\n>>> mask = np.ones(len(arr), dtype=bool)\n>>> mask[[0,2,4]] = False\n>>> result = arr[mask,...]\n\nIs equivalent to ``np.delete(arr, [0,2,4], axis=0)``, but allows further\nuse of `mask`.\n\nExamples\n--------\n>>> arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n>>> arr\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n>>> np.delete(arr, 1, 0)\narray([[ 1,  2,  3,  4],\n       [ 9, 10, 11, 12]])\n\n>>> np.delete(arr, np.s_[::2], 1)\narray([[ 2,  4],\n       [ 6,  8],\n       [10, 12]])\n>>> np.delete(arr, [1,3,5], None)\narray([ 1,  3,  5,  7,  8,  9, 10, 11, 12])", "Library": "NumPy"}
{"API_Name": "np.diag", "Docstring": "Extract a diagonal or construct a diagonal array.\n\nSee the more detailed documentation for ``numpy.diagonal`` if you use this\nfunction to extract a diagonal and wish to write to the resulting array;\nwhether it returns a copy or a view depends on what version of numpy you\nare using.\n\nParameters\n----------\nv : array_like\n    If `v` is a 2-D array, return a copy of its `k`-th diagonal.\n    If `v` is a 1-D array, return a 2-D array with `v` on the `k`-th\n    diagonal.\nk : int, optional\n    Diagonal in question. The default is 0. Use `k>0` for diagonals\n    above the main diagonal, and `k<0` for diagonals below the main\n    diagonal.\n\nReturns\n-------\nout : ndarray\n    The extracted diagonal or constructed diagonal array.\n\nSee Also\n--------\ndiagonal : Return specified diagonals.\ndiagflat : Create a 2-D array with the flattened input as a diagonal.\ntrace : Sum along diagonals.\ntriu : Upper triangle of an array.\ntril : Lower triangle of an array.\n\nExamples\n--------\n>>> x = np.arange(9).reshape((3,3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n>>> np.diag(x)\narray([0, 4, 8])\n>>> np.diag(x, k=1)\narray([1, 5])\n>>> np.diag(x, k=-1)\narray([3, 7])\n\n>>> np.diag(np.diag(x))\narray([[0, 0, 0],\n       [0, 4, 0],\n       [0, 0, 8]])", "Library": "NumPy"}
{"API_Name": "np.digitize", "Docstring": "Return the indices of the bins to which each value in input array belongs.\n\n=========  =============  ============================\n`right`    order of bins  returned index `i` satisfies\n=========  =============  ============================\n``False``  increasing     ``bins[i-1] <= x < bins[i]``\n``True``   increasing     ``bins[i-1] < x <= bins[i]``\n``False``  decreasing     ``bins[i-1] > x >= bins[i]``\n``True``   decreasing     ``bins[i-1] >= x > bins[i]``\n=========  =============  ============================\n\nIf values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is\nreturned as appropriate.\n\nParameters\n----------\nx : array_like\n    Input array to be binned. Prior to NumPy 1.10.0, this array had to\n    be 1-dimensional, but can now have any shape.\nbins : array_like\n    Array of bins. It has to be 1-dimensional and monotonic.\nright : bool, optional\n    Indicating whether the intervals include the right or the left bin\n    edge. Default behavior is (right==False) indicating that the interval\n    does not include the right edge. The left bin end is open in this\n    case, i.e., bins[i-1] <= x < bins[i] is the default behavior for\n    monotonically increasing bins.\n\nReturns\n-------\nindices : ndarray of ints\n    Output array of indices, of same shape as `x`.\n\nRaises\n------\nValueError\n    If `bins` is not monotonic.\nTypeError\n    If the type of the input is complex.\n\nSee Also\n--------\nbincount, histogram, unique, searchsorted\n\nNotes\n-----\nIf values in `x` are such that they fall outside the bin range,\nattempting to index `bins` with the indices that `digitize` returns\nwill result in an IndexError.\n\n.. versionadded:: 1.10.0\n\n`np.digitize` is  implemented in terms of `np.searchsorted`. This means\nthat a binary search is used to bin the values, which scales much better\nfor larger number of bins than the previous linear search. It also removes\nthe requirement for the input array to be 1-dimensional.\n\nFor monotonically _increasing_ `bins`, the following are equivalent::\n\n    np.digitize(x, bins, right=True)\n    np.searchsorted(bins, x, side='left')\n\nNote that as the order of the arguments are reversed, the side must be too.\nThe `searchsorted` call is marginally faster, as it does not do any\nmonotonicity checks. Perhaps more importantly, it supports all dtypes.\n\nExamples\n--------\n>>> x = np.array([0.2, 6.4, 3.0, 1.6])\n>>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])\n>>> inds = np.digitize(x, bins)\n>>> inds\narray([1, 4, 3, 2])\n>>> for n in range(x.size):\n...   print(bins[inds[n]-1], \"<=\", x[n], \"<\", bins[inds[n]])\n...\n0.0 <= 0.2 < 1.0\n4.0 <= 6.4 < 10.0\n2.5 <= 3.0 < 4.0\n1.0 <= 1.6 < 2.5\n\n>>> x = np.array([1.2, 10.0, 12.4, 15.5, 20.])\n>>> bins = np.array([0, 5, 10, 15, 20])\n>>> np.digitize(x,bins,right=True)\narray([1, 2, 3, 4, 4])\n>>> np.digitize(x,bins,right=False)\narray([1, 3, 3, 4, 5])", "Library": "NumPy"}
{"API_Name": "np.e", "Docstring": "Convert a string or number to a floating point number, if possible.", "Library": "NumPy"}
{"API_Name": "np.empty", "Docstring": "empty(shape, dtype=float, order='C', *, like=None)\n\nReturn a new array of given shape and type, without initializing entries.\n\nParameters\n----------\nshape : int or tuple of int\n    Shape of the empty array, e.g., ``(2, 3)`` or ``2``.\ndtype : data-type, optional\n    Desired output data-type for the array, e.g, `numpy.int8`. Default is\n    `numpy.float64`.\norder : {'C', 'F'}, optional, default: 'C'\n    Whether to store multi-dimensional data in row-major\n    (C-style) or column-major (Fortran-style) order in\n    memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Array of uninitialized (arbitrary) data of the given shape, dtype, and\n    order.  Object arrays will be initialized to None.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nNotes\n-----\n`empty`, unlike `zeros`, does not set the array values to zero,\nand may therefore be marginally faster.  On the other hand, it requires\nthe user to manually set all the values in the array, and should be\nused with caution.\n\nExamples\n--------\n>>> np.empty([2, 2])\narray([[ -9.74499359e+001,   6.69583040e-309],\n       [  2.13182611e-314,   3.06959433e-309]])         #uninitialized\n\n>>> np.empty([2, 2], dtype=int)\narray([[-1073741821, -1067949133],\n       [  496041986,    19249760]])                     #uninitialized", "Library": "NumPy"}
{"API_Name": "np.empty_like", "Docstring": "empty_like(prototype, dtype=None, order='K', subok=True, shape=None)\n\nReturn a new array with the same shape and type as a given array.\n\nParameters\n----------\nprototype : array_like\n    The shape and data-type of `prototype` define these same attributes\n    of the returned array.\ndtype : data-type, optional\n    Overrides the data type of the result.\n\n    .. versionadded:: 1.6.0\norder : {'C', 'F', 'A', or 'K'}, optional\n    Overrides the memory layout of the result. 'C' means C-order,\n    'F' means F-order, 'A' means 'F' if `prototype` is Fortran\n    contiguous, 'C' otherwise. 'K' means match the layout of `prototype`\n    as closely as possible.\n\n    .. versionadded:: 1.6.0\nsubok : bool, optional.\n    If True, then the newly created array will use the sub-class\n    type of `prototype`, otherwise it will be a base-class array. Defaults\n    to True.\nshape : int or sequence of ints, optional.\n    Overrides the shape of the result. If order='K' and the number of\n    dimensions is unchanged, will try to keep order, otherwise,\n    order='C' is implied.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nout : ndarray\n    Array of uninitialized (arbitrary) data with the same\n    shape and type as `prototype`.\n\nSee Also\n--------\nones_like : Return an array of ones with shape and type of input.\nzeros_like : Return an array of zeros with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\n\nNotes\n-----\nThis function does *not* initialize the returned array; to do that use\n`zeros_like` or `ones_like` instead.  It may be marginally faster than\nthe functions that do set the array values.\n\nExamples\n--------\n>>> a = ([1,2,3], [4,5,6])                         # a is array-like\n>>> np.empty_like(a)\narray([[-1073741821, -1073741821,           3],    # uninitialized\n       [          0,           0, -1073741821]])\n>>> a = np.array([[1., 2., 3.],[4.,5.,6.]])\n>>> np.empty_like(a)\narray([[ -2.00000715e+000,   1.48219694e-323,  -2.00000572e+000], # uninitialized\n       [  4.38791518e-305,  -2.00000715e+000,   4.17269252e-309]])", "Library": "NumPy"}
{"API_Name": "np.exp", "Docstring": "exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCalculate the exponential of all elements in the input array.\n\nParameters\n----------\nx : array_like\n    Input values.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nout : ndarray or scalar\n    Output array, element-wise exponential of `x`.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nexpm1 : Calculate ``exp(x) - 1`` for all elements in the array.\nexp2  : Calculate ``2**x`` for all elements in the array.\n\nNotes\n-----\nThe irrational number ``e`` is also known as Euler's number.  It is\napproximately 2.718281, and is the base of the natural logarithm,\n``ln`` (this means that, if :math:`x = \\ln y = \\log_e y`,\nthen :math:`e^x = y`. For real input, ``exp(x)`` is always positive.\n\nFor complex arguments, ``x = a + ib``, we can write\n:math:`e^x = e^a e^{ib}`.  The first term, :math:`e^a`, is already\nknown (it is the real argument, described above).  The second term,\n:math:`e^{ib}`, is :math:`\\cos b + i \\sin b`, a function with\nmagnitude 1 and a periodic phase.\n\nReferences\n----------\n.. [1] Wikipedia, \"Exponential function\",\n       https://en.wikipedia.org/wiki/Exponential_function\n.. [2] M. Abramovitz and I. A. Stegun, \"Handbook of Mathematical Functions\n       with Formulas, Graphs, and Mathematical Tables,\" Dover, 1964, p. 69,\n       https://personal.math.ubc.ca/~cbm/aands/page_69.htm\n\nExamples\n--------\nPlot the magnitude and phase of ``exp(x)`` in the complex plane:\n\n>>> import matplotlib.pyplot as plt\n\n>>> x = np.linspace(-2*np.pi, 2*np.pi, 100)\n>>> xx = x + 1j * x[:, np.newaxis] # a + ib over complex plane\n>>> out = np.exp(xx)\n\n>>> plt.subplot(121)\n>>> plt.imshow(np.abs(out),\n...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='gray')\n>>> plt.title('Magnitude of exp(x)')\n\n>>> plt.subplot(122)\n>>> plt.imshow(np.angle(out),\n...            extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='hsv')\n>>> plt.title('Phase (angle) of exp(x)')\n>>> plt.show()", "Library": "NumPy"}
{"API_Name": "np.ndarray.flatten", "Docstring": "a.flatten(order='C') Return a copy of the array collapsed into one dimension. Parameters ---------- order : {'C', 'F', 'A', 'K'}, optional 'C' means to flatten in row-major (C-style) order. 'F' means to flatten in column-major (Fortran- style) order. 'A' means to flatten in column-major order if `a` is Fortran *contiguous* in memory, row-major order otherwise. 'K' means to flatten `a` in the order the elements occur in memory. The default is 'C'. Returns ------- y : ndarray A copy of the input array, flattened to one dimension. See Also -------- ravel : Return a flattened array. flat : A 1-D flat iterator over the array. Examples -------- >>> a = np.array([[1,2], [3,4]]) >>> a.flatten() array([1, 2, 3, 4]) >>> a.flatten('F') array([1, 3, 2, 4])", "Library": "NumPy"}
{"API_Name": "np.flatnonzero", "Docstring": "Return indices that are non-zero in the flattened version of a.\n\nThis is equivalent to ``np.nonzero(np.ravel(a))[0]``.\n\nParameters\n----------\na : array_like\n    Input data.\n\nReturns\n-------\nres : ndarray\n    Output array, containing the indices of the elements of ``a.ravel()``\n    that are non-zero.\n\nSee Also\n--------\nnonzero : Return the indices of the non-zero elements of the input array.\nravel : Return a 1-D array containing the elements of the input array.\n\nExamples\n--------\n>>> x = np.arange(-2, 3)\n>>> x\narray([-2, -1,  0,  1,  2])\n>>> np.flatnonzero(x)\narray([0, 1, 3, 4])\n\nUse the indices of the non-zero elements as an index array to extract\nthese elements:\n\n>>> x.ravel()[np.flatnonzero(x)]\narray([-2, -1,  1,  2])", "Library": "NumPy"}
{"API_Name": "np.flip", "Docstring": "Reverse the order of elements in an array along the given axis.\n\nThe shape of the array is preserved, but the elements are reordered.\n\n.. versionadded:: 1.12.0\n\nParameters\n----------\nm : array_like\n    Input array.\naxis : None or int or tuple of ints, optional\n     Axis or axes along which to flip over. The default,\n     axis=None, will flip over all of the axes of the input array.\n     If axis is negative it counts from the last to the first axis.\n\n     If axis is a tuple of ints, flipping is performed on all of the axes\n     specified in the tuple.\n\n     .. versionchanged:: 1.15.0\n        None and tuples of axes are supported\n\nReturns\n-------\nout : array_like\n    A view of `m` with the entries of axis reversed.  Since a view is\n    returned, this operation is done in constant time.\n\nSee Also\n--------\nflipud : Flip an array vertically (axis=0).\nfliplr : Flip an array horizontally (axis=1).\n\nNotes\n-----\nflip(m, 0) is equivalent to flipud(m).\n\nflip(m, 1) is equivalent to fliplr(m).\n\nflip(m, n) corresponds to ``m[...,::-1,...]`` with ``::-1`` at position n.\n\nflip(m) corresponds to ``m[::-1,::-1,...,::-1]`` with ``::-1`` at all\npositions.\n\nflip(m, (0, 1)) corresponds to ``m[::-1,::-1,...]`` with ``::-1`` at\nposition 0 and position 1.\n\nExamples\n--------\n>>> A = np.arange(8).reshape((2,2,2))\n>>> A\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n>>> np.flip(A, 0)\narray([[[4, 5],\n        [6, 7]],\n       [[0, 1],\n        [2, 3]]])\n>>> np.flip(A, 1)\narray([[[2, 3],\n        [0, 1]],\n       [[6, 7],\n        [4, 5]]])\n>>> np.flip(A)\narray([[[7, 6],\n        [5, 4]],\n       [[3, 2],\n        [1, 0]]])\n>>> np.flip(A, (0, 2))\narray([[[5, 4],\n        [7, 6]],\n       [[1, 0],\n        [3, 2]]])\n>>> A = np.random.randn(3,4,5)\n>>> np.all(np.flip(A,2) == A[:,:,::-1,...])\nTrue", "Library": "NumPy"}
{"API_Name": "np.fliplr", "Docstring": "Reverse the order of elements along axis 1 (left/right).\n\nFor a 2-D array, this flips the entries in each row in the left/right\ndirection. Columns are preserved, but appear in a different order than\nbefore.\n\nParameters\n----------\nm : array_like\n    Input array, must be at least 2-D.\n\nReturns\n-------\nf : ndarray\n    A view of `m` with the columns reversed.  Since a view\n    is returned, this operation is :math:`\\mathcal O(1)`.\n\nSee Also\n--------\nflipud : Flip array in the up/down direction.\nflip : Flip array in one or more dimensions.\nrot90 : Rotate array counterclockwise.\n\nNotes\n-----\nEquivalent to ``m[:,::-1]`` or ``np.flip(m, axis=1)``.\nRequires the array to be at least 2-D.\n\nExamples\n--------\n>>> A = np.diag([1.,2.,3.])\n>>> A\narray([[1.,  0.,  0.],\n       [0.,  2.,  0.],\n       [0.,  0.,  3.]])\n>>> np.fliplr(A)\narray([[0.,  0.,  1.],\n       [0.,  2.,  0.],\n       [3.,  0.,  0.]])\n\n>>> A = np.random.randn(2,3,5)\n>>> np.all(np.fliplr(A) == A[:,::-1,...])\nTrue", "Library": "NumPy"}
{"API_Name": "np.floor", "Docstring": "floor(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the floor of the input, element-wise.\n\nThe floor of the scalar `x` is the largest integer `i`, such that\n`i <= x`.  It is often denoted as :math:`\\lfloor x \\rfloor`.\n\nParameters\n----------\nx : array_like\n    Input data.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray or scalar\n    The floor of each element in `x`.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nceil, trunc, rint, fix\n\nNotes\n-----\nSome spreadsheet programs calculate the \"floor-towards-zero\", where\n``floor(-2.5) == -2``.  NumPy instead uses the definition of\n`floor` where `floor(-2.5) == -3`. The \"floor-towards-zero\"\nfunction is called ``fix`` in NumPy.\n\nExamples\n--------\n>>> a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])\n>>> np.floor(a)\narray([-2., -2., -1.,  0.,  1.,  1.,  2.])", "Library": "NumPy"}
{"API_Name": "np.full", "Docstring": "Return a new array of given shape and type, filled with `fill_value`.\n\nParameters\n----------\nshape : int or sequence of ints\n    Shape of the new array, e.g., ``(2, 3)`` or ``2``.\nfill_value : scalar or array_like\n    Fill value.\ndtype : data-type, optional\n    The desired data-type for the array  The default, None, means\n     ``np.array(fill_value).dtype``.\norder : {'C', 'F'}, optional\n    Whether to store multidimensional data in C- or Fortran-contiguous\n    (row- or column-wise) order in memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Array of `fill_value` with the given shape, dtype, and order.\n\nSee Also\n--------\nfull_like : Return a new array with shape of input filled with value.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nzeros : Return a new array setting values to zero.\n\nExamples\n--------\n>>> np.full((2, 2), np.inf)\narray([[inf, inf],\n       [inf, inf]])\n>>> np.full((2, 2), 10)\narray([[10, 10],\n       [10, 10]])\n\n>>> np.full((2, 2), [1, 2])\narray([[1, 2],\n       [1, 2]])", "Library": "NumPy"}
{"API_Name": "np.gcd", "Docstring": "gcd(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturns the greatest common divisor of ``|x1|`` and ``|x2|``\n\nParameters\n----------\nx1, x2 : array_like, int\n    Arrays of values.\n    If ``x1.shape != x2.shape``, they must be broadcastable to a common\n    shape (which becomes the shape of the output).\n\nReturns\n-------\ny : ndarray or scalar\n    The greatest common divisor of the absolute value of the inputs\n    This is a scalar if both `x1` and `x2` are scalars.\n\nSee Also\n--------\nlcm : The lowest common multiple\n\nExamples\n--------\n>>> np.gcd(12, 20)\n4\n>>> np.gcd.reduce([15, 25, 35])\n5\n>>> np.gcd(np.arange(6), 20)\narray([20,  1,  2,  1,  4,  5])", "Library": "NumPy"}
{"API_Name": "np.hstack", "Docstring": "Stack arrays in sequence horizontally (column wise).\n\nThis is equivalent to concatenation along the second axis, except for 1-D\narrays where it concatenates along the first axis. Rebuilds arrays divided\nby `hsplit`.\n\nThis function makes most sense for arrays with up to 3 dimensions. For\ninstance, for pixel-data with a height (first axis), width (second axis),\nand r/g/b channels (third axis). The functions `concatenate`, `stack` and\n`block` provide more general stacking and concatenation operations.\n\nParameters\n----------\ntup : sequence of ndarrays\n    The arrays must have the same shape along all but the second axis,\n    except 1-D arrays which can be any length.\n\ndtype : str or dtype\n    If provided, the destination array will have this dtype. Cannot be\n    provided together with `out`.\n\n.. versionadded:: 1.24\n\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n\n.. versionadded:: 1.24\n\nReturns\n-------\nstacked : ndarray\n    The array formed by stacking the given arrays.\n\nSee Also\n--------\nconcatenate : Join a sequence of arrays along an existing axis.\nstack : Join a sequence of arrays along a new axis.\nblock : Assemble an nd-array from nested lists of blocks.\nvstack : Stack arrays in sequence vertically (row wise).\ndstack : Stack arrays in sequence depth wise (along third axis).\ncolumn_stack : Stack 1-D arrays as columns into a 2-D array.\nhsplit : Split an array into multiple sub-arrays horizontally (column-wise).\n\nExamples\n--------\n>>> a = np.array((1,2,3))\n>>> b = np.array((4,5,6))\n>>> np.hstack((a,b))\narray([1, 2, 3, 4, 5, 6])\n>>> a = np.array([[1],[2],[3]])\n>>> b = np.array([[4],[5],[6]])\n>>> np.hstack((a,b))\narray([[1, 4],\n       [2, 5],\n       [3, 6]])", "Library": "NumPy"}
{"API_Name": "np.imag", "Docstring": "Return the imaginary part of the complex argument.\n\nParameters\n----------\nval : array_like\n    Input array.\n\nReturns\n-------\nout : ndarray or scalar\n    The imaginary component of the complex argument. If `val` is real,\n    the type of `val` is used for the output.  If `val` has complex\n    elements, the returned type is float.\n\nSee Also\n--------\nreal, angle, real_if_close\n\nExamples\n--------\n>>> a = np.array([1+2j, 3+4j, 5+6j])\n>>> a.imag\narray([2.,  4.,  6.])\n>>> a.imag = np.array([8, 10, 12])\n>>> a\narray([1. +8.j,  3.+10.j,  5.+12.j])\n>>> np.imag(1 + 1j)\n1.0", "Library": "NumPy"}
{"API_Name": "np.in1d", "Docstring": "Test whether each element of a 1-D array is also present in a second array.\n\nReturns a boolean array the same length as `ar1` that is True\nwhere an element of `ar1` is in `ar2` and False otherwise.\n\nWe recommend using :func:`isin` instead of `in1d` for new code.\n\nParameters\n----------\nar1 : (M,) array_like\n    Input array.\nar2 : array_like\n    The values against which to test each value of `ar1`.\nassume_unique : bool, optional\n    If True, the input arrays are both assumed to be unique, which\n    can speed up the calculation.  Default is False.\ninvert : bool, optional\n    If True, the values in the returned array are inverted (that is,\n    False where an element of `ar1` is in `ar2` and True otherwise).\n    Default is False. ``np.in1d(a, b, invert=True)`` is equivalent\n    to (but is faster than) ``np.invert(in1d(a, b))``.\nkind : {None, 'sort', 'table'}, optional\n    The algorithm to use. This will not affect the final result,\n    but will affect the speed and memory use. The default, None,\n    will select automatically based on memory considerations.\n\n    * If 'sort', will use a mergesort-based approach. This will have\n      a memory usage of roughly 6 times the sum of the sizes of\n      `ar1` and `ar2`, not accounting for size of dtypes.\n    * If 'table', will use a lookup table approach similar\n      to a counting sort. This is only available for boolean and\n      integer arrays. This will have a memory usage of the\n      size of `ar1` plus the max-min value of `ar2`. `assume_unique`\n      has no effect when the 'table' option is used.\n    * If None, will automatically choose 'table' if\n      the required memory allocation is less than or equal to\n      6 times the sum of the sizes of `ar1` and `ar2`,\n      otherwise will use 'sort'. This is done to not use\n      a large amount of memory by default, even though\n      'table' may be faster in most cases. If 'table' is chosen,\n      `assume_unique` will have no effect.\n\n    .. versionadded:: 1.8.0\n\nReturns\n-------\nin1d : (M,) ndarray, bool\n    The values `ar1[in1d]` are in `ar2`.\n\nSee Also\n--------\nisin                  : Version of this function that preserves the\n                        shape of ar1.\nnumpy.lib.arraysetops : Module with a number of other functions for\n                        performing set operations on arrays.\n\nNotes\n-----\n`in1d` can be considered as an element-wise function version of the\npython keyword `in`, for 1-D sequences. ``in1d(a, b)`` is roughly\nequivalent to ``np.array([item in b for item in a])``.\nHowever, this idea fails if `ar2` is a set, or similar (non-sequence)\ncontainer:  As ``ar2`` is converted to an array, in those cases\n``asarray(ar2)`` is an object array rather than the expected array of\ncontained values.\n\nUsing ``kind='table'`` tends to be faster than `kind='sort'` if the\nfollowing relationship is true:\n``log10(len(ar2)) > (log10(max(ar2)-min(ar2)) - 2.27) / 0.927``,\nbut may use greater memory. The default value for `kind` will\nbe automatically selected based only on memory usage, so one may\nmanually set ``kind='table'`` if memory constraints can be relaxed.\n\n.. versionadded:: 1.4.0\n\nExamples\n--------\n>>> test = np.array([0, 1, 2, 5, 0])\n>>> states = [0, 2]\n>>> mask = np.in1d(test, states)\n>>> mask\narray([ True, False,  True, False,  True])\n>>> test[mask]\narray([0, 2, 0])\n>>> mask = np.in1d(test, states, invert=True)\n>>> mask\narray([False,  True, False,  True, False])\n>>> test[mask]\narray([1, 5])", "Library": "NumPy"}
{"API_Name": "np.inf", "Docstring": "Convert a string or number to a floating point number, if possible.", "Library": "NumPy"}
{"API_Name": "np.isclose", "Docstring": "Returns a boolean array where two arrays are element-wise equal within a\ntolerance.\n\nThe tolerance values are positive, typically very small numbers.  The\nrelative difference (`rtol` * abs(`b`)) and the absolute difference\n`atol` are added together to compare against the absolute difference\nbetween `a` and `b`.\n\n.. warning:: The default `atol` is not appropriate for comparing numbers\n             that are much smaller than one (see Notes).\n\nParameters\n----------\na, b : array_like\n    Input arrays to compare.\nrtol : float\n    The relative tolerance parameter (see Notes).\natol : float\n    The absolute tolerance parameter (see Notes).\nequal_nan : bool\n    Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n    considered equal to NaN's in `b` in the output array.\n\nReturns\n-------\ny : array_like\n    Returns a boolean array of where `a` and `b` are equal within the\n    given tolerance. If both `a` and `b` are scalars, returns a single\n    boolean value.\n\nSee Also\n--------\nallclose\nmath.isclose\n\nNotes\n-----\n.. versionadded:: 1.7.0\n\nFor finite values, isclose uses the following equation to test whether\ntwo floating point values are equivalent.\n\n absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\nUnlike the built-in `math.isclose`, the above equation is not symmetric\nin `a` and `b` -- it assumes `b` is the reference value -- so that\n`isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,\nthe default value of atol is not zero, and is used to determine what\nsmall values should be considered close to zero. The default value is\nappropriate for expected values of order unity: if the expected values\nare significantly smaller than one, it can result in false positives.\n`atol` should be carefully selected for the use case at hand. A zero value\nfor `atol` will result in `False` if either `a` or `b` is zero.\n\n`isclose` is not defined for non-numeric data types.\n`bool` is considered a numeric data-type for this purpose.\n\nExamples\n--------\n>>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])\narray([ True, False])\n>>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])\narray([ True, True])\n>>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])\narray([False,  True])\n>>> np.isclose([1.0, np.nan], [1.0, np.nan])\narray([ True, False])\n>>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\narray([ True, True])\n>>> np.isclose([1e-8, 1e-7], [0.0, 0.0])\narray([ True, False])\n>>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)\narray([False, False])\n>>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])\narray([ True,  True])\n>>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)\narray([False,  True])", "Library": "NumPy"}
{"API_Name": "np.isin", "Docstring": "Calculates ``element in test_elements``, broadcasting over `element` only.\nReturns a boolean array of the same shape as `element` that is True\nwhere an element of `element` is in `test_elements` and False otherwise.\n\nParameters\n----------\nelement : array_like\n    Input array.\ntest_elements : array_like\n    The values against which to test each value of `element`.\n    This argument is flattened if it is an array or array_like.\n    See notes for behavior with non-array-like parameters.\nassume_unique : bool, optional\n    If True, the input arrays are both assumed to be unique, which\n    can speed up the calculation.  Default is False.\ninvert : bool, optional\n    If True, the values in the returned array are inverted, as if\n    calculating `element not in test_elements`. Default is False.\n    ``np.isin(a, b, invert=True)`` is equivalent to (but faster\n    than) ``np.invert(np.isin(a, b))``.\nkind : {None, 'sort', 'table'}, optional\n    The algorithm to use. This will not affect the final result,\n    but will affect the speed and memory use. The default, None,\n    will select automatically based on memory considerations.\n\n    * If 'sort', will use a mergesort-based approach. This will have\n      a memory usage of roughly 6 times the sum of the sizes of\n      `ar1` and `ar2`, not accounting for size of dtypes.\n    * If 'table', will use a lookup table approach similar\n      to a counting sort. This is only available for boolean and\n      integer arrays. This will have a memory usage of the\n      size of `ar1` plus the max-min value of `ar2`. `assume_unique`\n      has no effect when the 'table' option is used.\n    * If None, will automatically choose 'table' if\n      the required memory allocation is less than or equal to\n      6 times the sum of the sizes of `ar1` and `ar2`,\n      otherwise will use 'sort'. This is done to not use\n      a large amount of memory by default, even though\n      'table' may be faster in most cases. If 'table' is chosen,\n      `assume_unique` will have no effect.\n\n\nReturns\n-------\nisin : ndarray, bool\n    Has the same shape as `element`. The values `element[isin]`\n    are in `test_elements`.\n\nSee Also\n--------\nin1d                  : Flattened version of this function.\nnumpy.lib.arraysetops : Module with a number of other functions for\n                        performing set operations on arrays.\n\nNotes\n-----\n\n`isin` is an element-wise function version of the python keyword `in`.\n``isin(a, b)`` is roughly equivalent to\n``np.array([item in b for item in a])`` if `a` and `b` are 1-D sequences.\n\n`element` and `test_elements` are converted to arrays if they are not\nalready. If `test_elements` is a set (or other non-sequence collection)\nit will be converted to an object array with one element, rather than an\narray of the values contained in `test_elements`. This is a consequence\nof the `array` constructor's way of handling non-sequence collections.\nConverting the set to a list usually gives the desired behavior.\n\nUsing ``kind='table'`` tends to be faster than `kind='sort'` if the\nfollowing relationship is true:\n``log10(len(ar2)) > (log10(max(ar2)-min(ar2)) - 2.27) / 0.927``,\nbut may use greater memory. The default value for `kind` will\nbe automatically selected based only on memory usage, so one may\nmanually set ``kind='table'`` if memory constraints can be relaxed.\n\n.. versionadded:: 1.13.0\n\nExamples\n--------\n>>> element = 2*np.arange(4).reshape((2, 2))\n>>> element\narray([[0, 2],\n       [4, 6]])\n>>> test_elements = [1, 2, 4, 8]\n>>> mask = np.isin(element, test_elements)\n>>> mask\narray([[False,  True],\n       [ True, False]])\n>>> element[mask]\narray([2, 4])\n\nThe indices of the matched values can be obtained with `nonzero`:\n\n>>> np.nonzero(mask)\n(array([0, 1]), array([1, 0]))\n\nThe test can also be inverted:\n\n>>> mask = np.isin(element, test_elements, invert=True)\n>>> mask\narray([[ True, False],\n       [False,  True]])\n>>> element[mask]\narray([0, 6])\n\nBecause of how `array` handles sets, the following does not\nwork as expected:\n\n>>> test_set = {1, 2, 4, 8}\n>>> np.isin(element, test_set)\narray([[False, False],\n       [False, False]])\n\nCasting the set to a list gives the expected result:\n\n>>> np.isin(element, list(test_set))\narray([[False,  True],\n       [ True, False]])", "Library": "NumPy"}
{"API_Name": "np.isnan", "Docstring": "isnan(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nTest element-wise for NaN and return result as a boolean array.\n\nParameters\n----------\nx : array_like\n    Input array.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray or bool\n    True where ``x`` is NaN, false otherwise.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nisinf, isneginf, isposinf, isfinite, isnat\n\nNotes\n-----\nNumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n(IEEE 754). This means that Not a Number is not equivalent to infinity.\n\nExamples\n--------\n>>> np.isnan(np.nan)\nTrue\n>>> np.isnan(np.inf)\nFalse\n>>> np.isnan([np.log(-1.),1.,np.log(0)])\narray([ True, False, False])", "Library": "NumPy"}
{"API_Name": "np.ix_", "Docstring": "Construct an open mesh from multiple sequences.\n\nThis function takes N 1-D sequences and returns N outputs with N\ndimensions each, such that the shape is 1 in all but one dimension\nand the dimension with the non-unit shape value cycles through all\nN dimensions.\n\nUsing `ix_` one can quickly construct index arrays that will index\nthe cross product. ``a[np.ix_([1,3],[2,5])]`` returns the array\n``[[a[1,2] a[1,5]], [a[3,2] a[3,5]]]``.\n\nParameters\n----------\nargs : 1-D sequences\n    Each sequence should be of integer or boolean type.\n    Boolean sequences will be interpreted as boolean masks for the\n    corresponding dimension (equivalent to passing in\n    ``np.nonzero(boolean_sequence)``).\n\nReturns\n-------\nout : tuple of ndarrays\n    N arrays with N dimensions each, with N the number of input\n    sequences. Together these arrays form an open mesh.\n\nSee Also\n--------\nogrid, mgrid, meshgrid\n\nExamples\n--------\n>>> a = np.arange(10).reshape(2, 5)\n>>> a\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n>>> ixgrid = np.ix_([0, 1], [2, 4])\n>>> ixgrid\n(array([[0],\n       [1]]), array([[2, 4]]))\n>>> ixgrid[0].shape, ixgrid[1].shape\n((2, 1), (1, 2))\n>>> a[ixgrid]\narray([[2, 4],\n       [7, 9]])\n\n>>> ixgrid = np.ix_([True, True], [2, 4])\n>>> a[ixgrid]\narray([[2, 4],\n       [7, 9]])\n>>> ixgrid = np.ix_([True, True], [False, False, True, False, True])\n>>> a[ixgrid]\narray([[2, 4],\n       [7, 9]])", "Library": "NumPy"}
{"API_Name": "np.lexsort", "Docstring": "lexsort(keys, axis=-1)\n\nPerform an indirect stable sort using a sequence of keys.\n\nGiven multiple sorting keys, which can be interpreted as columns in a\nspreadsheet, lexsort returns an array of integer indices that describes\nthe sort order by multiple columns. The last key in the sequence is used\nfor the primary sort order, the second-to-last key for the secondary sort\norder, and so on. The keys argument must be a sequence of objects that\ncan be converted to arrays of the same shape. If a 2D array is provided\nfor the keys argument, its rows are interpreted as the sorting keys and\nsorting is according to the last row, second last row etc.\n\nParameters\n----------\nkeys : (k, N) array or tuple containing k (N,)-shaped sequences\n    The `k` different \"columns\" to be sorted.  The last column (or row if\n    `keys` is a 2D array) is the primary sort key.\naxis : int, optional\n    Axis to be indirectly sorted.  By default, sort over the last axis.\n\nReturns\n-------\nindices : (N,) ndarray of ints\n    Array of indices that sort the keys along the specified axis.\n\nSee Also\n--------\nargsort : Indirect sort.\nndarray.sort : In-place sort.\nsort : Return a sorted copy of an array.\n\nExamples\n--------\nSort names: first by surname, then by name.\n\n>>> surnames =    ('Hertz',    'Galilei', 'Hertz')\n>>> first_names = ('Heinrich', 'Galileo', 'Gustav')\n>>> ind = np.lexsort((first_names, surnames))\n>>> ind\narray([1, 2, 0])\n\n>>> [surnames[i] + \", \" + first_names[i] for i in ind]\n['Galilei, Galileo', 'Hertz, Gustav', 'Hertz, Heinrich']\n\nSort two columns of numbers:\n\n>>> a = [1,5,1,4,3,4,4] # First column\n>>> b = [9,4,0,4,0,2,1] # Second column\n>>> ind = np.lexsort((b,a)) # Sort by a, then by b\n>>> ind\narray([2, 0, 4, 6, 5, 3, 1])\n\n>>> [(a[i],b[i]) for i in ind]\n[(1, 0), (1, 9), (3, 0), (4, 1), (4, 2), (4, 4), (5, 4)]\n\nNote that sorting is first according to the elements of ``a``.\nSecondary sorting is according to the elements of ``b``.\n\nA normal ``argsort`` would have yielded:\n\n>>> [(a[i],b[i]) for i in np.argsort(a)]\n[(1, 9), (1, 0), (3, 0), (4, 4), (4, 2), (4, 1), (5, 4)]\n\nStructured arrays are sorted lexically by ``argsort``:\n\n>>> x = np.array([(1,9), (5,4), (1,0), (4,4), (3,0), (4,2), (4,1)],\n...              dtype=np.dtype([('x', int), ('y', int)]))\n\n>>> np.argsort(x) # or np.argsort(x, order=('x', 'y'))\narray([2, 0, 4, 6, 5, 3, 1])", "Library": "NumPy"}
{"API_Name": "np.lib.stride_tricks.sliding_window_view", "Docstring": "Create a sliding window view into the array with the given window shape.\n\nAlso known as rolling or moving window, the window slides across all\ndimensions of the array and extracts subsets of the array at all window\npositions.\n\n.. versionadded:: 1.20.0\n\nParameters\n----------\nx : array_like\n    Array to create the sliding window view from.\nwindow_shape : int or tuple of int\n    Size of window over each axis that takes part in the sliding window.\n    If `axis` is not present, must have same length as the number of input\n    array dimensions. Single integers `i` are treated as if they were the\n    tuple `(i,)`.\naxis : int or tuple of int, optional\n    Axis or axes along which the sliding window is applied.\n    By default, the sliding window is applied to all axes and\n    `window_shape[i]` will refer to axis `i` of `x`.\n    If `axis` is given as a `tuple of int`, `window_shape[i]` will refer to\n    the axis `axis[i]` of `x`.\n    Single integers `i` are treated as if they were the tuple `(i,)`.\nsubok : bool, optional\n    If True, sub-classes will be passed-through, otherwise the returned\n    array will be forced to be a base-class array (default).\nwriteable : bool, optional\n    When true, allow writing to the returned view. The default is false,\n    as this should be used with caution: the returned view contains the\n    same memory location multiple times, so writing to one location will\n    cause others to change.\n\nReturns\n-------\nview : ndarray\n    Sliding window view of the array. The sliding window dimensions are\n    inserted at the end, and the original dimensions are trimmed as\n    required by the size of the sliding window.\n    That is, ``view.shape = x_shape_trimmed + window_shape``, where\n    ``x_shape_trimmed`` is ``x.shape`` with every entry reduced by one less\n    than the corresponding window size.\n\nSee Also\n--------\nlib.stride_tricks.as_strided: A lower-level and less safe routine for\n    creating arbitrary views from custom shape and strides.\nbroadcast_to: broadcast an array to a given shape.\n\nNotes\n-----\nFor many applications using a sliding window view can be convenient, but\npotentially very slow. Often specialized solutions exist, for example:\n\n- `scipy.signal.fftconvolve`\n\n- filtering functions in `scipy.ndimage`\n\n- moving window functions provided by\n  `bottleneck <https://github.com/pydata/bottleneck>`_.\n\nAs a rough estimate, a sliding window approach with an input size of `N`\nand a window size of `W` will scale as `O(N*W)` where frequently a special\nalgorithm can achieve `O(N)`. That means that the sliding window variant\nfor a window size of 100 can be a 100 times slower than a more specialized\nversion.\n\nNevertheless, for small window sizes, when no custom algorithm exists, or\nas a prototyping and developing tool, this function can be a good solution.\n\nExamples\n--------\n>>> x = np.arange(6)\n>>> x.shape\n(6,)\n>>> v = sliding_window_view(x, 3)\n>>> v.shape\n(4, 3)\n>>> v\narray([[0, 1, 2],\n       [1, 2, 3],\n       [2, 3, 4],\n       [3, 4, 5]])\n\nThis also works in more dimensions, e.g.\n\n>>> i, j = np.ogrid[:3, :4]\n>>> x = 10*i + j\n>>> x.shape\n(3, 4)\n>>> x\narray([[ 0,  1,  2,  3],\n       [10, 11, 12, 13],\n       [20, 21, 22, 23]])\n>>> shape = (2,2)\n>>> v = sliding_window_view(x, shape)\n>>> v.shape\n(2, 3, 2, 2)\n>>> v\narray([[[[ 0,  1],\n         [10, 11]],\n        [[ 1,  2],\n         [11, 12]],\n        [[ 2,  3],\n         [12, 13]]],\n       [[[10, 11],\n         [20, 21]],\n        [[11, 12],\n         [21, 22]],\n        [[12, 13],\n         [22, 23]]]])\n\nThe axis can be specified explicitly:\n\n>>> v = sliding_window_view(x, 3, 0)\n>>> v.shape\n(1, 4, 3)\n>>> v\narray([[[ 0, 10, 20],\n        [ 1, 11, 21],\n        [ 2, 12, 22],\n        [ 3, 13, 23]]])\n\nThe same axis can be used several times. In that case, every use reduces\nthe corresponding original dimension:\n\n>>> v = sliding_window_view(x, (2, 3), (1, 1))\n>>> v.shape\n(3, 1, 2, 3)\n>>> v\narray([[[[ 0,  1,  2],\n         [ 1,  2,  3]]],\n       [[[10, 11, 12],\n         [11, 12, 13]]],\n       [[[20, 21, 22],\n         [21, 22, 23]]]])\n\nCombining with stepped slicing (`::step`), this can be used to take sliding\nviews which skip elements:\n\n>>> x = np.arange(7)\n>>> sliding_window_view(x, 5)[:, ::2]\narray([[0, 2, 4],\n       [1, 3, 5],\n       [2, 4, 6]])\n\nor views which move by multiple elements\n\n>>> x = np.arange(7)\n>>> sliding_window_view(x, 3)[::2, :]\narray([[0, 1, 2],\n       [2, 3, 4],\n       [4, 5, 6]])\n\nA common application of `sliding_window_view` is the calculation of running\nstatistics. The simplest example is the\n`moving average <https://en.wikipedia.org/wiki/Moving_average>`_:\n\n>>> x = np.arange(6)\n>>> x.shape\n(6,)\n>>> v = sliding_window_view(x, 3)\n>>> v.shape\n(4, 3)\n>>> v\narray([[0, 1, 2],\n       [1, 2, 3],\n       [2, 3, 4],\n       [3, 4, 5]])\n>>> moving_average = v.mean(axis=-1)\n>>> moving_average\narray([1., 2., 3., 4.])\n\nNote that a sliding window approach is often **not** optimal (see Notes).", "Library": "NumPy"}
{"API_Name": "np.linalg.matrix_power", "Docstring": "Raise a square matrix to the (integer) power `n`.\n\nFor positive integers `n`, the power is computed by repeated matrix\nsquarings and matrix multiplications. If ``n == 0``, the identity matrix\nof the same shape as M is returned. If ``n < 0``, the inverse\nis computed and then raised to the ``abs(n)``.\n\n.. note:: Stacks of object matrices are not currently supported.\n\nParameters\n----------\na : (..., M, M) array_like\n    Matrix to be \"powered\".\nn : int\n    The exponent can be any integer or long integer, positive,\n    negative, or zero.\n\nReturns\n-------\na**n : (..., M, M) ndarray or matrix object\n    The return value is the same shape and type as `M`;\n    if the exponent is positive or zero then the type of the\n    elements is the same as those of `M`. If the exponent is\n    negative the elements are floating-point.\n\nRaises\n------\nLinAlgError\n    For matrices that are not square or that (for negative powers) cannot\n    be inverted numerically.\n\nExamples\n--------\n>>> from numpy.linalg import matrix_power\n>>> i = np.array([[0, 1], [-1, 0]]) # matrix equiv. of the imaginary unit\n>>> matrix_power(i, 3) # should = -i\narray([[ 0, -1],\n       [ 1,  0]])\n>>> matrix_power(i, 0)\narray([[1, 0],\n       [0, 1]])\n>>> matrix_power(i, -3) # should = 1/(-i) = i, but w/ f.p. elements\narray([[ 0.,  1.],\n       [-1.,  0.]])\n\nSomewhat more sophisticated example\n\n>>> q = np.zeros((4, 4))\n>>> q[0:2, 0:2] = -i\n>>> q[2:4, 2:4] = i\n>>> q # one of the three quaternion units not equal to 1\narray([[ 0., -1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.],\n       [ 0.,  0., -1.,  0.]])\n>>> matrix_power(q, 2) # = -np.eye(4)\narray([[-1.,  0.,  0.,  0.],\n       [ 0., -1.,  0.,  0.],\n       [ 0.,  0., -1.,  0.],\n       [ 0.,  0.,  0., -1.]])", "Library": "NumPy"}
{"API_Name": "np.linalg.matrix_rank", "Docstring": "Return matrix rank of array using SVD method\n\nRank of the array is the number of singular values of the array that are\ngreater than `tol`.\n\n.. versionchanged:: 1.14\n   Can now operate on stacks of matrices\n\nParameters\n----------\nA : {(M,), (..., M, N)} array_like\n    Input vector or stack of matrices.\ntol : (...) array_like, float, optional\n    Threshold below which SVD values are considered zero. If `tol` is\n    None, and ``S`` is an array with singular values for `M`, and\n    ``eps`` is the epsilon value for datatype of ``S``, then `tol` is\n    set to ``S.max() * max(M, N) * eps``.\n\n    .. versionchanged:: 1.14\n       Broadcasted against the stack of matrices\nhermitian : bool, optional\n    If True, `A` is assumed to be Hermitian (symmetric if real-valued),\n    enabling a more efficient method for finding singular values.\n    Defaults to False.\n\n    .. versionadded:: 1.14\n\nReturns\n-------\nrank : (...) array_like\n    Rank of A.\n\nNotes\n-----\nThe default threshold to detect rank deficiency is a test on the magnitude\nof the singular values of `A`.  By default, we identify singular values less\nthan ``S.max() * max(M, N) * eps`` as indicating rank deficiency (with\nthe symbols defined above). This is the algorithm MATLAB uses [1].  It also\nappears in *Numerical recipes* in the discussion of SVD solutions for linear\nleast squares [2].\n\nThis default threshold is designed to detect rank deficiency accounting for\nthe numerical errors of the SVD computation.  Imagine that there is a column\nin `A` that is an exact (in floating point) linear combination of other\ncolumns in `A`. Computing the SVD on `A` will not produce a singular value\nexactly equal to 0 in general: any difference of the smallest SVD value from\n0 will be caused by numerical imprecision in the calculation of the SVD.\nOur threshold for small SVD values takes this numerical imprecision into\naccount, and the default threshold will detect such numerical rank\ndeficiency.  The threshold may declare a matrix `A` rank deficient even if\nthe linear combination of some columns of `A` is not exactly equal to\nanother column of `A` but only numerically very close to another column of\n`A`.\n\nWe chose our default threshold because it is in wide use.  Other thresholds\nare possible.  For example, elsewhere in the 2007 edition of *Numerical\nrecipes* there is an alternative threshold of ``S.max() *\nnp.finfo(A.dtype).eps / 2. * np.sqrt(m + n + 1.)``. The authors describe\nthis threshold as being based on \"expected roundoff error\" (p 71).\n\nThe thresholds above deal with floating point roundoff error in the\ncalculation of the SVD.  However, you may have more information about the\nsources of error in `A` that would make you consider other tolerance values\nto detect *effective* rank deficiency.  The most useful measure of the\ntolerance depends on the operations you intend to use on your matrix.  For\nexample, if your data come from uncertain measurements with uncertainties\ngreater than floating point epsilon, choosing a tolerance near that\nuncertainty may be preferable.  The tolerance may be absolute if the\nuncertainties are absolute rather than relative.\n\nReferences\n----------\n.. [1] MATLAB reference documentation, \"Rank\"\n       https://www.mathworks.com/help/techdoc/ref/rank.html\n.. [2] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,\n       \"Numerical Recipes (3rd edition)\", Cambridge University Press, 2007,\n       page 795.\n\nExamples\n--------\n>>> from numpy.linalg import matrix_rank\n>>> matrix_rank(np.eye(4)) # Full rank matrix\n4\n>>> I=np.eye(4); I[-1,-1] = 0. # rank deficient matrix\n>>> matrix_rank(I)\n3\n>>> matrix_rank(np.ones((4,))) # 1 dimension - rank 1 unless all 0\n1\n>>> matrix_rank(np.zeros((4,)))\n0", "Library": "NumPy"}
{"API_Name": "np.linalg.norm", "Docstring": "Matrix or vector norm.\n\nThis function is able to return one of eight different matrix norms,\nor one of an infinite number of vector norms (described below), depending\non the value of the ``ord`` parameter.\n\nParameters\n----------\nx : array_like\n    Input array.  If `axis` is None, `x` must be 1-D or 2-D, unless `ord`\n    is None. If both `axis` and `ord` are None, the 2-norm of\n    ``x.ravel`` will be returned.\nord : {non-zero int, inf, -inf, 'fro', 'nuc'}, optional\n    Order of the norm (see table under ``Notes``). inf means numpy's\n    `inf` object. The default is None.\naxis : {None, int, 2-tuple of ints}, optional.\n    If `axis` is an integer, it specifies the axis of `x` along which to\n    compute the vector norms.  If `axis` is a 2-tuple, it specifies the\n    axes that hold 2-D matrices, and the matrix norms of these matrices\n    are computed.  If `axis` is None then either a vector norm (when `x`\n    is 1-D) or a matrix norm (when `x` is 2-D) is returned. The default\n    is None.\n\n    .. versionadded:: 1.8.0\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are normed over are left in the\n    result as dimensions with size one.  With this option the result will\n    broadcast correctly against the original `x`.\n\n    .. versionadded:: 1.10.0\n\nReturns\n-------\nn : float or ndarray\n    Norm of the matrix or vector(s).\n\nSee Also\n--------\nscipy.linalg.norm : Similar function in SciPy.\n\nNotes\n-----\nFor values of ``ord < 1``, the result is, strictly speaking, not a\nmathematical 'norm', but it may still be useful for various numerical\npurposes.\n\nThe following norms can be calculated:\n\n=====  ============================  ==========================\nord    norm for matrices             norm for vectors\n=====  ============================  ==========================\nNone   Frobenius norm                2-norm\n'fro'  Frobenius norm                --\n'nuc'  nuclear norm                  --\ninf    max(sum(abs(x), axis=1))      max(abs(x))\n-inf   min(sum(abs(x), axis=1))      min(abs(x))\n0      --                            sum(x != 0)\n1      max(sum(abs(x), axis=0))      as below\n-1     min(sum(abs(x), axis=0))      as below\n2      2-norm (largest sing. value)  as below\n-2     smallest singular value       as below\nother  --                            sum(abs(x)**ord)**(1./ord)\n=====  ============================  ==========================\n\nThe Frobenius norm is given by [1]_:\n\n    :math:`||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for\nmatrices and raise a ValueError when ``x.ndim != 2``.\n\nReferences\n----------\n.. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,\n       Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n\nExamples\n--------\n>>> from numpy import linalg as LA\n>>> a = np.arange(9) - 4\n>>> a\narray([-4, -3, -2, ...,  2,  3,  4])\n>>> b = a.reshape((3, 3))\n>>> b\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\n\n>>> LA.norm(a)\n7.745966692414834\n>>> LA.norm(b)\n7.745966692414834\n>>> LA.norm(b, 'fro')\n7.745966692414834\n>>> LA.norm(a, np.inf)\n4.0\n>>> LA.norm(b, np.inf)\n9.0\n>>> LA.norm(a, -np.inf)\n0.0\n>>> LA.norm(b, -np.inf)\n2.0\n\n>>> LA.norm(a, 1)\n20.0\n>>> LA.norm(b, 1)\n7.0\n>>> LA.norm(a, -1)\n-4.6566128774142013e-010\n>>> LA.norm(b, -1)\n6.0\n>>> LA.norm(a, 2)\n7.745966692414834\n>>> LA.norm(b, 2)\n7.3484692283495345\n\n>>> LA.norm(a, -2)\n0.0\n>>> LA.norm(b, -2)\n1.8570331885190563e-016 # may vary\n>>> LA.norm(a, 3)\n5.8480354764257312 # may vary\n>>> LA.norm(a, -3)\n0.0\n\nUsing the `axis` argument to compute vector norms:\n\n>>> c = np.array([[ 1, 2, 3],\n...               [-1, 1, 4]])\n>>> LA.norm(c, axis=0)\narray([ 1.41421356,  2.23606798,  5.        ])\n>>> LA.norm(c, axis=1)\narray([ 3.74165739,  4.24264069])\n>>> LA.norm(c, ord=1, axis=1)\narray([ 6.,  6.])\n\nUsing the `axis` argument to compute matrix norms:\n\n>>> m = np.arange(8).reshape(2,2,2)\n>>> LA.norm(m, axis=(1,2))\narray([  3.74165739,  11.22497216])\n>>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(3.7416573867739413, 11.224972160321824)", "Library": "NumPy"}
{"API_Name": "np.linalg.svd", "Docstring": "Singular Value Decomposition.\n\nWhen `a` is a 2D array, and ``full_matrices=False``, then it is\nfactorized as ``u @ np.diag(s) @ vh = (u * s) @ vh``, where\n`u` and the Hermitian transpose of `vh` are 2D arrays with\northonormal columns and `s` is a 1D array of `a`'s singular\nvalues. When `a` is higher-dimensional, SVD is applied in\nstacked mode as explained below.\n\nParameters\n----------\na : (..., M, N) array_like\n    A real or complex array with ``a.ndim >= 2``.\nfull_matrices : bool, optional\n    If True (default), `u` and `vh` have the shapes ``(..., M, M)`` and\n    ``(..., N, N)``, respectively.  Otherwise, the shapes are\n    ``(..., M, K)`` and ``(..., K, N)``, respectively, where\n    ``K = min(M, N)``.\ncompute_uv : bool, optional\n    Whether or not to compute `u` and `vh` in addition to `s`.  True\n    by default.\nhermitian : bool, optional\n    If True, `a` is assumed to be Hermitian (symmetric if real-valued),\n    enabling a more efficient method for finding singular values.\n    Defaults to False.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nu : { (..., M, M), (..., M, K) } array\n    Unitary array(s). The first ``a.ndim - 2`` dimensions have the same\n    size as those of the input `a`. The size of the last two dimensions\n    depends on the value of `full_matrices`. Only returned when\n    `compute_uv` is True.\ns : (..., K) array\n    Vector(s) with the singular values, within each vector sorted in\n    descending order. The first ``a.ndim - 2`` dimensions have the same\n    size as those of the input `a`.\nvh : { (..., N, N), (..., K, N) } array\n    Unitary array(s). The first ``a.ndim - 2`` dimensions have the same\n    size as those of the input `a`. The size of the last two dimensions\n    depends on the value of `full_matrices`. Only returned when\n    `compute_uv` is True.\n\nRaises\n------\nLinAlgError\n    If SVD computation does not converge.\n\nSee Also\n--------\nscipy.linalg.svd : Similar function in SciPy.\nscipy.linalg.svdvals : Compute singular values of a matrix.\n\nNotes\n-----\n\n.. versionchanged:: 1.8.0\n   Broadcasting rules apply, see the `numpy.linalg` documentation for\n   details.\n\nThe decomposition is performed using LAPACK routine ``_gesdd``.\n\nSVD is usually described for the factorization of a 2D matrix :math:`A`.\nThe higher-dimensional case will be discussed below. In the 2D case, SVD is\nwritten as :math:`A = U S V^H`, where :math:`A = a`, :math:`U= u`,\n:math:`S= \\mathtt{np.diag}(s)` and :math:`V^H = vh`. The 1D array `s`\ncontains the singular values of `a` and `u` and `vh` are unitary. The rows\nof `vh` are the eigenvectors of :math:`A^H A` and the columns of `u` are\nthe eigenvectors of :math:`A A^H`. In both cases the corresponding\n(possibly non-zero) eigenvalues are given by ``s**2``.\n\nIf `a` has more than two dimensions, then broadcasting rules apply, as\nexplained in :ref:`routines.linalg-broadcasting`. This means that SVD is\nworking in \"stacked\" mode: it iterates over all indices of the first\n``a.ndim - 2`` dimensions and for each combination SVD is applied to the\nlast two indices. The matrix `a` can be reconstructed from the\ndecomposition with either ``(u * s[..., None, :]) @ vh`` or\n``u @ (s[..., None] * vh)``. (The ``@`` operator can be replaced by the\nfunction ``np.matmul`` for python versions below 3.5.)\n\nIf `a` is a ``matrix`` object (as opposed to an ``ndarray``), then so are\nall the return values.\n\nExamples\n--------\n>>> a = np.random.randn(9, 6) + 1j*np.random.randn(9, 6)\n>>> b = np.random.randn(2, 7, 8, 3) + 1j*np.random.randn(2, 7, 8, 3)\n\nReconstruction based on full SVD, 2D case:\n\n>>> u, s, vh = np.linalg.svd(a, full_matrices=True)\n>>> u.shape, s.shape, vh.shape\n((9, 9), (6,), (6, 6))\n>>> np.allclose(a, np.dot(u[:, :6] * s, vh))\nTrue\n>>> smat = np.zeros((9, 6), dtype=complex)\n>>> smat[:6, :6] = np.diag(s)\n>>> np.allclose(a, np.dot(u, np.dot(smat, vh)))\nTrue\n\nReconstruction based on reduced SVD, 2D case:\n\n>>> u, s, vh = np.linalg.svd(a, full_matrices=False)\n>>> u.shape, s.shape, vh.shape\n((9, 6), (6,), (6, 6))\n>>> np.allclose(a, np.dot(u * s, vh))\nTrue\n>>> smat = np.diag(s)\n>>> np.allclose(a, np.dot(u, np.dot(smat, vh)))\nTrue\n\nReconstruction based on full SVD, 4D case:\n\n>>> u, s, vh = np.linalg.svd(b, full_matrices=True)\n>>> u.shape, s.shape, vh.shape\n((2, 7, 8, 8), (2, 7, 3), (2, 7, 3, 3))\n>>> np.allclose(b, np.matmul(u[..., :3] * s[..., None, :], vh))\nTrue\n>>> np.allclose(b, np.matmul(u[..., :3], s[..., None] * vh))\nTrue\n\nReconstruction based on reduced SVD, 4D case:\n\n>>> u, s, vh = np.linalg.svd(b, full_matrices=False)\n>>> u.shape, s.shape, vh.shape\n((2, 7, 8, 3), (2, 7, 3), (2, 7, 3, 3))\n>>> np.allclose(b, np.matmul(u * s[..., None, :], vh))\nTrue\n>>> np.allclose(b, np.matmul(u, s[..., None] * vh))\nTrue", "Library": "NumPy"}
{"API_Name": "np.linspace", "Docstring": "Return evenly spaced numbers over a specified interval.\n\nReturns `num` evenly spaced samples, calculated over the\ninterval [`start`, `stop`].\n\nThe endpoint of the interval can optionally be excluded.\n\n.. versionchanged:: 1.16.0\n    Non-scalar `start` and `stop` are now supported.\n\n.. versionchanged:: 1.20.0\n    Values are rounded towards ``-inf`` instead of ``0`` when an\n    integer ``dtype`` is specified. The old behavior can\n    still be obtained with ``np.linspace(start, stop, num).astype(int)``\n\nParameters\n----------\nstart : array_like\n    The starting value of the sequence.\nstop : array_like\n    The end value of the sequence, unless `endpoint` is set to False.\n    In that case, the sequence consists of all but the last of ``num + 1``\n    evenly spaced samples, so that `stop` is excluded.  Note that the step\n    size changes when `endpoint` is False.\nnum : int, optional\n    Number of samples to generate. Default is 50. Must be non-negative.\nendpoint : bool, optional\n    If True, `stop` is the last sample. Otherwise, it is not included.\n    Default is True.\nretstep : bool, optional\n    If True, return (`samples`, `step`), where `step` is the spacing\n    between samples.\ndtype : dtype, optional\n    The type of the output array.  If `dtype` is not given, the data type\n    is inferred from `start` and `stop`. The inferred dtype will never be\n    an integer; `float` is chosen even if the arguments would produce an\n    array of integers.\n\n    .. versionadded:: 1.9.0\n\naxis : int, optional\n    The axis in the result to store the samples.  Relevant only if start\n    or stop are array-like.  By default (0), the samples will be along a\n    new axis inserted at the beginning. Use -1 to get an axis at the end.\n\n    .. versionadded:: 1.16.0\n\nReturns\n-------\nsamples : ndarray\n    There are `num` equally spaced samples in the closed interval\n    ``[start, stop]`` or the half-open interval ``[start, stop)``\n    (depending on whether `endpoint` is True or False).\nstep : float, optional\n    Only returned if `retstep` is True\n\n    Size of spacing between samples.\n\n\nSee Also\n--------\narange : Similar to `linspace`, but uses a step size (instead of the\n         number of samples).\ngeomspace : Similar to `linspace`, but with numbers spaced evenly on a log\n            scale (a geometric progression).\nlogspace : Similar to `geomspace`, but with the end points specified as\n           logarithms.\n:ref:`how-to-partition`\n\nExamples\n--------\n>>> np.linspace(2.0, 3.0, num=5)\narray([2.  , 2.25, 2.5 , 2.75, 3.  ])\n>>> np.linspace(2.0, 3.0, num=5, endpoint=False)\narray([2. ,  2.2,  2.4,  2.6,  2.8])\n>>> np.linspace(2.0, 3.0, num=5, retstep=True)\n(array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\nGraphical illustration:\n\n>>> import matplotlib.pyplot as plt\n>>> N = 8\n>>> y = np.zeros(N)\n>>> x1 = np.linspace(0, 10, N, endpoint=True)\n>>> x2 = np.linspace(0, 10, N, endpoint=False)\n>>> plt.plot(x1, y, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.plot(x2, y + 0.5, 'o')\n[<matplotlib.lines.Line2D object at 0x...>]\n>>> plt.ylim([-0.5, 1])\n(-0.5, 1)\n>>> plt.show()", "Library": "NumPy"}
{"API_Name": "np.logical_and", "Docstring": "logical_and(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nCompute the truth value of x1 AND x2 element-wise.\n\nParameters\n----------\nx1, x2 : array_like\n    Input arrays.\n    If ``x1.shape != x2.shape``, they must be broadcastable to a common\n    shape (which becomes the shape of the output).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray or bool\n    Boolean result of the logical AND operation applied to the elements\n    of `x1` and `x2`; the shape is determined by broadcasting.\n    This is a scalar if both `x1` and `x2` are scalars.\n\nSee Also\n--------\nlogical_or, logical_not, logical_xor\nbitwise_and\n\nExamples\n--------\n>>> np.logical_and(True, False)\nFalse\n>>> np.logical_and([True, False], [False, False])\narray([False, False])\n\n>>> x = np.arange(5)\n>>> np.logical_and(x>1, x<4)\narray([False, False,  True,  True, False])\n\n\nThe ``&`` operator can be used as a shorthand for ``np.logical_and`` on\nboolean ndarrays.\n\n>>> a = np.array([True, False])\n>>> b = np.array([False, False])\n>>> a & b\narray([False, False])", "Library": "NumPy"}
{"API_Name": "np.ma.filled", "Docstring": "Return input as an array with masked data replaced by a fill value.\n\nIf `a` is not a `MaskedArray`, `a` itself is returned.\nIf `a` is a `MaskedArray` and `fill_value` is None, `fill_value` is set to\n``a.fill_value``.\n\nParameters\n----------\na : MaskedArray or array_like\n    An input object.\nfill_value : array_like, optional.\n    Can be scalar or non-scalar. If non-scalar, the\n    resulting filled array should be broadcastable\n    over input array. Default is None.\n\nReturns\n-------\na : ndarray\n    The filled array.\n\nSee Also\n--------\ncompressed\n\nExamples\n--------\n>>> x = np.ma.array(np.arange(9).reshape(3, 3), mask=[[1, 0, 0],\n...                                                   [1, 0, 0],\n...                                                   [0, 0, 0]])\n>>> x.filled()\narray([[999999,      1,      2],\n       [999999,      4,      5],\n       [     6,      7,      8]])\n>>> x.filled(fill_value=333)\narray([[333,   1,   2],\n       [333,   4,   5],\n       [  6,   7,   8]])\n>>> x.filled(fill_value=np.arange(3))\narray([[0, 1, 2],\n       [0, 4, 5],\n       [6, 7, 8]])", "Library": "NumPy"}
{"API_Name": "np.ma.masked_where", "Docstring": "Mask an array where a condition is met.\n\nReturn `a` as an array masked where `condition` is True.\nAny masked values of `a` or `condition` are also masked in the output.\n\nParameters\n----------\ncondition : array_like\n    Masking condition.  When `condition` tests floating point values for\n    equality, consider using ``masked_values`` instead.\na : array_like\n    Array to mask.\ncopy : bool\n    If True (default) make a copy of `a` in the result.  If False modify\n    `a` in place and return a view.\n\nReturns\n-------\nresult : MaskedArray\n    The result of masking `a` where `condition` is True.\n\nSee Also\n--------\nmasked_values : Mask using floating point equality.\nmasked_equal : Mask where equal to a given value.\nmasked_not_equal : Mask where `not` equal to a given value.\nmasked_less_equal : Mask where less than or equal to a given value.\nmasked_greater_equal : Mask where greater than or equal to a given value.\nmasked_less : Mask where less than a given value.\nmasked_greater : Mask where greater than a given value.\nmasked_inside : Mask inside a given interval.\nmasked_outside : Mask outside a given interval.\nmasked_invalid : Mask invalid values (NaNs or infs).\n\nExamples\n--------\n>>> import numpy.ma as ma\n>>> a = np.arange(4)\n>>> a\narray([0, 1, 2, 3])\n>>> ma.masked_where(a <= 2, a)\nmasked_array(data=[--, --, --, 3],\n             mask=[ True,  True,  True, False],\n       fill_value=999999)\n\nMask array `b` conditional on `a`.\n\n>>> b = ['a', 'b', 'c', 'd']\n>>> ma.masked_where(a == 2, b)\nmasked_array(data=['a', 'b', --, 'd'],\n             mask=[False, False,  True, False],\n       fill_value='N/A',\n            dtype='<U1')\n\nEffect of the `copy` argument.\n\n>>> c = ma.masked_where(a <= 2, a)\n>>> c\nmasked_array(data=[--, --, --, 3],\n             mask=[ True,  True,  True, False],\n       fill_value=999999)\n>>> c[0] = 99\n>>> c\nmasked_array(data=[99, --, --, 3],\n             mask=[False,  True,  True, False],\n       fill_value=999999)\n>>> a\narray([0, 1, 2, 3])\n>>> c = ma.masked_where(a <= 2, a, copy=False)\n>>> c[0] = 99\n>>> c\nmasked_array(data=[99, --, --, 3],\n             mask=[False,  True,  True, False],\n       fill_value=999999)\n>>> a\narray([99,  1,  2,  3])\n\nWhen `condition` or `a` contain masked values.\n\n>>> a = np.arange(4)\n>>> a = ma.masked_where(a == 2, a)\n>>> a\nmasked_array(data=[0, 1, --, 3],\n             mask=[False, False,  True, False],\n       fill_value=999999)\n>>> b = np.arange(4)\n>>> b = ma.masked_where(b == 0, b)\n>>> b\nmasked_array(data=[--, 1, 2, 3],\n             mask=[ True, False, False, False],\n       fill_value=999999)\n>>> ma.masked_where(a == 3, b)\nmasked_array(data=[--, 1, --, --],\n             mask=[ True, False,  True,  True],\n       fill_value=999999)", "Library": "NumPy"}
{"API_Name": "np.matrix", "Docstring": "matrix(data, dtype=None, copy=True)\n\n.. note:: It is no longer recommended to use this class, even for linear\n          algebra. Instead use regular arrays. The class may be removed\n          in the future.\n\nReturns a matrix from an array-like object, or from a string of data.\nA matrix is a specialized 2-D array that retains its 2-D nature\nthrough operations.  It has certain special operators, such as ``*``\n(matrix multiplication) and ``**`` (matrix power).\n\nParameters\n----------\ndata : array_like or string\n   If `data` is a string, it is interpreted as a matrix with commas\n   or spaces separating columns, and semicolons separating rows.\ndtype : data-type\n   Data-type of the output matrix.\ncopy : bool\n   If `data` is already an `ndarray`, then this flag determines\n   whether the data is copied (the default), or whether a view is\n   constructed.\n\nSee Also\n--------\narray\n\nExamples\n--------\n>>> a = np.matrix('1 2; 3 4')\n>>> a\nmatrix([[1, 2],\n        [3, 4]])\n\n>>> np.matrix([[1, 2], [3, 4]])\nmatrix([[1, 2],\n        [3, 4]])", "Library": "NumPy"}
{"API_Name": "np.max", "Docstring": "Return the maximum of an array or maximum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the maximum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amax` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The minimum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the maximum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namax : ndarray or scalar\n    Maximum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is an int, the result is an array of dimension\n    ``a.ndim - 1``. If `axis` is a tuple, the result is an array of \n    dimension ``a.ndim - len(axis)``.\n\nSee Also\n--------\namin :\n    The minimum value of an array along a given axis, propagating any NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignoring any NaNs.\nmaximum :\n    Element-wise maximum of two arrays, propagating any NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignoring any NaNs.\nargmax :\n    Return the indices of the maximum values.\n\nnanmin, minimum, fmin\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding max value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmax.\n\nDon't use `amax` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``maximum(a[0], a[1])`` is faster than\n``amax(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amax(a)           # Maximum of the flattened array\n3\n>>> np.amax(a, axis=0)   # Maxima along the first axis\narray([2, 3])\n>>> np.amax(a, axis=1)   # Maxima along the second axis\narray([1, 3])\n>>> np.amax(a, where=[False, True], initial=-1, axis=0)\narray([-1,  3])\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amax(b)\nnan\n>>> np.amax(b, where=~np.isnan(b), initial=-1)\n4.0\n>>> np.nanmax(b)\n4.0\n\nYou can use an initial value to compute the maximum of an empty slice, or\nto initialize it to a different value:\n\n>>> np.amax([[-50], [10]], axis=-1, initial=0)\narray([ 0, 10])\n\nNotice that the initial value is used as one of the elements for which the\nmaximum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\n>>> np.amax([5], initial=6)\n6\n>>> max([5], default=6)\n5", "Library": "NumPy"}
{"API_Name": "np.maximum", "Docstring": "maximum(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nElement-wise maximum of array elements.\n\nCompare two arrays and returns a new array containing the element-wise\nmaxima. If one of the elements being compared is a NaN, then that\nelement is returned. If both elements are NaNs then the first is\nreturned. The latter distinction is important for complex NaNs, which\nare defined as at least one of the real or imaginary parts being a NaN.\nThe net effect is that NaNs are propagated.\n\nParameters\n----------\nx1, x2 : array_like\n    The arrays holding the elements to be compared.\n    If ``x1.shape != x2.shape``, they must be broadcastable to a common\n    shape (which becomes the shape of the output).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray or scalar\n    The maximum of `x1` and `x2`, element-wise.\n    This is a scalar if both `x1` and `x2` are scalars.\n\nSee Also\n--------\nminimum :\n    Element-wise minimum of two arrays, propagates NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignores NaNs.\namax :\n    The maximum value of an array along a given axis, propagates NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignores NaNs.\n\nfmin, amin, nanmin\n\nNotes\n-----\nThe maximum is equivalent to ``np.where(x1 >= x2, x1, x2)`` when\nneither x1 nor x2 are nans, but it is faster and does proper\nbroadcasting.\n\nExamples\n--------\n>>> np.maximum([2, 3, 4], [1, 5, 2])\narray([2, 5, 4])\n\n>>> np.maximum(np.eye(2), [0.5, 2]) # broadcasting\narray([[ 1. ,  2. ],\n       [ 0.5,  2. ]])\n\n>>> np.maximum([np.nan, 0, np.nan], [0, np.nan, np.nan])\narray([nan, nan, nan])\n>>> np.maximum(np.Inf, 1)\ninf", "Library": "NumPy"}
{"API_Name": "np.maximum.accumulate", "Docstring": "accumulate(array, axis=0, dtype=None, out=None)\n\nAccumulate the result of applying the operator to all elements.\n\nFor a one-dimensional array, accumulate produces results equivalent to::\n\n  r = np.empty(len(A))\n  t = op.identity        # op = the ufunc being applied to A's  elements\n  for i in range(len(A)):\n      t = op(t, A[i])\n      r[i] = t\n  return r\n\nFor example, add.accumulate() is equivalent to np.cumsum().\n\nFor a multi-dimensional array, accumulate is applied along only one\naxis (axis zero by default; see Examples below) so repeated use is\nnecessary if one wants to accumulate over multiple axes.\n\nParameters\n----------\narray : array_like\n    The array to act on.\naxis : int, optional\n    The axis along which to apply the accumulation; default is zero.\ndtype : data-type code, optional\n    The data-type used to represent the intermediate results. Defaults\n    to the data-type of the output array if such is provided, or the\n    data-type of the input array if no output array is provided.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If not provided or None,\n    a freshly-allocated array is returned. For consistency with\n    ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n    1-element tuple.\n\n    .. versionchanged:: 1.13.0\n       Tuples are allowed for keyword argument.\n\nReturns\n-------\nr : ndarray\n    The accumulated values. If `out` was supplied, `r` is a reference to\n    `out`.\n\nExamples\n--------\n1-D array examples:\n\n>>> np.add.accumulate([2, 3, 5])\narray([ 2,  5, 10])\n>>> np.multiply.accumulate([2, 3, 5])\narray([ 2,  6, 30])\n\n2-D array examples:\n\n>>> I = np.eye(2)\n>>> I\narray([[1.,  0.],\n       [0.,  1.]])\n\nAccumulate along axis 0 (rows), down columns:\n\n>>> np.add.accumulate(I, 0)\narray([[1.,  0.],\n       [1.,  1.]])\n>>> np.add.accumulate(I) # no axis specified = axis zero\narray([[1.,  0.],\n       [1.,  1.]])\n\nAccumulate along axis 1 (columns), through rows:\n\n>>> np.add.accumulate(I, 1)\narray([[1.,  1.],\n       [0.,  1.]])", "Library": "NumPy"}
{"API_Name": "np.mean", "Docstring": "Compute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n\nParameters\n----------\na : array_like\n    Array containing numbers whose mean is desired. If `a` is not an\n    array, a conversion is attempted.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the means are computed. The default is to\n    compute the mean of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a mean is performed over multiple axes,\n    instead of a single axis or all the axes as before.\ndtype : data-type, optional\n    Type to use in computing the mean.  For integer inputs, the default\n    is `float64`; for floating point inputs, it is the same as the\n    input dtype.\nout : ndarray, optional\n    Alternate output array in which to place the result.  The default\n    is ``None``; if provided, it must have the same shape as the\n    expected output, but the type will be cast if necessary.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `mean` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the mean. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nm : ndarray, see dtype parameter above\n    If `out=None`, returns a new array containing the mean values,\n    otherwise a reference to the output array is returned.\n\nSee Also\n--------\naverage : Weighted average\nstd, var, nanmean, nanstd, nanvar\n\nNotes\n-----\nThe arithmetic mean is the sum of the elements along the axis divided\nby the number of elements.\n\nNote that for floating-point input, the mean is computed using the\nsame precision the input has.  Depending on the input data, this can\ncause the results to be inaccurate, especially for `float32` (see\nexample below).  Specifying a higher-precision accumulator using the\n`dtype` keyword can alleviate this issue.\n\nBy default, `float16` results are computed using `float32` intermediates\nfor extra precision.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.mean(a)\n2.5\n>>> np.mean(a, axis=0)\narray([2., 3.])\n>>> np.mean(a, axis=1)\narray([1.5, 3.5])\n\nIn single precision, `mean` can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.mean(a)\n0.54999924\n\nComputing the mean in float64 is more accurate:\n\n>>> np.mean(a, dtype=np.float64)\n0.55000000074505806 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])\n>>> np.mean(a)\n12.0\n>>> np.mean(a, where=[[True], [False], [False]])\n9.0", "Library": "NumPy"}
{"API_Name": "np.median", "Docstring": "Compute the median along the specified axis.\n\nReturns the median of the array elements.\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\naxis : {int, sequence of int, None}, optional\n    Axis or axes along which the medians are computed. The default\n    is to compute the median along a flattened version of the array.\n    A sequence of axes is supported since version 1.9.0.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output,\n    but the type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n   If True, then allow use of memory of input array `a` for\n   calculations. The input array will be modified by the call to\n   `median`. This will save memory when you do not need to preserve\n   the contents of the input array. Treat the input as undefined,\n   but it will probably be fully or partially sorted. Default is\n   False. If `overwrite_input` is ``True`` and `a` is not already an\n   `ndarray`, an error will be raised.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `arr`.\n\n    .. versionadded:: 1.9.0\n\nReturns\n-------\nmedian : ndarray\n    A new array holding the result. If the input contains integers\n    or floats smaller than ``float64``, then the output data-type is\n    ``np.float64``.  Otherwise, the data-type of the output is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nmean, percentile\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i\ne., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\ntwo middle values of ``V_sorted`` when ``N`` is even.\n\nExamples\n--------\n>>> a = np.array([[10, 7, 4], [3, 2, 1]])\n>>> a\narray([[10,  7,  4],\n       [ 3,  2,  1]])\n>>> np.median(a)\n3.5\n>>> np.median(a, axis=0)\narray([6.5, 4.5, 2.5])\n>>> np.median(a, axis=1)\narray([7.,  2.])\n>>> m = np.median(a, axis=0)\n>>> out = np.zeros_like(m)\n>>> np.median(a, axis=0, out=m)\narray([6.5,  4.5,  2.5])\n>>> m\narray([6.5,  4.5,  2.5])\n>>> b = a.copy()\n>>> np.median(b, axis=1, overwrite_input=True)\narray([7.,  2.])\n>>> assert not np.all(a==b)\n>>> b = a.copy()\n>>> np.median(b, axis=None, overwrite_input=True)\n3.5\n>>> assert not np.all(a==b)", "Library": "NumPy"}
{"API_Name": "np.min", "Docstring": "Return the minimum of an array or minimum along an axis.\n\nParameters\n----------\na : array_like\n    Input data.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which to operate.  By default, flattened input is\n    used.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, the minimum is selected over multiple axes,\n    instead of a single axis or all the axes as before.\nout : ndarray, optional\n    Alternative output array in which to place the result.  Must\n    be of the same shape and buffer length as the expected output.\n    See :ref:`ufuncs-output-type` for more details.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `amin` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\ninitial : scalar, optional\n    The maximum value of an output element. Must be present to allow\n    computation on empty slice. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to compare for the minimum. See `~numpy.ufunc.reduce`\n    for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\namin : ndarray or scalar\n    Minimum of `a`. If `axis` is None, the result is a scalar value.\n    If `axis` is an int, the result is an array of dimension\n    ``a.ndim - 1``.  If `axis` is a tuple, the result is an array of \n    dimension ``a.ndim - len(axis)``.\n\nSee Also\n--------\namax :\n    The maximum value of an array along a given axis, propagating any NaNs.\nnanmin :\n    The minimum value of an array along a given axis, ignoring any NaNs.\nminimum :\n    Element-wise minimum of two arrays, propagating any NaNs.\nfmin :\n    Element-wise minimum of two arrays, ignoring any NaNs.\nargmin :\n    Return the indices of the minimum values.\n\nnanmax, maximum, fmax\n\nNotes\n-----\nNaN values are propagated, that is if at least one item is NaN, the\ncorresponding min value will be NaN as well. To ignore NaN values\n(MATLAB behavior), please use nanmin.\n\nDon't use `amin` for element-wise comparison of 2 arrays; when\n``a.shape[0]`` is 2, ``minimum(a[0], a[1])`` is faster than\n``amin(a, axis=0)``.\n\nExamples\n--------\n>>> a = np.arange(4).reshape((2,2))\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> np.amin(a)           # Minimum of the flattened array\n0\n>>> np.amin(a, axis=0)   # Minima along the first axis\narray([0, 1])\n>>> np.amin(a, axis=1)   # Minima along the second axis\narray([0, 2])\n>>> np.amin(a, where=[False, True], initial=10, axis=0)\narray([10,  1])\n\n>>> b = np.arange(5, dtype=float)\n>>> b[2] = np.NaN\n>>> np.amin(b)\nnan\n>>> np.amin(b, where=~np.isnan(b), initial=10)\n0.0\n>>> np.nanmin(b)\n0.0\n\n>>> np.amin([[-50], [10]], axis=-1, initial=0)\narray([-50,   0])\n\nNotice that the initial value is used as one of the elements for which the\nminimum is determined, unlike for the default argument Python's max\nfunction, which is only used for empty iterables.\n\nNotice that this isn't the same as Python's ``default`` argument.\n\n>>> np.amin([6], initial=5)\n5\n>>> min([6], default=5)\n6", "Library": "NumPy"}
{"API_Name": "np.multiply.reduce", "Docstring": "reduce(array, axis=0, dtype=None, out=None, keepdims=False, initial=<no value>, where=True)\n\nReduces `array`'s dimension by one, by applying ufunc along one axis.\n\nLet :math:`array.shape = (N_0, ..., N_i, ..., N_{M-1})`.  Then\n:math:`ufunc.reduce(array, axis=i)[k_0, ..,k_{i-1}, k_{i+1}, .., k_{M-1}]` =\nthe result of iterating `j` over :math:`range(N_i)`, cumulatively applying\nufunc to each :math:`array[k_0, ..,k_{i-1}, j, k_{i+1}, .., k_{M-1}]`.\nFor a one-dimensional array, reduce produces results equivalent to:\n::\n\n r = op.identity # op = ufunc\n for i in range(len(A)):\n   r = op(r, A[i])\n return r\n\nFor example, add.reduce() is equivalent to sum().\n\nParameters\n----------\narray : array_like\n    The array to act on.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a reduction is performed.\n    The default (`axis` = 0) is perform a reduction over the first\n    dimension of the input array. `axis` may be negative, in\n    which case it counts from the last to the first axis.\n\n    .. versionadded:: 1.7.0\n\n    If this is None, a reduction is performed over all the axes.\n    If this is a tuple of ints, a reduction is performed on multiple\n    axes, instead of a single axis or all the axes as before.\n\n    For operations which are either not commutative or not associative,\n    doing a reduction over multiple axes is not well-defined. The\n    ufuncs do not currently raise an exception in this case, but will\n    likely do so in the future.\ndtype : data-type code, optional\n    The type used to represent the intermediate results. Defaults\n    to the data-type of the output array if this is provided, or\n    the data-type of the input array if no output array is provided.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If not provided or None,\n    a freshly-allocated array is returned. For consistency with\n    ``ufunc.__call__``, if given as a keyword, this may be wrapped in a\n    1-element tuple.\n\n    .. versionchanged:: 1.13.0\n       Tuples are allowed for keyword argument.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the original `array`.\n\n    .. versionadded:: 1.7.0\ninitial : scalar, optional\n    The value with which to start the reduction.\n    If the ufunc has no identity or the dtype is object, this defaults\n    to None - otherwise it defaults to ufunc.identity.\n    If ``None`` is given, the first element of the reduction is used,\n    and an error is thrown if the reduction is empty.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    A boolean array which is broadcasted to match the dimensions\n    of `array`, and selects elements to include in the reduction. Note\n    that for ufuncs like ``minimum`` that do not have an identity\n    defined, one has to pass in also ``initial``.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nr : ndarray\n    The reduced array. If `out` was supplied, `r` is a reference to it.\n\nExamples\n--------\n>>> np.multiply.reduce([2,3,5])\n30\n\nA multi-dimensional array example:\n\n>>> X = np.arange(8).reshape((2,2,2))\n>>> X\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n>>> np.add.reduce(X, 0)\narray([[ 4,  6],\n       [ 8, 10]])\n>>> np.add.reduce(X) # confirm: default axis value is 0\narray([[ 4,  6],\n       [ 8, 10]])\n>>> np.add.reduce(X, 1)\narray([[ 2,  4],\n       [10, 12]])\n>>> np.add.reduce(X, 2)\narray([[ 1,  5],\n       [ 9, 13]])\n\nYou can use the ``initial`` keyword argument to initialize the reduction\nwith a different value, and ``where`` to select specific elements to include:\n\n>>> np.add.reduce([10], initial=5)\n15\n>>> np.add.reduce(np.ones((2, 2, 2)), axis=(0, 2), initial=10)\narray([14., 14.])\n>>> a = np.array([10., np.nan, 10])\n>>> np.add.reduce(a, where=~np.isnan(a))\n20.0\n\nAllows reductions of empty arrays where they would normally fail, i.e.\nfor ufuncs without an identity.\n\n>>> np.minimum.reduce([], initial=np.inf)\ninf\n>>> np.minimum.reduce([[1., 2.], [3., 4.]], initial=10., where=[True, False])\narray([ 1., 10.])\n>>> np.minimum.reduce([])\nTraceback (most recent call last):\n    ...\nValueError: zero-size array to reduction operation minimum which has no identity", "Library": "NumPy"}
{"API_Name": "np.nan", "Docstring": "Convert a string or number to a floating point number, if possible.", "Library": "NumPy"}
{"API_Name": "np.nan_to_num", "Docstring": "Replace NaN with zero and infinity with large finite numbers (default\nbehaviour) or with the numbers defined by the user using the `nan`,\n`posinf` and/or `neginf` keywords.\n\nIf `x` is inexact, NaN is replaced by zero or by the user defined value in\n`nan` keyword, infinity is replaced by the largest finite floating point\nvalues representable by ``x.dtype`` or by the user defined value in\n`posinf` keyword and -infinity is replaced by the most negative finite\nfloating point values representable by ``x.dtype`` or by the user defined\nvalue in `neginf` keyword.\n\nFor complex dtypes, the above is applied to each of the real and\nimaginary components of `x` separately.\n\nIf `x` is not inexact, then no replacements are made.\n\nParameters\n----------\nx : scalar or array_like\n    Input data.\ncopy : bool, optional\n    Whether to create a copy of `x` (True) or to replace values\n    in-place (False). The in-place operation only occurs if\n    casting to an array does not require a copy.\n    Default is True.\n\n    .. versionadded:: 1.13\nnan : int, float, optional\n    Value to be used to fill NaN values. If no value is passed\n    then NaN values will be replaced with 0.0.\n\n    .. versionadded:: 1.17\nposinf : int, float, optional\n    Value to be used to fill positive infinity values. If no value is\n    passed then positive infinity values will be replaced with a very\n    large number.\n\n    .. versionadded:: 1.17\nneginf : int, float, optional\n    Value to be used to fill negative infinity values. If no value is\n    passed then negative infinity values will be replaced with a very\n    small (or negative) number.\n\n    .. versionadded:: 1.17\n\n\n\nReturns\n-------\nout : ndarray\n    `x`, with the non-finite values replaced. If `copy` is False, this may\n    be `x` itself.\n\nSee Also\n--------\nisinf : Shows which elements are positive or negative infinity.\nisneginf : Shows which elements are negative infinity.\nisposinf : Shows which elements are positive infinity.\nisnan : Shows which elements are Not a Number (NaN).\nisfinite : Shows which elements are finite (not NaN, not infinity)\n\nNotes\n-----\nNumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic\n(IEEE 754). This means that Not a Number is not equivalent to infinity.\n\nExamples\n--------\n>>> np.nan_to_num(np.inf)\n1.7976931348623157e+308\n>>> np.nan_to_num(-np.inf)\n-1.7976931348623157e+308\n>>> np.nan_to_num(np.nan)\n0.0\n>>> x = np.array([np.inf, -np.inf, np.nan, -128, 128])\n>>> np.nan_to_num(x)\narray([ 1.79769313e+308, -1.79769313e+308,  0.00000000e+000, # may vary\n       -1.28000000e+002,  1.28000000e+002])\n>>> np.nan_to_num(x, nan=-9999, posinf=33333333, neginf=33333333)\narray([ 3.3333333e+07,  3.3333333e+07, -9.9990000e+03,\n       -1.2800000e+02,  1.2800000e+02])\n>>> y = np.array([complex(np.inf, np.nan), np.nan, complex(np.nan, np.inf)])\narray([  1.79769313e+308,  -1.79769313e+308,   0.00000000e+000, # may vary\n     -1.28000000e+002,   1.28000000e+002])\n>>> np.nan_to_num(y)\narray([  1.79769313e+308 +0.00000000e+000j, # may vary\n         0.00000000e+000 +0.00000000e+000j,\n         0.00000000e+000 +1.79769313e+308j])\n>>> np.nan_to_num(y, nan=111111, posinf=222222)\narray([222222.+111111.j, 111111.     +0.j, 111111.+222222.j])", "Library": "NumPy"}
{"API_Name": "np.nanargmax", "Docstring": "Return the indices of the maximum values in the specified axis ignoring\nNaNs. For all-NaN slices ``ValueError`` is raised. Warning: the\nresults cannot be trusted if a slice contains only NaNs and -Infs.\n\n\nParameters\n----------\na : array_like\n    Input data.\naxis : int, optional\n    Axis along which to operate.  By default flattened input is used.\nout : array, optional\n    If provided, the result will be inserted into this array. It should\n    be of the appropriate shape and dtype.\n\n    .. versionadded:: 1.22.0\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the array.\n\n    .. versionadded:: 1.22.0\n\nReturns\n-------\nindex_array : ndarray\n    An array of indices or a single index value.\n\nSee Also\n--------\nargmax, nanargmin\n\nExamples\n--------\n>>> a = np.array([[np.nan, 4], [2, 3]])\n>>> np.argmax(a)\n0\n>>> np.nanargmax(a)\n1\n>>> np.nanargmax(a, axis=0)\narray([1, 0])\n>>> np.nanargmax(a, axis=1)\narray([1, 1])", "Library": "NumPy"}
{"API_Name": "np.nanpercentile", "Docstring": "Compute the qth percentile of the data along the specified axis,\nwhile ignoring nan values.\n\nReturns the qth percentile(s) of the array elements.\n\n.. versionadded:: 1.9.0\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array, containing\n    nan values to be ignored.\nq : array_like of float\n    Percentile or sequence of percentiles to compute, which must be\n    between 0 and 100 inclusive.\naxis : {int, tuple of int, None}, optional\n    Axis or axes along which the percentiles are computed. The default\n    is to compute the percentile(s) along a flattened version of the\n    array.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape and buffer length as the expected output, but the\n    type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n    If True, then allow the input array `a` to be modified by\n    intermediate calculations, to save memory. In this case, the\n    contents of the input `a` after this function completes is\n    undefined.\nmethod : str, optional\n    This parameter specifies the method to use for estimating the\n    percentile.  There are many different methods, some unique to NumPy.\n    See the notes for explanation.  The options sorted by their R type\n    as summarized in the H&F paper [1]_ are:\n\n    1. 'inverted_cdf'\n    2. 'averaged_inverted_cdf'\n    3. 'closest_observation'\n    4. 'interpolated_inverted_cdf'\n    5. 'hazen'\n    6. 'weibull'\n    7. 'linear'  (default)\n    8. 'median_unbiased'\n    9. 'normal_unbiased'\n\n    The first three methods are discontinuous.  NumPy further defines the\n    following discontinuous variations of the default 'linear' (7.) option:\n\n    * 'lower'\n    * 'higher',\n    * 'midpoint'\n    * 'nearest'\n\n    .. versionchanged:: 1.22.0\n        This argument was previously called \"interpolation\" and only\n        offered the \"linear\" default and last four options.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left in\n    the result as dimensions with size one. With this option, the\n    result will broadcast correctly against the original array `a`.\n\n    If this is anything but the default value it will be passed\n    through (in the special case of an empty array) to the\n    `mean` function of the underlying array.  If the array is\n    a sub-class and `mean` does not have the kwarg `keepdims` this\n    will raise a RuntimeError.\n\ninterpolation : str, optional\n    Deprecated name for the method keyword argument.\n\n    .. deprecated:: 1.22.0\n\nReturns\n-------\npercentile : scalar or ndarray\n    If `q` is a single percentile and `axis=None`, then the result\n    is a scalar. If multiple percentiles are given, first axis of\n    the result corresponds to the percentiles. The other axes are\n    the axes that remain after the reduction of `a`. If the input\n    contains integers or floats smaller than ``float64``, the output\n    data-type is ``float64``. Otherwise, the output data-type is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nnanmean\nnanmedian : equivalent to ``nanpercentile(..., 50)``\npercentile, median, mean\nnanquantile : equivalent to nanpercentile, except q in range [0, 1].\n\nNotes\n-----\nFor more information please see `numpy.percentile`\n\nExamples\n--------\n>>> a = np.array([[10., 7., 4.], [3., 2., 1.]])\n>>> a[0][1] = np.nan\n>>> a\narray([[10.,  nan,   4.],\n      [ 3.,   2.,   1.]])\n>>> np.percentile(a, 50)\nnan\n>>> np.nanpercentile(a, 50)\n3.0\n>>> np.nanpercentile(a, 50, axis=0)\narray([6.5, 2. , 2.5])\n>>> np.nanpercentile(a, 50, axis=1, keepdims=True)\narray([[7.],\n       [2.]])\n>>> m = np.nanpercentile(a, 50, axis=0)\n>>> out = np.zeros_like(m)\n>>> np.nanpercentile(a, 50, axis=0, out=out)\narray([6.5, 2. , 2.5])\n>>> m\narray([6.5,  2. ,  2.5])\n\n>>> b = a.copy()\n>>> np.nanpercentile(b, 50, axis=1, overwrite_input=True)\narray([7., 2.])\n>>> assert not np.all(a==b)\n\nReferences\n----------\n.. [1] R. J. Hyndman and Y. Fan,\n   \"Sample quantiles in statistical packages,\"\n   The American Statistician, 50(4), pp. 361-365, 1996", "Library": "NumPy"}
{"API_Name": "np.ndarray.astype", "Docstring": "a.astype(dtype, order='K', casting='unsafe', subok=True, copy=True)\n\nCopy of the array, cast to a specified type.\n\nParameters\n----------\ndtype : str or dtype\n    Typecode or data-type to which the array is cast.\norder : {'C', 'F', 'A', 'K'}, optional\n    Controls the memory layout order of the result.\n    'C' means C order, 'F' means Fortran order, 'A'\n    means 'F' order if all the arrays are Fortran contiguous,\n    'C' order otherwise, and 'K' means as close to the\n    order the array elements appear in memory as possible.\n    Default is 'K'.\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n    Controls what kind of data casting may occur. Defaults to 'unsafe'\n    for backwards compatibility.\n\n      * 'no' means the data types should not be cast at all.\n      * 'equiv' means only byte-order changes are allowed.\n      * 'safe' means only casts which can preserve values are allowed.\n      * 'same_kind' means only safe casts or casts within a kind,\n        like float64 to float32, are allowed.\n      * 'unsafe' means any data conversions may be done.\nsubok : bool, optional\n    If True, then sub-classes will be passed-through (default), otherwise\n    the returned array will be forced to be a base-class array.\ncopy : bool, optional\n    By default, astype always returns a newly allocated array. If this\n    is set to false, and the `dtype`, `order`, and `subok`\n    requirements are satisfied, the input array is returned instead\n    of a copy.\n\nReturns\n-------\narr_t : ndarray\n    Unless `copy` is False and the other conditions for returning the input\n    array are satisfied (see description for `copy` input parameter), `arr_t`\n    is a new array of the same shape as the input array, with dtype, order\n    given by `dtype`, `order`.\n\nNotes\n-----\n.. versionchanged:: 1.17.0\n   Casting between a simple data type and a structured one is possible only\n   for \"unsafe\" casting.  Casting to multiple fields is allowed, but\n   casting from multiple fields is not.\n\n.. versionchanged:: 1.9.0\n   Casting from numeric to string types in 'safe' casting mode requires\n   that the string dtype length is long enough to store the max\n   integer/float value converted.\n\nRaises\n------\nComplexWarning\n    When casting from complex to float or int. To avoid this,\n    one should use ``a.real.astype(t)``.\n\nExamples\n--------\n>>> x = np.array([1, 2, 2.5])\n>>> x\narray([1. ,  2. ,  2.5])\n\n>>> x.astype(int)\narray([1, 2, 2])", "Library": "NumPy"}
{"API_Name": "np.ndarray.copy", "Docstring": "a.copy(order='C')\n\nReturn a copy of the array.\n\nParameters\n----------\norder : {'C', 'F', 'A', 'K'}, optional\n    Controls the memory layout of the copy. 'C' means C-order,\n    'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n    'C' otherwise. 'K' means match the layout of `a` as closely\n    as possible. (Note that this function and :func:`numpy.copy` are very\n    similar but have different default values for their order=\n    arguments, and this function always passes sub-classes through.)\n\nSee also\n--------\nnumpy.copy : Similar function with different default behavior\nnumpy.copyto\n\nNotes\n-----\nThis function is the preferred method for creating an array copy.  The\nfunction :func:`numpy.copy` is similar, but it defaults to using order 'K',\nand will not pass sub-classes through by default.\n\nExamples\n--------\n>>> x = np.array([[1,2,3],[4,5,6]], order='F')\n\n>>> y = x.copy()\n\n>>> x.fill(0)\n\n>>> x\narray([[0, 0, 0],\n       [0, 0, 0]])\n\n>>> y\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n>>> y.flags['C_CONTIGUOUS']\nTrue", "Library": "NumPy"}
{"API_Name": "np.ndarray.flat", "Docstring": "A 1-D iterator over the array.\n\nThis is a `numpy.flatiter` instance, which acts similarly to, but is not\na subclass of, Python's built-in iterator object.\n\nSee Also\n--------\nflatten : Return a copy of the array collapsed into one dimension.\n\nflatiter\n\nExamples\n--------\n>>> x = np.arange(1, 7).reshape(2, 3)\n>>> x\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> x.flat[3]\n4\n>>> x.T\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n>>> x.T.flat[3]\n5\n>>> type(x.flat)\n<class 'numpy.flatiter'>\n\nAn assignment example:\n\n>>> x.flat = 3; x\narray([[3, 3, 3],\n       [3, 3, 3]])\n>>> x.flat[[1,4]] = 1; x\narray([[3, 1, 3],\n       [3, 1, 3]])", "Library": "NumPy"}
{"API_Name": "np.ndarray.nonzero", "Docstring": "a.nonzero()\n\nReturn the indices of the elements that are non-zero.\n\nRefer to `numpy.nonzero` for full documentation.\n\nSee Also\n--------\nnumpy.nonzero : equivalent function", "Library": "NumPy"}
{"API_Name": "np.ndarray.reshape", "Docstring": "a.reshape(shape, order='C')\n\nReturns an array containing the same data with a new shape.\n\nRefer to `numpy.reshape` for full documentation.\n\nSee Also\n--------\nnumpy.reshape : equivalent function\n\nNotes\n-----\nUnlike the free function `numpy.reshape`, this method on `ndarray` allows\nthe elements of the shape parameter to be passed in as separate arguments.\nFor example, ``a.reshape(10, 11)`` is equivalent to\n``a.reshape((10, 11))``.", "Library": "NumPy"}
{"API_Name": "np.ones", "Docstring": "Return a new array of given shape and type, filled with ones.\n\nParameters\n----------\nshape : int or sequence of ints\n    Shape of the new array, e.g., ``(2, 3)`` or ``2``.\ndtype : data-type, optional\n    The desired data-type for the array, e.g., `numpy.int8`.  Default is\n    `numpy.float64`.\norder : {'C', 'F'}, optional, default: C\n    Whether to store multi-dimensional data in row-major\n    (C-style) or column-major (Fortran-style) order in\n    memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Array of ones with the given shape, dtype, and order.\n\nSee Also\n--------\nones_like : Return an array of ones with shape and type of input.\nempty : Return a new uninitialized array.\nzeros : Return a new array setting values to zero.\nfull : Return a new array of given shape filled with value.\n\n\nExamples\n--------\n>>> np.ones(5)\narray([1., 1., 1., 1., 1.])\n\n>>> np.ones((5,), dtype=int)\narray([1, 1, 1, 1, 1])\n\n>>> np.ones((2, 1))\narray([[1.],\n       [1.]])\n\n>>> s = (2,2)\n>>> np.ones(s)\narray([[1.,  1.],\n       [1.,  1.]])", "Library": "NumPy"}
{"API_Name": "np.pad", "Docstring": "Pad an array.\n\nParameters\n----------\narray : array_like of rank N\n    The array to pad.\npad_width : {sequence, array_like, int}\n    Number of values padded to the edges of each axis.\n    ``((before_1, after_1), ... (before_N, after_N))`` unique pad widths\n    for each axis.\n    ``(before, after)`` or ``((before, after),)`` yields same before\n    and after pad for each axis.\n    ``(pad,)`` or ``int`` is a shortcut for before = after = pad width\n    for all axes.\nmode : str or function, optional\n    One of the following string values or a user supplied function.\n\n    'constant' (default)\n        Pads with a constant value.\n    'edge'\n        Pads with the edge values of array.\n    'linear_ramp'\n        Pads with the linear ramp between end_value and the\n        array edge value.\n    'maximum'\n        Pads with the maximum value of all or part of the\n        vector along each axis.\n    'mean'\n        Pads with the mean value of all or part of the\n        vector along each axis.\n    'median'\n        Pads with the median value of all or part of the\n        vector along each axis.\n    'minimum'\n        Pads with the minimum value of all or part of the\n        vector along each axis.\n    'reflect'\n        Pads with the reflection of the vector mirrored on\n        the first and last values of the vector along each\n        axis.\n    'symmetric'\n        Pads with the reflection of the vector mirrored\n        along the edge of the array.\n    'wrap'\n        Pads with the wrap of the vector along the axis.\n        The first values are used to pad the end and the\n        end values are used to pad the beginning.\n    'empty'\n        Pads with undefined values.\n\n        .. versionadded:: 1.17\n\n    <function>\n        Padding function, see Notes.\nstat_length : sequence or int, optional\n    Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n    values at edge of each axis used to calculate the statistic value.\n\n    ``((before_1, after_1), ... (before_N, after_N))`` unique statistic\n    lengths for each axis.\n\n    ``(before, after)`` or ``((before, after),)`` yields same before\n    and after statistic lengths for each axis.\n\n    ``(stat_length,)`` or ``int`` is a shortcut for\n    ``before = after = statistic`` length for all axes.\n\n    Default is ``None``, to use the entire axis.\nconstant_values : sequence or scalar, optional\n    Used in 'constant'.  The values to set the padded values for each\n    axis.\n\n    ``((before_1, after_1), ... (before_N, after_N))`` unique pad constants\n    for each axis.\n\n    ``(before, after)`` or ``((before, after),)`` yields same before\n    and after constants for each axis.\n\n    ``(constant,)`` or ``constant`` is a shortcut for\n    ``before = after = constant`` for all axes.\n\n    Default is 0.\nend_values : sequence or scalar, optional\n    Used in 'linear_ramp'.  The values used for the ending value of the\n    linear_ramp and that will form the edge of the padded array.\n\n    ``((before_1, after_1), ... (before_N, after_N))`` unique end values\n    for each axis.\n\n    ``(before, after)`` or ``((before, after),)`` yields same before\n    and after end values for each axis.\n\n    ``(constant,)`` or ``constant`` is a shortcut for\n    ``before = after = constant`` for all axes.\n\n    Default is 0.\nreflect_type : {'even', 'odd'}, optional\n    Used in 'reflect', and 'symmetric'.  The 'even' style is the\n    default with an unaltered reflection around the edge value.  For\n    the 'odd' style, the extended part of the array is created by\n    subtracting the reflected values from two times the edge value.\n\nReturns\n-------\npad : ndarray\n    Padded array of rank equal to `array` with shape increased\n    according to `pad_width`.\n\nNotes\n-----\n.. versionadded:: 1.7.0\n\nFor an array with rank greater than 1, some of the padding of later\naxes is calculated from padding of previous axes.  This is easiest to\nthink about with a rank 2 array where the corners of the padded array\nare calculated by using padded values from the first axis.\n\nThe padding function, if used, should modify a rank 1 array in-place. It\nhas the following signature::\n\n    padding_func(vector, iaxis_pad_width, iaxis, kwargs)\n\nwhere\n\n    vector : ndarray\n        A rank 1 array already padded with zeros.  Padded values are\n        vector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].\n    iaxis_pad_width : tuple\n        A 2-tuple of ints, iaxis_pad_width[0] represents the number of\n        values padded at the beginning of vector where\n        iaxis_pad_width[1] represents the number of values padded at\n        the end of vector.\n    iaxis : int\n        The axis currently being calculated.\n    kwargs : dict\n        Any keyword arguments the function requires.\n\nExamples\n--------\n>>> a = [1, 2, 3, 4, 5]\n>>> np.pad(a, (2, 3), 'constant', constant_values=(4, 6))\narray([4, 4, 1, ..., 6, 6, 6])\n\n>>> np.pad(a, (2, 3), 'edge')\narray([1, 1, 1, ..., 5, 5, 5])\n\n>>> np.pad(a, (2, 3), 'linear_ramp', end_values=(5, -4))\narray([ 5,  3,  1,  2,  3,  4,  5,  2, -1, -4])\n\n>>> np.pad(a, (2,), 'maximum')\narray([5, 5, 1, 2, 3, 4, 5, 5, 5])\n\n>>> np.pad(a, (2,), 'mean')\narray([3, 3, 1, 2, 3, 4, 5, 3, 3])\n\n>>> np.pad(a, (2,), 'median')\narray([3, 3, 1, 2, 3, 4, 5, 3, 3])\n\n>>> a = [[1, 2], [3, 4]]\n>>> np.pad(a, ((3, 2), (2, 3)), 'minimum')\narray([[1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1],\n       [3, 3, 3, 4, 3, 3, 3],\n       [1, 1, 1, 2, 1, 1, 1],\n       [1, 1, 1, 2, 1, 1, 1]])\n\n>>> a = [1, 2, 3, 4, 5]\n>>> np.pad(a, (2, 3), 'reflect')\narray([3, 2, 1, 2, 3, 4, 5, 4, 3, 2])\n\n>>> np.pad(a, (2, 3), 'reflect', reflect_type='odd')\narray([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n\n>>> np.pad(a, (2, 3), 'symmetric')\narray([2, 1, 1, 2, 3, 4, 5, 5, 4, 3])\n\n>>> np.pad(a, (2, 3), 'symmetric', reflect_type='odd')\narray([0, 1, 1, 2, 3, 4, 5, 5, 6, 7])\n\n>>> np.pad(a, (2, 3), 'wrap')\narray([4, 5, 1, 2, 3, 4, 5, 1, 2, 3])\n\n>>> def pad_with(vector, pad_width, iaxis, kwargs):\n...     pad_value = kwargs.get('padder', 10)\n...     vector[:pad_width[0]] = pad_value\n...     vector[-pad_width[1]:] = pad_value\n>>> a = np.arange(6)\n>>> a = a.reshape((2, 3))\n>>> np.pad(a, 2, pad_with)\narray([[10, 10, 10, 10, 10, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10],\n       [10, 10,  0,  1,  2, 10, 10],\n       [10, 10,  3,  4,  5, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10],\n       [10, 10, 10, 10, 10, 10, 10]])\n>>> np.pad(a, 2, pad_with, padder=100)\narray([[100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100],\n       [100, 100,   0,   1,   2, 100, 100],\n       [100, 100,   3,   4,   5, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100],\n       [100, 100, 100, 100, 100, 100, 100]])", "Library": "NumPy"}
{"API_Name": "np.percentile", "Docstring": "Compute the q-th percentile of the data along the specified axis.\n\nReturns the q-th percentile(s) of the array elements.\n\nParameters\n----------\na : array_like\n    Input array or object that can be converted to an array.\nq : array_like of float\n    Percentile or sequence of percentiles to compute, which must be between\n    0 and 100 inclusive.\naxis : {int, tuple of int, None}, optional\n    Axis or axes along which the percentiles are computed. The\n    default is to compute the percentile(s) along a flattened\n    version of the array.\n\n    .. versionchanged:: 1.9.0\n        A tuple of axes is supported\nout : ndarray, optional\n    Alternative output array in which to place the result. It must\n    have the same shape and buffer length as the expected output,\n    but the type (of the output) will be cast if necessary.\noverwrite_input : bool, optional\n    If True, then allow the input array `a` to be modified by intermediate\n    calculations, to save memory. In this case, the contents of the input\n    `a` after this function completes is undefined.\nmethod : str, optional\n    This parameter specifies the method to use for estimating the\n    percentile.  There are many different methods, some unique to NumPy.\n    See the notes for explanation.  The options sorted by their R type\n    as summarized in the H&F paper [1]_ are:\n\n    1. 'inverted_cdf'\n    2. 'averaged_inverted_cdf'\n    3. 'closest_observation'\n    4. 'interpolated_inverted_cdf'\n    5. 'hazen'\n    6. 'weibull'\n    7. 'linear'  (default)\n    8. 'median_unbiased'\n    9. 'normal_unbiased'\n\n    The first three methods are discontinuous.  NumPy further defines the\n    following discontinuous variations of the default 'linear' (7.) option:\n\n    * 'lower'\n    * 'higher',\n    * 'midpoint'\n    * 'nearest'\n\n    .. versionchanged:: 1.22.0\n        This argument was previously called \"interpolation\" and only\n        offered the \"linear\" default and last four options.\n\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left in\n    the result as dimensions with size one. With this option, the\n    result will broadcast correctly against the original array `a`.\n\n    .. versionadded:: 1.9.0\n\ninterpolation : str, optional\n    Deprecated name for the method keyword argument.\n\n    .. deprecated:: 1.22.0\n\nReturns\n-------\npercentile : scalar or ndarray\n    If `q` is a single percentile and `axis=None`, then the result\n    is a scalar. If multiple percentiles are given, first axis of\n    the result corresponds to the percentiles. The other axes are\n    the axes that remain after the reduction of `a`. If the input\n    contains integers or floats smaller than ``float64``, the output\n    data-type is ``float64``. Otherwise, the output data-type is the\n    same as that of the input. If `out` is specified, that array is\n    returned instead.\n\nSee Also\n--------\nmean\nmedian : equivalent to ``percentile(..., 50)``\nnanpercentile\nquantile : equivalent to percentile, except q in the range [0, 1].\n\nNotes\n-----\nGiven a vector ``V`` of length ``n``, the q-th percentile of ``V`` is\nthe value ``q/100`` of the way from the minimum to the maximum in a\nsorted copy of ``V``. The values and distances of the two nearest\nneighbors as well as the `method` parameter will determine the\npercentile if the normalized ranking does not match the location of\n``q`` exactly. This function is the same as the median if ``q=50``, the\nsame as the minimum if ``q=0`` and the same as the maximum if\n``q=100``.\n\nThe optional `method` parameter specifies the method to use when the\ndesired percentile lies between two indexes ``i`` and ``j = i + 1``.\nIn that case, we first determine ``i + g``, a virtual index that lies\nbetween ``i`` and ``j``, where  ``i`` is the floor and ``g`` is the\nfractional part of the index. The final result is, then, an interpolation\nof ``a[i]`` and ``a[j]`` based on ``g``. During the computation of ``g``,\n``i`` and ``j`` are modified using correction constants ``alpha`` and\n``beta`` whose choices depend on the ``method`` used. Finally, note that\nsince Python uses 0-based indexing, the code subtracts another 1 from the\nindex internally.\n\nThe following formula determines the virtual index ``i + g``, the location \nof the percentile in the sorted sample:\n\n.. math::\n    i + g = (q / 100) * ( n - alpha - beta + 1 ) + alpha\n\nThe different methods then work as follows\n\ninverted_cdf:\n    method 1 of H&F [1]_.\n    This method gives discontinuous results:\n\n    * if g > 0 ; then take j\n    * if g = 0 ; then take i\n\naveraged_inverted_cdf:\n    method 2 of H&F [1]_.\n    This method give discontinuous results:\n\n    * if g > 0 ; then take j\n    * if g = 0 ; then average between bounds\n\nclosest_observation:\n    method 3 of H&F [1]_.\n    This method give discontinuous results:\n\n    * if g > 0 ; then take j\n    * if g = 0 and index is odd ; then take j\n    * if g = 0 and index is even ; then take i\n\ninterpolated_inverted_cdf:\n    method 4 of H&F [1]_.\n    This method give continuous results using:\n\n    * alpha = 0\n    * beta = 1\n\nhazen:\n    method 5 of H&F [1]_.\n    This method give continuous results using:\n\n    * alpha = 1/2\n    * beta = 1/2\n\nweibull:\n    method 6 of H&F [1]_.\n    This method give continuous results using:\n\n    * alpha = 0\n    * beta = 0\n\nlinear:\n    method 7 of H&F [1]_.\n    This method give continuous results using:\n\n    * alpha = 1\n    * beta = 1\n\nmedian_unbiased:\n    method 8 of H&F [1]_.\n    This method is probably the best method if the sample\n    distribution function is unknown (see reference).\n    This method give continuous results using:\n\n    * alpha = 1/3\n    * beta = 1/3\n\nnormal_unbiased:\n    method 9 of H&F [1]_.\n    This method is probably the best method if the sample\n    distribution function is known to be normal.\n    This method give continuous results using:\n\n    * alpha = 3/8\n    * beta = 3/8\n\nlower:\n    NumPy method kept for backwards compatibility.\n    Takes ``i`` as the interpolation point.\n\nhigher:\n    NumPy method kept for backwards compatibility.\n    Takes ``j`` as the interpolation point.\n\nnearest:\n    NumPy method kept for backwards compatibility.\n    Takes ``i`` or ``j``, whichever is nearest.\n\nmidpoint:\n    NumPy method kept for backwards compatibility.\n    Uses ``(i + j) / 2``.\n\nExamples\n--------\n>>> a = np.array([[10, 7, 4], [3, 2, 1]])\n>>> a\narray([[10,  7,  4],\n       [ 3,  2,  1]])\n>>> np.percentile(a, 50)\n3.5\n>>> np.percentile(a, 50, axis=0)\narray([6.5, 4.5, 2.5])\n>>> np.percentile(a, 50, axis=1)\narray([7.,  2.])\n>>> np.percentile(a, 50, axis=1, keepdims=True)\narray([[7.],\n       [2.]])\n\n>>> m = np.percentile(a, 50, axis=0)\n>>> out = np.zeros_like(m)\n>>> np.percentile(a, 50, axis=0, out=out)\narray([6.5, 4.5, 2.5])\n>>> m\narray([6.5, 4.5, 2.5])\n\n>>> b = a.copy()\n>>> np.percentile(b, 50, axis=1, overwrite_input=True)\narray([7.,  2.])\n>>> assert not np.all(a == b)\n\nThe different methods can be visualized graphically:\n\n.. plot::\n\n    import matplotlib.pyplot as plt\n\n    a = np.arange(4)\n    p = np.linspace(0, 100, 6001)\n    ax = plt.gca()\n    lines = [\n        ('linear', '-', 'C0'),\n        ('inverted_cdf', ':', 'C1'),\n        # Almost the same as `inverted_cdf`:\n        ('averaged_inverted_cdf', '-.', 'C1'),\n        ('closest_observation', ':', 'C2'),\n        ('interpolated_inverted_cdf', '--', 'C1'),\n        ('hazen', '--', 'C3'),\n        ('weibull', '-.', 'C4'),\n        ('median_unbiased', '--', 'C5'),\n        ('normal_unbiased', '-.', 'C6'),\n        ]\n    for method, style, color in lines:\n        ax.plot(\n            p, np.percentile(a, p, method=method),\n            label=method, linestyle=style, color=color)\n    ax.set(\n        title='Percentiles for different methods and data: ' + str(a),\n        xlabel='Percentile',\n        ylabel='Estimated percentile value',\n        yticks=a)\n    ax.legend()\n    plt.show()\n\nReferences\n----------\n.. [1] R. J. Hyndman and Y. Fan,\n   \"Sample quantiles in statistical packages,\"\n   The American Statistician, 50(4), pp. 361-365, 1996", "Library": "NumPy"}
{"API_Name": "np.polyfit", "Docstring": "Least squares polynomial fit.\n\n.. note::\n   This forms part of the old polynomial API. Since version 1.4, the\n   new polynomial API defined in `numpy.polynomial` is preferred.\n   A summary of the differences can be found in the\n   :doc:`transition guide </reference/routines.polynomials>`.\n\nFit a polynomial ``p(x) = p[0] * x**deg + ... + p[deg]`` of degree `deg`\nto points `(x, y)`. Returns a vector of coefficients `p` that minimises\nthe squared error in the order `deg`, `deg-1`, ... `0`.\n\nThe `Polynomial.fit <numpy.polynomial.polynomial.Polynomial.fit>` class\nmethod is recommended for new code as it is more stable numerically. See\nthe documentation of the method for more information.\n\nParameters\n----------\nx : array_like, shape (M,)\n    x-coordinates of the M sample points ``(x[i], y[i])``.\ny : array_like, shape (M,) or (M, K)\n    y-coordinates of the sample points. Several data sets of sample\n    points sharing the same x-coordinates can be fitted at once by\n    passing in a 2D-array that contains one dataset per column.\ndeg : int\n    Degree of the fitting polynomial\nrcond : float, optional\n    Relative condition number of the fit. Singular values smaller than\n    this relative to the largest singular value will be ignored. The\n    default value is len(x)*eps, where eps is the relative precision of\n    the float type, about 2e-16 in most cases.\nfull : bool, optional\n    Switch determining nature of return value. When it is False (the\n    default) just the coefficients are returned, when True diagnostic\n    information from the singular value decomposition is also returned.\nw : array_like, shape (M,), optional\n    Weights. If not None, the weight ``w[i]`` applies to the unsquared\n    residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are\n    chosen so that the errors of the products ``w[i]*y[i]`` all have the\n    same variance.  When using inverse-variance weighting, use\n    ``w[i] = 1/sigma(y[i])``.  The default value is None.\ncov : bool or str, optional\n    If given and not `False`, return not just the estimate but also its\n    covariance matrix. By default, the covariance are scaled by\n    chi2/dof, where dof = M - (deg + 1), i.e., the weights are presumed\n    to be unreliable except in a relative sense and everything is scaled\n    such that the reduced chi2 is unity. This scaling is omitted if\n    ``cov='unscaled'``, as is relevant for the case that the weights are\n    w = 1/sigma, with sigma known to be a reliable estimate of the\n    uncertainty.\n\nReturns\n-------\np : ndarray, shape (deg + 1,) or (deg + 1, K)\n    Polynomial coefficients, highest power first.  If `y` was 2-D, the\n    coefficients for `k`-th data set are in ``p[:,k]``.\n\nresiduals, rank, singular_values, rcond\n    These values are only returned if ``full == True``\n\n    - residuals -- sum of squared residuals of the least squares fit\n    - rank -- the effective rank of the scaled Vandermonde\n       coefficient matrix\n    - singular_values -- singular values of the scaled Vandermonde\n       coefficient matrix\n    - rcond -- value of `rcond`.\n\n    For more details, see `numpy.linalg.lstsq`.\n\nV : ndarray, shape (M,M) or (M,M,K)\n    Present only if ``full == False`` and ``cov == True``.  The covariance\n    matrix of the polynomial coefficient estimates.  The diagonal of\n    this matrix are the variance estimates for each coefficient.  If y\n    is a 2-D array, then the covariance matrix for the `k`-th data set\n    are in ``V[:,:,k]``\n\n\nWarns\n-----\nRankWarning\n    The rank of the coefficient matrix in the least-squares fit is\n    deficient. The warning is only raised if ``full == False``.\n\n    The warnings can be turned off by\n\n    >>> import warnings\n    >>> warnings.simplefilter('ignore', np.RankWarning)\n\nSee Also\n--------\npolyval : Compute polynomial values.\nlinalg.lstsq : Computes a least-squares fit.\nscipy.interpolate.UnivariateSpline : Computes spline fits.\n\nNotes\n-----\nThe solution minimizes the squared error\n\n.. math::\n    E = \\sum_{j=0}^k |p(x_j) - y_j|^2\n\nin the equations::\n\n    x[0]**n * p[0] + ... + x[0] * p[n-1] + p[n] = y[0]\n    x[1]**n * p[0] + ... + x[1] * p[n-1] + p[n] = y[1]\n    ...\n    x[k]**n * p[0] + ... + x[k] * p[n-1] + p[n] = y[k]\n\nThe coefficient matrix of the coefficients `p` is a Vandermonde matrix.\n\n`polyfit` issues a `RankWarning` when the least-squares fit is badly\nconditioned. This implies that the best fit is not well-defined due\nto numerical error. The results may be improved by lowering the polynomial\ndegree or by replacing `x` by `x` - `x`.mean(). The `rcond` parameter\ncan also be set to a value smaller than its default, but the resulting\nfit may be spurious: including contributions from the small singular\nvalues can add numerical noise to the result.\n\nNote that fitting polynomial coefficients is inherently badly conditioned\nwhen the degree of the polynomial is large or the interval of sample points\nis badly centered. The quality of the fit should always be checked in these\ncases. When polynomial fits are not satisfactory, splines may be a good\nalternative.\n\nReferences\n----------\n.. [1] Wikipedia, \"Curve fitting\",\n       https://en.wikipedia.org/wiki/Curve_fitting\n.. [2] Wikipedia, \"Polynomial interpolation\",\n       https://en.wikipedia.org/wiki/Polynomial_interpolation\n\nExamples\n--------\n>>> import warnings\n>>> x = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0])\n>>> y = np.array([0.0, 0.8, 0.9, 0.1, -0.8, -1.0])\n>>> z = np.polyfit(x, y, 3)\n>>> z\narray([ 0.08703704, -0.81349206,  1.69312169, -0.03968254]) # may vary\n\nIt is convenient to use `poly1d` objects for dealing with polynomials:\n\n>>> p = np.poly1d(z)\n>>> p(0.5)\n0.6143849206349179 # may vary\n>>> p(3.5)\n-0.34732142857143039 # may vary\n>>> p(10)\n22.579365079365115 # may vary\n\nHigh-order polynomials may oscillate wildly:\n\n>>> with warnings.catch_warnings():\n...     warnings.simplefilter('ignore', np.RankWarning)\n...     p30 = np.poly1d(np.polyfit(x, y, 30))\n...\n>>> p30(4)\n-0.80000000000000204 # may vary\n>>> p30(5)\n-0.99999999999999445 # may vary\n>>> p30(4.5)\n-0.10547061179440398 # may vary\n\nIllustration:\n\n>>> import matplotlib.pyplot as plt\n>>> xp = np.linspace(-2, 6, 100)\n>>> _ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p30(xp), '--')\n>>> plt.ylim(-2,2)\n(-2, 2)\n>>> plt.show()", "Library": "NumPy"}
{"API_Name": "np.random.choice", "Docstring": "choice(a, size=None, replace=True, p=None)\n\nGenerates a random sample from a given 1-D array\n\n.. versionadded:: 1.7.0\n\n.. note::\n    New code should use the `~numpy.random.Generator.choice`\n    method of a `~numpy.random.Generator` instance instead;\n    please see the :ref:`random-quick-start`.\n\nParameters\n----------\na : 1-D array-like or int\n    If an ndarray, a random sample is generated from its elements.\n    If an int, the random sample is generated as if it were ``np.arange(a)``\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\nreplace : boolean, optional\n    Whether the sample is with or without replacement. Default is True,\n    meaning that a value of ``a`` can be selected multiple times.\np : 1-D array-like, optional\n    The probabilities associated with each entry in a.\n    If not given, the sample assumes a uniform distribution over all\n    entries in ``a``.\n\nReturns\n-------\nsamples : single item or ndarray\n    The generated random samples\n\nRaises\n------\nValueError\n    If a is an int and less than zero, if a or p are not 1-dimensional,\n    if a is an array-like of size 0, if p is not a vector of\n    probabilities, if a and p have different lengths, or if\n    replace=False and the sample size is greater than the population\n    size\n\nSee Also\n--------\nrandint, shuffle, permutation\nrandom.Generator.choice: which should be used in new code\n\nNotes\n-----\nSetting user-specified probabilities through ``p`` uses a more general but less\nefficient sampler than the default. The general sampler produces a different sample\nthan the optimized sampler even if each element of ``p`` is 1 / len(a).\n\nSampling random rows from a 2-D array is not possible with this function,\nbut is possible with `Generator.choice` through its ``axis`` keyword.\n\nExamples\n--------\nGenerate a uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3)\narray([0, 3, 4]) # random\n>>> #This is equivalent to np.random.randint(0,5,3)\n\nGenerate a non-uniform random sample from np.arange(5) of size 3:\n\n>>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\narray([3, 3, 0]) # random\n\nGenerate a uniform random sample from np.arange(5) of size 3 without\nreplacement:\n\n>>> np.random.choice(5, 3, replace=False)\narray([3,1,0]) # random\n>>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n\nGenerate a non-uniform random sample from np.arange(5) of size\n3 without replacement:\n\n>>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\narray([2, 3, 0]) # random\n\nAny of the above can be repeated with an arbitrary array-like\ninstead of just integers. For instance:\n\n>>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n>>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\narray(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n      dtype='<U11')", "Library": "NumPy"}
{"API_Name": "np.random.rand", "Docstring": "rand(d0, d1, ..., dn)\n\nRandom values in a given shape.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `random_sample`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\nCreate an array of the given shape and populate it with\nrandom samples from a uniform distribution\nover ``[0, 1)``.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nout : ndarray, shape ``(d0, d1, ..., dn)``\n    Random values.\n\nSee Also\n--------\nrandom\n\nExamples\n--------\n>>> np.random.rand(3,2)\narray([[ 0.14022471,  0.96360618],  #random\n       [ 0.37601032,  0.25528411],  #random\n       [ 0.49313049,  0.94909878]]) #random", "Library": "NumPy"}
{"API_Name": "np.random.randint", "Docstring": "randint(low, high=None, size=None, dtype=int)\n\nReturn random integers from `low` (inclusive) to `high` (exclusive).\n\nReturn random integers from the \"discrete uniform\" distribution of\nthe specified dtype in the \"half-open\" interval [`low`, `high`). If\n`high` is None (the default), then results are from [0, `low`).\n\n.. note::\n    New code should use the `~numpy.random.Generator.randint`\n    method of a `~numpy.random.Generator` instance instead;\n    please see the :ref:`random-quick-start`.\n\nParameters\n----------\nlow : int or array-like of ints\n    Lowest (signed) integers to be drawn from the distribution (unless\n    ``high=None``, in which case this parameter is one above the\n    *highest* such integer).\nhigh : int or array-like of ints, optional\n    If provided, one above the largest (signed) integer to be drawn\n    from the distribution (see above for behavior if ``high=None``).\n    If array-like, must contain integer values\nsize : int or tuple of ints, optional\n    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n    ``m * n * k`` samples are drawn.  Default is None, in which case a\n    single value is returned.\ndtype : dtype, optional\n    Desired dtype of the result. Byteorder must be native.\n    The default value is int.\n\n    .. versionadded:: 1.11.0\n\nReturns\n-------\nout : int or ndarray of ints\n    `size`-shaped array of random integers from the appropriate\n    distribution, or a single such random int if `size` not provided.\n\nSee Also\n--------\nrandom_integers : similar to `randint`, only for the closed\n    interval [`low`, `high`], and 1 is the lowest value if `high` is\n    omitted.\nrandom.Generator.integers: which should be used for new code.\n\nExamples\n--------\n>>> np.random.randint(2, size=10)\narray([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n>>> np.random.randint(1, size=10)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\nGenerate a 2 x 4 array of ints between 0 and 4, inclusive:\n\n>>> np.random.randint(5, size=(2, 4))\narray([[4, 0, 2, 1], # random\n       [3, 2, 2, 0]])\n\nGenerate a 1 x 3 array with 3 different upper bounds\n\n>>> np.random.randint(1, [3, 5, 10])\narray([2, 2, 9]) # random\n\nGenerate a 1 by 3 array with 3 different lower bounds\n\n>>> np.random.randint([1, 5, 7], 10)\narray([9, 8, 7]) # random\n\nGenerate a 2 by 4 array using broadcasting with dtype of uint8\n\n>>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\narray([[ 8,  6,  9,  7], # random\n       [ 1, 16,  9, 12]], dtype=uint8)", "Library": "NumPy"}
{"API_Name": "np.random.randn", "Docstring": "randn(d0, d1, ..., dn)\n\nReturn a sample (or samples) from the \"standard normal\" distribution.\n\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `standard_normal`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\n.. note::\n    New code should use the\n    `~numpy.random.Generator.standard_normal`\n    method of a `~numpy.random.Generator` instance instead;\n    please see the :ref:`random-quick-start`.\n\nIf positive int_like arguments are provided, `randn` generates an array\nof shape ``(d0, d1, ..., dn)``, filled\nwith random floats sampled from a univariate \"normal\" (Gaussian)\ndistribution of mean 0 and variance 1. A single float randomly sampled\nfrom the distribution is returned if no argument is provided.\n\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\n\nReturns\n-------\nZ : ndarray or float\n    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n    the standard normal distribution, or a single such float if\n    no parameters were supplied.\n\nSee Also\n--------\nstandard_normal : Similar, but takes a tuple as its argument.\nnormal : Also accepts mu and sigma arguments.\nrandom.Generator.standard_normal: which should be used for new code.\n\nNotes\n-----\nFor random samples from the normal distribution with mean ``mu`` and\nstandard deviation ``sigma``, use::\n\n    sigma * np.random.randn(...) + mu\n\nExamples\n--------\n>>> np.random.randn()\n2.1923875335537315  # random\n\nTwo-by-four array of samples from the normal distribution with\nmean 3 and standard deviation 2.5:\n\n>>> 3 + 2.5 * np.random.randn(2, 4)\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random", "Library": "NumPy"}
{"API_Name": "np.random.permutation", "Docstring": "permutation(x)\n\nRandomly permute a sequence, or return a permuted range.\n\nIf `x` is a multi-dimensional array, it is only shuffled along its\nfirst index.\n\n.. note::\n    New code should use the\n    `~numpy.random.Generator.permutation`\n    method of a `~numpy.random.Generator` instance instead;\n    please see the :ref:`random-quick-start`.\n\nParameters\n----------\nx : int or array_like\n    If `x` is an integer, randomly permute ``np.arange(x)``.\n    If `x` is an array, make a copy and shuffle the elements\n    randomly.\n\nReturns\n-------\nout : ndarray\n    Permuted sequence or array range.\n\nSee Also\n--------\nrandom.Generator.permutation: which should be used for new code.\n\nExamples\n--------\n>>> np.random.permutation(10)\narray([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random\n\n>>> np.random.permutation([1, 4, 9, 12, 15])\narray([15,  1,  9,  4, 12]) # random\n\n>>> arr = np.arange(9).reshape((3, 3))\n>>> np.random.permutation(arr)\narray([[6, 7, 8], # random\n       [0, 1, 2],\n       [3, 4, 5]])", "Library": "NumPy"}
{"API_Name": "np.random.seed", "Docstring": "seed(seed=None)\n\nReseed the singleton RandomState instance.\n\nNotes\n-----\nThis is a convenience, legacy function that exists to support\nolder code that uses the singleton RandomState. Best practice\nis to use a dedicated ``Generator`` instance rather than\nthe random variate generation methods exposed directly in\nthe random module.\n\nSee Also\n--------\nnumpy.random.Generator", "Library": "NumPy"}
{"API_Name": "np.ravel", "Docstring": "Return a contiguous flattened array.\n\nA 1-D array, containing the elements of the input, is returned.  A copy is\nmade only if needed.\n\nAs of NumPy 1.10, the returned array will have the same type as the input\narray. (for example, a masked array will be returned for a masked array\ninput)\n\nParameters\n----------\na : array_like\n    Input array.  The elements in `a` are read in the order specified by\n    `order`, and packed as a 1-D array.\norder : {'C','F', 'A', 'K'}, optional\n\n    The elements of `a` are read using this index order. 'C' means\n    to index the elements in row-major, C-style order,\n    with the last axis index changing fastest, back to the first\n    axis index changing slowest.  'F' means to index the elements\n    in column-major, Fortran-style order, with the\n    first index changing fastest, and the last index changing\n    slowest. Note that the 'C' and 'F' options take no account of\n    the memory layout of the underlying array, and only refer to\n    the order of axis indexing.  'A' means to read the elements in\n    Fortran-like index order if `a` is Fortran *contiguous* in\n    memory, C-like order otherwise.  'K' means to read the\n    elements in the order they occur in memory, except for\n    reversing the data when strides are negative.  By default, 'C'\n    index order is used.\n\nReturns\n-------\ny : array_like\n    y is an array of the same subtype as `a`, with shape ``(a.size,)``.\n    Note that matrices are special cased for backward compatibility, if `a`\n    is a matrix, then y is a 1-D ndarray.\n\nSee Also\n--------\nndarray.flat : 1-D iterator over an array.\nndarray.flatten : 1-D array copy of the elements of an array\n                  in row-major order.\nndarray.reshape : Change the shape of an array without changing its data.\n\nNotes\n-----\nIn row-major, C-style order, in two dimensions, the row index\nvaries the slowest, and the column index the quickest.  This can\nbe generalized to multiple dimensions, where row-major order\nimplies that the index along the first axis varies slowest, and\nthe index along the last quickest.  The opposite holds for\ncolumn-major, Fortran-style index ordering.\n\nWhen a view is desired in as many cases as possible, ``arr.reshape(-1)``\nmay be preferable.\n\nExamples\n--------\nIt is equivalent to ``reshape(-1, order=order)``.\n\n>>> x = np.array([[1, 2, 3], [4, 5, 6]])\n>>> np.ravel(x)\narray([1, 2, 3, 4, 5, 6])\n\n>>> x.reshape(-1)\narray([1, 2, 3, 4, 5, 6])\n\n>>> np.ravel(x, order='F')\narray([1, 4, 2, 5, 3, 6])\n\nWhen ``order`` is 'A', it will preserve the array's 'C' or 'F' ordering:\n\n>>> np.ravel(x.T)\narray([1, 4, 2, 5, 3, 6])\n>>> np.ravel(x.T, order='A')\narray([1, 2, 3, 4, 5, 6])\n\nWhen ``order`` is 'K', it will preserve orderings that are neither 'C'\nnor 'F', but won't reverse axes:\n\n>>> a = np.arange(3)[::-1]; a\narray([2, 1, 0])\n>>> a.ravel(order='C')\narray([2, 1, 0])\n>>> a.ravel(order='K')\narray([2, 1, 0])\n\n>>> a = np.arange(12).reshape(2,3,2).swapaxes(1,2); a\narray([[[ 0,  2,  4],\n        [ 1,  3,  5]],\n       [[ 6,  8, 10],\n        [ 7,  9, 11]]])\n>>> a.ravel(order='C')\narray([ 0,  2,  4,  1,  3,  5,  6,  8, 10,  7,  9, 11])\n>>> a.ravel(order='K')\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])", "Library": "NumPy"}
{"API_Name": "np.ravel_multi_index", "Docstring": "ravel_multi_index(multi_index, dims, mode='raise', order='C')\n\nConverts a tuple of index arrays into an array of flat\nindices, applying boundary modes to the multi-index.\n\nParameters\n----------\nmulti_index : tuple of array_like\n    A tuple of integer arrays, one array for each dimension.\ndims : tuple of ints\n    The shape of array into which the indices from ``multi_index`` apply.\nmode : {'raise', 'wrap', 'clip'}, optional\n    Specifies how out-of-bounds indices are handled.  Can specify\n    either one mode or a tuple of modes, one mode per index.\n\n    * 'raise' -- raise an error (default)\n    * 'wrap' -- wrap around\n    * 'clip' -- clip to the range\n\n    In 'clip' mode, a negative index which would normally\n    wrap will clip to 0 instead.\norder : {'C', 'F'}, optional\n    Determines whether the multi-index should be viewed as\n    indexing in row-major (C-style) or column-major\n    (Fortran-style) order.\n\nReturns\n-------\nraveled_indices : ndarray\n    An array of indices into the flattened version of an array\n    of dimensions ``dims``.\n\nSee Also\n--------\nunravel_index\n\nNotes\n-----\n.. versionadded:: 1.6.0\n\nExamples\n--------\n>>> arr = np.array([[3,6,6],[4,5,1]])\n>>> np.ravel_multi_index(arr, (7,6))\narray([22, 41, 37])\n>>> np.ravel_multi_index(arr, (7,6), order='F')\narray([31, 41, 13])\n>>> np.ravel_multi_index(arr, (4,6), mode='clip')\narray([22, 23, 19])\n>>> np.ravel_multi_index(arr, (4,4), mode=('clip','wrap'))\narray([12, 13, 13])\n\n>>> np.ravel_multi_index((3,1,4,1), (6,7,8,9))\n1621", "Library": "NumPy"}
{"API_Name": "np.real", "Docstring": "Return the real part of the complex argument.\n\nParameters\n----------\nval : array_like\n    Input array.\n\nReturns\n-------\nout : ndarray or scalar\n    The real component of the complex argument. If `val` is real, the type\n    of `val` is used for the output.  If `val` has complex elements, the\n    returned type is float.\n\nSee Also\n--------\nreal_if_close, imag, angle\n\nExamples\n--------\n>>> a = np.array([1+2j, 3+4j, 5+6j])\n>>> a.real\narray([1.,  3.,  5.])\n>>> a.real = 9\n>>> a\narray([9.+2.j,  9.+4.j,  9.+6.j])\n>>> a.real = np.array([9, 8, 7])\n>>> a\narray([9.+2.j,  8.+4.j,  7.+6.j])\n>>> np.real(1 + 1j)\n1.0", "Library": "NumPy"}
{"API_Name": "np.rec.fromarrays", "Docstring": "Create a record array from a (flat) list of arrays\n\nParameters\n----------\narrayList : list or tuple\n    List of array-like objects (such as lists, tuples,\n    and ndarrays).\ndtype : data-type, optional\n    valid dtype for all arrays\nshape : int or tuple of ints, optional\n    Shape of the resulting array. If not provided, inferred from\n    ``arrayList[0]``.\nformats, names, titles, aligned, byteorder :\n    If `dtype` is ``None``, these arguments are passed to\n    `numpy.format_parser` to construct a dtype. See that function for\n    detailed documentation.\n\nReturns\n-------\nnp.recarray\n    Record array consisting of given arrayList columns.\n\nExamples\n--------\n>>> x1=np.array([1,2,3,4])\n>>> x2=np.array(['a','dd','xyz','12'])\n>>> x3=np.array([1.1,2,3,4])\n>>> r = np.core.records.fromarrays([x1,x2,x3],names='a,b,c')\n>>> print(r[1])\n(2, 'dd', 2.0) # may vary\n>>> x1[1]=34\n>>> r.a\narray([1, 2, 3, 4])\n\n>>> x1 = np.array([1, 2, 3, 4])\n>>> x2 = np.array(['a', 'dd', 'xyz', '12'])\n>>> x3 = np.array([1.1, 2, 3,4])\n>>> r = np.core.records.fromarrays(\n...     [x1, x2, x3],\n...     dtype=np.dtype([('a', np.int32), ('b', 'S3'), ('c', np.float32)]))\n>>> r\nrec.array([(1, b'a', 1.1), (2, b'dd', 2. ), (3, b'xyz', 3. ),\n           (4, b'12', 4. )],\n          dtype=[('a', '<i4'), ('b', 'S3'), ('c', '<f4')])", "Library": "NumPy"}
{"API_Name": "np.repeat", "Docstring": "Repeat elements of an array.\n\nParameters\n----------\na : array_like\n    Input array.\nrepeats : int or array of ints\n    The number of repetitions for each element.  `repeats` is broadcasted\n    to fit the shape of the given axis.\naxis : int, optional\n    The axis along which to repeat values.  By default, use the\n    flattened input array, and return a flat output array.\n\nReturns\n-------\nrepeated_array : ndarray\n    Output array which has the same shape as `a`, except along\n    the given axis.\n\nSee Also\n--------\ntile : Tile an array.\nunique : Find the unique elements of an array.\n\nExamples\n--------\n>>> np.repeat(3, 4)\narray([3, 3, 3, 3])\n>>> x = np.array([[1,2],[3,4]])\n>>> np.repeat(x, 2)\narray([1, 1, 2, 2, 3, 3, 4, 4])\n>>> np.repeat(x, 3, axis=1)\narray([[1, 1, 1, 2, 2, 2],\n       [3, 3, 3, 4, 4, 4]])\n>>> np.repeat(x, [1, 2], axis=0)\narray([[1, 2],\n       [3, 4],\n       [3, 4]])", "Library": "NumPy"}
{"API_Name": "np.reshape", "Docstring": "Gives a new shape to an array without changing its data.\n\nParameters\n----------\na : array_like\n    Array to be reshaped.\nnewshape : int or tuple of ints\n    The new shape should be compatible with the original shape. If\n    an integer, then the result will be a 1-D array of that length.\n    One shape dimension can be -1. In this case, the value is\n    inferred from the length of the array and remaining dimensions.\norder : {'C', 'F', 'A'}, optional\n    Read the elements of `a` using this index order, and place the\n    elements into the reshaped array using this index order.  'C'\n    means to read / write the elements using C-like index order,\n    with the last axis index changing fastest, back to the first\n    axis index changing slowest. 'F' means to read / write the\n    elements using Fortran-like index order, with the first index\n    changing fastest, and the last index changing slowest. Note that\n    the 'C' and 'F' options take no account of the memory layout of\n    the underlying array, and only refer to the order of indexing.\n    'A' means to read / write the elements in Fortran-like index\n    order if `a` is Fortran *contiguous* in memory, C-like order\n    otherwise.\n\nReturns\n-------\nreshaped_array : ndarray\n    This will be a new view object if possible; otherwise, it will\n    be a copy.  Note there is no guarantee of the *memory layout* (C- or\n    Fortran- contiguous) of the returned array.\n\nSee Also\n--------\nndarray.reshape : Equivalent method.\n\nNotes\n-----\nIt is not always possible to change the shape of an array without\ncopying the data. If you want an error to be raised when the data is copied,\nyou should assign the new shape to the shape attribute of the array::\n\n >>> a = np.zeros((10, 2))\n\n # A transpose makes the array non-contiguous\n >>> b = a.T\n\n # Taking a view makes it possible to modify the shape without modifying\n # the initial object.\n >>> c = b.view()\n >>> c.shape = (20)\n Traceback (most recent call last):\n    ...\n AttributeError: Incompatible shape for in-place modification. Use\n `.reshape()` to make a copy with the desired shape.\n\nThe `order` keyword gives the index ordering both for *fetching* the values\nfrom `a`, and then *placing* the values into the output array.\nFor example, let's say you have an array:\n\n>>> a = np.arange(6).reshape((3, 2))\n>>> a\narray([[0, 1],\n       [2, 3],\n       [4, 5]])\n\nYou can think of reshaping as first raveling the array (using the given\nindex order), then inserting the elements from the raveled array into the\nnew array using the same kind of index ordering as was used for the\nraveling.\n\n>>> np.reshape(a, (2, 3)) # C-like index ordering\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.reshape(np.ravel(a), (2, 3)) # equivalent to C ravel then C reshape\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.reshape(a, (2, 3), order='F') # Fortran-like index ordering\narray([[0, 4, 3],\n       [2, 1, 5]])\n>>> np.reshape(np.ravel(a, order='F'), (2, 3), order='F')\narray([[0, 4, 3],\n       [2, 1, 5]])\n\nExamples\n--------\n>>> a = np.array([[1,2,3], [4,5,6]])\n>>> np.reshape(a, 6)\narray([1, 2, 3, 4, 5, 6])\n>>> np.reshape(a, 6, order='F')\narray([1, 4, 2, 5, 3, 6])\n\n>>> np.reshape(a, (3,-1))       # the unspecified value is inferred to be 2\narray([[1, 2],\n       [3, 4],\n       [5, 6]])", "Library": "NumPy"}
{"API_Name": "np.roll", "Docstring": "Roll array elements along a given axis.\n\nElements that roll beyond the last position are re-introduced at\nthe first.\n\nParameters\n----------\na : array_like\n    Input array.\nshift : int or tuple of ints\n    The number of places by which elements are shifted.  If a tuple,\n    then `axis` must be a tuple of the same size, and each of the\n    given axes is shifted by the corresponding number.  If an int\n    while `axis` is a tuple of ints, then the same value is used for\n    all given axes.\naxis : int or tuple of ints, optional\n    Axis or axes along which elements are shifted.  By default, the\n    array is flattened before shifting, after which the original\n    shape is restored.\n\nReturns\n-------\nres : ndarray\n    Output array, with the same shape as `a`.\n\nSee Also\n--------\nrollaxis : Roll the specified axis backwards, until it lies in a\n           given position.\n\nNotes\n-----\n.. versionadded:: 1.12.0\n\nSupports rolling over multiple dimensions simultaneously.\n\nExamples\n--------\n>>> x = np.arange(10)\n>>> np.roll(x, 2)\narray([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n>>> np.roll(x, -2)\narray([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])\n\n>>> x2 = np.reshape(x, (2, 5))\n>>> x2\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\n>>> np.roll(x2, 1)\narray([[9, 0, 1, 2, 3],\n       [4, 5, 6, 7, 8]])\n>>> np.roll(x2, -1)\narray([[1, 2, 3, 4, 5],\n       [6, 7, 8, 9, 0]])\n>>> np.roll(x2, 1, axis=0)\narray([[5, 6, 7, 8, 9],\n       [0, 1, 2, 3, 4]])\n>>> np.roll(x2, -1, axis=0)\narray([[5, 6, 7, 8, 9],\n       [0, 1, 2, 3, 4]])\n>>> np.roll(x2, 1, axis=1)\narray([[4, 0, 1, 2, 3],\n       [9, 5, 6, 7, 8]])\n>>> np.roll(x2, -1, axis=1)\narray([[1, 2, 3, 4, 0],\n       [6, 7, 8, 9, 5]])\n>>> np.roll(x2, (1, 1), axis=(1, 0))\narray([[9, 5, 6, 7, 8],\n       [4, 0, 1, 2, 3]])\n>>> np.roll(x2, (2, 1), axis=(1, 0))\narray([[8, 9, 5, 6, 7],\n       [3, 4, 0, 1, 2]])", "Library": "NumPy"}
{"API_Name": "np.round", "Docstring": "Round an array to the given number of decimals.\n\nSee Also\n--------\naround : equivalent function; see for details.", "Library": "NumPy"}
{"API_Name": "np.sin", "Docstring": "sin(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nTrigonometric sine, element-wise.\n\nParameters\n----------\nx : array_like\n    Angle, in radians (:math:`2 \\pi` rad equals 360 degrees).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : array_like\n    The sine of each element of x.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\narcsin, sinh, cos\n\nNotes\n-----\nThe sine is one of the fundamental functions of trigonometry (the\nmathematical study of triangles).  Consider a circle of radius 1\ncentered on the origin.  A ray comes in from the :math:`+x` axis, makes\nan angle at the origin (measured counter-clockwise from that axis), and\ndeparts from the origin.  The :math:`y` coordinate of the outgoing\nray's intersection with the unit circle is the sine of that angle.  It\nranges from -1 for :math:`x=3\\pi / 2` to +1 for :math:`\\pi / 2.`  The\nfunction has zeroes where the angle is a multiple of :math:`\\pi`.\nSines of angles between :math:`\\pi` and :math:`2\\pi` are negative.\nThe numerous properties of the sine and related functions are included\nin any standard trigonometry text.\n\nExamples\n--------\nPrint sine of one angle:\n\n>>> np.sin(np.pi/2.)\n1.0\n\nPrint sines of an array of angles given in degrees:\n\n>>> np.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180. )\narray([ 0.        ,  0.5       ,  0.70710678,  0.8660254 ,  1.        ])\n\nPlot the sine function:\n\n>>> import matplotlib.pylab as plt\n>>> x = np.linspace(-np.pi, np.pi, 201)\n>>> plt.plot(x, np.sin(x))\n>>> plt.xlabel('Angle [rad]')\n>>> plt.ylabel('sin(x)')\n>>> plt.axis('tight')\n>>> plt.show()", "Library": "NumPy"}
{"API_Name": "np.sort", "Docstring": "Return a sorted copy of an array.\n\nParameters\n----------\na : array_like\n    Array to be sorted.\naxis : int or None, optional\n    Axis along which to sort. If None, the array is flattened before\n    sorting. The default is -1, which sorts along the last axis.\nkind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, optional\n    Sorting algorithm. The default is 'quicksort'. Note that both 'stable'\n    and 'mergesort' use timsort or radix sort under the covers and, in general,\n    the actual implementation will vary with data type. The 'mergesort' option\n    is retained for backwards compatibility.\n\n    .. versionchanged:: 1.15.0.\n       The 'stable' option was added.\n\norder : str or list of str, optional\n    When `a` is an array with fields defined, this argument specifies\n    which fields to compare first, second, etc.  A single field can\n    be specified as a string, and not all fields need be specified,\n    but unspecified fields will still be used, in the order in which\n    they come up in the dtype, to break ties.\n\nReturns\n-------\nsorted_array : ndarray\n    Array of the same type and shape as `a`.\n\nSee Also\n--------\nndarray.sort : Method to sort an array in-place.\nargsort : Indirect sort.\nlexsort : Indirect stable sort on multiple keys.\nsearchsorted : Find elements in a sorted array.\npartition : Partial sort.\n\nNotes\n-----\nThe various sorting algorithms are characterized by their average speed,\nworst case performance, work space size, and whether they are stable. A\nstable sort keeps items with the same key in the same relative\norder. The four algorithms implemented in NumPy have the following\nproperties:\n\n=========== ======= ============= ============ ========\n   kind      speed   worst case    work space   stable\n=========== ======= ============= ============ ========\n'quicksort'    1     O(n^2)            0          no\n'heapsort'     3     O(n*log(n))       0          no\n'mergesort'    2     O(n*log(n))      ~n/2        yes\n'timsort'      2     O(n*log(n))      ~n/2        yes\n=========== ======= ============= ============ ========\n\n.. note:: The datatype determines which of 'mergesort' or 'timsort'\n   is actually used, even if 'mergesort' is specified. User selection\n   at a finer scale is not currently available.\n\nAll the sort algorithms make temporary copies of the data when\nsorting along any but the last axis.  Consequently, sorting along\nthe last axis is faster and uses less space than sorting along\nany other axis.\n\nThe sort order for complex numbers is lexicographic. If both the real\nand imaginary parts are non-nan then the order is determined by the\nreal parts except when they are equal, in which case the order is\ndetermined by the imaginary parts.\n\nPrevious to numpy 1.4.0 sorting real and complex arrays containing nan\nvalues led to undefined behaviour. In numpy versions >= 1.4.0 nan\nvalues are sorted to the end. The extended sort order is:\n\n  * Real: [R, nan]\n  * Complex: [R + Rj, R + nanj, nan + Rj, nan + nanj]\n\nwhere R is a non-nan real value. Complex values with the same nan\nplacements are sorted according to the non-nan part if it exists.\nNon-nan values are sorted as before.\n\n.. versionadded:: 1.12.0\n\nquicksort has been changed to `introsort <https://en.wikipedia.org/wiki/Introsort>`_.\nWhen sorting does not make enough progress it switches to\n`heapsort <https://en.wikipedia.org/wiki/Heapsort>`_.\nThis implementation makes quicksort O(n*log(n)) in the worst case.\n\n'stable' automatically chooses the best stable sorting algorithm\nfor the data type being sorted.\nIt, along with 'mergesort' is currently mapped to\n`timsort <https://en.wikipedia.org/wiki/Timsort>`_\nor `radix sort <https://en.wikipedia.org/wiki/Radix_sort>`_\ndepending on the data type.\nAPI forward compatibility currently limits the\nability to select the implementation and it is hardwired for the different\ndata types.\n\n.. versionadded:: 1.17.0\n\nTimsort is added for better performance on already or nearly\nsorted data. On random data timsort is almost identical to\nmergesort. It is now used for stable sort while quicksort is still the\ndefault sort if none is chosen. For timsort details, refer to\n`CPython listsort.txt <https://github.com/python/cpython/blob/3.7/Objects/listsort.txt>`_.\n'mergesort' and 'stable' are mapped to radix sort for integer data types. Radix sort is an\nO(n) sort instead of O(n log n).\n\n.. versionchanged:: 1.18.0\n\nNaT now sorts to the end of arrays for consistency with NaN.\n\nExamples\n--------\n>>> a = np.array([[1,4],[3,1]])\n>>> np.sort(a)                # sort along the last axis\narray([[1, 4],\n       [1, 3]])\n>>> np.sort(a, axis=None)     # sort the flattened array\narray([1, 1, 3, 4])\n>>> np.sort(a, axis=0)        # sort along the first axis\narray([[1, 1],\n       [3, 4]])\n\nUse the `order` keyword to specify a field to use when sorting a\nstructured array:\n\n>>> dtype = [('name', 'S10'), ('height', float), ('age', int)]\n>>> values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),\n...           ('Galahad', 1.7, 38)]\n>>> a = np.array(values, dtype=dtype)       # create a structured array\n>>> np.sort(a, order='height')                        # doctest: +SKIP\narray([('Galahad', 1.7, 38), ('Arthur', 1.8, 41),\n       ('Lancelot', 1.8999999999999999, 38)],\n      dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])\n\nSort by age, then height if ages are equal:\n\n>>> np.sort(a, order=['age', 'height'])               # doctest: +SKIP\narray([('Galahad', 1.7, 38), ('Lancelot', 1.8999999999999999, 38),\n       ('Arthur', 1.8, 41)],\n      dtype=[('name', '|S10'), ('height', '<f8'), ('age', '<i4')])", "Library": "NumPy"}
{"API_Name": "np.sqrt", "Docstring": "sqrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nReturn the non-negative square-root of an array, element-wise.\n\nParameters\n----------\nx : array_like\n    The values whose square-roots are required.\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray\n    An array of the same shape as `x`, containing the positive\n    square-root of each element in `x`.  If any element in `x` is\n    complex, a complex array is returned (and the square-roots of\n    negative reals are calculated).  If all of the elements in `x`\n    are real, so is `y`, with negative elements returning ``nan``.\n    If `out` was provided, `y` is a reference to it.\n    This is a scalar if `x` is a scalar.\n\nSee Also\n--------\nemath.sqrt\n    A version which returns complex numbers when given negative reals.\n    Note: 0.0 and -0.0 are handled differently for complex inputs.\n\nNotes\n-----\n*sqrt* has--consistent with common convention--as its branch cut the\nreal \"interval\" [`-inf`, 0), and is continuous from above on it.\nA branch cut is a curve in the complex plane across which a given\ncomplex function fails to be continuous.\n\nExamples\n--------\n>>> np.sqrt([1,4,9])\narray([ 1.,  2.,  3.])\n\n>>> np.sqrt([4, -1, -3+4J])\narray([ 2.+0.j,  0.+1.j,  1.+2.j])\n\n>>> np.sqrt([4, -1, np.inf])\narray([ 2., nan, inf])", "Library": "NumPy"}
{"API_Name": "np.std", "Docstring": "Compute the standard deviation along the specified axis.\n\nReturns the standard deviation, a measure of the spread of a distribution,\nof the array elements. The standard deviation is computed for the\nflattened array by default, otherwise over the specified axis.\n\nParameters\n----------\na : array_like\n    Calculate the standard deviation of these values.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which the standard deviation is computed. The\n    default is to compute the standard deviation of the flattened array.\n\n    .. versionadded:: 1.7.0\n\n    If this is a tuple of ints, a standard deviation is performed over\n    multiple axes, instead of a single axis or all the axes as before.\ndtype : dtype, optional\n    Type to use in computing the standard deviation. For arrays of\n    integer type the default is float64, for arrays of float types it is\n    the same as the array type.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output but the type (of the calculated\n    values) will be cast if necessary.\nddof : int, optional\n    Means Delta Degrees of Freedom.  The divisor used in calculations\n    is ``N - ddof``, where ``N`` represents the number of elements.\n    By default `ddof` is zero.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `std` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\n\nwhere : array_like of bool, optional\n    Elements to include in the standard deviation.\n    See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nstandard_deviation : ndarray, see dtype parameter above.\n    If `out` is None, return a new array containing the standard deviation,\n    otherwise return a reference to the output array.\n\nSee Also\n--------\nvar, mean, nanmean, nanstd, nanvar\n:ref:`ufuncs-output-type`\n\nNotes\n-----\nThe standard deviation is the square root of the average of the squared\ndeviations from the mean, i.e., ``std = sqrt(mean(x))``, where\n``x = abs(a - a.mean())**2``.\n\nThe average squared deviation is typically calculated as ``x.sum() / N``,\nwhere ``N = len(x)``. If, however, `ddof` is specified, the divisor\n``N - ddof`` is used instead. In standard statistical practice, ``ddof=1``\nprovides an unbiased estimator of the variance of the infinite population.\n``ddof=0`` provides a maximum likelihood estimate of the variance for\nnormally distributed variables. The standard deviation computed in this\nfunction is the square root of the estimated variance, so even with\n``ddof=1``, it will not be an unbiased estimate of the standard deviation\nper se.\n\nNote that, for complex numbers, `std` takes the absolute\nvalue before squaring, so that the result is always real and nonnegative.\n\nFor floating-point input, the *std* is computed using the same\nprecision the input has. Depending on the input data, this can cause\nthe results to be inaccurate, especially for float32 (see example below).\nSpecifying a higher-accuracy accumulator using the `dtype` keyword can\nalleviate this issue.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.std(a)\n1.1180339887498949 # may vary\n>>> np.std(a, axis=0)\narray([1.,  1.])\n>>> np.std(a, axis=1)\narray([0.5,  0.5])\n\nIn single precision, std() can be inaccurate:\n\n>>> a = np.zeros((2, 512*512), dtype=np.float32)\n>>> a[0, :] = 1.0\n>>> a[1, :] = 0.1\n>>> np.std(a)\n0.45000005\n\nComputing the standard deviation in float64 is more accurate:\n\n>>> np.std(a, dtype=np.float64)\n0.44999999925494177 # may vary\n\nSpecifying a where argument:\n\n>>> a = np.array([[14, 8, 11, 10], [7, 9, 10, 11], [10, 15, 5, 10]])\n>>> np.std(a)\n2.614064523559687 # may vary\n>>> np.std(a, where=[[True], [True], [False]])\n2.0", "Library": "NumPy"}
{"API_Name": "np.sum", "Docstring": "Sum of array elements over a given axis.\n\nParameters\n----------\na : array_like\n    Elements to sum.\naxis : None or int or tuple of ints, optional\n    Axis or axes along which a sum is performed.  The default,\n    axis=None, will sum all of the elements of the input array.  If\n    axis is negative it counts from the last to the first axis.\n\n    .. versionadded:: 1.7.0\n\n    If axis is a tuple of ints, a sum is performed on all of the axes\n    specified in the tuple instead of a single axis or all the axes as\n    before.\ndtype : dtype, optional\n    The type of the returned array and of the accumulator in which the\n    elements are summed.  The dtype of `a` is used by default unless `a`\n    has an integer dtype of less precision than the default platform\n    integer.  In that case, if `a` is signed then the platform integer\n    is used while if `a` is unsigned then an unsigned integer of the\n    same precision as the platform integer is used.\nout : ndarray, optional\n    Alternative output array in which to place the result. It must have\n    the same shape as the expected output, but the type of the output\n    values will be cast if necessary.\nkeepdims : bool, optional\n    If this is set to True, the axes which are reduced are left\n    in the result as dimensions with size one. With this option,\n    the result will broadcast correctly against the input array.\n\n    If the default value is passed, then `keepdims` will not be\n    passed through to the `sum` method of sub-classes of\n    `ndarray`, however any non-default value will be.  If the\n    sub-class' method does not implement `keepdims` any\n    exceptions will be raised.\ninitial : scalar, optional\n    Starting value for the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.15.0\n\nwhere : array_like of bool, optional\n    Elements to include in the sum. See `~numpy.ufunc.reduce` for details.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nsum_along_axis : ndarray\n    An array with the same shape as `a`, with the specified\n    axis removed.   If `a` is a 0-d array, or if `axis` is None, a scalar\n    is returned.  If an output array is specified, a reference to\n    `out` is returned.\n\nSee Also\n--------\nndarray.sum : Equivalent method.\n\nadd.reduce : Equivalent functionality of `add`.\n\ncumsum : Cumulative sum of array elements.\n\ntrapz : Integration of array values using the composite trapezoidal rule.\n\nmean, average\n\nNotes\n-----\nArithmetic is modular when using integer types, and no error is\nraised on overflow.\n\nThe sum of an empty array is the neutral element 0:\n\n>>> np.sum([])\n0.0\n\nFor floating point numbers the numerical precision of sum (and\n``np.add.reduce``) is in general limited by directly adding each number\nindividually to the result causing rounding errors in every step.\nHowever, often numpy will use a  numerically better approach (partial\npairwise summation) leading to improved precision in many use-cases.\nThis improved precision is always provided when no ``axis`` is given.\nWhen ``axis`` is given, it will depend on which axis is summed.\nTechnically, to provide the best speed possible, the improved precision\nis only used when the summation is along the fast axis in memory.\nNote that the exact precision may vary depending on other parameters.\nIn contrast to NumPy, Python's ``math.fsum`` function uses a slower but\nmore precise approach to summation.\nEspecially when summing a large number of lower precision floating point\nnumbers, such as ``float32``, numerical errors can become significant.\nIn such cases it can be advisable to use `dtype=\"float64\"` to use a higher\nprecision for the output.\n\nExamples\n--------\n>>> np.sum([0.5, 1.5])\n2.0\n>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n1\n>>> np.sum([[0, 1], [0, 5]])\n6\n>>> np.sum([[0, 1], [0, 5]], axis=0)\narray([0, 6])\n>>> np.sum([[0, 1], [0, 5]], axis=1)\narray([1, 5])\n>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\narray([1., 5.])\n\nIf the accumulator is too small, overflow occurs:\n\n>>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n-128\n\nYou can also start the sum with a value other than zero:\n\n>>> np.sum([10], initial=5)\n15", "Library": "NumPy"}
{"API_Name": "np.swapaxes", "Docstring": "Interchange two axes of an array.\n\nParameters\n----------\na : array_like\n    Input array.\naxis1 : int\n    First axis.\naxis2 : int\n    Second axis.\n\nReturns\n-------\na_swapped : ndarray\n    For NumPy >= 1.10.0, if `a` is an ndarray, then a view of `a` is\n    returned; otherwise a new array is created. For earlier NumPy\n    versions a view of `a` is returned only if the order of the\n    axes is changed, otherwise the input array is returned.\n\nExamples\n--------\n>>> x = np.array([[1,2,3]])\n>>> np.swapaxes(x,0,1)\narray([[1],\n       [2],\n       [3]])\n\n>>> x = np.array([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n\n>>> np.swapaxes(x,0,2)\narray([[[0, 4],\n        [2, 6]],\n       [[1, 5],\n        [3, 7]]])", "Library": "NumPy"}
{"API_Name": "np.tile", "Docstring": "Construct an array by repeating A the number of times given by reps.\n\nIf `reps` has length ``d``, the result will have dimension of\n``max(d, A.ndim)``.\n\nIf ``A.ndim < d``, `A` is promoted to be d-dimensional by prepending new\naxes. So a shape (3,) array is promoted to (1, 3) for 2-D replication,\nor shape (1, 1, 3) for 3-D replication. If this is not the desired\nbehavior, promote `A` to d-dimensions manually before calling this\nfunction.\n\nIf ``A.ndim > d``, `reps` is promoted to `A`.ndim by pre-pending 1's to it.\nThus for an `A` of shape (2, 3, 4, 5), a `reps` of (2, 2) is treated as\n(1, 1, 2, 2).\n\nNote : Although tile may be used for broadcasting, it is strongly\nrecommended to use numpy's broadcasting operations and functions.\n\nParameters\n----------\nA : array_like\n    The input array.\nreps : array_like\n    The number of repetitions of `A` along each axis.\n\nReturns\n-------\nc : ndarray\n    The tiled output array.\n\nSee Also\n--------\nrepeat : Repeat elements of an array.\nbroadcast_to : Broadcast an array to a new shape\n\nExamples\n--------\n>>> a = np.array([0, 1, 2])\n>>> np.tile(a, 2)\narray([0, 1, 2, 0, 1, 2])\n>>> np.tile(a, (2, 2))\narray([[0, 1, 2, 0, 1, 2],\n       [0, 1, 2, 0, 1, 2]])\n>>> np.tile(a, (2, 1, 2))\narray([[[0, 1, 2, 0, 1, 2]],\n       [[0, 1, 2, 0, 1, 2]]])\n\n>>> b = np.array([[1, 2], [3, 4]])\n>>> np.tile(b, 2)\narray([[1, 2, 1, 2],\n       [3, 4, 3, 4]])\n>>> np.tile(b, (2, 1))\narray([[1, 2],\n       [3, 4],\n       [1, 2],\n       [3, 4]])\n\n>>> c = np.array([1,2,3,4])\n>>> np.tile(c,(4,1))\narray([[1, 2, 3, 4],\n       [1, 2, 3, 4],\n       [1, 2, 3, 4],\n       [1, 2, 3, 4]])", "Library": "NumPy"}
{"API_Name": "np.transpose", "Docstring": "Returns an array with axes transposed.\n\nFor a 1-D array, this returns an unchanged view of the original array, as a\ntransposed vector is simply the same vector.\nTo convert a 1-D array into a 2-D column vector, an additional dimension\nmust be added, e.g., ``np.atleast2d(a).T`` achieves this, as does\n``a[:, np.newaxis]``.\nFor a 2-D array, this is the standard matrix transpose.\nFor an n-D array, if axes are given, their order indicates how the\naxes are permuted (see Examples). If axes are not provided, then\n``transpose(a).shape == a.shape[::-1]``.\n\nParameters\n----------\na : array_like\n    Input array.\naxes : tuple or list of ints, optional\n    If specified, it must be a tuple or list which contains a permutation\n    of [0,1,...,N-1] where N is the number of axes of `a`. The `i`'th axis\n    of the returned array will correspond to the axis numbered ``axes[i]``\n    of the input. If not specified, defaults to ``range(a.ndim)[::-1]``,\n    which reverses the order of the axes.\n\nReturns\n-------\np : ndarray\n    `a` with its axes permuted. A view is returned whenever possible.\n\nSee Also\n--------\nndarray.transpose : Equivalent method.\nmoveaxis : Move axes of an array to new positions.\nargsort : Return the indices that would sort an array.\n\nNotes\n-----\nUse ``transpose(a, argsort(axes))`` to invert the transposition of tensors\nwhen using the `axes` keyword argument.\n\nExamples\n--------\n>>> a = np.array([[1, 2], [3, 4]])\n>>> a\narray([[1, 2],\n       [3, 4]])\n>>> np.transpose(a)\narray([[1, 3],\n       [2, 4]])\n\n>>> a = np.array([1, 2, 3, 4])\n>>> a\narray([1, 2, 3, 4])\n>>> np.transpose(a)\narray([1, 2, 3, 4])\n\n>>> a = np.ones((1, 2, 3))\n>>> np.transpose(a, (1, 0, 2)).shape\n(2, 1, 3)\n\n>>> a = np.ones((2, 3, 4, 5))\n>>> np.transpose(a).shape\n(5, 4, 3, 2)", "Library": "NumPy"}
{"API_Name": "np.tril", "Docstring": "Lower triangle of an array.\n\nReturn a copy of an array with elements above the `k`-th diagonal zeroed.\nFor arrays with ``ndim`` exceeding 2, `tril` will apply to the final two\naxes.\n\nParameters\n----------\nm : array_like, shape (..., M, N)\n    Input array.\nk : int, optional\n    Diagonal above which to zero elements.  `k = 0` (the default) is the\n    main diagonal, `k < 0` is below it and `k > 0` is above.\n\nReturns\n-------\ntril : ndarray, shape (..., M, N)\n    Lower triangle of `m`, of same shape and data-type as `m`.\n\nSee Also\n--------\ntriu : same thing, only for the upper triangle\n\nExamples\n--------\n>>> np.tril([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)\narray([[ 0,  0,  0],\n       [ 4,  0,  0],\n       [ 7,  8,  0],\n       [10, 11, 12]])\n\n>>> np.tril(np.arange(3*4*5).reshape(3, 4, 5))\narray([[[ 0,  0,  0,  0,  0],\n        [ 5,  6,  0,  0,  0],\n        [10, 11, 12,  0,  0],\n        [15, 16, 17, 18,  0]],\n       [[20,  0,  0,  0,  0],\n        [25, 26,  0,  0,  0],\n        [30, 31, 32,  0,  0],\n        [35, 36, 37, 38,  0]],\n       [[40,  0,  0,  0,  0],\n        [45, 46,  0,  0,  0],\n        [50, 51, 52,  0,  0],\n        [55, 56, 57, 58,  0]]])", "Library": "NumPy"}
{"API_Name": "np.triu", "Docstring": "Upper triangle of an array.\n\nReturn a copy of an array with the elements below the `k`-th diagonal\nzeroed. For arrays with ``ndim`` exceeding 2, `triu` will apply to the\nfinal two axes.\n\nPlease refer to the documentation for `tril` for further details.\n\nSee Also\n--------\ntril : lower triangle of an array\n\nExamples\n--------\n>>> np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 0,  8,  9],\n       [ 0,  0, 12]])\n\n>>> np.triu(np.arange(3*4*5).reshape(3, 4, 5))\narray([[[ 0,  1,  2,  3,  4],\n        [ 0,  6,  7,  8,  9],\n        [ 0,  0, 12, 13, 14],\n        [ 0,  0,  0, 18, 19]],\n       [[20, 21, 22, 23, 24],\n        [ 0, 26, 27, 28, 29],\n        [ 0,  0, 32, 33, 34],\n        [ 0,  0,  0, 38, 39]],\n       [[40, 41, 42, 43, 44],\n        [ 0, 46, 47, 48, 49],\n        [ 0,  0, 52, 53, 54],\n        [ 0,  0,  0, 58, 59]]])", "Library": "NumPy"}
{"API_Name": "np.unique", "Docstring": "Find the unique elements of an array.\n\nReturns the sorted unique elements of an array. There are three optional\noutputs in addition to the unique elements:\n\n* the indices of the input array that give the unique values\n* the indices of the unique array that reconstruct the input array\n* the number of times each unique value comes up in the input array\n\nParameters\n----------\nar : array_like\n    Input array. Unless `axis` is specified, this will be flattened if it\n    is not already 1-D.\nreturn_index : bool, optional\n    If True, also return the indices of `ar` (along the specified axis,\n    if provided, or in the flattened array) that result in the unique array.\nreturn_inverse : bool, optional\n    If True, also return the indices of the unique array (for the specified\n    axis, if provided) that can be used to reconstruct `ar`.\nreturn_counts : bool, optional\n    If True, also return the number of times each unique item appears\n    in `ar`.\naxis : int or None, optional\n    The axis to operate on. If None, `ar` will be flattened. If an integer,\n    the subarrays indexed by the given axis will be flattened and treated\n    as the elements of a 1-D array with the dimension of the given axis,\n    see the notes for more details.  Object arrays or structured arrays\n    that contain objects are not supported if the `axis` kwarg is used. The\n    default is None.\n\n    .. versionadded:: 1.13.0\n\nequal_nan : bool, optional\n    If True, collapses multiple NaN values in the return array into one.\n\n    .. versionadded:: 1.24\n\nReturns\n-------\nunique : ndarray\n    The sorted unique values.\nunique_indices : ndarray, optional\n    The indices of the first occurrences of the unique values in the\n    original array. Only provided if `return_index` is True.\nunique_inverse : ndarray, optional\n    The indices to reconstruct the original array from the\n    unique array. Only provided if `return_inverse` is True.\nunique_counts : ndarray, optional\n    The number of times each of the unique values comes up in the\n    original array. Only provided if `return_counts` is True.\n\n    .. versionadded:: 1.9.0\n\nSee Also\n--------\nnumpy.lib.arraysetops : Module with a number of other functions for\n                        performing set operations on arrays.\nrepeat : Repeat elements of an array.\n\nNotes\n-----\nWhen an axis is specified the subarrays indexed by the axis are sorted.\nThis is done by making the specified axis the first dimension of the array\n(move the axis to the first dimension to keep the order of the other axes)\nand then flattening the subarrays in C order. The flattened subarrays are\nthen viewed as a structured type with each element given a label, with the\neffect that we end up with a 1-D array of structured types that can be\ntreated in the same way as any other 1-D array. The result is that the\nflattened subarrays are sorted in lexicographic order starting with the\nfirst element.\n\n.. versionchanged: NumPy 1.21\n    If nan values are in the input array, a single nan is put\n    to the end of the sorted unique values.\n\n    Also for complex arrays all NaN values are considered equivalent\n    (no matter whether the NaN is in the real or imaginary part).\n    As the representant for the returned array the smallest one in the\n    lexicographical order is chosen - see np.sort for how the lexicographical\n    order is defined for complex arrays.\n\nExamples\n--------\n>>> np.unique([1, 1, 2, 2, 3, 3])\narray([1, 2, 3])\n>>> a = np.array([[1, 1], [2, 3]])\n>>> np.unique(a)\narray([1, 2, 3])\n\nReturn the unique rows of a 2D array\n\n>>> a = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]])\n>>> np.unique(a, axis=0)\narray([[1, 0, 0], [2, 3, 4]])\n\nReturn the indices of the original array that give the unique values:\n\n>>> a = np.array(['a', 'b', 'b', 'c', 'a'])\n>>> u, indices = np.unique(a, return_index=True)\n>>> u\narray(['a', 'b', 'c'], dtype='<U1')\n>>> indices\narray([0, 1, 3])\n>>> a[indices]\narray(['a', 'b', 'c'], dtype='<U1')\n\nReconstruct the input array from the unique values and inverse:\n\n>>> a = np.array([1, 2, 6, 4, 2, 3, 2])\n>>> u, indices = np.unique(a, return_inverse=True)\n>>> u\narray([1, 2, 3, 4, 6])\n>>> indices\narray([0, 1, 4, 3, 1, 2, 1])\n>>> u[indices]\narray([1, 2, 6, 4, 2, 3, 2])\n\nReconstruct the input values from the unique values and counts:\n\n>>> a = np.array([1, 2, 6, 4, 2, 3, 2])\n>>> values, counts = np.unique(a, return_counts=True)\n>>> values\narray([1, 2, 3, 4, 6])\n>>> counts\narray([1, 3, 1, 1, 1])\n>>> np.repeat(values, counts)\narray([1, 2, 2, 2, 3, 4, 6])    # original order not preserved", "Library": "NumPy"}
{"API_Name": "np.unravel_index", "Docstring": "unravel_index(indices, shape, order='C')\n\nConverts a flat index or array of flat indices into a tuple\nof coordinate arrays.\n\nParameters\n----------\nindices : array_like\n    An integer array whose elements are indices into the flattened\n    version of an array of dimensions ``shape``. Before version 1.6.0,\n    this function accepted just one index value.\nshape : tuple of ints\n    The shape of the array to use for unraveling ``indices``.\n\n    .. versionchanged:: 1.16.0\n        Renamed from ``dims`` to ``shape``.\n\norder : {'C', 'F'}, optional\n    Determines whether the indices should be viewed as indexing in\n    row-major (C-style) or column-major (Fortran-style) order.\n\n    .. versionadded:: 1.6.0\n\nReturns\n-------\nunraveled_coords : tuple of ndarray\n    Each array in the tuple has the same shape as the ``indices``\n    array.\n\nSee Also\n--------\nravel_multi_index\n\nExamples\n--------\n>>> np.unravel_index([22, 41, 37], (7,6))\n(array([3, 6, 6]), array([4, 5, 1]))\n>>> np.unravel_index([31, 41, 13], (7,6), order='F')\n(array([3, 6, 6]), array([4, 5, 1]))\n\n>>> np.unravel_index(1621, (6,7,8,9))\n(3, 1, 4, 1)", "Library": "NumPy"}
{"API_Name": "np.vectorize", "Docstring": "vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,\n          signature=None)\n\nGeneralized function class.\n\nDefine a vectorized function which takes a nested sequence of objects or\nnumpy arrays as inputs and returns a single numpy array or a tuple of numpy\narrays. The vectorized function evaluates `pyfunc` over successive tuples\nof the input arrays like the python map function, except it uses the\nbroadcasting rules of numpy.\n\nThe data type of the output of `vectorized` is determined by calling\nthe function with the first element of the input.  This can be avoided\nby specifying the `otypes` argument.\n\nParameters\n----------\npyfunc : callable\n    A python function or method.\notypes : str or list of dtypes, optional\n    The output data type. It must be specified as either a string of\n    typecode characters or a list of data type specifiers. There should\n    be one data type specifier for each output.\ndoc : str, optional\n    The docstring for the function. If None, the docstring will be the\n    ``pyfunc.__doc__``.\nexcluded : set, optional\n    Set of strings or integers representing the positional or keyword\n    arguments for which the function will not be vectorized.  These will be\n    passed directly to `pyfunc` unmodified.\n\n    .. versionadded:: 1.7.0\n\ncache : bool, optional\n    If `True`, then cache the first function call that determines the number\n    of outputs if `otypes` is not provided.\n\n    .. versionadded:: 1.7.0\n\nsignature : string, optional\n    Generalized universal function signature, e.g., ``(m,n),(n)->(m)`` for\n    vectorized matrix-vector multiplication. If provided, ``pyfunc`` will\n    be called with (and expected to return) arrays with shapes given by the\n    size of corresponding core dimensions. By default, ``pyfunc`` is\n    assumed to take scalars as input and output.\n\n    .. versionadded:: 1.12.0\n\nReturns\n-------\nvectorized : callable\n    Vectorized function.\n\nSee Also\n--------\nfrompyfunc : Takes an arbitrary Python function and returns a ufunc\n\nNotes\n-----\nThe `vectorize` function is provided primarily for convenience, not for\nperformance. The implementation is essentially a for loop.\n\nIf `otypes` is not specified, then a call to the function with the\nfirst argument will be used to determine the number of outputs.  The\nresults of this call will be cached if `cache` is `True` to prevent\ncalling the function twice.  However, to implement the cache, the\noriginal function must be wrapped which will slow down subsequent\ncalls, so only do this if your function is expensive.\n\nThe new keyword argument interface and `excluded` argument support\nfurther degrades performance.\n\nReferences\n----------\n.. [1] :doc:`/reference/c-api/generalized-ufuncs`\n\nExamples\n--------\n>>> def myfunc(a, b):\n...     \"Return a-b if a>b, otherwise return a+b\"\n...     if a > b:\n...         return a - b\n...     else:\n...         return a + b\n\n>>> vfunc = np.vectorize(myfunc)\n>>> vfunc([1, 2, 3, 4], 2)\narray([3, 4, 1, 2])\n\nThe docstring is taken from the input function to `vectorize` unless it\nis specified:\n\n>>> vfunc.__doc__\n'Return a-b if a>b, otherwise return a+b'\n>>> vfunc = np.vectorize(myfunc, doc='Vectorized `myfunc`')\n>>> vfunc.__doc__\n'Vectorized `myfunc`'\n\nThe output type is determined by evaluating the first element of the input,\nunless it is specified:\n\n>>> out = vfunc([1, 2, 3, 4], 2)\n>>> type(out[0])\n<class 'numpy.int64'>\n>>> vfunc = np.vectorize(myfunc, otypes=[float])\n>>> out = vfunc([1, 2, 3, 4], 2)\n>>> type(out[0])\n<class 'numpy.float64'>\n\nThe `excluded` argument can be used to prevent vectorizing over certain\narguments.  This can be useful for array-like arguments of a fixed length\nsuch as the coefficients for a polynomial as in `polyval`:\n\n>>> def mypolyval(p, x):\n...     _p = list(p)\n...     res = _p.pop(0)\n...     while _p:\n...         res = res*x + _p.pop(0)\n...     return res\n>>> vpolyval = np.vectorize(mypolyval, excluded=['p'])\n>>> vpolyval(p=[1, 2, 3], x=[0, 1])\narray([3, 6])\n\nPositional arguments may also be excluded by specifying their position:\n\n>>> vpolyval.excluded.add(0)\n>>> vpolyval([1, 2, 3], x=[0, 1])\narray([3, 6])\n\nThe `signature` argument allows for vectorizing functions that act on\nnon-scalar arrays of fixed length. For example, you can use it for a\nvectorized calculation of Pearson correlation coefficient and its p-value:\n\n>>> import scipy.stats\n>>> pearsonr = np.vectorize(scipy.stats.pearsonr,\n...                 signature='(n),(n)->(),()')\n>>> pearsonr([[0, 1, 2, 3]], [[1, 2, 3, 4], [4, 3, 2, 1]])\n(array([ 1., -1.]), array([ 0.,  0.]))\n\nOr for a vectorized convolution:\n\n>>> convolve = np.vectorize(np.convolve, signature='(n),(m)->(k)')\n>>> convolve(np.eye(4), [1, 2, 1])\narray([[1., 2., 1., 0., 0., 0.],\n       [0., 1., 2., 1., 0., 0.],\n       [0., 0., 1., 2., 1., 0.],\n       [0., 0., 0., 1., 2., 1.]])", "Library": "NumPy"}
{"API_Name": "np.vstack", "Docstring": "Stack arrays in sequence vertically (row wise).\n\nThis is equivalent to concatenation along the first axis after 1-D arrays\nof shape `(N,)` have been reshaped to `(1,N)`. Rebuilds arrays divided by\n`vsplit`.\n\nThis function makes most sense for arrays with up to 3 dimensions. For\ninstance, for pixel-data with a height (first axis), width (second axis),\nand r/g/b channels (third axis). The functions `concatenate`, `stack` and\n`block` provide more general stacking and concatenation operations.\n\n``np.row_stack`` is an alias for `vstack`. They are the same function.\n\nParameters\n----------\ntup : sequence of ndarrays\n    The arrays must have the same shape along all but the first axis.\n    1-D arrays must have the same length.\n\ndtype : str or dtype\n    If provided, the destination array will have this dtype. Cannot be\n    provided together with `out`.\n\n.. versionadded:: 1.24\n\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n\n.. versionadded:: 1.24\n\nReturns\n-------\nstacked : ndarray\n    The array formed by stacking the given arrays, will be at least 2-D.\n\nSee Also\n--------\nconcatenate : Join a sequence of arrays along an existing axis.\nstack : Join a sequence of arrays along a new axis.\nblock : Assemble an nd-array from nested lists of blocks.\nhstack : Stack arrays in sequence horizontally (column wise).\ndstack : Stack arrays in sequence depth wise (along third axis).\ncolumn_stack : Stack 1-D arrays as columns into a 2-D array.\nvsplit : Split an array into multiple sub-arrays vertically (row-wise).\n\nExamples\n--------\n>>> a = np.array([1, 2, 3])\n>>> b = np.array([4, 5, 6])\n>>> np.vstack((a,b))\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n>>> a = np.array([[1], [2], [3]])\n>>> b = np.array([[4], [5], [6]])\n>>> np.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [4],\n       [5],\n       [6]])", "Library": "NumPy"}
{"API_Name": "np.where", "Docstring": "where(condition, [x, y], /)\n\nReturn elements chosen from `x` or `y` depending on `condition`.\n\n.. note::\n    When only `condition` is provided, this function is a shorthand for\n    ``np.asarray(condition).nonzero()``. Using `nonzero` directly should be\n    preferred, as it behaves correctly for subclasses. The rest of this\n    documentation covers only the case where all three arguments are\n    provided.\n\nParameters\n----------\ncondition : array_like, bool\n    Where True, yield `x`, otherwise yield `y`.\nx, y : array_like\n    Values from which to choose. `x`, `y` and `condition` need to be\n    broadcastable to some shape.\n\nReturns\n-------\nout : ndarray\n    An array with elements from `x` where `condition` is True, and elements\n    from `y` elsewhere.\n\nSee Also\n--------\nchoose\nnonzero : The function that is called when x and y are omitted\n\nNotes\n-----\nIf all the arrays are 1-D, `where` is equivalent to::\n\n    [xv if c else yv\n     for c, xv, yv in zip(condition, x, y)]\n\nExamples\n--------\n>>> a = np.arange(10)\n>>> a\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> np.where(a < 5, a, 10*a)\narray([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\nThis can be used on multidimensional arrays too:\n\n>>> np.where([[True, False], [True, True]],\n...          [[1, 2], [3, 4]],\n...          [[9, 8], [7, 6]])\narray([[1, 8],\n       [3, 4]])\n\nThe shapes of x, y, and the condition are broadcast together:\n\n>>> x, y = np.ogrid[:3, :4]\n>>> np.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\narray([[10,  0,  0,  0],\n       [10, 11,  1,  1],\n       [10, 11, 12,  2]])\n\n>>> a = np.array([[0, 1, 2],\n...               [0, 2, 4],\n...               [0, 3, 6]])\n>>> np.where(a < 4, a, -1)  # -1 is broadcast\narray([[ 0,  1,  2],\n       [ 0,  2, -1],\n       [ 0,  3, -1]])", "Library": "NumPy"}
{"API_Name": "np.zeros", "Docstring": "zeros(shape, dtype=float, order='C', *, like=None)\n\nReturn a new array of given shape and type, filled with zeros.\n\nParameters\n----------\nshape : int or tuple of ints\n    Shape of the new array, e.g., ``(2, 3)`` or ``2``.\ndtype : data-type, optional\n    The desired data-type for the array, e.g., `numpy.int8`.  Default is\n    `numpy.float64`.\norder : {'C', 'F'}, optional, default: 'C'\n    Whether to store multi-dimensional data in row-major\n    (C-style) or column-major (Fortran-style) order in\n    memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Array of zeros with the given shape, dtype, and order.\n\nSee Also\n--------\nzeros_like : Return an array of zeros with shape and type of input.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nfull : Return a new array of given shape filled with value.\n\nExamples\n--------\n>>> np.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])\n\n>>> np.zeros((5,), dtype=int)\narray([0, 0, 0, 0, 0])\n\n>>> np.zeros((2, 1))\narray([[ 0.],\n       [ 0.]])\n\n>>> s = (2,2)\n>>> np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n\n>>> np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\narray([(0, 0), (0, 0)],\n      dtype=[('x', '<i4'), ('y', '<i4')])", "Library": "NumPy"}
{"API_Name": "np.zeros_like", "Docstring": "Return an array of zeros with the same shape and type as a given array.\n\nParameters\n----------\na : array_like\n    The shape and data-type of `a` define these same attributes of\n    the returned array.\ndtype : data-type, optional\n    Overrides the data type of the result.\n\n    .. versionadded:: 1.6.0\norder : {'C', 'F', 'A', or 'K'}, optional\n    Overrides the memory layout of the result. 'C' means C-order,\n    'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,\n    'C' otherwise. 'K' means match the layout of `a` as closely\n    as possible.\n\n    .. versionadded:: 1.6.0\nsubok : bool, optional.\n    If True, then the newly created array will use the sub-class\n    type of `a`, otherwise it will be a base-class array. Defaults\n    to True.\nshape : int or sequence of ints, optional.\n    Overrides the shape of the result. If order='K' and the number of\n    dimensions is unchanged, will try to keep order, otherwise,\n    order='C' is implied.\n\n    .. versionadded:: 1.17.0\n\nReturns\n-------\nout : ndarray\n    Array of zeros with the same shape and type as `a`.\n\nSee Also\n--------\nempty_like : Return an empty array with shape and type of input.\nones_like : Return an array of ones with shape and type of input.\nfull_like : Return a new array with shape of input filled with value.\nzeros : Return a new array setting values to zero.\n\nExamples\n--------\n>>> x = np.arange(6)\n>>> x = x.reshape((2, 3))\n>>> x\narray([[0, 1, 2],\n       [3, 4, 5]])\n>>> np.zeros_like(x)\narray([[0, 0, 0],\n       [0, 0, 0]])\n\n>>> y = np.arange(3, dtype=float)\n>>> y\narray([0., 1., 2.])\n>>> np.zeros_like(y)\narray([0.,  0.,  0.])", "Library": "NumPy"}
{"API_Name": "pd.concat", "Docstring": "Concatenate pandas objects along a particular axis.\n\nAllows optional set logic along the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis,\nwhich may be useful if the labels are the same (or overlapping) on\nthe passed axis number.\n\nParameters\n----------\nobjs : a sequence or mapping of Series or DataFrame objects\n    If a mapping is passed, the sorted keys will be used as the `keys`\n    argument, unless it is passed, in which case the values will be\n    selected (see below). Any None objects will be dropped silently unless\n    they are all None in which case a ValueError will be raised.\naxis : {0/'index', 1/'columns'}, default 0\n    The axis to concatenate along.\njoin : {'inner', 'outer'}, default 'outer'\n    How to handle indexes on other axis (or axes).\nignore_index : bool, default False\n    If True, do not use the index values along the concatenation axis. The\n    resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n    concatenating objects where the concatenation axis does not have\n    meaningful indexing information. Note the index values on the other\n    axes are still respected in the join.\nkeys : sequence, default None\n    If multiple levels passed, should contain tuples. Construct\n    hierarchical index using the passed keys as the outermost level.\nlevels : list of sequences, default None\n    Specific levels (unique values) to use for constructing a\n    MultiIndex. Otherwise they will be inferred from the keys.\nnames : list, default None\n    Names for the levels in the resulting hierarchical index.\nverify_integrity : bool, default False\n    Check whether the new concatenated axis contains duplicates. This can\n    be very expensive relative to the actual data concatenation.\nsort : bool, default False\n    Sort non-concatenation axis if it is not already aligned when `join`\n    is 'outer'.\n    This has no effect when ``join='inner'``, which already preserves\n    the order of the non-concatenation axis.\n\n    .. versionchanged:: 1.0.0\n\n       Changed to not sort by default.\n\ncopy : bool, default True\n    If False, do not copy data unnecessarily.\n\nReturns\n-------\nobject, type of objs\n    When concatenating all ``Series`` along the index (axis=0), a\n    ``Series`` is returned. When ``objs`` contains at least one\n    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n    the columns (axis=1), a ``DataFrame`` is returned.\n\nSee Also\n--------\nDataFrame.join : Join DataFrames using indexes.\nDataFrame.merge : Merge DataFrames by indexes or columns.\n\nNotes\n-----\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining\npandas objects can be found `here\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n\nIt is not recommended to build DataFrames by adding single rows in a\nfor loop. Build a list of rows and make a DataFrame in a single concat.\n\nExamples\n--------\nCombine two ``Series``.\n\n>>> s1 = pd.Series(['a', 'b'])\n>>> s2 = pd.Series(['c', 'd'])\n>>> pd.concat([s1, s2])\n0    a\n1    b\n0    c\n1    d\ndtype: object\n\nClear the existing index and reset it in the result\nby setting the ``ignore_index`` option to ``True``.\n\n>>> pd.concat([s1, s2], ignore_index=True)\n0    a\n1    b\n2    c\n3    d\ndtype: object\n\nAdd a hierarchical index at the outermost level of\nthe data with the ``keys`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'])\ns1  0    a\n    1    b\ns2  0    c\n    1    d\ndtype: object\n\nLabel the index keys you create with the ``names`` option.\n\n>>> pd.concat([s1, s2], keys=['s1', 's2'],\n...           names=['Series name', 'Row ID'])\nSeries name  Row ID\ns1           0         a\n             1         b\ns2           0         c\n             1         d\ndtype: object\n\nCombine two ``DataFrame`` objects with identical columns.\n\n>>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n...                    columns=['letter', 'number'])\n>>> df1\n  letter  number\n0      a       1\n1      b       2\n>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n...                    columns=['letter', 'number'])\n>>> df2\n  letter  number\n0      c       3\n1      d       4\n>>> pd.concat([df1, df2])\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects with overlapping columns\nand return everything. Columns outside the intersection will\nbe filled with ``NaN`` values.\n\n>>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n...                    columns=['letter', 'number', 'animal'])\n>>> df3\n  letter  number animal\n0      c       3    cat\n1      d       4    dog\n>>> pd.concat([df1, df3], sort=False)\n  letter  number animal\n0      a       1    NaN\n1      b       2    NaN\n0      c       3    cat\n1      d       4    dog\n\nCombine ``DataFrame`` objects with overlapping columns\nand return only those that are shared by passing ``inner`` to\nthe ``join`` keyword argument.\n\n>>> pd.concat([df1, df3], join=\"inner\")\n  letter  number\n0      a       1\n1      b       2\n0      c       3\n1      d       4\n\nCombine ``DataFrame`` objects horizontally along the x axis by\npassing in ``axis=1``.\n\n>>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n...                    columns=['animal', 'name'])\n>>> pd.concat([df1, df4], axis=1)\n  letter  number  animal    name\n0      a       1    bird   polly\n1      b       2  monkey  george\n\nPrevent the result from including duplicate index values with the\n``verify_integrity`` option.\n\n>>> df5 = pd.DataFrame([1], index=['a'])\n>>> df5\n   0\na  1\n>>> df6 = pd.DataFrame([2], index=['a'])\n>>> df6\n   0\na  2\n>>> pd.concat([df5, df6], verify_integrity=True)\nTraceback (most recent call last):\n    ...\nValueError: Indexes have overlapping values: ['a']\n\nAppend a single row to the end of a ``DataFrame`` object.\n\n>>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n>>> df7\n    a   b\n0   1   2\n>>> new_row = pd.Series({'a': 3, 'b': 4})\n>>> new_row\na    3\nb    4\ndtype: int64\n>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n    a   b\n0   1   2\n1   3   4", "Library": "Pandas"}
{"API_Name": "pd.cut", "Docstring": "Bin values into discrete intervals.\n\nUse `cut` when you need to segment and sort data values into bins. This\nfunction is also useful for going from a continuous variable to a\ncategorical variable. For example, `cut` could convert ages to groups of\nage ranges. Supports binning into an equal number of bins, or a\npre-specified array of bins.\n\nParameters\n----------\nx : array-like\n    The input array to be binned. Must be 1-dimensional.\nbins : int, sequence of scalars, or IntervalIndex\n    The criteria to bin by.\n\n    * int : Defines the number of equal-width bins in the range of `x`. The\n      range of `x` is extended by .1% on each side to include the minimum\n      and maximum values of `x`.\n    * sequence of scalars : Defines the bin edges allowing for non-uniform\n      width. No extension of the range of `x` is done.\n    * IntervalIndex : Defines the exact bins to be used. Note that\n      IntervalIndex for `bins` must be non-overlapping.\n\nright : bool, default True\n    Indicates whether `bins` includes the rightmost edge or not. If\n    ``right == True`` (the default), then the `bins` ``[1, 2, 3, 4]``\n    indicate (1,2], (2,3], (3,4]. This argument is ignored when\n    `bins` is an IntervalIndex.\nlabels : array or False, default None\n    Specifies the labels for the returned bins. Must be the same length as\n    the resulting bins. If False, returns only integer indicators of the\n    bins. This affects the type of the output container (see below).\n    This argument is ignored when `bins` is an IntervalIndex. If True,\n    raises an error. When `ordered=False`, labels must be provided.\nretbins : bool, default False\n    Whether to return the bins or not. Useful when bins is provided\n    as a scalar.\nprecision : int, default 3\n    The precision at which to store and display the bins labels.\ninclude_lowest : bool, default False\n    Whether the first interval should be left-inclusive or not.\nduplicates : {default 'raise', 'drop'}, optional\n    If bin edges are not unique, raise ValueError or drop non-uniques.\nordered : bool, default True\n    Whether the labels are ordered or not. Applies to returned types\n    Categorical and Series (with Categorical dtype). If True,\n    the resulting categorical will be ordered. If False, the resulting\n    categorical will be unordered (labels must be provided).\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nout : Categorical, Series, or ndarray\n    An array-like object representing the respective bin for each value\n    of `x`. The type depends on the value of `labels`.\n\n    * None (default) : returns a Series for Series `x` or a\n      Categorical for all other inputs. The values stored within\n      are Interval dtype.\n\n    * sequence of scalars : returns a Series for Series `x` or a\n      Categorical for all other inputs. The values stored within\n      are whatever the type in the sequence is.\n\n    * False : returns an ndarray of integers.\n\nbins : numpy.ndarray or IntervalIndex.\n    The computed or specified bins. Only returned when `retbins=True`.\n    For scalar or sequence `bins`, this is an ndarray with the computed\n    bins. If set `duplicates=drop`, `bins` will drop non-unique bin. For\n    an IntervalIndex `bins`, this is equal to `bins`.\n\nSee Also\n--------\nqcut : Discretize variable into equal-sized buckets based on rank\n    or based on sample quantiles.\nCategorical : Array type for storing data that come from a\n    fixed set of values.\nSeries : One-dimensional array with axis labels (including time series).\nIntervalIndex : Immutable Index implementing an ordered, sliceable set.\n\nNotes\n-----\nAny NA values will be NA in the result. Out of bounds values will be NA in\nthe resulting Series or Categorical object.\n\nReference :ref:`the user guide <reshaping.tile.cut>` for more examples.\n\nExamples\n--------\nDiscretize into three equal-sized bins.\n\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\n... # doctest: +ELLIPSIS\n[(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\nCategories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...\n\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)\n... # doctest: +ELLIPSIS\n([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\nCategories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...\narray([0.994, 3.   , 5.   , 7.   ]))\n\nDiscovers the same bins, but assign them specific labels. Notice that\nthe returned Categorical's categories are `labels` and is ordered.\n\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]),\n...        3, labels=[\"bad\", \"medium\", \"good\"])\n['bad', 'good', 'medium', 'medium', 'good', 'bad']\nCategories (3, object): ['bad' < 'medium' < 'good']\n\n``ordered=False`` will result in unordered categories when labels are passed.\nThis parameter can be used to allow non-unique labels:\n\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3,\n...        labels=[\"B\", \"A\", \"B\"], ordered=False)\n['B', 'B', 'A', 'A', 'B', 'B']\nCategories (2, object): ['A', 'B']\n\n``labels=False`` implies you just want the bins back.\n\n>>> pd.cut([0, 1, 1, 2], bins=4, labels=False)\narray([0, 1, 1, 3])\n\nPassing a Series as an input returns a Series with categorical dtype:\n\n>>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\n...               index=['a', 'b', 'c', 'd', 'e'])\n>>> pd.cut(s, 3)\n... # doctest: +ELLIPSIS\na    (1.992, 4.667]\nb    (1.992, 4.667]\nc    (4.667, 7.333]\nd     (7.333, 10.0]\ne     (7.333, 10.0]\ndtype: category\nCategories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ...\n\nPassing a Series as an input returns a Series with mapping value.\nIt is used to map numerically to intervals based on bins.\n\n>>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\n...               index=['a', 'b', 'c', 'd', 'e'])\n>>> pd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False)\n... # doctest: +ELLIPSIS\n(a    1.0\n b    2.0\n c    3.0\n d    4.0\n e    NaN\n dtype: float64,\n array([ 0,  2,  4,  6,  8, 10]))\n\nUse `drop` optional when bins is not unique\n\n>>> pd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True,\n...        right=False, duplicates='drop')\n... # doctest: +ELLIPSIS\n(a    1.0\n b    2.0\n c    3.0\n d    3.0\n e    NaN\n dtype: float64,\n array([ 0,  2,  4,  6, 10]))\n\nPassing an IntervalIndex for `bins` results in those categories exactly.\nNotice that values not covered by the IntervalIndex are set to NaN. 0\nis to the left of the first bin (which is closed on the right), and 1.5\nfalls between two bins.\n\n>>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\n>>> pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)\n[NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]]\nCategories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]]", "Library": "Pandas"}
{"API_Name": "pd.date_range", "Docstring": "Return a fixed frequency DatetimeIndex.\n\nReturns the range of equally spaced time points (where the difference between any\ntwo adjacent points is specified by the given frequency) such that they all\nsatisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,\nthe first and last time points in that range that fall on the boundary of ``freq``\n(if given as a frequency string) or that are valid for ``freq`` (if given as a\n:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,\n``end``, or ``freq`` is *not* specified, this missing parameter can be computed\ngiven ``periods``, the number of timesteps in the range. See the note below.)\n\nParameters\n----------\nstart : str or datetime-like, optional\n    Left bound for generating dates.\nend : str or datetime-like, optional\n    Right bound for generating dates.\nperiods : int, optional\n    Number of periods to generate.\nfreq : str or DateOffset, default 'D'\n    Frequency strings can have multiples, e.g. '5H'. See\n    :ref:`here <timeseries.offset_aliases>` for a list of\n    frequency aliases.\ntz : str or tzinfo, optional\n    Time zone name for returning localized DatetimeIndex, for example\n    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is\n    timezone-naive.\nnormalize : bool, default False\n    Normalize start/end dates to midnight before generating date range.\nname : str, default None\n    Name of the resulting DatetimeIndex.\nclosed : {None, 'left', 'right'}, optional\n    Make the interval closed with respect to the given frequency to\n    the 'left', 'right', or both sides (None, the default).\n\n    .. deprecated:: 1.4.0\n       Argument `closed` has been deprecated to standardize boundary inputs.\n       Use `inclusive` instead, to set each bound as closed or open.\ninclusive : {\"both\", \"neither\", \"left\", \"right\"}, default \"both\"\n    Include boundaries; Whether to set each bound as closed or open.\n\n    .. versionadded:: 1.4.0\n**kwargs\n    For compatibility. Has no effect on the result.\n\nReturns\n-------\nrng : DatetimeIndex\n\nSee Also\n--------\nDatetimeIndex : An immutable container for datetimes.\ntimedelta_range : Return a fixed frequency TimedeltaIndex.\nperiod_range : Return a fixed frequency PeriodIndex.\ninterval_range : Return a fixed frequency IntervalIndex.\n\nNotes\n-----\nOf the four parameters ``start``, ``end``, ``periods``, and ``freq``,\nexactly three must be specified. If ``freq`` is omitted, the resulting\n``DatetimeIndex`` will have ``periods`` linearly spaced elements between\n``start`` and ``end`` (closed on both sides).\n\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\n**Specifying the values**\n\nThe next four examples generate the same `DatetimeIndex`, but vary\nthe combination of `start`, `end` and `periods`.\n\nSpecify `start` and `end`, with the default daily frequency.\n\n>>> pd.date_range(start='1/1/2018', end='1/08/2018')\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start` and `periods`, the number of periods (days).\n\n>>> pd.date_range(start='1/1/2018', periods=8)\nDatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',\n               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `end` and `periods`, the number of periods (days).\n\n>>> pd.date_range(end='1/1/2018', periods=8)\nDatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',\n               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],\n              dtype='datetime64[ns]', freq='D')\n\nSpecify `start`, `end`, and `periods`; the frequency is generated\nautomatically (linearly spaced).\n\n>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)\nDatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',\n               '2018-04-27 00:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\n**Other Parameters**\n\nChanged the `freq` (frequency) to ``'M'`` (month end frequency).\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='M')\nDatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n               '2018-05-31'],\n              dtype='datetime64[ns]', freq='M')\n\nMultiples are allowed\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq='3M')\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3M')\n\n`freq` can also be specified as an Offset object.\n\n>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))\nDatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',\n               '2019-01-31'],\n              dtype='datetime64[ns]', freq='3M')\n\nSpecify `tz` to set the timezone.\n\n>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')\nDatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',\n               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',\n               '2018-01-05 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq='D')\n\n`inclusive` controls whether to include `start` and `end` that are on the\nboundary. The default, \"both\", includes boundary points on either end.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=\"both\")\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='left'`` to exclude `end` if it falls on the boundary.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],\n              dtype='datetime64[ns]', freq='D')\n\nUse ``inclusive='right'`` to exclude `start` if it falls on the boundary, and\nsimilarly ``inclusive='neither'`` will exclude both `start` and `end`.\n\n>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')\nDatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],\n              dtype='datetime64[ns]', freq='D')", "Library": "Pandas"}
{"API_Name": "pd.DataFrame", "Docstring": "Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns).\nArithmetic operations align on both row and column labels. Can be\nthought of as a dict-like container for Series objects. The primary\npandas data structure.\n\nParameters\n----------\ndata : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n    data is a dict, column order follows insertion-order. If a dict contains Series\n    which have an index defined, it is aligned by its index.\n\n    .. versionchanged:: 0.25.0\n       If data is a list of dicts, column order follows insertion-order.\n\nindex : Index or array-like\n    Index to use for resulting frame. Will default to RangeIndex if\n    no indexing information part of input data and no index provided.\ncolumns : Index or array-like\n    Column labels to use for resulting frame when data does not have them,\n    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n    will perform column selection instead.\ndtype : dtype, default None\n    Data type to force. Only a single dtype is allowed. If None, infer.\ncopy : bool or None, default None\n    Copy data from inputs.\n    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n    or 2d ndarray input, the default of None behaves like ``copy=False``.\n    If data is a dict containing one or more Series (possibly of different dtypes),\n    ``copy=False`` will ensure that these inputs are not copied.\n\n    .. versionchanged:: 1.3.0\n\nSee Also\n--------\nDataFrame.from_records : Constructor from tuples, also record arrays.\nDataFrame.from_dict : From dicts of Series, arrays, or dicts.\nread_csv : Read a comma-separated values (csv) file into DataFrame.\nread_table : Read general delimited file into DataFrame.\nread_clipboard : Read text from clipboard into DataFrame.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.dataframe>` for more information.\n\nExamples\n--------\nConstructing DataFrame from a dictionary.\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df\n   col1  col2\n0     1     3\n1     2     4\n\nNotice that the inferred dtype is int64.\n\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nTo enforce a single dtype:\n\n>>> df = pd.DataFrame(data=d, dtype=np.int8)\n>>> df.dtypes\ncol1    int8\ncol2    int8\ndtype: object\n\nConstructing DataFrame from a dictionary including Series:\n\n>>> d = {'col1': [0, 1, 2, 3], 'col2': pd.Series([2, 3], index=[2, 3])}\n>>> pd.DataFrame(data=d, index=[0, 1, 2, 3])\n   col1  col2\n0     0   NaN\n1     1   NaN\n2     2   2.0\n3     3   3.0\n\nConstructing DataFrame from numpy ndarray:\n\n>>> df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n...                    columns=['a', 'b', 'c'])\n>>> df2\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\n>>> data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n...                 dtype=[(\"a\", \"i4\"), (\"b\", \"i4\"), (\"c\", \"i4\")])\n>>> df3 = pd.DataFrame(data, columns=['c', 'a'])\n...\n>>> df3\n   c  a\n0  3  1\n1  6  4\n2  9  7\n\nConstructing DataFrame from dataclass:\n\n>>> from dataclasses import make_dataclass\n>>> Point = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n>>> pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n   x  y\n0  0  0\n1  0  3\n2  2  3", "Library": "Pandas"}
{"API_Name": "pd.DataFrame.copy", "Docstring": "Make a copy of this object's indices and data.\n\nWhen ``deep=True`` (default), a new object will be created with a\ncopy of the calling object's data and indices. Modifications to\nthe data or indices of the copy will not be reflected in the\noriginal object (see notes below).\n\nWhen ``deep=False``, a new object will be created without copying\nthe calling object's data or index (only references to the data\nand index are copied). Any changes to the data of the original\nwill be reflected in the shallow copy (and vice versa).\n\nParameters\n----------\ndeep : bool, default True\n    Make a deep copy, including a copy of the data and the indices.\n    With ``deep=False`` neither the indices nor the data are copied.\n\nReturns\n-------\ncopy : Series or DataFrame\n    Object type matches caller.\n\nNotes\n-----\nWhen ``deep=True``, data is copied but actual Python objects\nwill not be copied recursively, only the reference to the object.\nThis is in contrast to `copy.deepcopy` in the Standard Library,\nwhich recursively copies object data (see examples below).\n\nWhile ``Index`` objects are copied when ``deep=True``, the underlying\nnumpy array is not copied for performance reasons. Since ``Index`` is\nimmutable, the underlying data can be safely shared and a copy\nis not needed.\n\nSince pandas is not thread safe, see the\n:ref:`gotchas <gotchas.thread-safety>` when copying in a threading\nenvironment.\n\nExamples\n--------\n>>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n>>> s\na    1\nb    2\ndtype: int64\n\n>>> s_copy = s.copy()\n>>> s_copy\na    1\nb    2\ndtype: int64\n\n**Shallow copy versus default (deep) copy:**\n\n>>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n>>> deep = s.copy()\n>>> shallow = s.copy(deep=False)\n\nShallow copy shares data and index with original.\n\n>>> s is shallow\nFalse\n>>> s.values is shallow.values and s.index is shallow.index\nTrue\n\nDeep copy has own copy of data and index.\n\n>>> s is deep\nFalse\n>>> s.values is deep.values or s.index is deep.index\nFalse\n\nUpdates to the data shared by shallow copy and original is reflected\nin both; deep copy remains unchanged.\n\n>>> s[0] = 3\n>>> shallow[1] = 4\n>>> s\na    3\nb    4\ndtype: int64\n>>> shallow\na    3\nb    4\ndtype: int64\n>>> deep\na    1\nb    2\ndtype: int64\n\nNote that when copying an object containing Python objects, a deep copy\nwill copy the data, but will not do so recursively. Updating a nested\ndata object will be reflected in the deep copy.\n\n>>> s = pd.Series([[1, 2], [3, 4]])\n>>> deep = s.copy()\n>>> s[0][0] = 10\n>>> s\n0    [10, 2]\n1     [3, 4]\ndtype: object\n>>> deep\n0    [10, 2]\n1     [3, 4]\ndtype: object", "Library": "Pandas"}
{"API_Name": "pd.DataFrame.from_dict", "Docstring": "Construct DataFrame from dict of array-like or dicts.\n\nCreates DataFrame object from dictionary by columns or by index\nallowing dtype specification.\n\nParameters\n----------\ndata : dict\n    Of the form {field : array-like} or {field : dict}.\norient : {'columns', 'index', 'tight'}, default 'columns'\n    The \"orientation\" of the data. If the keys of the passed dict\n    should be the columns of the resulting DataFrame, pass 'columns'\n    (default). Otherwise if the keys should be rows, pass 'index'.\n    If 'tight', assume a dict with keys ['index', 'columns', 'data',\n    'index_names', 'column_names'].\n\n    .. versionadded:: 1.4.0\n       'tight' as an allowed value for the ``orient`` argument\n\ndtype : dtype, default None\n    Data type to force, otherwise infer.\ncolumns : list, default None\n    Column labels to use when ``orient='index'``. Raises a ValueError\n    if used with ``orient='columns'`` or ``orient='tight'``.\n\nReturns\n-------\nDataFrame\n\nSee Also\n--------\nDataFrame.from_records : DataFrame from structured ndarray, sequence\n    of tuples or dicts, or DataFrame.\nDataFrame : DataFrame object creation using constructor.\nDataFrame.to_dict : Convert the DataFrame to a dictionary.\n\nExamples\n--------\nBy default the keys of the dict become the DataFrame columns:\n\n>>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n>>> pd.DataFrame.from_dict(data)\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d\n\nSpecify ``orient='index'`` to create the DataFrame using dictionary\nkeys as rows:\n\n>>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']}\n>>> pd.DataFrame.from_dict(data, orient='index')\n       0  1  2  3\nrow_1  3  2  1  0\nrow_2  a  b  c  d\n\nWhen using the 'index' orientation, the column names can be\nspecified manually:\n\n>>> pd.DataFrame.from_dict(data, orient='index',\n...                        columns=['A', 'B', 'C', 'D'])\n       A  B  C  D\nrow_1  3  2  1  0\nrow_2  a  b  c  d\n\nSpecify ``orient='tight'`` to create the DataFrame using a 'tight'\nformat:\n\n>>> data = {'index': [('a', 'b'), ('a', 'c')],\n...         'columns': [('x', 1), ('y', 2)],\n...         'data': [[1, 3], [2, 4]],\n...         'index_names': ['n1', 'n2'],\n...         'column_names': ['z1', 'z2']}\n>>> pd.DataFrame.from_dict(data, orient='tight')\nz1     x  y\nz2     1  2\nn1 n2\na  b   1  3\n   c   2  4", "Library": "Pandas"}
{"API_Name": "pd.isna", "Docstring": "Detect missing values for an array-like object.\n\nThis function takes a scalar or array-like object and indicates\nwhether values are missing (``NaN`` in numeric arrays, ``None`` or ``NaN``\nin object arrays, ``NaT`` in datetimelike).\n\nParameters\n----------\nobj : scalar or array-like\n    Object to check for null or missing values.\n\nReturns\n-------\nbool or array-like of bool\n    For scalar input, returns a scalar boolean.\n    For array input, returns an array of boolean indicating whether each\n    corresponding element is missing.\n\nSee Also\n--------\nnotna : Boolean inverse of pandas.isna.\nSeries.isna : Detect missing values in a Series.\nDataFrame.isna : Detect missing values in a DataFrame.\nIndex.isna : Detect missing values in an Index.\n\nExamples\n--------\nScalar arguments (including strings) result in a scalar boolean.\n\n>>> pd.isna('dog')\nFalse\n\n>>> pd.isna(pd.NA)\nTrue\n\n>>> pd.isna(np.nan)\nTrue\n\nndarrays result in an ndarray of booleans.\n\n>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n>>> array\narray([[ 1., nan,  3.],\n       [ 4.,  5., nan]])\n>>> pd.isna(array)\narray([[False,  True, False],\n       [False, False,  True]])\n\nFor indexes, an ndarray of booleans is returned.\n\n>>> index = pd.DatetimeIndex([\"2017-07-05\", \"2017-07-06\", None,\n...                           \"2017-07-08\"])\n>>> index\nDatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],\n              dtype='datetime64[ns]', freq=None)\n>>> pd.isna(index)\narray([False, False,  True, False])\n\nFor Series and DataFrame, the same type is returned, containing booleans.\n\n>>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])\n>>> df\n     0     1    2\n0  ant   bee  cat\n1  dog  None  fly\n>>> pd.isna(df)\n       0      1      2\n0  False  False  False\n1  False   True  False\n\n>>> pd.isna(df[1])\n0    False\n1     True\nName: 1, dtype: bool", "Library": "Pandas"}
{"API_Name": "pd.melt", "Docstring": "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n\nThis function is useful to massage a DataFrame into a format where one\nor more columns are identifier variables (`id_vars`), while all other\ncolumns, considered measured variables (`value_vars`), are \"unpivoted\" to\nthe row axis, leaving just two non-identifier columns, 'variable' and\n'value'.\n\nParameters\n----------\nid_vars : tuple, list, or ndarray, optional\n    Column(s) to use as identifier variables.\nvalue_vars : tuple, list, or ndarray, optional\n    Column(s) to unpivot. If not specified, uses all columns that\n    are not set as `id_vars`.\nvar_name : scalar\n    Name to use for the 'variable' column. If None it uses\n    ``frame.columns.name`` or 'variable'.\nvalue_name : scalar, default 'value'\n    Name to use for the 'value' column.\ncol_level : int or str, optional\n    If columns are a MultiIndex then use this level to melt.\nignore_index : bool, default True\n    If True, original index is ignored. If False, the original index is retained.\n    Index labels will be repeated as necessary.\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nDataFrame\n    Unpivoted DataFrame.\n\nSee Also\n--------\nDataFrame.melt : Identical method.\npivot_table : Create a spreadsheet-style pivot table as a DataFrame.\nDataFrame.pivot : Return reshaped DataFrame organized\n    by given index / column values.\nDataFrame.explode : Explode a DataFrame from list-like\n        columns to long format.\n\nNotes\n-----\nReference :ref:`the user guide <reshaping.melt>` for more examples.\n\nExamples\n--------\n>>> df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n...                    'B': {0: 1, 1: 3, 2: 5},\n...                    'C': {0: 2, 1: 4, 2: 6}})\n>>> df\n   A  B  C\n0  a  1  2\n1  b  3  4\n2  c  5  6\n\n>>> pd.melt(df, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n\n>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n3  a        C      2\n4  b        C      4\n5  c        C      6\n\nThe names of 'variable' and 'value' columns can be customized:\n\n>>> pd.melt(df, id_vars=['A'], value_vars=['B'],\n...         var_name='myVarname', value_name='myValname')\n   A myVarname  myValname\n0  a         B          1\n1  b         B          3\n2  c         B          5\n\nOriginal index values can be kept around:\n\n>>> pd.melt(df, id_vars=['A'], value_vars=['B', 'C'], ignore_index=False)\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n0  a        C      2\n1  b        C      4\n2  c        C      6\n\nIf you have multi-index columns:\n\n>>> df.columns = [list('ABC'), list('DEF')]\n>>> df\n   A  B  C\n   D  E  F\n0  a  1  2\n1  b  3  4\n2  c  5  6\n\n>>> pd.melt(df, col_level=0, id_vars=['A'], value_vars=['B'])\n   A variable  value\n0  a        B      1\n1  b        B      3\n2  c        B      5\n\n>>> pd.melt(df, id_vars=[('A', 'D')], value_vars=[('B', 'E')])\n  (A, D) variable_0 variable_1  value\n0      a          B          E      1\n1      b          B          E      3\n2      c          B          E      5", "Library": "Pandas"}
{"API_Name": "pd.MultiIndex.from_product", "Docstring": "Make a MultiIndex from the cartesian product of multiple iterables.\n\nParameters\n----------\niterables : list / sequence of iterables\n    Each iterable has unique labels for each level of the index.\nsortorder : int or None\n    Level of sortedness (must be lexicographically sorted by that\n    level).\nnames : list / sequence of str, optional\n    Names for the levels in the index.\n\n    .. versionchanged:: 1.0.0\n\n       If not explicitly provided, names will be inferred from the\n       elements of iterables if an element has a name attribute\n\nReturns\n-------\nMultiIndex\n\nSee Also\n--------\nMultiIndex.from_arrays : Convert list of arrays to MultiIndex.\nMultiIndex.from_tuples : Convert list of tuples to MultiIndex.\nMultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\nExamples\n--------\n>>> numbers = [0, 1, 2]\n>>> colors = ['green', 'purple']\n>>> pd.MultiIndex.from_product([numbers, colors],\n...                            names=['number', 'color'])\nMultiIndex([(0,  'green'),\n            (0, 'purple'),\n            (1,  'green'),\n            (1, 'purple'),\n            (2,  'green'),\n            (2, 'purple')],\n           names=['number', 'color'])", "Library": "Pandas"}
{"API_Name": "pd.MultiIndex.from_tuples", "Docstring": "Convert list of tuples to MultiIndex.\n\nParameters\n----------\ntuples : list / sequence of tuple-likes\n    Each tuple is the index of one row/column.\nsortorder : int or None\n    Level of sortedness (must be lexicographically sorted by that\n    level).\nnames : list / sequence of str, optional\n    Names for the levels in the index.\n\nReturns\n-------\nMultiIndex\n\nSee Also\n--------\nMultiIndex.from_arrays : Convert list of arrays to MultiIndex.\nMultiIndex.from_product : Make a MultiIndex from cartesian product\n                          of iterables.\nMultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\nExamples\n--------\n>>> tuples = [(1, 'red'), (1, 'blue'),\n...           (2, 'red'), (2, 'blue')]\n>>> pd.MultiIndex.from_tuples(tuples, names=('number', 'color'))\nMultiIndex([(1,  'red'),\n            (1, 'blue'),\n            (2,  'red'),\n            (2, 'blue')],\n           names=['number', 'color'])", "Library": "Pandas"}
{"API_Name": "pd.pivot_table", "Docstring": "Create a spreadsheet-style pivot table as a DataFrame.\n\nThe levels in the pivot table will be stored in MultiIndex objects\n(hierarchical indexes) on the index and columns of the result DataFrame.\n\nParameters\n----------\ndata : DataFrame\nvalues : column to aggregate, optional\nindex : column, Grouper, array, or list of the previous\n    If an array is passed, it must be the same length as the data. The\n    list can contain any of the other types (except list).\n    Keys to group by on the pivot table index.  If an array is passed,\n    it is being used as the same manner as column values.\ncolumns : column, Grouper, array, or list of the previous\n    If an array is passed, it must be the same length as the data. The\n    list can contain any of the other types (except list).\n    Keys to group by on the pivot table column.  If an array is passed,\n    it is being used as the same manner as column values.\naggfunc : function, list of functions, dict, default numpy.mean\n    If list of functions passed, the resulting pivot table will have\n    hierarchical columns whose top level are the function names\n    (inferred from the function objects themselves)\n    If dict is passed, the key is column to aggregate and value\n    is function or list of functions.\nfill_value : scalar, default None\n    Value to replace missing values with (in the resulting pivot table,\n    after aggregation).\nmargins : bool, default False\n    Add all row / columns (e.g. for subtotal / grand totals).\ndropna : bool, default True\n    Do not include columns whose entries are all NaN. If True,\n    rows with a NaN value in any column will be omitted before\n    computing margins.\nmargins_name : str, default 'All'\n    Name of the row / column that will contain the totals\n    when margins is True.\nobserved : bool, default False\n    This only applies if any of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show all values for categorical groupers.\n\n    .. versionchanged:: 0.25.0\n\nsort : bool, default True\n    Specifies if the result should be sorted.\n\n    .. versionadded:: 1.3.0\n\nReturns\n-------\nDataFrame\n    An Excel style pivot table.\n\nSee Also\n--------\nDataFrame.pivot : Pivot without aggregation that can handle\n    non-numeric data.\nDataFrame.melt: Unpivot a DataFrame from wide to long format,\n    optionally leaving identifiers set.\nwide_to_long : Wide panel to long format. Less flexible but more\n    user-friendly than melt.\n\nNotes\n-----\nReference :ref:`the user guide <reshaping.pivot>` for more examples.\n\nExamples\n--------\n>>> df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n...                          \"bar\", \"bar\", \"bar\", \"bar\"],\n...                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n...                          \"one\", \"one\", \"two\", \"two\"],\n...                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n...                          \"small\", \"large\", \"small\", \"small\",\n...                          \"large\"],\n...                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n...                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n>>> df\n     A    B      C  D  E\n0  foo  one  small  1  2\n1  foo  one  large  2  4\n2  foo  one  large  2  5\n3  foo  two  small  3  5\n4  foo  two  small  3  6\n5  bar  one  large  4  6\n6  bar  one  small  5  8\n7  bar  two  small  6  9\n8  bar  two  large  7  9\n\nThis first example aggregates values by taking the sum.\n\n>>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n...                     columns=['C'], aggfunc=np.sum)\n>>> table\nC        large  small\nA   B\nbar one    4.0    5.0\n    two    7.0    6.0\nfoo one    4.0    1.0\n    two    NaN    6.0\n\nWe can also fill missing values using the `fill_value` parameter.\n\n>>> table = pd.pivot_table(df, values='D', index=['A', 'B'],\n...                     columns=['C'], aggfunc=np.sum, fill_value=0)\n>>> table\nC        large  small\nA   B\nbar one      4      5\n    two      7      6\nfoo one      4      1\n    two      0      6\n\nThe next example aggregates by taking the mean across multiple columns.\n\n>>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n...                     aggfunc={'D': np.mean,\n...                              'E': np.mean})\n>>> table\n                D         E\nA   C\nbar large  5.500000  7.500000\n    small  5.500000  8.500000\nfoo large  2.000000  4.500000\n    small  2.333333  4.333333\n\nWe can also calculate multiple types of aggregations for any given\nvalue column.\n\n>>> table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'],\n...                     aggfunc={'D': np.mean,\n...                              'E': [min, max, np.mean]})\n>>> table\n                  D   E\n               mean max      mean  min\nA   C\nbar large  5.500000   9  7.500000    6\n    small  5.500000   9  8.500000    8\nfoo large  2.000000   5  4.500000    4\n    small  2.333333   6  4.333333    2", "Library": "Pandas"}
{"API_Name": "pd.Series", "Docstring": "One-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object\nsupports both integer- and label-based indexing and provides a host of\nmethods for performing operations involving the index. Statistical\nmethods from ndarray have been overridden to automatically exclude\nmissing data (currently represented as NaN).\n\nOperations between Series (+, -, /, \\*, \\*\\*) align values based on their\nassociated index values-- they need not be the same length. The result\nindex will be the sorted union of the two indexes.\n\nParameters\n----------\ndata : array-like, Iterable, dict, or scalar value\n    Contains data stored in Series. If data is a dict, argument order is\n    maintained.\nindex : array-like or Index (1d)\n    Values must be hashable and have the same length as `data`.\n    Non-unique index values are allowed. Will default to\n    RangeIndex (0, 1, 2, ..., n) if not provided. If data is dict-like\n    and index is None, then the keys in the data are used as the index. If the\n    index is not None, the resulting Series is reindexed with the index values.\ndtype : str, numpy.dtype, or ExtensionDtype, optional\n    Data type for the output Series. If not specified, this will be\n    inferred from `data`.\n    See the :ref:`user guide <basics.dtypes>` for more usages.\nname : str, optional\n    The name to give to the Series.\ncopy : bool, default False\n    Copy input data. Only affects Series or 1d ndarray input. See examples.\n\nNotes\n-----\nPlease reference the :ref:`User Guide <basics.series>` for more information.\n\nExamples\n--------\nConstructing Series from a dictionary with an Index specified\n\n>>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['a', 'b', 'c'])\n>>> ser\na   1\nb   2\nc   3\ndtype: int64\n\nThe keys of the dictionary match with the Index values, hence the Index\nvalues have no effect.\n\n>>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> ser = pd.Series(data=d, index=['x', 'y', 'z'])\n>>> ser\nx   NaN\ny   NaN\nz   NaN\ndtype: float64\n\nNote that the Index is first build with the keys from the dictionary.\nAfter this the Series is reindexed with the given Index values, hence we\nget all NaN as a result.\n\nConstructing Series from a list with `copy=False`.\n\n>>> r = [1, 2]\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\n[1, 2]\n>>> ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `copy` of\nthe original data even though `copy=False`, so\nthe data is unchanged.\n\nConstructing Series from a 1d ndarray with `copy=False`.\n\n>>> r = np.array([1, 2])\n>>> ser = pd.Series(r, copy=False)\n>>> ser.iloc[0] = 999\n>>> r\narray([999,   2])\n>>> ser\n0    999\n1      2\ndtype: int64\n\nDue to input data type the Series has a `view` on\nthe original data, so\nthe data is changed as well.", "Library": "Pandas"}
{"API_Name": "pd.Series.sum", "Docstring": "Return the sum of the values over the requested axis.\n\nThis is equivalent to the method ``numpy.sum``.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\nmin_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``min_count`` non-NA values are present the result will be NA.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Series (if level specified)\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.sum()\n14\n\nBy default, the sum of an empty or all-NA Series is ``0``.\n\n>>> pd.Series([], dtype=\"float64\").sum()  # min_count=0 is the default\n0.0\n\nThis can be controlled with the ``min_count`` parameter. For example, if\nyou'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n\n>>> pd.Series([], dtype=\"float64\").sum(min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\nempty series identically.\n\n>>> pd.Series([np.nan]).sum()\n0.0\n\n>>> pd.Series([np.nan]).sum(min_count=1)\nnan", "Library": "Pandas"}
{"API_Name": "pd.Timestamp", "Docstring": "Pandas replacement for python datetime.datetime object.\n\nTimestamp is the pandas equivalent of python's Datetime\nand is interchangeable with it in most cases. It's the type used\nfor the entries that make up a DatetimeIndex, and other timeseries\noriented data structures in pandas.\n\nParameters\n----------\nts_input : datetime-like, str, int, float\n    Value to be converted to Timestamp.\nfreq : str, DateOffset\n    Offset which Timestamp will have.\ntz : str, pytz.timezone, dateutil.tz.tzfile or None\n    Time zone for time which Timestamp will have.\nunit : str\n    Unit used for conversion if ts_input is of type int or float. The\n    valid values are 'D', 'h', 'm', 's', 'ms', 'us', and 'ns'. For\n    example, 's' means seconds and 'ms' means milliseconds.\nyear, month, day : int\nhour, minute, second, microsecond : int, optional, default 0\nnanosecond : int, optional, default 0\ntzinfo : datetime.tzinfo, optional, default None\nfold : {0, 1}, default None, keyword-only\n    Due to daylight saving time, one wall clock time can occur twice\n    when shifting from summer to winter time; fold describes whether the\n    datetime-like corresponds  to the first (0) or the second time (1)\n    the wall clock hits the ambiguous time.\n\n    .. versionadded:: 1.1.0\n\nNotes\n-----\nThere are essentially three calling conventions for the constructor. The\nprimary form accepts four parameters. They can be passed by position or\nkeyword.\n\nThe other two forms mimic the parameters from ``datetime.datetime``. They\ncan be passed by either position or keyword, but not both mixed together.\n\nExamples\n--------\nUsing the primary calling convention:\n\nThis converts a datetime-like string\n\n>>> pd.Timestamp('2017-01-01T12')\nTimestamp('2017-01-01 12:00:00')\n\nThis converts a float representing a Unix epoch in units of seconds\n\n>>> pd.Timestamp(1513393355.5, unit='s')\nTimestamp('2017-12-16 03:02:35.500000')\n\nThis converts an int representing a Unix-epoch in units of seconds\nand for a particular timezone\n\n>>> pd.Timestamp(1513393355, unit='s', tz='US/Pacific')\nTimestamp('2017-12-15 19:02:35-0800', tz='US/Pacific')\n\nUsing the other two forms that mimic the API for ``datetime.datetime``:\n\n>>> pd.Timestamp(2017, 1, 1, 12)\nTimestamp('2017-01-01 12:00:00')\n\n>>> pd.Timestamp(year=2017, month=1, day=1, hour=12)\nTimestamp('2017-01-01 12:00:00')", "Library": "Pandas"}
{"API_Name": "pd.to_datetime", "Docstring": "Convert argument to datetime.\n\nThis function converts a scalar, array-like, :class:`Series` or\n:class:`DataFrame`/dict-like to a pandas datetime object.\n\nParameters\n----------\narg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like\n    The object to convert to a datetime. If a :class:`DataFrame` is provided, the\n    method expects minimally the following columns: :const:`\"year\"`,\n    :const:`\"month\"`, :const:`\"day\"`.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If :const:`'raise'`, then invalid parsing will raise an exception.\n    - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`.\n    - If :const:`'ignore'`, then invalid parsing will return the input.\ndayfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n    If :const:`True`, parses dates with the day first, e.g. :const:`\"10/11/12\"`\n    is parsed as :const:`2012-11-10`.\n\n    .. warning::\n\n        ``dayfirst=True`` is not strict, but will prefer to parse\n        with day first. If a delimited date string cannot be parsed in\n        accordance with the given `dayfirst` option, e.g.\n        ``to_datetime(['31-12-2021'])``, then a warning will be shown.\n\nyearfirst : bool, default False\n    Specify a date parse order if `arg` is str or is list-like.\n\n    - If :const:`True` parses dates with the year first, e.g.\n      :const:`\"10/11/12\"` is parsed as :const:`2010-11-12`.\n    - If both `dayfirst` and `yearfirst` are :const:`True`, `yearfirst` is\n      preceded (same as :mod:`dateutil`).\n\n    .. warning::\n\n        ``yearfirst=True`` is not strict, but will prefer to parse\n        with year first.\n\nutc : bool, default None\n    Control timezone-related parsing, localization and conversion.\n\n    - If :const:`True`, the function *always* returns a timezone-aware\n      UTC-localized :class:`Timestamp`, :class:`Series` or\n      :class:`DatetimeIndex`. To do this, timezone-naive inputs are\n      *localized* as UTC, while timezone-aware inputs are *converted* to UTC.\n\n    - If :const:`False` (default), inputs will not be coerced to UTC.\n      Timezone-naive inputs will remain naive, while timezone-aware ones\n      will keep their time offsets. Limitations exist for mixed\n      offsets (typically, daylight savings), see :ref:`Examples\n      <to_datetime_tz_examples>` section for details.\n\n    See also: pandas general documentation about `timezone conversion and\n    localization\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n    #time-zone-handling>`_.\n\nformat : str, default None\n    The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. Note that\n    :const:`\"%f\"` will parse all the way up to nanoseconds. See\n    `strftime documentation\n    <https://docs.python.org/3/library/datetime.html\n    #strftime-and-strptime-behavior>`_ for more information on choices.\nexact : bool, default True\n    Control how `format` is used:\n\n    - If :const:`True`, require an exact `format` match.\n    - If :const:`False`, allow the `format` to match anywhere in the target\n      string.\n\nunit : str, default 'ns'\n    The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n    integer or float number. This will be based off the origin.\n    Example, with ``unit='ms'`` and ``origin='unix'``, this would calculate\n    the number of milliseconds to the unix epoch start.\ninfer_datetime_format : bool, default False\n    If :const:`True` and no `format` is given, attempt to infer the format\n    of the datetime strings based on the first non-NaN element,\n    and if it can be inferred, switch to a faster method of parsing them.\n    In some cases this can increase the parsing speed by ~5-10x.\norigin : scalar, default 'unix'\n    Define the reference date. The numeric values would be parsed as number\n    of units (defined by `unit`) since this reference date.\n\n    - If :const:`'unix'` (or POSIX) time; origin is set to 1970-01-01.\n    - If :const:`'julian'`, unit must be :const:`'D'`, and origin is set to\n      beginning of Julian Calendar. Julian day number :const:`0` is assigned\n      to the day starting at noon on January 1, 4713 BC.\n    - If Timestamp convertible, origin is set to Timestamp identified by\n      origin.\ncache : bool, default True\n    If :const:`True`, use a cache of unique, converted dates to apply the\n    datetime conversion. May produce significant speed-up when parsing\n    duplicate date strings, especially ones with timezone offsets. The cache\n    is only used when there are at least 50 values. The presence of\n    out-of-bounds values will render the cache unusable and may slow down\n    parsing.\n\n    .. versionchanged:: 0.25.0\n        changed default value from :const:`False` to :const:`True`.\n\nReturns\n-------\ndatetime\n    If parsing succeeded.\n    Return type depends on input (types in parenthesis correspond to\n    fallback in case of unsuccessful timezone or out-of-range timestamp\n    parsing):\n\n    - scalar: :class:`Timestamp` (or :class:`datetime.datetime`)\n    - array-like: :class:`DatetimeIndex` (or :class:`Series` with\n      :class:`object` dtype containing :class:`datetime.datetime`)\n    - Series: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n    - DataFrame: :class:`Series` of :class:`datetime64` dtype (or\n      :class:`Series` of :class:`object` dtype containing\n      :class:`datetime.datetime`)\n\nRaises\n------\nParserError\n    When parsing a date from string fails.\nValueError\n    When another datetime conversion error happens. For example when one\n    of 'year', 'month', day' columns is missing in a :class:`DataFrame`, or\n    when a Timezone-aware :class:`datetime.datetime` is found in an array-like\n    of mixed time offsets, and ``utc=False``.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_timedelta : Convert argument to timedelta.\nconvert_dtypes : Convert dtypes.\n\nNotes\n-----\n\nMany input types are supported, and lead to different output types:\n\n- **scalars** can be int, float, str, datetime object (from stdlib :mod:`datetime`\n  module or :mod:`numpy`). They are converted to :class:`Timestamp` when\n  possible, otherwise they are converted to :class:`datetime.datetime`.\n  None/NaN/null scalars are converted to :const:`NaT`.\n\n- **array-like** can contain int, float, str, datetime objects. They are\n  converted to :class:`DatetimeIndex` when possible, otherwise they are\n  converted to :class:`Index` with :class:`object` dtype, containing\n  :class:`datetime.datetime`. None/NaN/null entries are converted to\n  :const:`NaT` in both cases.\n\n- **Series** are converted to :class:`Series` with :class:`datetime64`\n  dtype when possible, otherwise they are converted to :class:`Series` with\n  :class:`object` dtype, containing :class:`datetime.datetime`. None/NaN/null\n  entries are converted to :const:`NaT` in both cases.\n\n- **DataFrame/dict-like** are converted to :class:`Series` with\n  :class:`datetime64` dtype. For each row a datetime is created from assembling\n  the various dataframe columns. Column keys can be common abbreviations\n  like [\u2018year\u2019, \u2018month\u2019, \u2018day\u2019, \u2018minute\u2019, \u2018second\u2019, \u2018ms\u2019, \u2018us\u2019, \u2018ns\u2019]) or\n  plurals of the same.\n\nThe following causes are responsible for :class:`datetime.datetime` objects\nbeing returned (possibly inside an :class:`Index` or a :class:`Series` with\n:class:`object` dtype) instead of a proper pandas designated type\n(:class:`Timestamp`, :class:`DatetimeIndex` or :class:`Series`\nwith :class:`datetime64` dtype):\n\n- when any input element is before :const:`Timestamp.min` or after\n  :const:`Timestamp.max`, see `timestamp limitations\n  <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n  #timeseries-timestamp-limits>`_.\n\n- when ``utc=False`` (default) and the input is an array-like or\n  :class:`Series` containing mixed naive/aware datetime, or aware with mixed\n  time offsets. Note that this happens in the (quite frequent) situation when\n  the timezone has a daylight savings policy. In that case you may wish to\n  use ``utc=True``.\n\nExamples\n--------\n\n**Handling various input formats**\n\nAssembling a datetime from multiple columns of a :class:`DataFrame`. The keys\ncan be common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n'ms', 'us', 'ns']) or plurals of the same\n\n>>> df = pd.DataFrame({'year': [2015, 2016],\n...                    'month': [2, 3],\n...                    'day': [4, 5]})\n>>> pd.to_datetime(df)\n0   2015-02-04\n1   2016-03-05\ndtype: datetime64[ns]\n\nPassing ``infer_datetime_format=True`` can often-times speedup a parsing\nif its not an ISO8601 format exactly, but in a regular format.\n\n>>> s = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n>>> s.head()\n0    3/11/2000\n1    3/12/2000\n2    3/13/2000\n3    3/11/2000\n4    3/12/2000\ndtype: object\n\n>>> %timeit pd.to_datetime(s, infer_datetime_format=True)  # doctest: +SKIP\n100 loops, best of 3: 10.4 ms per loop\n\n>>> %timeit pd.to_datetime(s, infer_datetime_format=False)  # doctest: +SKIP\n1 loop, best of 3: 471 ms per loop\n\nUsing a unix epoch time\n\n>>> pd.to_datetime(1490195805, unit='s')\nTimestamp('2017-03-22 15:16:45')\n>>> pd.to_datetime(1490195805433502912, unit='ns')\nTimestamp('2017-03-22 15:16:45.433502912')\n\n.. warning:: For float arg, precision rounding might happen. To prevent\n    unexpected behavior use a fixed-width exact type.\n\nUsing a non-unix epoch origin\n\n>>> pd.to_datetime([1, 2, 3], unit='D',\n...                origin=pd.Timestamp('1960-01-01'))\nDatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],\n              dtype='datetime64[ns]', freq=None)\n\n**Non-convertible date/times**\n\nIf a date does not meet the `timestamp limitations\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n#timeseries-timestamp-limits>`_, passing ``errors='ignore'``\nwill return the original input instead of raising any exception.\n\nPassing ``errors='coerce'`` will force an out-of-bounds date to :const:`NaT`,\nin addition to forcing non-dates (or non-parseable dates) to :const:`NaT`.\n\n>>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore')\ndatetime.datetime(1300, 1, 1, 0, 0)\n>>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\nNaT\n\n.. _to_datetime_tz_examples:\n\n**Timezones and time offsets**\n\nThe default behaviour (``utc=False``) is as follows:\n\n- Timezone-naive inputs are converted to timezone-naive :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00:15'])\nDatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],\n              dtype='datetime64[ns]', freq=None)\n\n- Timezone-aware inputs *with constant time offset* are converted to\n  timezone-aware :class:`DatetimeIndex`:\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])\nDatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],\n              dtype='datetime64[ns, pytz.FixedOffset(-300)]', freq=None)\n\n- However, timezone-aware inputs *with mixed time offsets* (for example\n  issued from a timezone with daylight savings, such as Europe/Paris)\n  are **not successfully converted** to a :class:`DatetimeIndex`. Instead a\n  simple :class:`Index` containing :class:`datetime.datetime` objects is\n  returned:\n\n>>> pd.to_datetime(['2020-10-25 02:00 +0200', '2020-10-25 04:00 +0100'])\nIndex([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],\n      dtype='object')\n\n- A mix of timezone-aware and timezone-naive inputs is converted to\n  a timezone-aware :class:`DatetimeIndex` if the offsets of the timezone-aware\n  are constant:\n\n>>> from datetime import datetime\n>>> pd.to_datetime([\"2020-01-01 01:00 -01:00\", datetime(2020, 1, 1, 3, 0)])\nDatetimeIndex(['2020-01-01 01:00:00-01:00', '2020-01-01 02:00:00-01:00'],\n              dtype='datetime64[ns, pytz.FixedOffset(-60)]', freq=None)\n\n|\n\nSetting ``utc=True`` solves most of the above issues:\n\n- Timezone-naive inputs are *localized* as UTC\n\n>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Timezone-aware inputs are *converted* to UTC (the output represents the\n  exact same datetime, but viewed from the UTC time offset `+00:00`).\n\n>>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],\n...                utc=True)\nDatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)\n\n- Inputs can contain both naive and aware, string or datetime, the above\n  rules still apply\n\n>>> from datetime import timezone, timedelta\n>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 12:00 -0530',\n...                datetime(2020, 1, 1, 18),\n...                datetime(2020, 1, 1, 18,\n...                tzinfo=timezone(-timedelta(hours=1)))],\n...                utc=True)\nDatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 17:30:00+00:00',\n               '2020-01-01 18:00:00+00:00', '2020-01-01 19:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', freq=None)", "Library": "Pandas"}
{"API_Name": "pd.to_numeric", "Docstring": "Convert argument to a numeric type.\n\nThe default return dtype is `float64` or `int64`\ndepending on the data supplied. Use the `downcast` parameter\nto obtain other dtypes.\n\nPlease note that precision loss may occur if really large numbers\nare passed in. Due to the internal limitations of `ndarray`, if\nnumbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)\nor larger than `18446744073709551615` (np.iinfo(np.uint64).max) are\npassed in, it is very likely they will be converted to float so that\nthey can stored in an `ndarray`. These warnings apply similarly to\n`Series` since it internally leverages `ndarray`.\n\nParameters\n----------\narg : scalar, list, tuple, 1-d array, or Series\n    Argument to be converted.\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If 'raise', then invalid parsing will raise an exception.\n    - If 'coerce', then invalid parsing will be set as NaN.\n    - If 'ignore', then invalid parsing will return the input.\ndowncast : str, default None\n    Can be 'integer', 'signed', 'unsigned', or 'float'.\n    If not None, and if the data has been successfully cast to a\n    numerical dtype (or if the data was numeric to begin with),\n    downcast that resulting data to the smallest numerical dtype\n    possible according to the following rules:\n\n    - 'integer' or 'signed': smallest signed int dtype (min.: np.int8)\n    - 'unsigned': smallest unsigned int dtype (min.: np.uint8)\n    - 'float': smallest float dtype (min.: np.float32)\n\n    As this behaviour is separate from the core conversion to\n    numeric values, any errors raised during the downcasting\n    will be surfaced regardless of the value of the 'errors' input.\n\n    In addition, downcasting will only occur if the size\n    of the resulting data's dtype is strictly larger than\n    the dtype it is to be cast to, so if none of the dtypes\n    checked satisfy that specification, no downcasting will be\n    performed on the data.\n\nReturns\n-------\nret\n    Numeric if parsing succeeded.\n    Return type depends on input.  Series if Series, otherwise ndarray.\n\nSee Also\n--------\nDataFrame.astype : Cast argument to a specified dtype.\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\nDataFrame.convert_dtypes : Convert dtypes.\n\nExamples\n--------\nTake separate series and convert to numeric, coercing when told to\n\n>>> s = pd.Series(['1.0', '2', -3])\n>>> pd.to_numeric(s)\n0    1.0\n1    2.0\n2   -3.0\ndtype: float64\n>>> pd.to_numeric(s, downcast='float')\n0    1.0\n1    2.0\n2   -3.0\ndtype: float32\n>>> pd.to_numeric(s, downcast='signed')\n0    1\n1    2\n2   -3\ndtype: int8\n>>> s = pd.Series(['apple', '1.0', '2', -3])\n>>> pd.to_numeric(s, errors='ignore')\n0    apple\n1      1.0\n2        2\n3       -3\ndtype: object\n>>> pd.to_numeric(s, errors='coerce')\n0    NaN\n1    1.0\n2    2.0\n3   -3.0\ndtype: float64\n\nDowncasting of nullable integer and floating dtypes is supported:\n\n>>> s = pd.Series([1, 2, 3], dtype=\"Int64\")\n>>> pd.to_numeric(s, downcast=\"integer\")\n0    1\n1    2\n2    3\ndtype: Int8\n>>> s = pd.Series([1.0, 2.1, 3.0], dtype=\"Float64\")\n>>> pd.to_numeric(s, downcast=\"float\")\n0    1.0\n1    2.1\n2    3.0\ndtype: Float32", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.abs", "Docstring": "Return a Series/DataFrame with absolute numeric value of each element.\n\nThis function only applies to elements that are all numeric.\n\nReturns\n-------\nabs\n    Series/DataFrame containing the absolute value of each element.\n\nSee Also\n--------\nnumpy.absolute : Calculate the absolute value element-wise.\n\nNotes\n-----\nFor ``complex`` inputs, ``1.2 + 1j``, the absolute value is\n:math:`\\sqrt{ a^2 + b^2 }`.\n\nExamples\n--------\nAbsolute numeric values in a Series.\n\n>>> s = pd.Series([-1.10, 2, -3.33, 4])\n>>> s.abs()\n0    1.10\n1    2.00\n2    3.33\n3    4.00\ndtype: float64\n\nAbsolute numeric values in a Series with complex numbers.\n\n>>> s = pd.Series([1.2 + 1j])\n>>> s.abs()\n0    1.56205\ndtype: float64\n\nAbsolute numeric values in a Series with a Timedelta element.\n\n>>> s = pd.Series([pd.Timedelta('1 days')])\n>>> s.abs()\n0   1 days\ndtype: timedelta64[ns]\n\nSelect rows with data closest to certain value using argsort (from\n`StackOverflow <https://stackoverflow.com/a/17758115>`__).\n\n>>> df = pd.DataFrame({\n...     'a': [4, 5, 6, 7],\n...     'b': [10, 20, 30, 40],\n...     'c': [100, 50, -30, -50]\n... })\n>>> df\n     a    b    c\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n>>> df.loc[(df.c - 43).abs().argsort()]\n     a    b    c\n1    5   20   50\n0    4   10  100\n2    6   30  -30\n3    7   40  -50", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.add_prefix", "Docstring": "Prefix labels with string `prefix`.\n\nFor Series, the row labels are prefixed.\nFor DataFrame, the column labels are prefixed.\n\nParameters\n----------\nprefix : str\n    The string to add before each label.\n\nReturns\n-------\nSeries or DataFrame\n    New Series or DataFrame with updated labels.\n\nSee Also\n--------\nSeries.add_suffix: Suffix row labels with string `suffix`.\nDataFrame.add_suffix: Suffix column labels with string `suffix`.\n\nExamples\n--------\n>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64\n\n>>> s.add_prefix('item_')\nitem_0    1\nitem_1    2\nitem_2    3\nitem_3    4\ndtype: int64\n\n>>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6\n\n>>> df.add_prefix('col_')\n     col_A  col_B\n0       1       3\n1       2       4\n2       3       5\n3       4       6", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.add_suffix", "Docstring": "Suffix labels with string `suffix`.\n\nFor Series, the row labels are suffixed.\nFor DataFrame, the column labels are suffixed.\n\nParameters\n----------\nsuffix : str\n    The string to add after each label.\n\nReturns\n-------\nSeries or DataFrame\n    New Series or DataFrame with updated labels.\n\nSee Also\n--------\nSeries.add_prefix: Prefix row labels with string `prefix`.\nDataFrame.add_prefix: Prefix column labels with string `prefix`.\n\nExamples\n--------\n>>> s = pd.Series([1, 2, 3, 4])\n>>> s\n0    1\n1    2\n2    3\n3    4\ndtype: int64\n\n>>> s.add_suffix('_item')\n0_item    1\n1_item    2\n2_item    3\n3_item    4\ndtype: int64\n\n>>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]})\n>>> df\n   A  B\n0  1  3\n1  2  4\n2  3  5\n3  4  6\n\n>>> df.add_suffix('_col')\n     A_col  B_col\n0       1       3\n1       2       4\n2       3       5\n3       4       6", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.all", "Docstring": "Return whether all elements are True, potentially over an axis.\n\nReturns True unless there at least one element within a series or\nalong a Dataframe axis that is False or equivalent (e.g. zero or\nempty).\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns', None}, default 0\n    Indicate which axis or axes should be reduced. For `Series` this parameter\n    is unused and defaults to 0.\n\n    * 0 / 'index' : reduce the index, return a Series whose index is the\n      original column labels.\n    * 1 / 'columns' : reduce the columns, return a Series whose index is the\n      original index.\n    * None : reduce all axes, return a scalar.\n\nbool_only : bool, default None\n    Include only boolean columns. If None, will attempt to use everything,\n    then use only boolean data. Not implemented for Series.\nskipna : bool, default True\n    Exclude NA/null values. If the entire row/column is NA and skipna is\n    True, then the result will be True, as for an empty row/column.\n    If skipna is False, then NA are treated as True, because these are not\n    equal to zero.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\n**kwargs : any, default None\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\nSeries or DataFrame\n    If level is specified, then, DataFrame is returned; otherwise, Series\n    is returned.\n\nSee Also\n--------\nSeries.all : Return True if all elements are True.\nDataFrame.any : Return True if one (or more) elements are True.\n\nExamples\n--------\n**Series**\n\n>>> pd.Series([True, True]).all()\nTrue\n>>> pd.Series([True, False]).all()\nFalse\n>>> pd.Series([], dtype=\"float64\").all()\nTrue\n>>> pd.Series([np.nan]).all()\nTrue\n>>> pd.Series([np.nan]).all(skipna=False)\nTrue\n\n**DataFrames**\n\nCreate a dataframe from a dictionary.\n\n>>> df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]})\n>>> df\n   col1   col2\n0  True   True\n1  True  False\n\nDefault behaviour checks if values in each column all return True.\n\n>>> df.all()\ncol1     True\ncol2    False\ndtype: bool\n\nSpecify ``axis='columns'`` to check if values in each row all return True.\n\n>>> df.all(axis='columns')\n0     True\n1    False\ndtype: bool\n\nOr ``axis=None`` for whether every value is True.\n\n>>> df.all(axis=None)\nFalse", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.any", "Docstring": "Return whether any element is True, potentially over an axis.\n\nReturns False unless there is at least one element within a series or\nalong a Dataframe axis that is True or equivalent (e.g. non-zero or\nnon-empty).\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns', None}, default 0\n    Indicate which axis or axes should be reduced. For `Series` this parameter\n    is unused and defaults to 0.\n\n    * 0 / 'index' : reduce the index, return a Series whose index is the\n      original column labels.\n    * 1 / 'columns' : reduce the columns, return a Series whose index is the\n      original index.\n    * None : reduce all axes, return a scalar.\n\nbool_only : bool, default None\n    Include only boolean columns. If None, will attempt to use everything,\n    then use only boolean data. Not implemented for Series.\nskipna : bool, default True\n    Exclude NA/null values. If the entire row/column is NA and skipna is\n    True, then the result will be False, as for an empty row/column.\n    If skipna is False, then NA are treated as True, because these are not\n    equal to zero.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\n**kwargs : any, default None\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\nSeries or DataFrame\n    If level is specified, then, DataFrame is returned; otherwise, Series\n    is returned.\n\nSee Also\n--------\nnumpy.any : Numpy version of this method.\nSeries.any : Return whether any element is True.\nSeries.all : Return whether all elements are True.\nDataFrame.any : Return whether any element is True over requested axis.\nDataFrame.all : Return whether all elements are True over requested axis.\n\nExamples\n--------\n**Series**\n\nFor Series input, the output is a scalar indicating whether any element\nis True.\n\n>>> pd.Series([False, False]).any()\nFalse\n>>> pd.Series([True, False]).any()\nTrue\n>>> pd.Series([], dtype=\"float64\").any()\nFalse\n>>> pd.Series([np.nan]).any()\nFalse\n>>> pd.Series([np.nan]).any(skipna=False)\nTrue\n\n**DataFrame**\n\nWhether each column contains at least one True element (the default).\n\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]})\n>>> df\n   A  B  C\n0  1  0  0\n1  2  2  0\n\n>>> df.any()\nA     True\nB     True\nC    False\ndtype: bool\n\nAggregating over the columns.\n\n>>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]})\n>>> df\n       A  B\n0   True  1\n1  False  2\n\n>>> df.any(axis='columns')\n0    True\n1    True\ndtype: bool\n\n>>> df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]})\n>>> df\n       A  B\n0   True  1\n1  False  0\n\n>>> df.any(axis='columns')\n0    True\n1    False\ndtype: bool\n\nAggregating over the entire DataFrame with ``axis=None``.\n\n>>> df.any(axis=None)\nTrue\n\n`any` for an empty DataFrame is an empty Series.\n\n>>> pd.DataFrame([]).any()\nSeries([], dtype: bool)", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.apply", "Docstring": "Apply a function along an axis of the DataFrame.\n\nObjects passed to the function are Series objects whose index is\neither the DataFrame's index (``axis=0``) or the DataFrame's columns\n(``axis=1``). By default (``result_type=None``), the final return type\nis inferred from the return type of the applied function. Otherwise,\nit depends on the `result_type` argument.\n\nParameters\n----------\nfunc : function\n    Function to apply to each column or row.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Axis along which the function is applied:\n\n    * 0 or 'index': apply function to each column.\n    * 1 or 'columns': apply function to each row.\n\nraw : bool, default False\n    Determines if row or column is passed as a Series or ndarray object:\n\n    * ``False`` : passes each row or column as a Series to the\n      function.\n    * ``True`` : the passed function will receive ndarray objects\n      instead.\n      If you are just applying a NumPy reduction function this will\n      achieve much better performance.\n\nresult_type : {'expand', 'reduce', 'broadcast', None}, default None\n    These only act when ``axis=1`` (columns):\n\n    * 'expand' : list-like results will be turned into columns.\n    * 'reduce' : returns a Series if possible rather than expanding\n      list-like results. This is the opposite of 'expand'.\n    * 'broadcast' : results will be broadcast to the original shape\n      of the DataFrame, the original index and columns will be\n      retained.\n\n    The default behaviour (None) depends on the return value of the\n    applied function: list-like results will be returned as a Series\n    of those. However if the apply function returns a Series these\n    are expanded to columns.\nargs : tuple\n    Positional arguments to pass to `func` in addition to the\n    array/series.\n**kwargs\n    Additional keyword arguments to pass as keywords arguments to\n    `func`.\n\nReturns\n-------\nSeries or DataFrame\n    Result of applying ``func`` along the given axis of the\n    DataFrame.\n\nSee Also\n--------\nDataFrame.applymap: For elementwise operations.\nDataFrame.aggregate: Only perform aggregating type operations.\nDataFrame.transform: Only perform transforming type operations.\n\nNotes\n-----\nFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\nfor more details.\n\nExamples\n--------\n>>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n>>> df\n   A  B\n0  4  9\n1  4  9\n2  4  9\n\nUsing a numpy universal function (in this case the same as\n``np.sqrt(df)``):\n\n>>> df.apply(np.sqrt)\n     A    B\n0  2.0  3.0\n1  2.0  3.0\n2  2.0  3.0\n\nUsing a reducing function on either axis\n\n>>> df.apply(np.sum, axis=0)\nA    12\nB    27\ndtype: int64\n\n>>> df.apply(np.sum, axis=1)\n0    13\n1    13\n2    13\ndtype: int64\n\nReturning a list-like will result in a Series\n\n>>> df.apply(lambda x: [1, 2], axis=1)\n0    [1, 2]\n1    [1, 2]\n2    [1, 2]\ndtype: object\n\nPassing ``result_type='expand'`` will expand list-like results\nto columns of a Dataframe\n\n>>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n   0  1\n0  1  2\n1  1  2\n2  1  2\n\nReturning a Series inside the function is similar to passing\n``result_type='expand'``. The resulting column names\nwill be the Series index.\n\n>>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n   foo  bar\n0    1    2\n1    1    2\n2    1    2\n\nPassing ``result_type='broadcast'`` will ensure the same shape\nresult, whether list-like or scalar is returned by the function,\nand broadcast it along the axis. The resulting column names will\nbe the originals.\n\n>>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n   A  B\n0  1  2\n1  1  2\n2  1  2", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.asfreq", "Docstring": "Convert time series to specified frequency.\n\nReturns the original data conformed to a new index with the specified\nfrequency.\n\nIf the index of this DataFrame is a :class:`~pandas.PeriodIndex`, the new index\nis the result of transforming the original index with\n:meth:`PeriodIndex.asfreq <pandas.PeriodIndex.asfreq>` (so the original index\nwill map one-to-one to the new index).\n\nOtherwise, the new index will be equivalent to ``pd.date_range(start, end,\nfreq=freq)`` where ``start`` and ``end`` are, respectively, the first and\nlast entries in the original index (see :func:`pandas.date_range`). The\nvalues corresponding to any timesteps in the new index which were not present\nin the original index will be null (``NaN``), unless a method for filling\nsuch unknowns is provided (see the ``method`` parameter below).\n\nThe :meth:`resample` method is more appropriate if an operation on each group of\ntimesteps (such as an aggregate) is necessary to represent the data at the new\nfrequency.\n\nParameters\n----------\nfreq : DateOffset or str\n    Frequency DateOffset or string.\nmethod : {'backfill'/'bfill', 'pad'/'ffill'}, default None\n    Method to use for filling holes in reindexed Series (note this\n    does not fill NaNs that already were present):\n\n    * 'pad' / 'ffill': propagate last valid observation forward to next\n      valid\n    * 'backfill' / 'bfill': use NEXT valid observation to fill.\nhow : {'start', 'end'}, default end\n    For PeriodIndex only (see PeriodIndex.asfreq).\nnormalize : bool, default False\n    Whether to reset output index to midnight.\nfill_value : scalar, optional\n    Value to use for missing values, applied during upsampling (note\n    this does not fill NaNs that already were present).\n\nReturns\n-------\nDataFrame\n    DataFrame object reindexed to the specified frequency.\n\nSee Also\n--------\nreindex : Conform DataFrame to new index with optional filling logic.\n\nNotes\n-----\nTo learn more about the frequency strings, please see `this link\n<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n\nExamples\n--------\nStart by creating a series with 4 one minute timestamps.\n\n>>> index = pd.date_range('1/1/2000', periods=4, freq='T')\n>>> series = pd.Series([0.0, None, 2.0, 3.0], index=index)\n>>> df = pd.DataFrame({'s': series})\n>>> df\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:01:00    NaN\n2000-01-01 00:02:00    2.0\n2000-01-01 00:03:00    3.0\n\nUpsample the series into 30 second bins.\n\n>>> df.asfreq(freq='30S')\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    NaN\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    NaN\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    NaN\n2000-01-01 00:03:00    3.0\n\nUpsample again, providing a ``fill value``.\n\n>>> df.asfreq(freq='30S', fill_value=9.0)\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    9.0\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    9.0\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    9.0\n2000-01-01 00:03:00    3.0\n\nUpsample again, providing a ``method``.\n\n>>> df.asfreq(freq='30S', method='bfill')\n                       s\n2000-01-01 00:00:00    0.0\n2000-01-01 00:00:30    NaN\n2000-01-01 00:01:00    NaN\n2000-01-01 00:01:30    2.0\n2000-01-01 00:02:00    2.0\n2000-01-01 00:02:30    3.0\n2000-01-01 00:03:00    3.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.astype", "Docstring": "Cast a pandas object to a specified dtype ``dtype``.\n\nParameters\n----------\ndtype : data type, or dict of column name -> data type\n    Use a numpy.dtype or Python type to cast entire pandas object to\n    the same type. Alternatively, use {col: dtype, ...}, where col is a\n    column label and dtype is a numpy.dtype or Python type to cast one\n    or more of the DataFrame's columns to column-specific types.\ncopy : bool, default True\n    Return a copy when ``copy=True`` (be very careful setting\n    ``copy=False`` as changes to values then may propagate to other\n    pandas objects).\nerrors : {'raise', 'ignore'}, default 'raise'\n    Control raising of exceptions on invalid data for provided dtype.\n\n    - ``raise`` : allow exceptions to be raised\n    - ``ignore`` : suppress exceptions. On error return original object.\n\nReturns\n-------\ncasted : same type as caller\n\nSee Also\n--------\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nto_numeric : Convert argument to a numeric type.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\n\nNotes\n-----\n.. deprecated:: 1.3.0\n\n    Using ``astype`` to convert from timezone-naive dtype to\n    timezone-aware dtype is deprecated and will raise in a\n    future version.  Use :meth:`Series.dt.tz_localize` instead.\n\nExamples\n--------\nCreate a DataFrame:\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nCast all columns to int32:\n\n>>> df.astype('int32').dtypes\ncol1    int32\ncol2    int32\ndtype: object\n\nCast col1 to int32 using a dictionary:\n\n>>> df.astype({'col1': 'int32'}).dtypes\ncol1    int32\ncol2    int64\ndtype: object\n\nCreate a series:\n\n>>> ser = pd.Series([1, 2], dtype='int32')\n>>> ser\n0    1\n1    2\ndtype: int32\n>>> ser.astype('int64')\n0    1\n1    2\ndtype: int64\n\nConvert to categorical type:\n\n>>> ser.astype('category')\n0    1\n1    2\ndtype: category\nCategories (2, int64): [1, 2]\n\nConvert to ordered categorical type with custom ordering:\n\n>>> from pandas.api.types import CategoricalDtype\n>>> cat_dtype = CategoricalDtype(\n...     categories=[2, 1], ordered=True)\n>>> ser.astype(cat_dtype)\n0    1\n1    2\ndtype: category\nCategories (2, int64): [2 < 1]\n\nNote that using ``copy=False`` and changing data on a new\npandas object may propagate changes:\n\n>>> s1 = pd.Series([1, 2])\n>>> s2 = s1.astype('int64', copy=False)\n>>> s2[0] = 10\n>>> s1  # note that s1[0] has changed too\n0    10\n1     2\ndtype: int64\n\nCreate a series of dates:\n\n>>> ser_date = pd.Series(pd.date_range('20200101', periods=3))\n>>> ser_date\n0   2020-01-01\n1   2020-01-02\n2   2020-01-03\ndtype: datetime64[ns]", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.at", "Docstring": "Access a single value for a row/column label pair.\n\nSimilar to ``loc``, in that both provide label-based lookups. Use\n``at`` if you only need to get or set a single value in a DataFrame\nor Series.\n\nRaises\n------\nKeyError\n    * If getting a value and 'label' does not exist in a DataFrame or\n        Series.\nValueError\n    * If row/column label pair is not a tuple or if any label from\n        the pair is not a scalar for DataFrame.\n    * If label is list-like (*excluding* NamedTuple) for Series.\n\nSee Also\n--------\nDataFrame.at : Access a single value for a row/column pair by label.\nDataFrame.iat : Access a single value for a row/column pair by integer\n    position.\nDataFrame.loc : Access a group of rows and columns by label(s).\nDataFrame.iloc : Access a group of rows and columns by integer\n    position(s).\nSeries.at : Access a single value by label.\nSeries.iat : Access a single value by integer position.\nSeries.loc : Access a group of rows by label(s).\nSeries.iloc : Access a group of rows by integer position(s).\n\nNotes\n-----\nSee :ref:`Fast scalar value getting and setting <indexing.basics.get_value>`\nfor more details.\n\nExamples\n--------\n>>> df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n...                   index=[4, 5, 6], columns=['A', 'B', 'C'])\n>>> df\n    A   B   C\n4   0   2   3\n5   0   4   1\n6  10  20  30\n\nGet value at specified row/column pair\n\n>>> df.at[4, 'B']\n2\n\nSet value at specified row/column pair\n\n>>> df.at[4, 'B'] = 10\n>>> df.at[4, 'B']\n10\n\nGet value within a Series\n\n>>> df.loc[5].at['B']\n4", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.columns", "Docstring": "The column labels of the DataFrame.", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.copy", "Docstring": "Make a copy of this object's indices and data.\n\nWhen ``deep=True`` (default), a new object will be created with a\ncopy of the calling object's data and indices. Modifications to\nthe data or indices of the copy will not be reflected in the\noriginal object (see notes below).\n\nWhen ``deep=False``, a new object will be created without copying\nthe calling object's data or index (only references to the data\nand index are copied). Any changes to the data of the original\nwill be reflected in the shallow copy (and vice versa).\n\nParameters\n----------\ndeep : bool, default True\n    Make a deep copy, including a copy of the data and the indices.\n    With ``deep=False`` neither the indices nor the data are copied.\n\nReturns\n-------\ncopy : Series or DataFrame\n    Object type matches caller.\n\nNotes\n-----\nWhen ``deep=True``, data is copied but actual Python objects\nwill not be copied recursively, only the reference to the object.\nThis is in contrast to `copy.deepcopy` in the Standard Library,\nwhich recursively copies object data (see examples below).\n\nWhile ``Index`` objects are copied when ``deep=True``, the underlying\nnumpy array is not copied for performance reasons. Since ``Index`` is\nimmutable, the underlying data can be safely shared and a copy\nis not needed.\n\nSince pandas is not thread safe, see the\n:ref:`gotchas <gotchas.thread-safety>` when copying in a threading\nenvironment.\n\nExamples\n--------\n>>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n>>> s\na    1\nb    2\ndtype: int64\n\n>>> s_copy = s.copy()\n>>> s_copy\na    1\nb    2\ndtype: int64\n\n**Shallow copy versus default (deep) copy:**\n\n>>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n>>> deep = s.copy()\n>>> shallow = s.copy(deep=False)\n\nShallow copy shares data and index with original.\n\n>>> s is shallow\nFalse\n>>> s.values is shallow.values and s.index is shallow.index\nTrue\n\nDeep copy has own copy of data and index.\n\n>>> s is deep\nFalse\n>>> s.values is deep.values or s.index is deep.index\nFalse\n\nUpdates to the data shared by shallow copy and original is reflected\nin both; deep copy remains unchanged.\n\n>>> s[0] = 3\n>>> shallow[1] = 4\n>>> s\na    3\nb    4\ndtype: int64\n>>> shallow\na    3\nb    4\ndtype: int64\n>>> deep\na    1\nb    2\ndtype: int64\n\nNote that when copying an object containing Python objects, a deep copy\nwill copy the data, but will not do so recursively. Updating a nested\ndata object will be reflected in the deep copy.\n\n>>> s = pd.Series([[1, 2], [3, 4]])\n>>> deep = s.copy()\n>>> s[0][0] = 10\n>>> s\n0    [10, 2]\n1     [3, 4]\ndtype: object\n>>> deep\n0    [10, 2]\n1     [3, 4]\ndtype: object", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.corr", "Docstring": "Compute pairwise correlation of columns, excluding NA/null values.\n\nParameters\n----------\nmethod : {'pearson', 'kendall', 'spearman'} or callable\n    Method of correlation:\n\n    * pearson : standard correlation coefficient\n    * kendall : Kendall Tau correlation coefficient\n    * spearman : Spearman rank correlation\n    * callable: callable with input two 1d ndarrays\n        and returning a float. Note that the returned matrix from corr\n        will have 1 along the diagonals and will be symmetric\n        regardless of the callable's behavior.\nmin_periods : int, optional\n    Minimum number of observations required per pair of columns\n    to have a valid result. Currently only available for Pearson\n    and Spearman correlation.\nnumeric_only : bool, default True\n    Include only `float`, `int` or `boolean` data.\n\n    .. versionadded:: 1.5.0\n\n    .. deprecated:: 1.5.0\n        The default value of ``numeric_only`` will be ``False`` in a future\n        version of pandas.\n\nReturns\n-------\nDataFrame\n    Correlation matrix.\n\nSee Also\n--------\nDataFrame.corrwith : Compute pairwise correlation with another\n    DataFrame or Series.\nSeries.corr : Compute the correlation between two Series.\n\nNotes\n-----\nPearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.\n\n* `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_\n* `Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_\n* `Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_\n\nExamples\n--------\n>>> def histogram_intersection(a, b):\n...     v = np.minimum(a, b).sum().round(decimals=1)\n...     return v\n>>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(method=histogram_intersection)\n      dogs  cats\ndogs   1.0   0.3\ncats   0.3   1.0\n\n>>> df = pd.DataFrame([(1, 1), (2, np.nan), (np.nan, 3), (4, 4)],\n...                   columns=['dogs', 'cats'])\n>>> df.corr(min_periods=3)\n      dogs  cats\ndogs   1.0   NaN\ncats   NaN   1.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.count", "Docstring": "Count non-NA cells for each column or row.\n\nThe values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending\non `pandas.options.mode.use_inf_as_na`) are considered NA.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    If 0 or 'index' counts are generated for each column.\n    If 1 or 'columns' counts are generated for each row.\nlevel : int or str, optional\n    If the axis is a `MultiIndex` (hierarchical), count along a\n    particular `level`, collapsing into a `DataFrame`.\n    A `str` specifies the level name.\nnumeric_only : bool, default False\n    Include only `float`, `int` or `boolean` data.\n\nReturns\n-------\nSeries or DataFrame\n    For each column/row the number of non-NA/null entries.\n    If `level` is specified returns a `DataFrame`.\n\nSee Also\n--------\nSeries.count: Number of non-NA elements in a Series.\nDataFrame.value_counts: Count unique combinations of columns.\nDataFrame.shape: Number of DataFrame rows and columns (including NA\n    elements).\nDataFrame.isna: Boolean same-sized DataFrame showing places of NA\n    elements.\n\nExamples\n--------\nConstructing DataFrame from a dictionary:\n\n>>> df = pd.DataFrame({\"Person\":\n...                    [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"],\n...                    \"Age\": [24., np.nan, 21., 33, 26],\n...                    \"Single\": [False, True, True, True, False]})\n>>> df\n   Person   Age  Single\n0    John  24.0   False\n1    Myla   NaN    True\n2   Lewis  21.0    True\n3    John  33.0    True\n4    Myla  26.0   False\n\nNotice the uncounted NA values:\n\n>>> df.count()\nPerson    5\nAge       4\nSingle    5\ndtype: int64\n\nCounts for each **row**:\n\n>>> df.count(axis='columns')\n0    3\n1    2\n2    3\n3    3\n4    3\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.cumsum", "Docstring": "Return cumulative sum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nsum.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The index or the name of the axis. 0 is equivalent to None or 'index'.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\n*args, **kwargs\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\nSeries or DataFrame\n    Return cumulative sum of Series or DataFrame.\n\nSee Also\n--------\ncore.window.expanding.Expanding.sum : Similar functionality\n    but ignores ``NaN`` values.\nDataFrame.sum : Return the sum over\n    DataFrame axis.\nDataFrame.cummax : Return cumulative maximum over DataFrame axis.\nDataFrame.cummin : Return cumulative minimum over DataFrame axis.\nDataFrame.cumsum : Return cumulative sum over DataFrame axis.\nDataFrame.cumprod : Return cumulative product over DataFrame axis.\n\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumsum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumsum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the sum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cumsum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0\n\nTo iterate over columns and find the sum in each row,\nuse ``axis=1``\n\n>>> df.cumsum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.diff", "Docstring": "First discrete difference of element.\n\nCalculates the difference of a DataFrame element compared with another\nelement in the DataFrame (default is element in previous row).\n\nParameters\n----------\nperiods : int, default 1\n    Periods to shift for calculating difference, accepts negative\n    values.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Take difference over rows (0) or columns (1).\n\nReturns\n-------\nDataFrame\n    First differences of the Series.\n\nSee Also\n--------\nDataFrame.pct_change: Percent change over given number of periods.\nDataFrame.shift: Shift index by desired number of periods with an\n    optional time freq.\nSeries.diff: First discrete difference of object.\n\nNotes\n-----\nFor boolean dtypes, this uses :meth:`operator.xor` rather than\n:meth:`operator.sub`.\nThe result is calculated according to current dtype in DataFrame,\nhowever dtype of the result is always float64.\n\nExamples\n--------\n\nDifference with previous row\n\n>>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n...                    'b': [1, 1, 2, 3, 5, 8],\n...                    'c': [1, 4, 9, 16, 25, 36]})\n>>> df\n   a  b   c\n0  1  1   1\n1  2  1   4\n2  3  2   9\n3  4  3  16\n4  5  5  25\n5  6  8  36\n\n>>> df.diff()\n     a    b     c\n0  NaN  NaN   NaN\n1  1.0  0.0   3.0\n2  1.0  1.0   5.0\n3  1.0  1.0   7.0\n4  1.0  2.0   9.0\n5  1.0  3.0  11.0\n\nDifference with previous column\n\n>>> df.diff(axis=1)\n    a  b   c\n0 NaN  0   0\n1 NaN -1   3\n2 NaN -1   7\n3 NaN -1  13\n4 NaN  0  20\n5 NaN  2  28\n\nDifference with 3rd previous row\n\n>>> df.diff(periods=3)\n     a    b     c\n0  NaN  NaN   NaN\n1  NaN  NaN   NaN\n2  NaN  NaN   NaN\n3  3.0  2.0  15.0\n4  3.0  4.0  21.0\n5  3.0  6.0  27.0\n\nDifference with following row\n\n>>> df.diff(periods=-1)\n     a    b     c\n0 -1.0  0.0  -3.0\n1 -1.0 -1.0  -5.0\n2 -1.0 -1.0  -7.0\n3 -1.0 -2.0  -9.0\n4 -1.0 -3.0 -11.0\n5  NaN  NaN   NaN\n\nOverflow in input dtype\n\n>>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8)\n>>> df.diff()\n       a\n0    NaN\n1  255.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.div", "Docstring": "Get Floating division of dataframe and other, element-wise (binary operator `truediv`).\n\nEquivalent to ``dataframe / other``, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, `rtruediv`.\n\nAmong flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\narithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n\nParameters\n----------\nother : scalar, sequence, Series, dict or DataFrame\n    Any single or multiple element data structure, or list-like object.\naxis : {0 or 'index', 1 or 'columns'}\n    Whether to compare by the index (0 or 'index') or columns.\n    (1 or 'columns'). For Series input, axis to match Series index on.\nlevel : int or label\n    Broadcast across a level, matching Index values on the\n    passed MultiIndex level.\nfill_value : float or None, default None\n    Fill existing missing (NaN) values, and any new element needed for\n    successful DataFrame alignment, with this value before computation.\n    If data in both corresponding DataFrame locations is missing\n    the result will be missing.\n\nReturns\n-------\nDataFrame\n    Result of the arithmetic operation.\n\nSee Also\n--------\nDataFrame.add : Add DataFrames.\nDataFrame.sub : Subtract DataFrames.\nDataFrame.mul : Multiply DataFrames.\nDataFrame.div : Divide DataFrames (float division).\nDataFrame.truediv : Divide DataFrames (float division).\nDataFrame.floordiv : Divide DataFrames (integer division).\nDataFrame.mod : Calculate modulo (remainder after division).\nDataFrame.pow : Calculate exponential power.\n\nNotes\n-----\nMismatched indices will be unioned together.\n\nExamples\n--------\n>>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360\n\nAdd a scalar with operator version which return the same\nresults.\n\n>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361\n\n>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361\n\nDivide by constant with reverse version.\n\n>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0\n\n>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778\n\nSubtract a list and Series by axis with operator version.\n\n>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358\n\n>>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358\n\n>>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359\n\nMultiply a dictionary by axis.\n\n>>> df.mul({'angles': 0, 'degrees': 2})\n            angles      degrees\ncircle           0          720\ntriangle             0      360\nrectangle            0      720\n\n>>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles      degrees\ncircle               0        0\ntriangle             6      360\nrectangle           12     1080\n\nMultiply a DataFrame of different shape with operator version.\n\n>>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4\n\n>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN\n\n>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0\n\nDivide by a MultiIndex by level.\n\n>>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720\n\n>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.drop", "Docstring": "Drop specified labels from rows or columns.\n\nRemove rows or columns by specifying label names and corresponding\naxis, or by specifying directly index or column names. When using a\nmulti-index, labels on different levels can be removed by specifying\nthe level. See the `user guide <advanced.shown_levels>`\nfor more information about the now unused levels.\n\nParameters\n----------\nlabels : single label or list-like\n    Index or column labels to drop. A tuple will be used as a single\n    label and not treated as a list-like.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Whether to drop labels from the index (0 or 'index') or\n    columns (1 or 'columns').\nindex : single label or list-like\n    Alternative to specifying axis (``labels, axis=0``\n    is equivalent to ``index=labels``).\ncolumns : single label or list-like\n    Alternative to specifying axis (``labels, axis=1``\n    is equivalent to ``columns=labels``).\nlevel : int or level name, optional\n    For MultiIndex, level from which the labels will be removed.\ninplace : bool, default False\n    If False, return a copy. Otherwise, do operation\n    inplace and return None.\nerrors : {'ignore', 'raise'}, default 'raise'\n    If 'ignore', suppress error and only existing labels are\n    dropped.\n\nReturns\n-------\nDataFrame or None\n    DataFrame without the removed index or column labels or\n    None if ``inplace=True``.\n\nRaises\n------\nKeyError\n    If any of the labels is not found in the selected axis.\n\nSee Also\n--------\nDataFrame.loc : Label-location based indexer for selection by label.\nDataFrame.dropna : Return DataFrame with labels on given axis omitted\n    where (all or any) data are missing.\nDataFrame.drop_duplicates : Return DataFrame with duplicate rows\n    removed, optionally only considering certain columns.\nSeries.drop : Return Series with specified index labels removed.\n\nExamples\n--------\n>>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n...                   columns=['A', 'B', 'C', 'D'])\n>>> df\n   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n\nDrop columns\n\n>>> df.drop(['B', 'C'], axis=1)\n   A   D\n0  0   3\n1  4   7\n2  8  11\n\n>>> df.drop(columns=['B', 'C'])\n   A   D\n0  0   3\n1  4   7\n2  8  11\n\nDrop a row by index\n\n>>> df.drop([0, 1])\n   A  B   C   D\n2  8  9  10  11\n\nDrop columns and/or rows of MultiIndex DataFrame\n\n>>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n...                              ['speed', 'weight', 'length']],\n...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n>>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n...                         [250, 150], [1.5, 0.8], [320, 250],\n...                         [1, 0.8], [0.3, 0.2]])\n>>> df\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8\n        length  0.3     0.2\n\nDrop a specific index combination from the MultiIndex\nDataFrame, i.e., drop the combination ``'falcon'`` and\n``'weight'``, which deletes only the corresponding row\n\n>>> df.drop(index=('falcon', 'weight'))\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\n        length  1.5     1.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\n        length  1.5     0.8\nfalcon  speed   320.0   250.0\n        length  0.3     0.2\n\n>>> df.drop(index='cow', columns='small')\n                big\nlama    speed   45.0\n        weight  200.0\n        length  1.5\nfalcon  speed   320.0\n        weight  1.0\n        length  0.3\n\n>>> df.drop(index='length', level=1)\n                big     small\nlama    speed   45.0    30.0\n        weight  200.0   100.0\ncow     speed   30.0    20.0\n        weight  250.0   150.0\nfalcon  speed   320.0   250.0\n        weight  1.0     0.8", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.drop_duplicates", "Docstring": "Convert to Index using specified date_format. Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in `python string format doc <https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior>`__. Formats supported by the C `strftime` API but not by the python string format doc (such as `\"%R\"`, `\"%r\"`) are not officially supported and should be preferably replaced with their supported equivalents (such as `\"%H:%M\"`, `\"%I:%M:%S %p\"`). Note that `PeriodIndex` support additional directives, detailed in `Period.strftime`. Parameters ---------- date_format : str Date format string (e.g. \"%Y-%m-%d\"). Returns ------- ndarray[object] NumPy ndarray of formatted strings. See Also -------- to_datetime : Convert the given argument to datetime. DatetimeIndex.normalize : Return DatetimeIndex with times to midnight. DatetimeIndex.round : Round the DatetimeIndex to the specified freq. DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq. Timestamp.strftime : Format a single Timestamp. Period.strftime : Format a single Period. Examples -------- >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"), ...                     periods=3, freq='s') >>> rng.strftime('%B %d, %Y, %r') Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM', 'March 10, 2018, 09:00:02 AM'], dtype='object')", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.dropna", "Docstring": "Remove missing values.\n\nSee the :ref:`User Guide <missing_data>` for more on which values are\nconsidered missing, and how to work with missing data.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Determine if rows or columns which contain missing values are\n    removed.\n\n    * 0, or 'index' : Drop rows which contain missing values.\n    * 1, or 'columns' : Drop columns which contain missing value.\n\n    .. versionchanged:: 1.0.0\n\n       Pass tuple or list to drop on multiple axes.\n       Only a single axis is allowed.\n\nhow : {'any', 'all'}, default 'any'\n    Determine if row or column is removed from DataFrame, when we have\n    at least one NA or all NA.\n\n    * 'any' : If any NA values are present, drop that row or column.\n    * 'all' : If all values are NA, drop that row or column.\n\nthresh : int, optional\n    Require that many non-NA values. Cannot be combined with how.\nsubset : column label or sequence of labels, optional\n    Labels along other axis to consider, e.g. if you are dropping rows\n    these would be a list of columns to include.\ninplace : bool, default False\n    Whether to modify the DataFrame rather than creating a new one.\n\nReturns\n-------\nDataFrame or None\n    DataFrame with NA entries dropped from it or None if ``inplace=True``.\n\nSee Also\n--------\nDataFrame.isna: Indicate missing values.\nDataFrame.notna : Indicate existing (non-missing) values.\nDataFrame.fillna : Replace missing values.\nSeries.dropna : Drop missing values.\nIndex.dropna : Drop missing indices.\n\nExamples\n--------\n>>> df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n...                    \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n...                    \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"),\n...                             pd.NaT]})\n>>> df\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n\nDrop the rows where at least one element is missing.\n\n>>> df.dropna()\n     name        toy       born\n1  Batman  Batmobile 1940-04-25\n\nDrop the columns where at least one element is missing.\n\n>>> df.dropna(axis='columns')\n       name\n0    Alfred\n1    Batman\n2  Catwoman\n\nDrop the rows where all elements are missing.\n\n>>> df.dropna(how='all')\n       name        toy       born\n0    Alfred        NaN        NaT\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n\nKeep only the rows with at least 2 non-NA values.\n\n>>> df.dropna(thresh=2)\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n\nDefine in which columns to look for missing values.\n\n>>> df.dropna(subset=['name', 'toy'])\n       name        toy       born\n1    Batman  Batmobile 1940-04-25\n2  Catwoman   Bullwhip        NaT\n\nKeep the DataFrame with valid entries in the same variable.\n\n>>> df.dropna(inplace=True)\n>>> df\n     name        toy       born\n1  Batman  Batmobile 1940-04-25", "Library": "Pandas"}
{"API_Name": "pandas.Series.dt.strftime", "Docstring": "Convert to Index using specified date_format. Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in `python string format doc <https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior>`__. Formats supported by the C `strftime` API but not by the python string format doc (such as `\"%R\"`, `\"%r\"`) are not officially supported and should be preferably replaced with their supported equivalents (such as `\"%H:%M\"`, `\"%I:%M:%S %p\"`). Note that `PeriodIndex` support additional directives, detailed in `Period.strftime`. Parameters ---------- date_format : str Date format string (e.g. \"%Y-%m-%d\"). Returns ------- ndarray[object] NumPy ndarray of formatted strings. See Also -------- to_datetime : Convert the given argument to datetime. DatetimeIndex.normalize : Return DatetimeIndex with times to midnight. DatetimeIndex.round : Round the DatetimeIndex to the specified freq. DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq. Timestamp.strftime : Format a single Timestamp. Period.strftime : Format a single Period. Examples -------- >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"), ...                     periods=3, freq='s') >>> rng.strftime('%B %d, %Y, %r') Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM', 'March 10, 2018, 09:00:02 AM'], dtype='object')", "Library": "Pandas"}
{"API_Name": "pandas.Series.dt.tz_localize", "Docstring": "Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index. This method takes a time zone (tz) naive Datetime Array/Index object and makes this time zone aware. It does not move the time to another time zone. This method can also be used to do the inverse -- to create a time zone unaware object from an aware object. To that end, pass `tz=None`. Parameters ---------- tz : str, pytz.timezone, dateutil.tz.tzfile, datetime.tzinfo or None Time zone to convert timestamps to. Passing ``None`` will remove the time zone information preserving local time. ambiguous : 'infer', 'NaT', bool array, default 'raise' When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the `ambiguous` parameter dictates how ambiguous times should be handled. - 'infer' will attempt to infer fall dst-transition hours based on order - bool-ndarray where True signifies a DST time, False signifies a non-DST time (note that this flag is only applicable for ambiguous times) - 'NaT' will return NaT where there are ambiguous times - 'raise' will raise an AmbiguousTimeError if there are ambiguous times. nonexistent : 'shift_forward', 'shift_backward, 'NaT', timedelta, default 'raise' A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. - 'shift_forward' will shift the nonexistent time forward to the closest existing time - 'shift_backward' will shift the nonexistent time backward to the closest existing time - 'NaT' will return NaT where there are nonexistent times - timedelta objects will shift nonexistent times by the timedelta - 'raise' will raise an NonExistentTimeError if there are nonexistent times. Returns ------- Same type as self Array/Index converted to the specified time zone. Raises ------ TypeError If the Datetime Array/Index is tz-aware and tz is not None. See Also -------- DatetimeIndex.tz_convert : Convert tz-aware DatetimeIndex from one time zone to another. Examples -------- >>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3) >>> tz_naive DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00', '2018-03-03 09:00:00'], dtype='datetime64[ns]', freq='D') Localize DatetimeIndex in US/Eastern time zone: >>> tz_aware = tz_naive.tz_localize(tz='US/Eastern') >>> tz_aware DatetimeIndex(['2018-03-01 09:00:00-05:00', '2018-03-02 09:00:00-05:00', '2018-03-03 09:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None) With the ``tz=None``, we can remove the time zone information while keeping the local time (not converted to UTC): >>> tz_aware.tz_localize(None) DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00', '2018-03-03 09:00:00'], dtype='datetime64[ns]', freq=None) Be careful with DST changes. When there is sequential data, pandas can infer the DST time: >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00', ...                               '2018-10-28 02:00:00', ...                               '2018-10-28 02:30:00', ...                               '2018-10-28 02:00:00', ...                               '2018-10-28 02:30:00', ...                               '2018-10-28 03:00:00', ...                               '2018-10-28 03:30:00'])) >>> s.dt.tz_localize('CET', ambiguous='infer') 0   2018-10-28 01:30:00+02:00 1   2018-10-28 02:00:00+02:00 2   2018-10-28 02:30:00+02:00 3   2018-10-28 02:00:00+01:00 4   2018-10-28 02:30:00+01:00 5   2018-10-28 03:00:00+01:00 6   2018-10-28 03:30:00+01:00 dtype: datetime64[ns, CET] In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly >>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00', ...                               '2018-10-28 02:36:00', ...                               '2018-10-28 03:46:00'])) >>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False])) 0   2018-10-28 01:20:00+02:00 1   2018-10-28 02:36:00+02:00 2   2018-10-28 03:46:00+01:00 dtype: datetime64[ns, CET] If the DST transition causes nonexistent times, you can shift these dates forward or backwards with a timedelta object or `'shift_forward'` or `'shift_backwards'`. >>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00', ...                               '2015-03-29 03:30:00'])) >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward') 0   2015-03-29 03:00:00+02:00 1   2015-03-29 03:30:00+02:00 dtype: datetime64[ns, Europe/Warsaw] >>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward') 0   2015-03-29 01:59:59.999999999+01:00 1   2015-03-29 03:30:00+02:00 dtype: datetime64[ns, Europe/Warsaw] >>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H')) 0   2015-03-29 03:30:00+02:00 1   2015-03-29 03:30:00+02:00 dtype: datetime64[ns, Europe/Warsaw]", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.duplicated", "Docstring": "Return boolean Series denoting duplicate rows.\n\nConsidering certain columns is optional.\n\nParameters\n----------\nsubset : column label or sequence of labels, optional\n    Only consider certain columns for identifying duplicates, by\n    default use all of the columns.\nkeep : {'first', 'last', False}, default 'first'\n    Determines which duplicates (if any) to mark.\n\n    - ``first`` : Mark duplicates as ``True`` except for the first occurrence.\n    - ``last`` : Mark duplicates as ``True`` except for the last occurrence.\n    - False : Mark all duplicates as ``True``.\n\nReturns\n-------\nSeries\n    Boolean series for each duplicated rows.\n\nSee Also\n--------\nIndex.duplicated : Equivalent method on index.\nSeries.duplicated : Equivalent method on Series.\nSeries.drop_duplicates : Remove duplicate values from Series.\nDataFrame.drop_duplicates : Remove duplicate values from DataFrame.\n\nExamples\n--------\nConsider dataset containing ramen rating.\n\n>>> df = pd.DataFrame({\n...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n...     'rating': [4, 4, 3.5, 15, 5]\n... })\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\n\nBy default, for each set of duplicated values, the first occurrence\nis set on False and all others on True.\n\n>>> df.duplicated()\n0    False\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\nBy using 'last', the last occurrence of each set of duplicated values\nis set on False and all others on True.\n\n>>> df.duplicated(keep='last')\n0     True\n1    False\n2    False\n3    False\n4    False\ndtype: bool\n\nBy setting ``keep`` on False, all duplicates are True.\n\n>>> df.duplicated(keep=False)\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\nTo find duplicates on specific column(s), use ``subset``.\n\n>>> df.duplicated(subset=['brand'])\n0    False\n1     True\n2    False\n3     True\n4     True\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.Series.str.extract", "Docstring": "Extract capture groups in the regex `pat` as columns in a DataFrame. For each subject string in the Series, extract groups from the first match of regular expression `pat`. Parameters ---------- pat : str Regular expression pattern with capturing groups. flags : int, default 0 (no flags) Flags from the ``re`` module, e.g. ``re.IGNORECASE``, that modify regular expression matching for things like case, spaces, etc. For more details, see :mod:`re`. expand : bool, default True If True, return DataFrame with one column per capture group. If False, return a Series/Index if there is one capture group or DataFrame if there are multiple capture groups. Returns ------- DataFrame or Series or Index A DataFrame with one row for each subject string, and one column for each group. Any capture group names in regular expression pat will be used for column names; otherwise capture group numbers will be used. The dtype of each result column is always object, even when no match is found. If ``expand=False`` and pat has only one capture group, then return a Series (if subject is a Series) or Index (if subject is an Index). See Also -------- extractall : Returns all matches (not just the first match). Examples -------- A pattern with two groups will return a DataFrame with two columns. Non-matches will be NaN. >>> s = pd.Series(['a1', 'b2', 'c3']) >>> s.str.extract(r'([ab])(\\d)') 0    1 0    a    1 1    b    2 2  NaN  NaN A pattern may contain optional groups. >>> s.str.extract(r'([ab])?(\\d)') 0  1 0    a  1 1    b  2 2  NaN  3 Named groups will become column names in the result. >>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)') letter digit 0      a     1 1      b     2 2    NaN   NaN A pattern with one group will return a DataFrame with one column if expand=True. >>> s.str.extract(r'[ab](\\d)', expand=True) 0 0    1 1    2 2  NaN A pattern with one group will return a Series if expand=False. >>> s.str.extract(r'[ab](\\d)', expand=False) 0      1 1      2 2    NaN dtype: object", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.eq", "Docstring": "Get Equal to of dataframe and other, element-wise (binary operator `eq`).\n\nAmong flexible wrappers (`eq`, `ne`, `le`, `lt`, `ge`, `gt`) to comparison\noperators.\n\nEquivalent to `==`, `!=`, `<=`, `<`, `>=`, `>` with support to choose axis\n(rows or columns) and level for comparison.\n\nParameters\n----------\nother : scalar, sequence, Series, or DataFrame\n    Any single or multiple element data structure, or list-like object.\naxis : {0 or 'index', 1 or 'columns'}, default 'columns'\n    Whether to compare by the index (0 or 'index') or columns\n    (1 or 'columns').\nlevel : int or label\n    Broadcast across a level, matching Index values on the passed\n    MultiIndex level.\n\nReturns\n-------\nDataFrame of bool\n    Result of the comparison.\n\nSee Also\n--------\nDataFrame.eq : Compare DataFrames for equality elementwise.\nDataFrame.ne : Compare DataFrames for inequality elementwise.\nDataFrame.le : Compare DataFrames for less than inequality\n    or equality elementwise.\nDataFrame.lt : Compare DataFrames for strictly less than\n    inequality elementwise.\nDataFrame.ge : Compare DataFrames for greater than inequality\n    or equality elementwise.\nDataFrame.gt : Compare DataFrames for strictly greater than\n    inequality elementwise.\n\nNotes\n-----\nMismatched indices will be unioned together.\n`NaN` values are considered different (i.e. `NaN` != `NaN`).\n\nExamples\n--------\n>>> df = pd.DataFrame({'cost': [250, 150, 100],\n...                    'revenue': [100, 250, 300]},\n...                   index=['A', 'B', 'C'])\n>>> df\n   cost  revenue\nA   250      100\nB   150      250\nC   100      300\n\nComparison with a scalar, using either the operator or method:\n\n>>> df == 100\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False\n\n>>> df.eq(100)\n    cost  revenue\nA  False     True\nB  False    False\nC   True    False\n\nWhen `other` is a :class:`Series`, the columns of a DataFrame are aligned\nwith the index of `other` and broadcast:\n\n>>> df != pd.Series([100, 250], index=[\"cost\", \"revenue\"])\n    cost  revenue\nA   True     True\nB   True    False\nC  False     True\n\nUse the method to control the broadcast axis:\n\n>>> df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index')\n   cost  revenue\nA  True    False\nB  True     True\nC  True     True\nD  True     True\n\nWhen comparing to an arbitrary sequence, the number of columns must\nmatch the number elements in `other`:\n\n>>> df == [250, 100]\n    cost  revenue\nA   True     True\nB  False    False\nC  False    False\n\nUse the method to control the axis:\n\n>>> df.eq([250, 250, 100], axis='index')\n    cost  revenue\nA   True    False\nB  False     True\nC   True    False\n\nCompare to a DataFrame of different shape.\n\n>>> other = pd.DataFrame({'revenue': [300, 250, 100, 150]},\n...                      index=['A', 'B', 'C', 'D'])\n>>> other\n   revenue\nA      300\nB      250\nC      100\nD      150\n\n>>> df.gt(other)\n    cost  revenue\nA  False    False\nB  False    False\nC  False     True\nD  False    False\n\nCompare to a MultiIndex by level.\n\n>>> df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220],\n...                              'revenue': [100, 250, 300, 200, 175, 225]},\n...                             index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'],\n...                                    ['A', 'B', 'C', 'A', 'B', 'C']])\n>>> df_multindex\n      cost  revenue\nQ1 A   250      100\n   B   150      250\n   C   100      300\nQ2 A   150      200\n   B   300      175\n   C   220      225\n\n>>> df.le(df_multindex, level=1)\n       cost  revenue\nQ1 A   True     True\n   B   True     True\n   C   True     True\nQ2 A  False     True\n   B   True    False\n   C   True    False", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.equals", "Docstring": "Test whether two objects contain the same elements.\n\nThis function allows two Series or DataFrames to be compared against\neach other to see if they have the same shape and elements. NaNs in\nthe same location are considered equal.\n\nThe row/column index do not need to have the same type, as long\nas the values are considered equal. Corresponding columns must be of\nthe same dtype.\n\nParameters\n----------\nother : Series or DataFrame\n    The other Series or DataFrame to be compared with the first.\n\nReturns\n-------\nbool\n    True if all elements are the same in both objects, False\n    otherwise.\n\nSee Also\n--------\nSeries.eq : Compare two Series objects of the same length\n    and return a Series where each element is True if the element\n    in each Series is equal, False otherwise.\nDataFrame.eq : Compare two DataFrame objects of the same shape and\n    return a DataFrame where each element is True if the respective\n    element in each DataFrame is equal, False otherwise.\ntesting.assert_series_equal : Raises an AssertionError if left and\n    right are not equal. Provides an easy interface to ignore\n    inequality in dtypes, indexes and precision among others.\ntesting.assert_frame_equal : Like assert_series_equal, but targets\n    DataFrames.\nnumpy.array_equal : Return True if two arrays have the same shape\n    and elements, False otherwise.\n\nExamples\n--------\n>>> df = pd.DataFrame({1: [10], 2: [20]})\n>>> df\n    1   2\n0  10  20\n\nDataFrames df and exactly_equal have the same types and values for\ntheir elements and column labels, which will return True.\n\n>>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n>>> exactly_equal\n    1   2\n0  10  20\n>>> df.equals(exactly_equal)\nTrue\n\nDataFrames df and different_column_type have the same element\ntypes and values, but have different types for the column labels,\nwhich will still return True.\n\n>>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n>>> different_column_type\n   1.0  2.0\n0   10   20\n>>> df.equals(different_column_type)\nTrue\n\nDataFrames df and different_data_type have different types for the\nsame values for their elements, and will return False even though\ntheir column labels are the same values and types.\n\n>>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n>>> different_data_type\n      1     2\n0  10.0  20.0\n>>> df.equals(different_data_type)\nFalse", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.fillna", "Docstring": "Fill NA/NaN values using the specified method.\n\nParameters\n----------\nvalue : scalar, dict, Series, or DataFrame\n    Value to use to fill holes (e.g. 0), alternately a\n    dict/Series/DataFrame of values specifying which value to use for\n    each index (for a Series) or column (for a DataFrame).  Values not\n    in the dict/Series/DataFrame will not be filled. This value cannot\n    be a list.\nmethod : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n    Method to use for filling holes in reindexed Series\n    pad / ffill: propagate last valid observation forward to next valid\n    backfill / bfill: use next valid observation to fill gap.\naxis : {0 or 'index', 1 or 'columns'}\n    Axis along which to fill missing values. For `Series`\n    this parameter is unused and defaults to 0.\ninplace : bool, default False\n    If True, fill in-place. Note: this will modify any\n    other views on this object (e.g., a no-copy slice for a column in a\n    DataFrame).\nlimit : int, default None\n    If method is specified, this is the maximum number of consecutive\n    NaN values to forward/backward fill. In other words, if there is\n    a gap with more than this number of consecutive NaNs, it will only\n    be partially filled. If method is not specified, this is the\n    maximum number of entries along the entire axis where NaNs will be\n    filled. Must be greater than 0 if not None.\ndowncast : dict, default is None\n    A dict of item->dtype of what to downcast if possible,\n    or the string 'infer' which will try to downcast to an appropriate\n    equal type (e.g. float64 to int64 if possible).\n\nReturns\n-------\nDataFrame or None\n    Object with missing values filled or None if ``inplace=True``.\n\nSee Also\n--------\ninterpolate : Fill NaN values using interpolation.\nreindex : Conform object to new index.\nasfreq : Convert TimeSeries to specified frequency.\n\nExamples\n--------\n>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n...                    [3, 4, np.nan, 1],\n...                    [np.nan, np.nan, np.nan, np.nan],\n...                    [np.nan, 3, np.nan, 4]],\n...                   columns=list(\"ABCD\"))\n>>> df\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  NaN  NaN NaN  NaN\n3  NaN  3.0 NaN  4.0\n\nReplace all NaN elements with 0s.\n\n>>> df.fillna(0)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  0.0\n3  0.0  3.0  0.0  4.0\n\nWe can also propagate non-null values forward or backward.\n\n>>> df.fillna(method=\"ffill\")\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  3.0  4.0 NaN  1.0\n3  3.0  3.0 NaN  4.0\n\nReplace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n2, and 3 respectively.\n\n>>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n>>> df.fillna(value=values)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  2.0  1.0\n2  0.0  1.0  2.0  3.0\n3  0.0  3.0  2.0  4.0\n\nOnly replace the first NaN element.\n\n>>> df.fillna(value=values, limit=1)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  NaN  1.0\n2  NaN  1.0  NaN  3.0\n3  NaN  3.0  NaN  4.0\n\nWhen filling using a DataFrame, replacement happens along\nthe same column names and same indices\n\n>>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\n>>> df.fillna(df2)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  NaN\n3  0.0  3.0  0.0  4.0\n\nNote that column D is not affected since it is not present in df2.", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.filter", "Docstring": "Subset the dataframe rows or columns according to the specified index labels.\n\nNote that this routine does not filter a dataframe on its\ncontents. The filter is applied to the labels of the index.\n\nParameters\n----------\nitems : list-like\n    Keep labels from axis which are in items.\nlike : str\n    Keep labels from axis for which \"like in label == True\".\nregex : str (regular expression)\n    Keep labels from axis for which re.search(regex, label) == True.\naxis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n    The axis to filter on, expressed either as an index (int)\n    or axis name (str). By default this is the info axis, 'columns' for\n    DataFrame. For `Series` this parameter is unused and defaults to `None`.\n\nReturns\n-------\nsame type as input object\n\nSee Also\n--------\nDataFrame.loc : Access a group of rows and columns\n    by label(s) or a boolean array.\n\nNotes\n-----\nThe ``items``, ``like``, and ``regex`` parameters are\nenforced to be mutually exclusive.\n\n``axis`` defaults to the info axis that is used when indexing\nwith ``[]``.\n\nExamples\n--------\n>>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])),\n...                   index=['mouse', 'rabbit'],\n...                   columns=['one', 'two', 'three'])\n>>> df\n        one  two  three\nmouse     1    2      3\nrabbit    4    5      6\n\n>>> # select columns by name\n>>> df.filter(items=['one', 'three'])\n         one  three\nmouse     1      3\nrabbit    4      6\n\n>>> # select columns by regular expression\n>>> df.filter(regex='e$', axis=1)\n         one  three\nmouse     1      3\nrabbit    4      6\n\n>>> # select rows containing 'bbi'\n>>> df.filter(like='bbi', axis=0)\n         one  two  three\nrabbit    4    5      6", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.from_records", "Docstring": "Convert structured or record ndarray to DataFrame.\n\nCreates a DataFrame object from a structured ndarray, sequence of\ntuples or dicts, or DataFrame.\n\nParameters\n----------\ndata : structured ndarray, sequence of tuples or dicts, or DataFrame\n    Structured input data.\nindex : str, list of fields, array-like\n    Field of array to use as the index, alternately a specific set of\n    input labels to use.\nexclude : sequence, default None\n    Columns or fields to exclude.\ncolumns : sequence, default None\n    Column names to use. If the passed data do not have names\n    associated with them, this argument provides names for the\n    columns. Otherwise this argument indicates the order of the columns\n    in the result (any names not found in the data will become all-NA\n    columns).\ncoerce_float : bool, default False\n    Attempt to convert values of non-string, non-numeric objects (like\n    decimal.Decimal) to floating point, useful for SQL result sets.\nnrows : int, default None\n    Number of rows to read if data is an iterator.\n\nReturns\n-------\nDataFrame\n\nSee Also\n--------\nDataFrame.from_dict : DataFrame from dict of array-like or dicts.\nDataFrame : DataFrame object creation using constructor.\n\nExamples\n--------\nData can be provided as a structured ndarray:\n\n>>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')],\n...                 dtype=[('col_1', 'i4'), ('col_2', 'U1')])\n>>> pd.DataFrame.from_records(data)\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d\n\nData can be provided as a list of dicts:\n\n>>> data = [{'col_1': 3, 'col_2': 'a'},\n...         {'col_1': 2, 'col_2': 'b'},\n...         {'col_1': 1, 'col_2': 'c'},\n...         {'col_1': 0, 'col_2': 'd'}]\n>>> pd.DataFrame.from_records(data)\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d\n\nData can be provided as a list of tuples with corresponding columns:\n\n>>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')]\n>>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2'])\n   col_1 col_2\n0      3     a\n1      2     b\n2      1     c\n3      0     d", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.groupby", "Docstring": "Group DataFrame using a mapper or by a Series of columns.\n\nA groupby operation involves some combination of splitting the\nobject, applying a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mapping, function, label, or list of labels\n    Used to determine the groups for the groupby.\n    If ``by`` is a function, it's called on each value of the object's\n    index. If a dict or Series is passed, the Series or dict VALUES\n    will be used to determine the groups (the Series' values are first\n    aligned; see ``.align()`` method). If a list or ndarray of length\n    equal to the selected axis is passed (see the `groupby user guide\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups>`_),\n    the values are used as-is to determine the groups. A label or list\n    of labels may be passed to group by the columns in ``self``.\n    Notice that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1). For `Series` this parameter\n    is unused and defaults to 0.\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels. Do not specify both ``by`` and ``level``.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for DataFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, optional\n    When calling apply and the ``by`` argument produces a like-indexed\n    (i.e. :ref:`a transform <groupby.transform>`) result, add group keys to\n    index to identify pieces. By default group keys are not included\n    when the result's index (and column) labels match the inputs, and\n    are included otherwise. This argument has no effect if the result produced\n    is not like-indexed with respect to the input.\n\n    .. versionchanged:: 1.5.0\n\n       Warns that `group_keys` will no longer be ignored when the\n       result from ``apply`` is a like-indexed Series or DataFrame.\n       Specify ``group_keys`` explicitly to include the group keys or\n       not.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if any of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show all values for categorical groupers.\ndropna : bool, default True\n    If True, and if group keys contain NA values, NA values together\n    with row/column will be dropped.\n    If False, NA values will also be treated as the key in groups.\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nDataFrameGroupBy\n    Returns a groupby object that contains information about the groups.\n\nSee Also\n--------\nresample : Convenience method for frequency conversion and resampling\n    of time series.\n\nNotes\n-----\nSee the `user guide\n<https://pandas.pydata.org/pandas-docs/stable/groupby.html>`__ for more\ndetailed usage and examples, including splitting an object into groups,\niterating through groups, selecting a group, aggregation, and more.\n\nExamples\n--------\n>>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> df\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0\n>>> df.groupby(['Animal']).mean()\n        Max Speed\nAnimal\nFalcon      375.0\nParrot       25.0\n\n**Hierarchical Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n...                   index=index)\n>>> df\n                Max Speed\nAnimal Type\nFalcon Captive      390.0\n       Wild         350.0\nParrot Captive       30.0\n       Wild          20.0\n>>> df.groupby(level=0).mean()\n        Max Speed\nAnimal\nFalcon      370.0\nParrot       25.0\n>>> df.groupby(level=\"Type\").mean()\n         Max Speed\nType\nCaptive      210.0\nWild         185.0\n\nWe can also choose to include NA in group keys or not by setting\n`dropna` parameter, the default setting is `True`.\n\n>>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n>>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> df.groupby(by=[\"b\"]).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\n\n>>> df.groupby(by=[\"b\"], dropna=False).sum()\n    a   c\nb\n1.0 2   3\n2.0 2   5\nNaN 1   4\n\n>>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n>>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n\n>>> df.groupby(by=\"a\").sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\n\n>>> df.groupby(by=\"a\", dropna=False).sum()\n    b     c\na\na   13.0   13.0\nb   12.3  123.0\nNaN 12.3   33.0\n\nWhen using ``.apply()``, use ``group_keys`` to include or exclude the group keys.\nThe ``group_keys`` argument defaults to ``True`` (include).\n\n>>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n...                               'Parrot', 'Parrot'],\n...                    'Max Speed': [380., 370., 24., 26.]})\n>>> df.groupby(\"Animal\", group_keys=True).apply(lambda x: x)\n          Animal  Max Speed\nAnimal\nFalcon 0  Falcon      380.0\n       1  Falcon      370.0\nParrot 2  Parrot       24.0\n       3  Parrot       26.0\n\n>>> df.groupby(\"Animal\", group_keys=False).apply(lambda x: x)\n   Animal  Max Speed\n0  Falcon      380.0\n1  Falcon      370.0\n2  Parrot       24.0\n3  Parrot       26.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.head", "Docstring": "Return the first `n` rows.\n\nThis function returns the first `n` rows for the object based\non position. It is useful for quickly testing if your object\nhas the right type of data in it.\n\nFor negative values of `n`, this function returns all rows except\nthe last `|n|` rows, equivalent to ``df[:n]``.\n\nIf n is larger than the number of rows, this function returns all rows.\n\nParameters\n----------\nn : int, default 5\n    Number of rows to select.\n\nReturns\n-------\nsame type as caller\n    The first `n` rows of the caller object.\n\nSee Also\n--------\nDataFrame.tail: Returns the last `n` rows.\n\nExamples\n--------\n>>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n>>> df\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\n6      shark\n7      whale\n8      zebra\n\nViewing the first 5 lines\n\n>>> df.head()\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n\nViewing the first `n` lines (three in this case)\n\n>>> df.head(3)\n      animal\n0  alligator\n1        bee\n2     falcon\n\nFor negative values of `n`\n\n>>> df.head(-3)\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.idxmax", "Docstring": "Return index of first occurrence of maximum over requested axis.\n\nNA/null values are excluded.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\nnumeric_only : bool, default False\n    Include only `float`, `int` or `boolean` data.\n\n    .. versionadded:: 1.5.0\n\nReturns\n-------\nSeries\n    Indexes of maxima along the specified axis.\n\nRaises\n------\nValueError\n    * If the row/column is empty\n\nSee Also\n--------\nSeries.idxmax : Return index of the maximum element.\n\nNotes\n-----\nThis method is the DataFrame version of ``ndarray.argmax``.\n\nExamples\n--------\nConsider a dataset containing food consumption in Argentina.\n\n>>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48],\n...                    'co2_emissions': [37.2, 19.66, 1712]},\n...                    index=['Pork', 'Wheat Products', 'Beef'])\n\n>>> df\n                consumption  co2_emissions\nPork                  10.51         37.20\nWheat Products       103.11         19.66\nBeef                  55.48       1712.00\n\nBy default, it returns the index for the maximum value in each column.\n\n>>> df.idxmax()\nconsumption     Wheat Products\nco2_emissions             Beef\ndtype: object\n\nTo return the index for the maximum value in each row, use ``axis=\"columns\"``.\n\n>>> df.idxmax(axis=\"columns\")\nPork              co2_emissions\nWheat Products     consumption\nBeef              co2_emissions\ndtype: object", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.iloc", "Docstring": "Purely integer-location based indexing for selection by position.\n\n``.iloc[]`` is primarily integer position based (from ``0`` to\n``length-1`` of the axis), but may also be used with a boolean\narray.\n\nAllowed inputs are:\n\n- An integer, e.g. ``5``.\n- A list or array of integers, e.g. ``[4, 3, 0]``.\n- A slice object with ints, e.g. ``1:7``.\n- A boolean array.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above).\n  This is useful in method chains, when you don't have a reference to the\n  calling object, but would like to base your selection on some value.\n- A tuple of row and column indexes. The tuple elements consist of one of the\n  above inputs, e.g. ``(0, 1)``.\n\n``.iloc`` will raise ``IndexError`` if a requested indexer is\nout-of-bounds, except *slice* indexers which allow out-of-bounds\nindexing (this conforms with python/numpy *slice* semantics).\n\nSee more at :ref:`Selection by Position <indexing.integer>`.\n\nSee Also\n--------\nDataFrame.iat : Fast integer location scalar accessor.\nDataFrame.loc : Purely label-location based indexer for selection by label.\nSeries.iloc : Purely integer-location based indexing for\n               selection by position.\n\nExamples\n--------\n>>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n>>> df = pd.DataFrame(mydict)\n>>> df\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\n\n**Indexing just the rows**\n\nWith a scalar integer.\n\n>>> type(df.iloc[0])\n<class 'pandas.core.series.Series'>\n>>> df.iloc[0]\na    1\nb    2\nc    3\nd    4\nName: 0, dtype: int64\n\nWith a list of integers.\n\n>>> df.iloc[[0]]\n   a  b  c  d\n0  1  2  3  4\n>>> type(df.iloc[[0]])\n<class 'pandas.core.frame.DataFrame'>\n\n>>> df.iloc[[0, 1]]\n     a    b    c    d\n0    1    2    3    4\n1  100  200  300  400\n\nWith a `slice` object.\n\n>>> df.iloc[:3]\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\n\nWith a boolean mask the same length as the index.\n\n>>> df.iloc[[True, False, True]]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\n\nWith a callable, useful in method chains. The `x` passed\nto the ``lambda`` is the DataFrame being sliced. This selects\nthe rows whose index label even.\n\n>>> df.iloc[lambda x: x.index % 2 == 0]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\n\n**Indexing both axes**\n\nYou can mix the indexer types for the index and columns. Use ``:`` to\nselect the entire axis.\n\nWith scalar integers.\n\n>>> df.iloc[0, 1]\n2\n\nWith lists of integers.\n\n>>> df.iloc[[0, 2], [1, 3]]\n      b     d\n0     2     4\n2  2000  4000\n\nWith `slice` objects.\n\n>>> df.iloc[1:3, 0:3]\n      a     b     c\n1   100   200   300\n2  1000  2000  3000\n\nWith a boolean array whose length matches the columns.\n\n>>> df.iloc[:, [True, False, True, False]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000\n\nWith a callable function that expects the Series or DataFrame.\n\n>>> df.iloc[:, lambda df: [0, 2]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.index", "Docstring": "The index (row labels) of the DataFrame.", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.isna", "Docstring": "Detect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA.\nNA values, such as None or :attr:`numpy.NaN`, gets mapped to True\nvalues.\nEverything else gets mapped to False values. Characters such as empty\nstrings ``''`` or :attr:`numpy.inf` are not considered NA values\n(unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\nReturns\n-------\nDataFrame\n    Mask of bool values for each element in DataFrame that\n    indicates whether an element is an NA value.\n\nSee Also\n--------\nDataFrame.isnull : Alias of isna.\nDataFrame.notna : Boolean inverse of isna.\nDataFrame.dropna : Omit axes labels with missing values.\nisna : Top-level isna.\n\nExamples\n--------\nShow which entries in a DataFrame are NA.\n\n>>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                          pd.Timestamp('1940-04-25')],\n...                    name=['Alfred', 'Batman', ''],\n...                    toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker\n\n>>> df.isna()\n     age   born   name    toy\n0  False   True  False   True\n1  False  False  False  False\n2   True  False  False  False\n\nShow which entries in a Series are NA.\n\n>>> ser = pd.Series([5, 6, np.NaN])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64\n\n>>> ser.isna()\n0    False\n1    False\n2     True\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.isnull", "Docstring": "DataFrame.isnull is an alias for DataFrame.isna.\n\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA.\nNA values, such as None or :attr:`numpy.NaN`, gets mapped to True\nvalues.\nEverything else gets mapped to False values. Characters such as empty\nstrings ``''`` or :attr:`numpy.inf` are not considered NA values\n(unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\nReturns\n-------\nDataFrame\n    Mask of bool values for each element in DataFrame that\n    indicates whether an element is an NA value.\n\nSee Also\n--------\nDataFrame.isnull : Alias of isna.\nDataFrame.notna : Boolean inverse of isna.\nDataFrame.dropna : Omit axes labels with missing values.\nisna : Top-level isna.\n\nExamples\n--------\nShow which entries in a DataFrame are NA.\n\n>>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                          pd.Timestamp('1940-04-25')],\n...                    name=['Alfred', 'Batman', ''],\n...                    toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker\n\n>>> df.isna()\n     age   born   name    toy\n0  False   True  False   True\n1  False  False  False  False\n2   True  False  False  False\n\nShow which entries in a Series are NA.\n\n>>> ser = pd.Series([5, 6, np.NaN])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64\n\n>>> ser.isna()\n0    False\n1    False\n2     True\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.join", "Docstring": "Join columns of another DataFrame.\n\nJoin columns with `other` DataFrame either on index or on a key\ncolumn. Efficiently join multiple DataFrame objects by index at once by\npassing a list.\n\nParameters\n----------\nother : DataFrame, Series, or a list containing any combination of them\n    Index should be similar to one of the columns in this one. If a\n    Series is passed, its name attribute must be set, and that will be\n    used as the column name in the resulting joined DataFrame.\non : str, list of str, or array-like, optional\n    Column or index level name(s) in the caller to join on the index\n    in `other`, otherwise joins index-on-index. If multiple\n    values given, the `other` DataFrame must have a MultiIndex. Can\n    pass an array as the join key if it is not already contained in\n    the calling DataFrame. Like an Excel VLOOKUP operation.\nhow : {'left', 'right', 'outer', 'inner'}, default 'left'\n    How to handle the operation of the two objects.\n\n    * left: use calling frame's index (or column if on is specified)\n    * right: use `other`'s index.\n    * outer: form union of calling frame's index (or column if on is\n      specified) with `other`'s index, and sort it.\n      lexicographically.\n    * inner: form intersection of calling frame's index (or column if\n      on is specified) with `other`'s index, preserving the order\n      of the calling's one.\n    * cross: creates the cartesian product from both frames, preserves the order\n      of the left keys.\n\n      .. versionadded:: 1.2.0\n\nlsuffix : str, default ''\n    Suffix to use from left frame's overlapping columns.\nrsuffix : str, default ''\n    Suffix to use from right frame's overlapping columns.\nsort : bool, default False\n    Order result DataFrame lexicographically by the join key. If False,\n    the order of the join key depends on the join type (how keyword).\nvalidate : str, optional\n    If specified, checks if join is of specified type.\n    * \"one_to_one\" or \"1:1\": check if join keys are unique in both left\n    and right datasets.\n    * \"one_to_many\" or \"1:m\": check if join keys are unique in left dataset.\n    * \"many_to_one\" or \"m:1\": check if join keys are unique in right dataset.\n    * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n    .. versionadded:: 1.5.0\n\nReturns\n-------\nDataFrame\n    A dataframe containing columns from both the caller and `other`.\n\nSee Also\n--------\nDataFrame.merge : For column(s)-on-column(s) operations.\n\nNotes\n-----\nParameters `on`, `lsuffix`, and `rsuffix` are not supported when\npassing a list of `DataFrame` objects.\n\nSupport for specifying index levels as the `on` parameter was added\nin version 0.23.0.\n\nExamples\n--------\n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K2  A2\n3  K3  A3\n4  K4  A4\n5  K5  A5\n\n>>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n...                       'B': ['B0', 'B1', 'B2']})\n\n>>> other\n  key   B\n0  K0  B0\n1  K1  B1\n2  K2  B2\n\nJoin DataFrames using their indexes.\n\n>>> df.join(other, lsuffix='_caller', rsuffix='_other')\n  key_caller   A key_other    B\n0         K0  A0        K0   B0\n1         K1  A1        K1   B1\n2         K2  A2        K2   B2\n3         K3  A3       NaN  NaN\n4         K4  A4       NaN  NaN\n5         K5  A5       NaN  NaN\n\nIf we want to join using the key columns, we need to set key to be\nthe index in both `df` and `other`. The joined DataFrame will have\nkey as its index.\n\n>>> df.set_index('key').join(other.set_index('key'))\n      A    B\nkey\nK0   A0   B0\nK1   A1   B1\nK2   A2   B2\nK3   A3  NaN\nK4   A4  NaN\nK5   A5  NaN\n\nAnother option to join using the key columns is to use the `on`\nparameter. DataFrame.join always uses `other`'s index but we can use\nany column in `df`. This method preserves the original DataFrame's\nindex in the result.\n\n>>> df.join(other.set_index('key'), on='key')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K2  A2   B2\n3  K3  A3  NaN\n4  K4  A4  NaN\n5  K5  A5  NaN\n\nUsing non-unique key values shows how they are matched.\n\n>>> df = pd.DataFrame({'key': ['K0', 'K1', 'K1', 'K3', 'K0', 'K1'],\n...                    'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']})\n\n>>> df\n  key   A\n0  K0  A0\n1  K1  A1\n2  K1  A2\n3  K3  A3\n4  K0  A4\n5  K1  A5\n\n>>> df.join(other.set_index('key'), on='key', validate='m:1')\n  key   A    B\n0  K0  A0   B0\n1  K1  A1   B1\n2  K1  A2   B1\n3  K3  A3  NaN\n4  K0  A4   B0\n5  K1  A5   B1", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.loc", "Docstring": "Access a group of rows and columns by label(s) or a boolean array.\n\n``.loc[]`` is primarily label based, but may also be used with a\nboolean array.\n\nAllowed inputs are:\n\n- A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is\n  interpreted as a *label* of the index, and **never** as an\n  integer position along the index).\n- A list or array of labels, e.g. ``['a', 'b', 'c']``.\n- A slice object with labels, e.g. ``'a':'f'``.\n\n  .. warning:: Note that contrary to usual python slices, **both** the\n      start and the stop are included\n\n- A boolean array of the same length as the axis being sliced,\n  e.g. ``[True, False, True]``.\n- An alignable boolean Series. The index of the key will be aligned before\n  masking.\n- An alignable Index. The Index of the returned selection will be the input.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above)\n\nSee more at :ref:`Selection by Label <indexing.label>`.\n\nRaises\n------\nKeyError\n    If any items are not found.\nIndexingError\n    If an indexed key is passed and its index is unalignable to the frame index.\n\nSee Also\n--------\nDataFrame.at : Access a single value for a row/column label pair.\nDataFrame.iloc : Access group of rows and columns by integer position(s).\nDataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n    Series/DataFrame.\nSeries.loc : Access group of values using labels.\n\nExamples\n--------\n**Getting values**\n\n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=['cobra', 'viper', 'sidewinder'],\n...      columns=['max_speed', 'shield'])\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\n\nSingle label. Note this returns the row as a Series.\n\n>>> df.loc['viper']\nmax_speed    4\nshield       5\nName: viper, dtype: int64\n\nList of labels. Note using ``[[]]`` returns a DataFrame.\n\n>>> df.loc[['viper', 'sidewinder']]\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\n\nSingle label for row and column\n\n>>> df.loc['cobra', 'shield']\n2\n\nSlice with labels for row and single label for column. As mentioned\nabove, note that both the start and stop of the slice are included.\n\n>>> df.loc['cobra':'viper', 'max_speed']\ncobra    1\nviper    4\nName: max_speed, dtype: int64\n\nBoolean list with the same length as the row axis\n\n>>> df.loc[[False, False, True]]\n            max_speed  shield\nsidewinder          7       8\n\nAlignable boolean Series:\n\n>>> df.loc[pd.Series([False, True, False],\n...        index=['viper', 'sidewinder', 'cobra'])]\n            max_speed  shield\nsidewinder          7       8\n\nIndex (same behavior as ``df.reindex``)\n\n>>> df.loc[pd.Index([\"cobra\", \"viper\"], name=\"foo\")]\n       max_speed  shield\nfoo\ncobra          1       2\nviper          4       5\n\nConditional that returns a boolean Series\n\n>>> df.loc[df['shield'] > 6]\n            max_speed  shield\nsidewinder          7       8\n\nConditional that returns a boolean Series with column labels specified\n\n>>> df.loc[df['shield'] > 6, ['max_speed']]\n            max_speed\nsidewinder          7\n\nCallable that returns a boolean Series\n\n>>> df.loc[lambda df: df['shield'] == 8]\n            max_speed  shield\nsidewinder          7       8\n\n**Setting values**\n\nSet value for all items matching the list of labels\n\n>>> df.loc[['viper', 'sidewinder'], ['shield']] = 50\n>>> df\n            max_speed  shield\ncobra               1       2\nviper               4      50\nsidewinder          7      50\n\nSet value for an entire row\n\n>>> df.loc['cobra'] = 10\n>>> df\n            max_speed  shield\ncobra              10      10\nviper               4      50\nsidewinder          7      50\n\nSet value for an entire column\n\n>>> df.loc[:, 'max_speed'] = 30\n>>> df\n            max_speed  shield\ncobra              30      10\nviper              30      50\nsidewinder         30      50\n\nSet value for rows matching callable condition\n\n>>> df.loc[df['shield'] > 35] = 0\n>>> df\n            max_speed  shield\ncobra              30      10\nviper               0       0\nsidewinder          0       0\n\n**Getting values on a DataFrame with an index that has integer labels**\n\nAnother example using integers for the index\n\n>>> df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n>>> df\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n\nSlice with integer labels for rows. As mentioned above, note that both\nthe start and stop of the slice are included.\n\n>>> df.loc[7:9]\n   max_speed  shield\n7          1       2\n8          4       5\n9          7       8\n\n**Getting values with a MultiIndex**\n\nA number of examples using a DataFrame with a MultiIndex\n\n>>> tuples = [\n...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n...    ('viper', 'mark ii'), ('viper', 'mark iii')\n... ]\n>>> index = pd.MultiIndex.from_tuples(tuples)\n>>> values = [[12, 2], [0, 4], [10, 20],\n...         [1, 4], [7, 1], [16, 36]]\n>>> df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n>>> df\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n\nSingle label. Note this returns a DataFrame with a single index.\n\n>>> df.loc['cobra']\n         max_speed  shield\nmark i          12       2\nmark ii          0       4\n\nSingle index tuple. Note this returns a Series.\n\n>>> df.loc[('cobra', 'mark ii')]\nmax_speed    0\nshield       4\nName: (cobra, mark ii), dtype: int64\n\nSingle label for row and column. Similar to passing in a tuple, this\nreturns a Series.\n\n>>> df.loc['cobra', 'mark i']\nmax_speed    12\nshield        2\nName: (cobra, mark i), dtype: int64\n\nSingle tuple. Note using ``[[]]`` returns a DataFrame.\n\n>>> df.loc[[('cobra', 'mark ii')]]\n               max_speed  shield\ncobra mark ii          0       4\n\nSingle tuple for the index with a single label for the column\n\n>>> df.loc[('cobra', 'mark i'), 'shield']\n2\n\nSlice from index tuple to single label\n\n>>> df.loc[('cobra', 'mark i'):'viper']\n                     max_speed  shield\ncobra      mark i           12       2\n           mark ii           0       4\nsidewinder mark i           10      20\n           mark ii           1       4\nviper      mark ii           7       1\n           mark iii         16      36\n\nSlice from index tuple to index tuple\n\n>>> df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n                    max_speed  shield\ncobra      mark i          12       2\n           mark ii          0       4\nsidewinder mark i          10      20\n           mark ii          1       4\nviper      mark ii          7       1\n\nPlease see the :ref:`user guide<advanced.advanced_hierarchical>`\nfor more details and explanations of advanced indexing.", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.mask", "Docstring": "Replace values where the condition is True.\n\nParameters\n----------\ncond : bool Series/DataFrame, array-like, or callable\n    Where `cond` is False, keep the original value. Where\n    True, replace with corresponding value from `other`.\n    If `cond` is callable, it is computed on the Series/DataFrame and\n    should return boolean Series/DataFrame or array. The callable must\n    not change input Series/DataFrame (though pandas doesn't check it).\nother : scalar, Series/DataFrame, or callable\n    Entries where `cond` is True are replaced with\n    corresponding value from `other`.\n    If other is callable, it is computed on the Series/DataFrame and\n    should return scalar or Series/DataFrame. The callable must not\n    change input Series/DataFrame (though pandas doesn't check it).\ninplace : bool, default False\n    Whether to perform the operation in place on the data.\naxis : int, default None\n    Alignment axis if needed. For `Series` this parameter is\n    unused and defaults to 0.\nlevel : int, default None\n    Alignment level if needed.\nerrors : str, {'raise', 'ignore'}, default 'raise'\n    Note that currently this parameter won't affect\n    the results and will always coerce to a suitable dtype.\n\n    - 'raise' : allow exceptions to be raised.\n    - 'ignore' : suppress exceptions. On error return original object.\n\n    .. deprecated:: 1.5.0\n       This argument had no effect.\n\ntry_cast : bool, default None\n    Try to cast the result back to the input type (if possible).\n\n    .. deprecated:: 1.3.0\n        Manually cast back if necessary.\n\nReturns\n-------\nSame type as caller or None if ``inplace=True``.\n\nSee Also\n--------\n:func:`DataFrame.where` : Return an object of same shape as\n    self.\n\nNotes\n-----\nThe mask method is an application of the if-then idiom. For each\nelement in the calling DataFrame, if ``cond`` is ``False`` the\nelement is used; otherwise the corresponding element from the DataFrame\n``other`` is used. If the axis of ``other`` does not align with axis of\n``cond`` Series/DataFrame, the misaligned index positions will be filled with\nTrue.\n\nThe signature for :func:`DataFrame.where` differs from\n:func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n``np.where(m, df1, df2)``.\n\nFor further details and examples see the ``mask`` documentation in\n:ref:`indexing <indexing.where_mask>`.\n\nThe dtype of the object takes precedence. The fill value is casted to\nthe object's dtype, if this can be done losslessly.\n\nExamples\n--------\n>>> s = pd.Series(range(5))\n>>> s.where(s > 0)\n0    NaN\n1    1.0\n2    2.0\n3    3.0\n4    4.0\ndtype: float64\n>>> s.mask(s > 0)\n0    0.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n>>> s = pd.Series(range(5))\n>>> t = pd.Series([True, False])\n>>> s.where(t, 99)\n0     0\n1    99\n2    99\n3    99\n4    99\ndtype: int64\n>>> s.mask(t, 99)\n0    99\n1     1\n2    99\n3    99\n4    99\ndtype: int64\n\n>>> s.where(s > 1, 10)\n0    10\n1    10\n2    2\n3    3\n4    4\ndtype: int64\n>>> s.mask(s > 1, 10)\n0     0\n1     1\n2    10\n3    10\n4    10\ndtype: int64\n\n>>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n>>> df\n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n>>> m = df % 3 == 0\n>>> df.where(m, -df)\n   A  B\n0  0 -1\n1 -2  3\n2 -4 -5\n3  6 -7\n4 -8  9\n>>> df.where(m, -df) == np.where(m, df, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True\n>>> df.where(m, -df) == df.mask(~m, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.max", "Docstring": "Return the maximum of the values over the requested axis.\n\nIf you want the *index* of the maximum, use ``idxmax``. This is the equivalent of the ``numpy.ndarray`` method ``argmax``.\n\nParameters\n----------\naxis : {index (0), columns (1)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nSeries or DataFrame (if level specified)\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.max()\n8", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.mean", "Docstring": "Return the mean of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0), columns (1)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nSeries or DataFrame (if level specified)", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.median", "Docstring": "Return the median of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0), columns (1)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nSeries or DataFrame (if level specified)", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.merge", "Docstring": "Merge DataFrame or named Series objects with a database-style join.\n\nA named Series object is treated as a DataFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on\ncolumns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\non indexes or indexes on a column or columns, the index will be passed on.\nWhen performing a cross merge, no column specifications to merge on are\nallowed.\n\n.. warning::\n\n    If both key columns contain rows where the key is a null value, those\n    rows will be matched against each other. This is different from usual SQL\n    join behaviour and can lead to unexpected results.\n\nParameters\n----------\nright : DataFrame or named Series\n    Object to merge with.\nhow : {'left', 'right', 'outer', 'inner', 'cross'}, default 'inner'\n    Type of merge to be performed.\n\n    * left: use only keys from left frame, similar to a SQL left outer join;\n      preserve key order.\n    * right: use only keys from right frame, similar to a SQL right outer join;\n      preserve key order.\n    * outer: use union of keys from both frames, similar to a SQL full outer\n      join; sort keys lexicographically.\n    * inner: use intersection of keys from both frames, similar to a SQL inner\n      join; preserve the order of the left keys.\n    * cross: creates the cartesian product from both frames, preserves the order\n      of the left keys.\n\n      .. versionadded:: 1.2.0\n\non : label or list\n    Column or index level names to join on. These must be found in both\n    DataFrames. If `on` is None and not merging on indexes then this defaults\n    to the intersection of the columns in both DataFrames.\nleft_on : label or list, or array-like\n    Column or index level names to join on in the left DataFrame. Can also\n    be an array or list of arrays of the length of the left DataFrame.\n    These arrays are treated as if they are columns.\nright_on : label or list, or array-like\n    Column or index level names to join on in the right DataFrame. Can also\n    be an array or list of arrays of the length of the right DataFrame.\n    These arrays are treated as if they are columns.\nleft_index : bool, default False\n    Use the index from the left DataFrame as the join key(s). If it is a\n    MultiIndex, the number of keys in the other DataFrame (either the index\n    or a number of columns) must match the number of levels.\nright_index : bool, default False\n    Use the index from the right DataFrame as the join key. Same caveats as\n    left_index.\nsort : bool, default False\n    Sort the join keys lexicographically in the result DataFrame. If False,\n    the order of the join keys depends on the join type (how keyword).\nsuffixes : list-like, default is (\"_x\", \"_y\")\n    A length-2 sequence where each element is optionally a string\n    indicating the suffix to add to overlapping column names in\n    `left` and `right` respectively. Pass a value of `None` instead\n    of a string to indicate that the column name from `left` or\n    `right` should be left as-is, with no suffix. At least one of the\n    values must not be None.\ncopy : bool, default True\n    If False, avoid copy if possible.\nindicator : bool or str, default False\n    If True, adds a column to the output DataFrame called \"_merge\" with\n    information on the source of each row. The column can be given a different\n    name by providing a string argument. The column will have a Categorical\n    type with the value of \"left_only\" for observations whose merge key only\n    appears in the left DataFrame, \"right_only\" for observations\n    whose merge key only appears in the right DataFrame, and \"both\"\n    if the observation's merge key is found in both DataFrames.\n\nvalidate : str, optional\n    If specified, checks if merge is of specified type.\n\n    * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n      left and right datasets.\n    * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n      dataset.\n    * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n      dataset.\n    * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.\n\nReturns\n-------\nDataFrame\n    A DataFrame of the two merged objects.\n\nSee Also\n--------\nmerge_ordered : Merge with optional filling/interpolation.\nmerge_asof : Merge on nearest keys.\nDataFrame.join : Similar method using indices.\n\nNotes\n-----\nSupport for specifying index levels as the `on`, `left_on`, and\n`right_on` parameters was added in version 0.23.0\nSupport for merging named Series objects was added in version 0.24.0\n\nExamples\n--------\n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n...                     'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\n\nMerge df1 and df2 on the lkey and rkey columns. The value columns have\nthe default suffixes, _x and _y, appended.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\n\nMerge DataFrames df1 and df2 with specified left and right suffixes\nappended to any overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n...           suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\n\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n\n>>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4\n\n>>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n\n>>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n\n>>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8\n\n>>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.min", "Docstring": "Return the minimum of the values over the requested axis.\n\nIf you want the *index* of the minimum, use ``idxmin``. This is the equivalent of the ``numpy.ndarray`` method ``argmin``.\n\nParameters\n----------\naxis : {index (0), columns (1)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nSeries or DataFrame (if level specified)\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.min()\n0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.mode", "Docstring": "Get the mode(s) of each element along the selected axis.\n\nThe mode of a set of values is the value that appears most often.\nIt can be multiple values.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The axis to iterate over while searching for the mode:\n\n    * 0 or 'index' : get mode of each column\n    * 1 or 'columns' : get mode of each row.\n\nnumeric_only : bool, default False\n    If True, only apply to numeric columns.\ndropna : bool, default True\n    Don't consider counts of NaN/NaT.\n\nReturns\n-------\nDataFrame\n    The modes of each column or row.\n\nSee Also\n--------\nSeries.mode : Return the highest frequency value in a Series.\nSeries.value_counts : Return the counts of values in a Series.\n\nExamples\n--------\n>>> df = pd.DataFrame([('bird', 2, 2),\n...                    ('mammal', 4, np.nan),\n...                    ('arthropod', 8, 0),\n...                    ('bird', 2, np.nan)],\n...                   index=('falcon', 'horse', 'spider', 'ostrich'),\n...                   columns=('species', 'legs', 'wings'))\n>>> df\n           species  legs  wings\nfalcon        bird     2    2.0\nhorse       mammal     4    NaN\nspider   arthropod     8    0.0\nostrich       bird     2    NaN\n\nBy default, missing values are not considered, and the mode of wings\nare both 0 and 2. Because the resulting DataFrame has two rows,\nthe second row of ``species`` and ``legs`` contains ``NaN``.\n\n>>> df.mode()\n  species  legs  wings\n0    bird   2.0    0.0\n1     NaN   NaN    2.0\n\nSetting ``dropna=False`` ``NaN`` values are considered and they can be\nthe mode (like for wings).\n\n>>> df.mode(dropna=False)\n  species  legs  wings\n0    bird     2    NaN\n\nSetting ``numeric_only=True``, only the mode of numeric columns is\ncomputed, and columns of other types are ignored.\n\n>>> df.mode(numeric_only=True)\n   legs  wings\n0   2.0    0.0\n1   NaN    2.0\n\nTo compute the mode over columns and not rows, use the axis parameter:\n\n>>> df.mode(axis='columns', numeric_only=True)\n           0    1\nfalcon   2.0  NaN\nhorse    4.0  NaN\nspider   0.0  8.0\nostrich  2.0  NaN", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.pop", "Docstring": "Return item and drop from frame. Raise KeyError if not found.\n\nParameters\n----------\nitem : label\n    Label of column to be popped.\n\nReturns\n-------\nSeries\n\nExamples\n--------\n>>> df = pd.DataFrame([('falcon', 'bird', 389.0),\n...                    ('parrot', 'bird', 24.0),\n...                    ('lion', 'mammal', 80.5),\n...                    ('monkey', 'mammal', np.nan)],\n...                   columns=('name', 'class', 'max_speed'))\n>>> df\n     name   class  max_speed\n0  falcon    bird      389.0\n1  parrot    bird       24.0\n2    lion  mammal       80.5\n3  monkey  mammal        NaN\n\n>>> df.pop('class')\n0      bird\n1      bird\n2    mammal\n3    mammal\nName: class, dtype: object\n\n>>> df\n     name  max_speed\n0  falcon      389.0\n1  parrot       24.0\n2    lion       80.5\n3  monkey        NaN", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.pow", "Docstring": "Get Exponential power of dataframe and other, element-wise (binary operator `pow`).\n\nEquivalent to ``dataframe ** other``, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, `rpow`.\n\nAmong flexible wrappers (`add`, `sub`, `mul`, `div`, `mod`, `pow`) to\narithmetic operators: `+`, `-`, `*`, `/`, `//`, `%`, `**`.\n\nParameters\n----------\nother : scalar, sequence, Series, dict or DataFrame\n    Any single or multiple element data structure, or list-like object.\naxis : {0 or 'index', 1 or 'columns'}\n    Whether to compare by the index (0 or 'index') or columns.\n    (1 or 'columns'). For Series input, axis to match Series index on.\nlevel : int or label\n    Broadcast across a level, matching Index values on the\n    passed MultiIndex level.\nfill_value : float or None, default None\n    Fill existing missing (NaN) values, and any new element needed for\n    successful DataFrame alignment, with this value before computation.\n    If data in both corresponding DataFrame locations is missing\n    the result will be missing.\n\nReturns\n-------\nDataFrame\n    Result of the arithmetic operation.\n\nSee Also\n--------\nDataFrame.add : Add DataFrames.\nDataFrame.sub : Subtract DataFrames.\nDataFrame.mul : Multiply DataFrames.\nDataFrame.div : Divide DataFrames (float division).\nDataFrame.truediv : Divide DataFrames (float division).\nDataFrame.floordiv : Divide DataFrames (integer division).\nDataFrame.mod : Calculate modulo (remainder after division).\nDataFrame.pow : Calculate exponential power.\n\nNotes\n-----\nMismatched indices will be unioned together.\n\nExamples\n--------\n>>> df = pd.DataFrame({'angles': [0, 3, 4],\n...                    'degrees': [360, 180, 360]},\n...                   index=['circle', 'triangle', 'rectangle'])\n>>> df\n           angles  degrees\ncircle          0      360\ntriangle        3      180\nrectangle       4      360\n\nAdd a scalar with operator version which return the same\nresults.\n\n>>> df + 1\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361\n\n>>> df.add(1)\n           angles  degrees\ncircle          1      361\ntriangle        4      181\nrectangle       5      361\n\nDivide by constant with reverse version.\n\n>>> df.div(10)\n           angles  degrees\ncircle        0.0     36.0\ntriangle      0.3     18.0\nrectangle     0.4     36.0\n\n>>> df.rdiv(10)\n             angles   degrees\ncircle          inf  0.027778\ntriangle   3.333333  0.055556\nrectangle  2.500000  0.027778\n\nSubtract a list and Series by axis with operator version.\n\n>>> df - [1, 2]\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358\n\n>>> df.sub([1, 2], axis='columns')\n           angles  degrees\ncircle         -1      358\ntriangle        2      178\nrectangle       3      358\n\n>>> df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']),\n...        axis='index')\n           angles  degrees\ncircle         -1      359\ntriangle        2      179\nrectangle       3      359\n\nMultiply a dictionary by axis.\n\n>>> df.mul({'angles': 0, 'degrees': 2})\n            angles      degrees\ncircle           0          720\ntriangle             0      360\nrectangle            0      720\n\n>>> df.mul({'circle': 0, 'triangle': 2, 'rectangle': 3}, axis='index')\n            angles      degrees\ncircle               0        0\ntriangle             6      360\nrectangle           12     1080\n\nMultiply a DataFrame of different shape with operator version.\n\n>>> other = pd.DataFrame({'angles': [0, 3, 4]},\n...                      index=['circle', 'triangle', 'rectangle'])\n>>> other\n           angles\ncircle          0\ntriangle        3\nrectangle       4\n\n>>> df * other\n           angles  degrees\ncircle          0      NaN\ntriangle        9      NaN\nrectangle      16      NaN\n\n>>> df.mul(other, fill_value=0)\n           angles  degrees\ncircle          0      0.0\ntriangle        9      0.0\nrectangle      16      0.0\n\nDivide by a MultiIndex by level.\n\n>>> df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6],\n...                              'degrees': [360, 180, 360, 360, 540, 720]},\n...                             index=[['A', 'A', 'A', 'B', 'B', 'B'],\n...                                    ['circle', 'triangle', 'rectangle',\n...                                     'square', 'pentagon', 'hexagon']])\n>>> df_multindex\n             angles  degrees\nA circle          0      360\n  triangle        3      180\n  rectangle       4      360\nB square          4      360\n  pentagon        5      540\n  hexagon         6      720\n\n>>> df.div(df_multindex, level=1, fill_value=0)\n             angles  degrees\nA circle        NaN      1.0\n  triangle      1.0      1.0\n  rectangle     1.0      1.0\nB square        0.0      0.0\n  pentagon      0.0      0.0\n  hexagon       0.0      0.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.prod", "Docstring": "Return the product of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0), columns (1)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\nmin_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``min_count`` non-NA values are present the result will be NA.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nSeries or DataFrame (if level specified)\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\n\nExamples\n--------\nBy default, the product of an empty or all-NA Series is ``1``\n\n>>> pd.Series([], dtype=\"float64\").prod()\n1.0\n\nThis can be controlled with the ``min_count`` parameter\n\n>>> pd.Series([], dtype=\"float64\").prod(min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\nempty series identically.\n\n>>> pd.Series([np.nan]).prod()\n1.0\n\n>>> pd.Series([np.nan]).prod(min_count=1)\nnan", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.query", "Docstring": "Query the columns of a DataFrame with a boolean expression.\n\nParameters\n----------\nexpr : str\n    The query string to evaluate.\n\n    You can refer to variables\n    in the environment by prefixing them with an '@' character like\n    ``@a + b``.\n\n    You can refer to column names that are not valid Python variable names\n    by surrounding them in backticks. Thus, column names containing spaces\n    or punctuations (besides underscores) or starting with digits must be\n    surrounded by backticks. (For example, a column named \"Area (cm^2)\" would\n    be referenced as ```Area (cm^2)```). Column names which are Python keywords\n    (like \"list\", \"for\", \"import\", etc) cannot be used.\n\n    For example, if one of your columns is called ``a a`` and you want\n    to sum it with ``b``, your query should be ```a a` + b``.\n\n    .. versionadded:: 0.25.0\n        Backtick quoting introduced.\n\n    .. versionadded:: 1.0.0\n        Expanding functionality of backtick quoting for more than only spaces.\n\ninplace : bool\n    Whether to modify the DataFrame rather than creating a new one.\n**kwargs\n    See the documentation for :func:`eval` for complete details\n    on the keyword arguments accepted by :meth:`DataFrame.query`.\n\nReturns\n-------\nDataFrame or None\n    DataFrame resulting from the provided query expression or\n    None if ``inplace=True``.\n\nSee Also\n--------\neval : Evaluate a string describing operations on\n    DataFrame columns.\nDataFrame.eval : Evaluate a string describing operations on\n    DataFrame columns.\n\nNotes\n-----\nThe result of the evaluation of this expression is first passed to\n:attr:`DataFrame.loc` and if that fails because of a\nmultidimensional key (e.g., a DataFrame) then the result will be passed\nto :meth:`DataFrame.__getitem__`.\n\nThis method uses the top-level :func:`eval` function to\nevaluate the passed query.\n\nThe :meth:`~pandas.DataFrame.query` method uses a slightly\nmodified Python syntax by default. For example, the ``&`` and ``|``\n(bitwise) operators have the precedence of their boolean cousins,\n:keyword:`and` and :keyword:`or`. This *is* syntactically valid Python,\nhowever the semantics are different.\n\nYou can change the semantics of the expression by passing the keyword\nargument ``parser='python'``. This enforces the same semantics as\nevaluation in Python space. Likewise, you can pass ``engine='python'``\nto evaluate an expression using Python itself as a backend. This is not\nrecommended as it is inefficient compared to using ``numexpr`` as the\nengine.\n\nThe :attr:`DataFrame.index` and\n:attr:`DataFrame.columns` attributes of the\n:class:`~pandas.DataFrame` instance are placed in the query namespace\nby default, which allows you to treat both the index and columns of the\nframe as a column in the frame.\nThe identifier ``index`` is used for the frame index; you can also\nuse the name of the index to identify it in a query. Please note that\nPython keywords may not be used as identifiers.\n\nFor further details and examples see the ``query`` documentation in\n:ref:`indexing <indexing.query>`.\n\n*Backtick quoted variables*\n\nBacktick quoted variables are parsed as literal Python code and\nare converted internally to a Python valid identifier.\nThis can lead to the following problems.\n\nDuring parsing a number of disallowed characters inside the backtick\nquoted string are replaced by strings that are allowed as a Python identifier.\nThese characters include all operators in Python, the space character, the\nquestion mark, the exclamation mark, the dollar sign, and the euro sign.\nFor other characters that fall outside the ASCII range (U+0001..U+007F)\nand those that are not further specified in PEP 3131,\nthe query parser will raise an error.\nThis excludes whitespace different than the space character,\nbut also the hashtag (as it is used for comments) and the backtick\nitself (backtick can also not be escaped).\n\nIn a special case, quotes that make a pair around a backtick can\nconfuse the parser.\nFor example, ```it's` > `that's``` will raise an error,\nas it forms a quoted string (``'s > `that'``) with a backtick inside.\n\nSee also the Python documentation about lexical analysis\n(https://docs.python.org/3/reference/lexical_analysis.html)\nin combination with the source code in :mod:`pandas.core.computation.parsing`.\n\nExamples\n--------\n>>> df = pd.DataFrame({'A': range(1, 6),\n...                    'B': range(10, 0, -2),\n...                    'C C': range(10, 5, -1)})\n>>> df\n   A   B  C C\n0  1  10   10\n1  2   8    9\n2  3   6    8\n3  4   4    7\n4  5   2    6\n>>> df.query('A > B')\n   A  B  C C\n4  5  2    6\n\nThe previous expression is equivalent to\n\n>>> df[df.A > df.B]\n   A  B  C C\n4  5  2    6\n\nFor columns with spaces in their name, you can use backtick quoting.\n\n>>> df.query('B == `C C`')\n   A   B  C C\n0  1  10   10\n\nThe previous expression is equivalent to\n\n>>> df[df.B == df['C C']]\n   A   B  C C\n0  1  10   10", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.reindex", "Docstring": "Conform Series/DataFrame to new index with optional filling logic.\n\nPlaces NA/NaN in locations having no value in the previous index. A new object\nis produced unless the new index is equivalent to the current one and\n``copy=False``.\n\nParameters\n----------\n\nkeywords for axes : array-like, optional\n    New labels / index to conform to, should be specified using\n    keywords. Preferably an Index object to avoid duplicating data.\n\nmethod : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'}\n    Method to use for filling holes in reindexed DataFrame.\n    Please note: this is only applicable to DataFrames/Series with a\n    monotonically increasing/decreasing index.\n\n    * None (default): don't fill gaps\n    * pad / ffill: Propagate last valid observation forward to next\n      valid.\n    * backfill / bfill: Use next valid observation to fill gap.\n    * nearest: Use nearest valid observations to fill gap.\n\ncopy : bool, default True\n    Return a new object, even if the passed indexes are the same.\nlevel : int or name\n    Broadcast across a level, matching Index values on the\n    passed MultiIndex level.\nfill_value : scalar, default np.NaN\n    Value to use for missing values. Defaults to NaN, but can be any\n    \"compatible\" value.\nlimit : int, default None\n    Maximum number of consecutive elements to forward or backward fill.\ntolerance : optional\n    Maximum distance between original and new labels for inexact\n    matches. The values of the index at the matching locations most\n    satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n\n    Tolerance may be a scalar value, which applies the same tolerance\n    to all values, or list-like, which applies variable tolerance per\n    element. List-like includes list, tuple, array, Series, and must be\n    the same size as the index and its dtype must exactly match the\n    index's type.\n\nReturns\n-------\nSeries/DataFrame with changed index.\n\nSee Also\n--------\nDataFrame.set_index : Set row labels.\nDataFrame.reset_index : Remove row labels or move them to new columns.\nDataFrame.reindex_like : Change to same indices as other DataFrame.\n\nExamples\n--------\n``DataFrame.reindex`` supports two calling conventions\n\n* ``(index=index_labels, columns=column_labels, ...)``\n* ``(labels, axis={'index', 'columns'}, ...)``\n\nWe *highly* recommend using keyword arguments to clarify your\nintent.\n\nCreate a dataframe with some fictional data.\n\n>>> index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror']\n>>> df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301],\n...                   'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]},\n...                   index=index)\n>>> df\n           http_status  response_time\nFirefox            200           0.04\nChrome             200           0.02\nSafari             404           0.07\nIE10               404           0.08\nKonqueror          301           1.00\n\nCreate a new index and reindex the dataframe. By default\nvalues in the new index that do not have corresponding\nrecords in the dataframe are assigned ``NaN``.\n\n>>> new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10',\n...              'Chrome']\n>>> df.reindex(new_index)\n               http_status  response_time\nSafari               404.0           0.07\nIceweasel              NaN            NaN\nComodo Dragon          NaN            NaN\nIE10                 404.0           0.08\nChrome               200.0           0.02\n\nWe can fill in the missing values by passing a value to\nthe keyword ``fill_value``. Because the index is not monotonically\nincreasing or decreasing, we cannot use arguments to the keyword\n``method`` to fill the ``NaN`` values.\n\n>>> df.reindex(new_index, fill_value=0)\n               http_status  response_time\nSafari                 404           0.07\nIceweasel                0           0.00\nComodo Dragon            0           0.00\nIE10                   404           0.08\nChrome                 200           0.02\n\n>>> df.reindex(new_index, fill_value='missing')\n              http_status response_time\nSafari                404          0.07\nIceweasel         missing       missing\nComodo Dragon     missing       missing\nIE10                  404          0.08\nChrome                200          0.02\n\nWe can also reindex the columns.\n\n>>> df.reindex(columns=['http_status', 'user_agent'])\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN\n\nOr we can use \"axis-style\" keyword arguments\n\n>>> df.reindex(['http_status', 'user_agent'], axis=\"columns\")\n           http_status  user_agent\nFirefox            200         NaN\nChrome             200         NaN\nSafari             404         NaN\nIE10               404         NaN\nKonqueror          301         NaN\n\nTo further illustrate the filling functionality in\n``reindex``, we will create a dataframe with a\nmonotonically increasing index (for example, a sequence\nof dates).\n\n>>> date_index = pd.date_range('1/1/2010', periods=6, freq='D')\n>>> df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]},\n...                    index=date_index)\n>>> df2\n            prices\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n\nSuppose we decide to expand the dataframe to cover a wider\ndate range.\n\n>>> date_index2 = pd.date_range('12/29/2009', periods=10, freq='D')\n>>> df2.reindex(date_index2)\n            prices\n2009-12-29     NaN\n2009-12-30     NaN\n2009-12-31     NaN\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN\n\nThe index entries that did not have a value in the original data frame\n(for example, '2009-12-29') are by default filled with ``NaN``.\nIf desired, we can fill in the missing values using one of several\noptions.\n\nFor example, to back-propagate the last valid value to fill the ``NaN``\nvalues, pass ``bfill`` as an argument to the ``method`` keyword.\n\n>>> df2.reindex(date_index2, method='bfill')\n            prices\n2009-12-29   100.0\n2009-12-30   100.0\n2009-12-31   100.0\n2010-01-01   100.0\n2010-01-02   101.0\n2010-01-03     NaN\n2010-01-04   100.0\n2010-01-05    89.0\n2010-01-06    88.0\n2010-01-07     NaN\n\nPlease note that the ``NaN`` value present in the original dataframe\n(at index value 2010-01-03) will not be filled by any of the\nvalue propagation schemes. This is because filling while reindexing\ndoes not look at dataframe values, but only compares the original and\ndesired indexes. If you do want to fill in the ``NaN`` values present\nin the original dataframe, use the ``fillna()`` method.\n\nSee the :ref:`user guide <basics.reindexing>` for more.", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.rename", "Docstring": "Alter axes labels.\n\nFunction / dict values must be unique (1-to-1). Labels not contained in\na dict / Series will be left as-is. Extra labels listed don't throw an\nerror.\n\nSee the :ref:`user guide <basics.rename>` for more.\n\nParameters\n----------\nmapper : dict-like or function\n    Dict-like or function transformations to apply to\n    that axis' values. Use either ``mapper`` and ``axis`` to\n    specify the axis to target with ``mapper``, or ``index`` and\n    ``columns``.\nindex : dict-like or function\n    Alternative to specifying axis (``mapper, axis=0``\n    is equivalent to ``index=mapper``).\ncolumns : dict-like or function\n    Alternative to specifying axis (``mapper, axis=1``\n    is equivalent to ``columns=mapper``).\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Axis to target with ``mapper``. Can be either the axis name\n    ('index', 'columns') or number (0, 1). The default is 'index'.\ncopy : bool, default True\n    Also copy underlying data.\ninplace : bool, default False\n    Whether to modify the DataFrame rather than creating a new one.\n    If True then value of copy is ignored.\nlevel : int or level name, default None\n    In case of a MultiIndex, only rename labels in the specified\n    level.\nerrors : {'ignore', 'raise'}, default 'ignore'\n    If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`,\n    or `columns` contains labels that are not present in the Index\n    being transformed.\n    If 'ignore', existing keys will be renamed and extra keys will be\n    ignored.\n\nReturns\n-------\nDataFrame or None\n    DataFrame with the renamed axis labels or None if ``inplace=True``.\n\nRaises\n------\nKeyError\n    If any of the labels is not found in the selected axis and\n    \"errors='raise'\".\n\nSee Also\n--------\nDataFrame.rename_axis : Set the name of the axis.\n\nExamples\n--------\n``DataFrame.rename`` supports two calling conventions\n\n* ``(index=index_mapper, columns=columns_mapper, ...)``\n* ``(mapper, axis={'index', 'columns'}, ...)``\n\nWe *highly* recommend using keyword arguments to clarify your\nintent.\n\nRename columns using a mapping:\n\n>>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n>>> df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n   a  c\n0  1  4\n1  2  5\n2  3  6\n\nRename index using a mapping:\n\n>>> df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"})\n   A  B\nx  1  4\ny  2  5\nz  3  6\n\nCast index labels to a different type:\n\n>>> df.index\nRangeIndex(start=0, stop=3, step=1)\n>>> df.rename(index=str).index\nIndex(['0', '1', '2'], dtype='object')\n\n>>> df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\")\nTraceback (most recent call last):\nKeyError: ['C'] not found in axis\n\nUsing axis-style parameters:\n\n>>> df.rename(str.lower, axis='columns')\n   a  b\n0  1  4\n1  2  5\n2  3  6\n\n>>> df.rename({1: 2, 2: 4}, axis='index')\n   A  B\n0  1  4\n2  2  5\n4  3  6", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.rename_axis", "Docstring": "Set the name of the axis for the index or columns.\n\nParameters\n----------\nmapper : scalar, list-like, optional\n    Value to set the axis name attribute.\nindex, columns : scalar, list-like, dict-like or function, optional\n    A scalar, list-like, dict-like or functions transformations to\n    apply to that axis' values.\n    Note that the ``columns`` parameter is not allowed if the\n    object is a Series. This parameter only apply for DataFrame\n    type objects.\n\n    Use either ``mapper`` and ``axis`` to\n    specify the axis to target with ``mapper``, or ``index``\n    and/or ``columns``.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The axis to rename. For `Series` this parameter is unused and defaults to 0.\ncopy : bool, default True\n    Also copy underlying data.\ninplace : bool, default False\n    Modifies the object directly, instead of creating a new Series\n    or DataFrame.\n\nReturns\n-------\nSeries, DataFrame, or None\n    The same type as the caller or None if ``inplace=True``.\n\nSee Also\n--------\nSeries.rename : Alter Series index labels or name.\nDataFrame.rename : Alter DataFrame index labels or name.\nIndex.rename : Set new names on index.\n\nNotes\n-----\n``DataFrame.rename_axis`` supports two calling conventions\n\n* ``(index=index_mapper, columns=columns_mapper, ...)``\n* ``(mapper, axis={'index', 'columns'}, ...)``\n\nThe first calling convention will only modify the names of\nthe index and/or the names of the Index object that is the columns.\nIn this case, the parameter ``copy`` is ignored.\n\nThe second calling convention will modify the names of the\ncorresponding index if mapper is a list or a scalar.\nHowever, if mapper is dict-like or a function, it will use the\ndeprecated behavior of modifying the axis *labels*.\n\nWe *highly* recommend using keyword arguments to clarify your\nintent.\n\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([\"dog\", \"cat\", \"monkey\"])\n>>> s\n0       dog\n1       cat\n2    monkey\ndtype: object\n>>> s.rename_axis(\"animal\")\nanimal\n0    dog\n1    cat\n2    monkey\ndtype: object\n\n**DataFrame**\n\n>>> df = pd.DataFrame({\"num_legs\": [4, 4, 2],\n...                    \"num_arms\": [0, 0, 2]},\n...                   [\"dog\", \"cat\", \"monkey\"])\n>>> df\n        num_legs  num_arms\ndog            4         0\ncat            4         0\nmonkey         2         2\n>>> df = df.rename_axis(\"animal\")\n>>> df\n        num_legs  num_arms\nanimal\ndog            4         0\ncat            4         0\nmonkey         2         2\n>>> df = df.rename_axis(\"limbs\", axis=\"columns\")\n>>> df\nlimbs   num_legs  num_arms\nanimal\ndog            4         0\ncat            4         0\nmonkey         2         2\n\n**MultiIndex**\n\n>>> df.index = pd.MultiIndex.from_product([['mammal'],\n...                                        ['dog', 'cat', 'monkey']],\n...                                       names=['type', 'name'])\n>>> df\nlimbs          num_legs  num_arms\ntype   name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2\n\n>>> df.rename_axis(index={'type': 'class'})\nlimbs          num_legs  num_arms\nclass  name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2\n\n>>> df.rename_axis(columns=str.upper)\nLIMBS          num_legs  num_arms\ntype   name\nmammal dog            4         0\n       cat            4         0\n       monkey         2         2", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.replace", "Docstring": "Replace values given in `to_replace` with `value`.\n\nValues of the DataFrame are replaced with other values dynamically.\n\nThis differs from updating with ``.loc`` or ``.iloc``, which require\nyou to specify a location to update with some value.\n\nParameters\n----------\nto_replace : str, regex, list, dict, Series, int, float, or None\n    How to find the values that will be replaced.\n\n    * numeric, str or regex:\n\n        - numeric: numeric values equal to `to_replace` will be\n          replaced with `value`\n        - str: string exactly matching `to_replace` will be replaced\n          with `value`\n        - regex: regexs matching `to_replace` will be replaced with\n          `value`\n\n    * list of str, regex, or numeric:\n\n        - First, if `to_replace` and `value` are both lists, they\n          **must** be the same length.\n        - Second, if ``regex=True`` then all of the strings in **both**\n          lists will be interpreted as regexs otherwise they will match\n          directly. This doesn't matter much for `value` since there\n          are only a few possible substitution regexes you can use.\n        - str, regex and numeric rules apply as above.\n\n    * dict:\n\n        - Dicts can be used to specify different replacement values\n          for different existing values. For example,\n          ``{'a': 'b', 'y': 'z'}`` replaces the value 'a' with 'b' and\n          'y' with 'z'. To use a dict in this way, the optional `value`\n          parameter should not be given.\n        - For a DataFrame a dict can specify that different values\n          should be replaced in different columns. For example,\n          ``{'a': 1, 'b': 'z'}`` looks for the value 1 in column 'a'\n          and the value 'z' in column 'b' and replaces these values\n          with whatever is specified in `value`. The `value` parameter\n          should not be ``None`` in this case. You can treat this as a\n          special case of passing two lists except that you are\n          specifying the column to search in.\n        - For a DataFrame nested dictionaries, e.g.,\n          ``{'a': {'b': np.nan}}``, are read as follows: look in column\n          'a' for the value 'b' and replace it with NaN. The optional `value`\n          parameter should not be specified to use a nested dict in this\n          way. You can nest regular expressions as well. Note that\n          column names (the top-level dictionary keys in a nested\n          dictionary) **cannot** be regular expressions.\n\n    * None:\n\n        - This means that the `regex` argument must be a string,\n          compiled regular expression, or list, dict, ndarray or\n          Series of such elements. If `value` is also ``None`` then\n          this **must** be a nested dictionary or Series.\n\n    See the examples section for examples of each of these.\nvalue : scalar, dict, list, str, regex, default None\n    Value to replace any values matching `to_replace` with.\n    For a DataFrame a dict of values can be used to specify which\n    value to use for each column (columns not in the dict will not be\n    filled). Regular expressions, strings and lists or dicts of such\n    objects are also allowed.\n\ninplace : bool, default False\n    Whether to modify the DataFrame rather than creating a new one.\nlimit : int, default None\n    Maximum size gap to forward or backward fill.\nregex : bool or same types as `to_replace`, default False\n    Whether to interpret `to_replace` and/or `value` as regular\n    expressions. If this is ``True`` then `to_replace` *must* be a\n    string. Alternatively, this could be a regular expression or a\n    list, dict, or array of regular expressions in which case\n    `to_replace` must be ``None``.\nmethod : {'pad', 'ffill', 'bfill'}\n    The method to use when for replacement, when `to_replace` is a\n    scalar, list or tuple and `value` is ``None``.\n\n    .. versionchanged:: 0.23.0\n        Added to DataFrame.\n\nReturns\n-------\nDataFrame\n    Object after replacement.\n\nRaises\n------\nAssertionError\n    * If `regex` is not a ``bool`` and `to_replace` is not\n      ``None``.\n\nTypeError\n    * If `to_replace` is not a scalar, array-like, ``dict``, or ``None``\n    * If `to_replace` is a ``dict`` and `value` is not a ``list``,\n      ``dict``, ``ndarray``, or ``Series``\n    * If `to_replace` is ``None`` and `regex` is not compilable\n      into a regular expression or is a list, dict, ndarray, or\n      Series.\n    * When replacing multiple ``bool`` or ``datetime64`` objects and\n      the arguments to `to_replace` does not match the type of the\n      value being replaced\n\nValueError\n    * If a ``list`` or an ``ndarray`` is passed to `to_replace` and\n      `value` but they are not the same length.\n\nSee Also\n--------\nDataFrame.fillna : Fill NA values.\nDataFrame.where : Replace values based on boolean condition.\nSeries.str.replace : Simple string replacement.\n\nNotes\n-----\n* Regex substitution is performed under the hood with ``re.sub``. The\n  rules for substitution for ``re.sub`` are the same.\n* Regular expressions will only substitute on strings, meaning you\n  cannot provide, for example, a regular expression matching floating\n  point numbers and expect the columns in your frame that have a\n  numeric dtype to be matched. However, if those floating point\n  numbers *are* strings, then you can do this.\n* This method has *a lot* of options. You are encouraged to experiment\n  and play with this method to gain intuition about how it works.\n* When dict is used as the `to_replace` value, it is like\n  key(s) in the dict are the to_replace part and\n  value(s) in the dict are the value parameter.\n\nExamples\n--------\n\n**Scalar `to_replace` and `value`**\n\n>>> s = pd.Series([1, 2, 3, 4, 5])\n>>> s.replace(1, 5)\n0    5\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n>>> df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n>>> df.replace(0, 5)\n    A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n\n**List-like `to_replace`**\n\n>>> df.replace([0, 1, 2, 3], 4)\n    A  B  C\n0  4  5  a\n1  4  6  b\n2  4  7  c\n3  4  8  d\n4  4  9  e\n\n>>> df.replace([0, 1, 2, 3], [4, 3, 2, 1])\n    A  B  C\n0  4  5  a\n1  3  6  b\n2  2  7  c\n3  1  8  d\n4  4  9  e\n\n>>> s.replace([1, 2], method='bfill')\n0    3\n1    3\n2    3\n3    4\n4    5\ndtype: int64\n\n**dict-like `to_replace`**\n\n>>> df.replace({0: 10, 1: 100})\n        A  B  C\n0   10  5  a\n1  100  6  b\n2    2  7  c\n3    3  8  d\n4    4  9  e\n\n>>> df.replace({'A': 0, 'B': 5}, 100)\n        A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n\n>>> df.replace({'A': {0: 100, 4: 400}})\n        A  B  C\n0  100  5  a\n1    1  6  b\n2    2  7  c\n3    3  8  d\n4  400  9  e\n\n**Regular expression `to_replace`**\n\n>>> df = pd.DataFrame({'A': ['bat', 'foo', 'bait'],\n...                    'B': ['abc', 'bar', 'xyz']})\n>>> df.replace(to_replace=r'^ba.$', value='new', regex=True)\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n\n>>> df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True)\n        A    B\n0   new  abc\n1   foo  bar\n2  bait  xyz\n\n>>> df.replace(regex=r'^ba.$', value='new')\n        A    B\n0   new  abc\n1   foo  new\n2  bait  xyz\n\n>>> df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'})\n        A    B\n0   new  abc\n1   xyz  new\n2  bait  xyz\n\n>>> df.replace(regex=[r'^ba.$', 'foo'], value='new')\n        A    B\n0   new  abc\n1   new  new\n2  bait  xyz\n\nCompare the behavior of ``s.replace({'a': None})`` and\n``s.replace('a', None)`` to understand the peculiarities\nof the `to_replace` parameter:\n\n>>> s = pd.Series([10, 'a', 'a', 'b', 'a'])\n\nWhen one uses a dict as the `to_replace` value, it is like the\nvalue(s) in the dict are equal to the `value` parameter.\n``s.replace({'a': None})`` is equivalent to\n``s.replace(to_replace={'a': None}, value=None, method=None)``:\n\n>>> s.replace({'a': None})\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n\nWhen ``value`` is not explicitly passed and `to_replace` is a scalar, list\nor tuple, `replace` uses the method parameter (default 'pad') to do the\nreplacement. So this is why the 'a' values are being replaced by 10\nin rows 1 and 2 and 'b' in row 4 in this case.\n\n>>> s.replace('a')\n0    10\n1    10\n2    10\n3     b\n4     b\ndtype: object\n\nOn the other hand, if ``None`` is explicitly passed for ``value``, it will\nbe respected:\n\n>>> s.replace('a', None)\n0      10\n1    None\n2    None\n3       b\n4    None\ndtype: object\n\n    .. versionchanged:: 1.4.0\n        Previously the explicit ``None`` was silently ignored.", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.reset_index", "Docstring": "Reset the index, or a level of it.\n\nReset the index of the DataFrame, and use the default one instead.\nIf the DataFrame has a MultiIndex, this method can remove one or more\nlevels.\n\nParameters\n----------\nlevel : int, str, tuple, or list, default None\n    Only remove the given levels from the index. Removes all levels by\n    default.\ndrop : bool, default False\n    Do not try to insert index into dataframe columns. This resets\n    the index to the default integer index.\ninplace : bool, default False\n    Whether to modify the DataFrame rather than creating a new one.\ncol_level : int or str, default 0\n    If the columns have multiple levels, determines which level the\n    labels are inserted into. By default it is inserted into the first\n    level.\ncol_fill : object, default ''\n    If the columns have multiple levels, determines how the other\n    levels are named. If None then the index name is repeated.\nallow_duplicates : bool, optional, default lib.no_default\n    Allow duplicate column labels to be created.\n\n    .. versionadded:: 1.5.0\n\nnames : int, str or 1-dimensional list, default None\n    Using the given string, rename the DataFrame column which contains the\n    index data. If the DataFrame has a MultiIndex, this has to be a list or\n    tuple with length equal to the number of levels.\n\n    .. versionadded:: 1.5.0\n\nReturns\n-------\nDataFrame or None\n    DataFrame with the new index or None if ``inplace=True``.\n\nSee Also\n--------\nDataFrame.set_index : Opposite of reset_index.\nDataFrame.reindex : Change to new indices or expand indices.\nDataFrame.reindex_like : Change to same indices as other DataFrame.\n\nExamples\n--------\n>>> df = pd.DataFrame([('bird', 389.0),\n...                    ('bird', 24.0),\n...                    ('mammal', 80.5),\n...                    ('mammal', np.nan)],\n...                   index=['falcon', 'parrot', 'lion', 'monkey'],\n...                   columns=('class', 'max_speed'))\n>>> df\n         class  max_speed\nfalcon    bird      389.0\nparrot    bird       24.0\nlion    mammal       80.5\nmonkey  mammal        NaN\n\nWhen we reset the index, the old index is added as a column, and a\nnew sequential index is used:\n\n>>> df.reset_index()\n    index   class  max_speed\n0  falcon    bird      389.0\n1  parrot    bird       24.0\n2    lion  mammal       80.5\n3  monkey  mammal        NaN\n\nWe can use the `drop` parameter to avoid the old index being added as\na column:\n\n>>> df.reset_index(drop=True)\n    class  max_speed\n0    bird      389.0\n1    bird       24.0\n2  mammal       80.5\n3  mammal        NaN\n\nYou can also use `reset_index` with `MultiIndex`.\n\n>>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'),\n...                                    ('bird', 'parrot'),\n...                                    ('mammal', 'lion'),\n...                                    ('mammal', 'monkey')],\n...                                   names=['class', 'name'])\n>>> columns = pd.MultiIndex.from_tuples([('speed', 'max'),\n...                                      ('species', 'type')])\n>>> df = pd.DataFrame([(389.0, 'fly'),\n...                    ( 24.0, 'fly'),\n...                    ( 80.5, 'run'),\n...                    (np.nan, 'jump')],\n...                   index=index,\n...                   columns=columns)\n>>> df\n               speed species\n                 max    type\nclass  name\nbird   falcon  389.0     fly\n       parrot   24.0     fly\nmammal lion     80.5     run\n       monkey    NaN    jump\n\nUsing the `names` parameter, choose a name for the index column:\n\n>>> df.reset_index(names=['classes', 'names'])\n  classes   names  speed species\n                     max    type\n0    bird  falcon  389.0     fly\n1    bird  parrot   24.0     fly\n2  mammal    lion   80.5     run\n3  mammal  monkey    NaN    jump\n\nIf the index has multiple levels, we can reset a subset of them:\n\n>>> df.reset_index(level='class')\n         class  speed species\n                  max    type\nname\nfalcon    bird  389.0     fly\nparrot    bird   24.0     fly\nlion    mammal   80.5     run\nmonkey  mammal    NaN    jump\n\nIf we are not dropping the index, by default, it is placed in the top\nlevel. We can place it in another level:\n\n>>> df.reset_index(level='class', col_level=1)\n                speed species\n         class    max    type\nname\nfalcon    bird  389.0     fly\nparrot    bird   24.0     fly\nlion    mammal   80.5     run\nmonkey  mammal    NaN    jump\n\nWhen the index is inserted under another level, we can specify under\nwhich one with the parameter `col_fill`:\n\n>>> df.reset_index(level='class', col_level=1, col_fill='species')\n              species  speed species\n                class    max    type\nname\nfalcon           bird  389.0     fly\nparrot           bird   24.0     fly\nlion           mammal   80.5     run\nmonkey         mammal    NaN    jump\n\nIf we specify a nonexistent level for `col_fill`, it is created:\n\n>>> df.reset_index(level='class', col_level=1, col_fill='genus')\n                genus  speed species\n                class    max    type\nname\nfalcon           bird  389.0     fly\nparrot           bird   24.0     fly\nlion           mammal   80.5     run\nmonkey         mammal    NaN    jump", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.sample", "Docstring": "Return a random sample of items from an axis of object.\n\nYou can use `random_state` for reproducibility.\n\nParameters\n----------\nn : int, optional\n    Number of items from axis to return. Cannot be used with `frac`.\n    Default = 1 if `frac` = None.\nfrac : float, optional\n    Fraction of axis items to return. Cannot be used with `n`.\nreplace : bool, default False\n    Allow or disallow sampling of the same row more than once.\nweights : str or ndarray-like, optional\n    Default 'None' results in equal probability weighting.\n    If passed a Series, will align with target object on index. Index\n    values in weights not found in sampled object will be ignored and\n    index values in sampled object not in weights will be assigned\n    weights of zero.\n    If called on a DataFrame, will accept the name of a column\n    when axis = 0.\n    Unless weights are a Series, weights must be same length as axis\n    being sampled.\n    If weights do not sum to 1, they will be normalized to sum to 1.\n    Missing values in the weights column will be treated as zero.\n    Infinite values not allowed.\nrandom_state : int, array-like, BitGenerator, np.random.RandomState, np.random.Generator, optional\n    If int, array-like, or BitGenerator, seed for random number generator.\n    If np.random.RandomState or np.random.Generator, use as given.\n\n    .. versionchanged:: 1.1.0\n\n        array-like and BitGenerator object now passed to np.random.RandomState()\n        as seed\n\n    .. versionchanged:: 1.4.0\n\n        np.random.Generator objects now accepted\n\naxis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None\n    Axis to sample. Accepts axis number or name. Default is stat axis\n    for given data type. For `Series` this parameter is unused and defaults to `None`.\nignore_index : bool, default False\n    If True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\n    .. versionadded:: 1.3.0\n\nReturns\n-------\nSeries or DataFrame\n    A new object of same type as caller containing `n` items randomly\n    sampled from the caller object.\n\nSee Also\n--------\nDataFrameGroupBy.sample: Generates random samples from each group of a\n    DataFrame object.\nSeriesGroupBy.sample: Generates random samples from each group of a\n    Series object.\nnumpy.random.choice: Generates a random sample from a given 1-D numpy\n    array.\n\nNotes\n-----\nIf `frac` > 1, `replacement` should be set to `True`.\n\nExamples\n--------\n>>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0],\n...                    'num_wings': [2, 0, 0, 0],\n...                    'num_specimen_seen': [10, 2, 1, 8]},\n...                   index=['falcon', 'dog', 'spider', 'fish'])\n>>> df\n        num_legs  num_wings  num_specimen_seen\nfalcon         2          2                 10\ndog            4          0                  2\nspider         8          0                  1\nfish           0          0                  8\n\nExtract 3 random elements from the ``Series`` ``df['num_legs']``:\nNote that we use `random_state` to ensure the reproducibility of\nthe examples.\n\n>>> df['num_legs'].sample(n=3, random_state=1)\nfish      0\nspider    8\nfalcon    2\nName: num_legs, dtype: int64\n\nA random 50% sample of the ``DataFrame`` with replacement:\n\n>>> df.sample(frac=0.5, replace=True, random_state=1)\n      num_legs  num_wings  num_specimen_seen\ndog          4          0                  2\nfish         0          0                  8\n\nAn upsample sample of the ``DataFrame`` with replacement:\nNote that `replace` parameter has to be `True` for `frac` parameter > 1.\n\n>>> df.sample(frac=2, replace=True, random_state=1)\n        num_legs  num_wings  num_specimen_seen\ndog            4          0                  2\nfish           0          0                  8\nfalcon         2          2                 10\nfalcon         2          2                 10\nfish           0          0                  8\ndog            4          0                  2\nfish           0          0                  8\ndog            4          0                  2\n\nUsing a DataFrame column as weights. Rows with larger value in the\n`num_specimen_seen` column are more likely to be sampled.\n\n>>> df.sample(n=2, weights='num_specimen_seen', random_state=1)\n        num_legs  num_wings  num_specimen_seen\nfalcon         2          2                 10\nfish           0          0                  8", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.set_axis", "Docstring": "Assign desired index to given axis.\n\nIndexes for column or row labels can be changed by assigning\na list-like or Index.\n\nParameters\n----------\nlabels : list-like, Index\n    The values for the new index.\n\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The axis to update. The value 0 identifies the rows. For `Series`\n    this parameter is unused and defaults to 0.\n\ninplace : bool, default False\n    Whether to return a new DataFrame instance.\n\n    .. deprecated:: 1.5.0\n\ncopy : bool, default True\n    Whether to make a copy of the underlying data.\n\n    .. versionadded:: 1.5.0\n\nReturns\n-------\nrenamed : DataFrame or None\n    An object of type DataFrame or None if ``inplace=True``.\n\nSee Also\n--------\nDataFrame.rename_axis : Alter the name of the index or columns.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n        Change the row labels.\n\n        >>> df.set_axis(['a', 'b', 'c'], axis='index')\n           A  B\n        a  1  4\n        b  2  5\n        c  3  6\n\n        Change the column labels.\n\n        >>> df.set_axis(['I', 'II'], axis='columns')\n           I  II\n        0  1   4\n        1  2   5\n        2  3   6\n\n        Now, update the labels without copying the underlying data.\n\n        >>> df.set_axis(['i', 'ii'], axis='columns', copy=False)\n           i  ii\n        0  1   4\n        1  2   5\n        2  3   6", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.set_index", "Docstring": "Set the DataFrame index using existing columns.\n\nSet the DataFrame index (row labels) using one or more existing\ncolumns or arrays (of the correct length). The index can replace the\nexisting index or expand on it.\n\nParameters\n----------\nkeys : label or array-like or list of labels/arrays\n    This parameter can be either a single column key, a single array of\n    the same length as the calling DataFrame, or a list containing an\n    arbitrary combination of column keys and arrays. Here, \"array\"\n    encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n    instances of :class:`~collections.abc.Iterator`.\ndrop : bool, default True\n    Delete columns to be used as the new index.\nappend : bool, default False\n    Whether to append columns to existing index.\ninplace : bool, default False\n    Whether to modify the DataFrame rather than creating a new one.\nverify_integrity : bool, default False\n    Check the new index for duplicates. Otherwise defer the check until\n    necessary. Setting to False will improve the performance of this\n    method.\n\nReturns\n-------\nDataFrame or None\n    Changed row labels or None if ``inplace=True``.\n\nSee Also\n--------\nDataFrame.reset_index : Opposite of set_index.\nDataFrame.reindex : Change to new indices or expand indices.\nDataFrame.reindex_like : Change to same indices as other DataFrame.\n\nExamples\n--------\n>>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n...                    'year': [2012, 2014, 2013, 2014],\n...                    'sale': [55, 40, 84, 31]})\n>>> df\n   month  year  sale\n0      1  2012    55\n1      4  2014    40\n2      7  2013    84\n3     10  2014    31\n\nSet the index to become the 'month' column:\n\n>>> df.set_index('month')\n       year  sale\nmonth\n1      2012    55\n4      2014    40\n7      2013    84\n10     2014    31\n\nCreate a MultiIndex using columns 'year' and 'month':\n\n>>> df.set_index(['year', 'month'])\n            sale\nyear  month\n2012  1     55\n2014  4     40\n2013  7     84\n2014  10    31\n\nCreate a MultiIndex using an Index and a column:\n\n>>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n         month  sale\n   year\n1  2012  1      55\n2  2014  4      40\n3  2013  7      84\n4  2014  10     31\n\nCreate a MultiIndex using two Series:\n\n>>> s = pd.Series([1, 2, 3, 4])\n>>> df.set_index([s, s**2])\n      month  year  sale\n1 1       1  2012    55\n2 4       4  2014    40\n3 9       7  2013    84\n4 16     10  2014    31", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.shape", "Docstring": "Return a tuple representing the dimensionality of the DataFrame.\n\nSee Also\n--------\nndarray.shape : Tuple of array dimensions.\n\nExamples\n--------\n>>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n>>> df.shape\n(2, 2)\n\n>>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4],\n...                    'col3': [5, 6]})\n>>> df.shape\n(2, 3)", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.shift", "Docstring": "Shift index by desired number of periods with an optional time `freq`.\n\nWhen `freq` is not passed, shift the index without realigning the data.\nIf `freq` is passed (in this case, the index must be date or datetime,\nor it will raise a `NotImplementedError`), the index will be\nincreased using the periods and the `freq`. `freq` can be inferred\nwhen specified as \"infer\" as long as either freq or inferred_freq\nattribute is set in the index.\n\nParameters\n----------\nperiods : int\n    Number of periods to shift. Can be positive or negative.\nfreq : DateOffset, tseries.offsets, timedelta, or str, optional\n    Offset to use from the tseries module or time rule (e.g. 'EOM').\n    If `freq` is specified then the index values are shifted but the\n    data is not realigned. That is, use `freq` if you would like to\n    extend the index when shifting and preserve the original data.\n    If `freq` is specified as \"infer\" then it will be inferred from\n    the freq or inferred_freq attributes of the index. If neither of\n    those attributes exist, a ValueError is thrown.\naxis : {0 or 'index', 1 or 'columns', None}, default None\n    Shift direction. For `Series` this parameter is unused and defaults to 0.\nfill_value : object, optional\n    The scalar value to use for newly introduced missing values.\n    the default depends on the dtype of `self`.\n    For numeric data, ``np.nan`` is used.\n    For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n    For extension dtypes, ``self.dtype.na_value`` is used.\n\n    .. versionchanged:: 1.1.0\n\nReturns\n-------\nDataFrame\n    Copy of input object, shifted.\n\nSee Also\n--------\nIndex.shift : Shift values of Index.\nDatetimeIndex.shift : Shift values of DatetimeIndex.\nPeriodIndex.shift : Shift values of PeriodIndex.\ntshift : Shift the time index, using the index's frequency if\n    available.\n\nExamples\n--------\n>>> df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45],\n...                    \"Col2\": [13, 23, 18, 33, 48],\n...                    \"Col3\": [17, 27, 22, 37, 52]},\n...                   index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n>>> df\n            Col1  Col2  Col3\n2020-01-01    10    13    17\n2020-01-02    20    23    27\n2020-01-03    15    18    22\n2020-01-04    30    33    37\n2020-01-05    45    48    52\n\n>>> df.shift(periods=3)\n            Col1  Col2  Col3\n2020-01-01   NaN   NaN   NaN\n2020-01-02   NaN   NaN   NaN\n2020-01-03   NaN   NaN   NaN\n2020-01-04  10.0  13.0  17.0\n2020-01-05  20.0  23.0  27.0\n\n>>> df.shift(periods=1, axis=\"columns\")\n            Col1  Col2  Col3\n2020-01-01   NaN    10    13\n2020-01-02   NaN    20    23\n2020-01-03   NaN    15    18\n2020-01-04   NaN    30    33\n2020-01-05   NaN    45    48\n\n>>> df.shift(periods=3, fill_value=0)\n            Col1  Col2  Col3\n2020-01-01     0     0     0\n2020-01-02     0     0     0\n2020-01-03     0     0     0\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n\n>>> df.shift(periods=3, freq=\"D\")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52\n\n>>> df.shift(periods=3, freq=\"infer\")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.sort_index", "Docstring": "Sort object by labels (along an axis).\n\nReturns a new DataFrame sorted by label if `inplace` argument is\n``False``, otherwise updates the original DataFrame and returns None.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The axis along which to sort.  The value 0 identifies the rows,\n    and 1 identifies the columns.\nlevel : int or level name or list of ints or list of level names\n    If not None, sort on values in specified index level(s).\nascending : bool or list-like of bools, default True\n    Sort ascending vs. descending. When the index is a MultiIndex the\n    sort direction can be controlled for each level individually.\ninplace : bool, default False\n    Whether to modify the DataFrame rather than creating a new one.\nkind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n    Choice of sorting algorithm. See also :func:`numpy.sort` for more\n    information. `mergesort` and `stable` are the only stable algorithms. For\n    DataFrames, this option is only applied when sorting on a single\n    column or label.\nna_position : {'first', 'last'}, default 'last'\n    Puts NaNs at the beginning if `first`; `last` puts NaNs at the end.\n    Not implemented for MultiIndex.\nsort_remaining : bool, default True\n    If True and sorting by level and index is multilevel, sort by other\n    levels too (in order) after sorting by specified level.\nignore_index : bool, default False\n    If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n    .. versionadded:: 1.0.0\n\nkey : callable, optional\n    If not None, apply the key function to the index values\n    before sorting. This is similar to the `key` argument in the\n    builtin :meth:`sorted` function, with the notable difference that\n    this `key` function should be *vectorized*. It should expect an\n    ``Index`` and return an ``Index`` of the same shape. For MultiIndex\n    inputs, the key is applied *per level*.\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nDataFrame or None\n    The original DataFrame sorted by the labels or None if ``inplace=True``.\n\nSee Also\n--------\nSeries.sort_index : Sort Series by the index.\nDataFrame.sort_values : Sort DataFrame by the value.\nSeries.sort_values : Sort Series by the value.\n\nExamples\n--------\n>>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150],\n...                   columns=['A'])\n>>> df.sort_index()\n     A\n1    4\n29   2\n100  1\n150  5\n234  3\n\nBy default, it sorts in ascending order, to sort in descending order,\nuse ``ascending=False``\n\n>>> df.sort_index(ascending=False)\n     A\n234  3\n150  5\n100  1\n29   2\n1    4\n\nA key function can be specified which is applied to the index before\nsorting. For a ``MultiIndex`` this is applied to each level separately.\n\n>>> df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd'])\n>>> df.sort_index(key=lambda x: x.str.lower())\n   a\nA  1\nb  2\nC  3\nd  4", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.sort_values", "Docstring": "Sort by the values along either axis.\n\nParameters\n----------\n        by : str or list of str\n            Name or list of names to sort by.\n\n            - if `axis` is 0 or `'index'` then `by` may contain index\n              levels and/or column labels.\n            - if `axis` is 1 or `'columns'` then `by` may contain column\n              levels and/or index labels.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n     Axis to be sorted.\nascending : bool or list of bool, default True\n     Sort ascending vs. descending. Specify list for multiple sort\n     orders.  If this is a list of bools, must match the length of\n     the by.\ninplace : bool, default False\n     If True, perform operation in-place.\nkind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n     Choice of sorting algorithm. See also :func:`numpy.sort` for more\n     information. `mergesort` and `stable` are the only stable algorithms. For\n     DataFrames, this option is only applied when sorting on a single\n     column or label.\nna_position : {'first', 'last'}, default 'last'\n     Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n     end.\nignore_index : bool, default False\n     If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n     .. versionadded:: 1.0.0\n\nkey : callable, optional\n    Apply the key function to the values\n    before sorting. This is similar to the `key` argument in the\n    builtin :meth:`sorted` function, with the notable difference that\n    this `key` function should be *vectorized*. It should expect a\n    ``Series`` and return a Series with the same shape as the input.\n    It will be applied to each column in `by` independently.\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nDataFrame or None\n    DataFrame with sorted values or None if ``inplace=True``.\n\nSee Also\n--------\nDataFrame.sort_index : Sort a DataFrame by the index.\nSeries.sort_values : Similar method for a Series.\n\nExamples\n--------\n>>> df = pd.DataFrame({\n...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n...     'col2': [2, 1, 9, 8, 7, 4],\n...     'col3': [0, 1, 9, 4, 2, 3],\n...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n... })\n>>> df\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n\nSort by col1\n\n>>> df.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\n\nSort by multiple columns\n\n>>> df.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\n\nSort Descending\n\n>>> df.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\n\nPutting NAs first\n\n>>> df.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n\nSorting with a key function\n\n>>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n\nNatural sort with the key argument,\nusing the `natsort <https://github.com/SethMMorton/natsort>` package.\n\n>>> df = pd.DataFrame({\n...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n...    \"value\": [10, 20, 30, 40, 50]\n... })\n>>> df\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\n>>> from natsort import index_natsorted\n>>> df.sort_values(\n...    by=\"time\",\n...    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n... )\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.stack", "Docstring": "Stack the prescribed level(s) from columns to index.\n\nReturn a reshaped DataFrame or Series having a multi-level\nindex with one or more new inner-most levels compared to the current\nDataFrame. The new inner-most levels are created by pivoting the\ncolumns of the current dataframe:\n\n  - if the columns have a single level, the output is a Series;\n  - if the columns have multiple levels, the new index\n    level(s) is (are) taken from the prescribed level(s) and\n    the output is a DataFrame.\n\nParameters\n----------\nlevel : int, str, list, default -1\n    Level(s) to stack from the column axis onto the index\n    axis, defined as one index or label, or a list of indices\n    or labels.\ndropna : bool, default True\n    Whether to drop rows in the resulting Frame/Series with\n    missing values. Stacking a column level onto the index\n    axis can create combinations of index and column values\n    that are missing from the original dataframe. See Examples\n    section.\n\nReturns\n-------\nDataFrame or Series\n    Stacked dataframe or series.\n\nSee Also\n--------\nDataFrame.unstack : Unstack prescribed level(s) from index axis\n     onto column axis.\nDataFrame.pivot : Reshape dataframe from long format to wide\n     format.\nDataFrame.pivot_table : Create a spreadsheet-style pivot table\n     as a DataFrame.\n\nNotes\n-----\nThe function is named by analogy with a collection of books\nbeing reorganized from being side by side on a horizontal\nposition (the columns of the dataframe) to being stacked\nvertically on top of each other (in the index of the\ndataframe).\n\nReference :ref:`the user guide <reshaping.stacking>` for more examples.\n\nExamples\n--------\n**Single level columns**\n\n>>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]],\n...                                     index=['cat', 'dog'],\n...                                     columns=['weight', 'height'])\n\nStacking a dataframe with a single level column axis returns a Series:\n\n>>> df_single_level_cols\n     weight height\ncat       0      1\ndog       2      3\n>>> df_single_level_cols.stack()\ncat  weight    0\n     height    1\ndog  weight    2\n     height    3\ndtype: int64\n\n**Multi level columns: simple case**\n\n>>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n...                                        ('weight', 'pounds')])\n>>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]],\n...                                     index=['cat', 'dog'],\n...                                     columns=multicol1)\n\nStacking a dataframe with a multi-level column axis:\n\n>>> df_multi_level_cols1\n     weight\n         kg    pounds\ncat       1        2\ndog       2        4\n>>> df_multi_level_cols1.stack()\n            weight\ncat kg           1\n    pounds       2\ndog kg           2\n    pounds       4\n\n**Missing values**\n\n>>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'),\n...                                        ('height', 'm')])\n>>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]],\n...                                     index=['cat', 'dog'],\n...                                     columns=multicol2)\n\nIt is common to have missing values when stacking a dataframe\nwith multi-level columns, as the stacked dataframe typically\nhas more values than the original dataframe. Missing values\nare filled with NaNs:\n\n>>> df_multi_level_cols2\n    weight height\n        kg      m\ncat    1.0    2.0\ndog    3.0    4.0\n>>> df_multi_level_cols2.stack()\n        height  weight\ncat kg     NaN     1.0\n    m      2.0     NaN\ndog kg     NaN     3.0\n    m      4.0     NaN\n\n**Prescribing the level(s) to be stacked**\n\nThe first parameter controls which level or levels are stacked:\n\n>>> df_multi_level_cols2.stack(0)\n             kg    m\ncat height  NaN  2.0\n    weight  1.0  NaN\ndog height  NaN  4.0\n    weight  3.0  NaN\n>>> df_multi_level_cols2.stack([0, 1])\ncat  height  m     2.0\n     weight  kg    1.0\ndog  height  m     4.0\n     weight  kg    3.0\ndtype: float64\n\n**Dropping missing values**\n\n>>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]],\n...                                     index=['cat', 'dog'],\n...                                     columns=multicol2)\n\nNote that rows where all values are missing are dropped by\ndefault but this behaviour can be controlled via the dropna\nkeyword parameter:\n\n>>> df_multi_level_cols3\n    weight height\n        kg      m\ncat    NaN    1.0\ndog    2.0    3.0\n>>> df_multi_level_cols3.stack(dropna=False)\n        height  weight\ncat kg     NaN     NaN\n    m      1.0     NaN\ndog kg     NaN     2.0\n    m      3.0     NaN\n>>> df_multi_level_cols3.stack(dropna=True)\n        height  weight\ncat m      1.0     NaN\ndog kg     NaN     2.0\n    m      3.0     NaN", "Library": "Pandas"}
{"API_Name": "pandas.Series.str.rsplit", "Docstring": "Split strings around given separator/delimiter. Splits the string in the Series/Index from the end, at the specified delimiter string. Parameters ---------- pat : str, optional String to split on. If not specified, split on whitespace. n : int, default -1 (all) Limit number of splits in output. ``None``, 0 and -1 will be interpreted as return all splits. expand : bool, default False Expand the split strings into separate columns. - If ``True``, return DataFrame/MultiIndex expanding dimensionality. - If ``False``, return Series/Index, containing lists of strings. Returns ------- Series, Index, DataFrame or MultiIndex Type matches caller unless ``expand=True`` (see Notes). See Also -------- Series.str.split : Split strings around given separator/delimiter. Series.str.rsplit : Splits string around given separator/delimiter, starting from the right. Series.str.join : Join lists contained as elements in the Series/Index with passed delimiter. str.split : Standard library version for split. str.rsplit : Standard library version for rsplit. Notes ----- The handling of the `n` keyword depends on the number of found splits: - If found splits > `n`,  make first `n` splits only - If found splits <= `n`, make all splits - If for a certain row the number of found splits < `n`, append `None` for padding up to `n` if ``expand=True`` If using ``expand=True``, Series and Index callers return DataFrame and MultiIndex objects, respectively. Examples -------- >>> s = pd.Series( ...     [ ...         \"this is a regular sentence\", ...         \"https://docs.python.org/3/tutorial/index.html\", ...         np.nan ...     ] ... ) >>> s 0                       this is a regular sentence 1    https://docs.python.org/3/tutorial/index.html 2                                              NaN dtype: object In the default setting, the string is split by whitespace. >>> s.str.split() 0                   [this, is, a, regular, sentence] 1    [https://docs.python.org/3/tutorial/index.html] 2                                                NaN dtype: object Without the `n` parameter, the outputs of `rsplit` and `split` are identical. >>> s.str.rsplit() 0                   [this, is, a, regular, sentence] 1    [https://docs.python.org/3/tutorial/index.html] 2                                                NaN dtype: object The `n` parameter can be used to limit the number of splits on the delimiter. The outputs of `split` and `rsplit` are different. >>> s.str.split(n=2) 0                     [this, is, a regular sentence] 1    [https://docs.python.org/3/tutorial/index.html] 2                                                NaN dtype: object >>> s.str.rsplit(n=2) 0                     [this is a, regular, sentence] 1    [https://docs.python.org/3/tutorial/index.html] 2                                                NaN dtype: object The `pat` parameter can be used to split by other characters. >>> s.str.split(pat=\"/\") 0                         [this is a regular sentence] 1    [https:, , docs.python.org, 3, tutorial, index... 2                                                  NaN dtype: object When using ``expand=True``, the split elements will expand out into separate columns. If NaN is present, it is propagated throughout the columns during the split. >>> s.str.split(expand=True) 0     1     2        3         4 0                                           this    is     a  regular  sentence 1  https://docs.python.org/3/tutorial/index.html  None  None     None      None 2                                            NaN   NaN   NaN      NaN       NaN For slightly more complex use cases like splitting the html document name from a url, a combination of parameter settings can be used. >>> s.str.rsplit(\"/\", n=1, expand=True) 0           1 0          this is a regular sentence        None 1  https://docs.python.org/3/tutorial  index.html 2                                 NaN         NaN", "Library": "Pandas"}
{"API_Name": "pandas.Series.str.strip", "Docstring": "Remove leading and trailing characters. Strip whitespaces (including newlines) or a set of specified characters from each string in the Series/Index from left and right sides. Replaces any non-strings in Series with NaNs. Equivalent to :meth:`str.strip`. Parameters ---------- to_strip : str or None, default None Specifying the set of characters to be removed. All combinations of this set of characters will be stripped. If None then whitespaces are removed. Returns ------- Series or Index of object See Also -------- Series.str.strip : Remove leading and trailing characters in Series/Index. Series.str.lstrip : Remove leading characters in Series/Index. Series.str.rstrip : Remove trailing characters in Series/Index. Examples -------- >>> s = pd.Series(['1. Ant.  ', '2. Bee!\\n', '3. Cat?\\t', np.nan, 10, True]) >>> s 0    1. Ant. 1    2. Bee!\\n 2    3. Cat?\\t 3          NaN 4           10 5         True dtype: object >>> s.str.strip() 0    1. Ant. 1    2. Bee! 2    3. Cat? 3        NaN 4        NaN 5        NaN dtype: object >>> s.str.lstrip('123.') 0    Ant. 1    Bee!\\n 2    Cat?\\t 3       NaN 4       NaN 5       NaN dtype: object >>> s.str.rstrip('.!? \\n\\t') 0    1. Ant 1    2. Bee 2    3. Cat 3       NaN 4       NaN 5       NaN dtype: object >>> s.str.strip('123.!? \\n\\t') 0    Ant 1    Bee 2    Cat 3    NaN 4    NaN 5    NaN dtype: object", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.sum", "Docstring": "Return the sum of the values over the requested axis.\n\nThis is equivalent to the method ``numpy.sum``.\n\nParameters\n----------\naxis : {index (0), columns (1)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a Series.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\nmin_count : int, default 0\n    The required number of valid values to perform the operation. If fewer than\n    ``min_count`` non-NA values are present the result will be NA.\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nSeries or DataFrame (if level specified)\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.sum()\n14\n\nBy default, the sum of an empty or all-NA Series is ``0``.\n\n>>> pd.Series([], dtype=\"float64\").sum()  # min_count=0 is the default\n0.0\n\nThis can be controlled with the ``min_count`` parameter. For example, if\nyou'd like the sum of an empty series to be NaN, pass ``min_count=1``.\n\n>>> pd.Series([], dtype=\"float64\").sum(min_count=1)\nnan\n\nThanks to the ``skipna`` parameter, ``min_count`` handles all-NA and\nempty series identically.\n\n>>> pd.Series([np.nan]).sum()\n0.0\n\n>>> pd.Series([np.nan]).sum(min_count=1)\nnan", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.T", "Docstring": "The transpose of the DataFrame. Returns ------- DataFrame The transposed DataFrame. See Also -------- DataFrame.transpose : Transpose index and columns. Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) >>> df col1  col2 0     1     3 1     2     4 >>> df.T 0  1 col1  1  2 col2  3  4", "Library": "Pandas"}
{"API_Name": "pandas.Series.to_frame", "Docstring": "Convert Series to DataFrame. Parameters ---------- name : object, optional The passed name should substitute for the series name (if it has one). Returns ------- DataFrame DataFrame representation of Series. Examples -------- >>> s = pd.Series([\"a\", \"b\", \"c\"], ...               name=\"vals\") >>> s.to_frame() vals 0    a 1    b 2    c", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.to_numpy", "Docstring": "Convert the DataFrame to a NumPy array.\n\nBy default, the dtype of the returned array will be the common NumPy\ndtype of all types in the DataFrame. For example, if the dtypes are\n``float16`` and ``float32``, the results dtype will be ``float32``.\nThis may require copying data and coercing values, which may be\nexpensive.\n\nParameters\n----------\ndtype : str or numpy.dtype, optional\n    The dtype to pass to :meth:`numpy.asarray`.\ncopy : bool, default False\n    Whether to ensure that the returned value is not a view on\n    another array. Note that ``copy=False`` does not *ensure* that\n    ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that\n    a copy is made, even if not strictly necessary.\nna_value : Any, optional\n    The value to use for missing values. The default value depends\n    on `dtype` and the dtypes of the DataFrame columns.\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nnumpy.ndarray\n\nSee Also\n--------\nSeries.to_numpy : Similar method for Series.\n\nExamples\n--------\n>>> pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy()\narray([[1, 3],\n       [2, 4]])\n\nWith heterogeneous data, the lowest common type will have to\nbe used.\n\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]})\n>>> df.to_numpy()\narray([[1. , 3. ],\n       [2. , 4.5]])\n\nFor a mix of numeric and non-numeric types, the output array will\nhave object dtype.\n\n>>> df['C'] = pd.date_range('2000', periods=2)\n>>> df.to_numpy()\narray([[1, 3.0, Timestamp('2000-01-01 00:00:00')],\n       [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object)", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.unstack", "Docstring": "Pivot a level of the (necessarily hierarchical) index labels.\n\nReturns a DataFrame having a new level of column labels whose inner-most level\nconsists of the pivoted index labels.\n\nIf the index is not a MultiIndex, the output will be a Series\n(the analogue of stack when the columns are not a MultiIndex).\n\nParameters\n----------\nlevel : int, str, or list of these, default -1 (last level)\n    Level(s) of index to unstack, can pass level name.\nfill_value : int, str or dict\n    Replace NaN with this value if the unstack produces missing values.\n\nReturns\n-------\nSeries or DataFrame\n\nSee Also\n--------\nDataFrame.pivot : Pivot a table based on column values.\nDataFrame.stack : Pivot a level of the column labels (inverse operation\n    from `unstack`).\n\nNotes\n-----\nReference :ref:`the user guide <reshaping.stacking>` for more examples.\n\nExamples\n--------\n>>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),\n...                                    ('two', 'a'), ('two', 'b')])\n>>> s = pd.Series(np.arange(1.0, 5.0), index=index)\n>>> s\none  a   1.0\n     b   2.0\ntwo  a   3.0\n     b   4.0\ndtype: float64\n\n>>> s.unstack(level=-1)\n     a   b\none  1.0  2.0\ntwo  3.0  4.0\n\n>>> s.unstack(level=0)\n   one  two\na  1.0   3.0\nb  2.0   4.0\n\n>>> df = s.unstack(level=0)\n>>> df.unstack()\none  a  1.0\n     b  2.0\ntwo  a  3.0\n     b  4.0\ndtype: float64", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.update", "Docstring": "Modify in place using non-NA values from another DataFrame.\n\nAligns on indices. There is no return value.\n\nParameters\n----------\nother : DataFrame, or object coercible into a DataFrame\n    Should have at least one matching index/column label\n    with the original DataFrame. If a Series is passed,\n    its name attribute must be set, and that will be\n    used as the column name to align with the original DataFrame.\njoin : {'left'}, default 'left'\n    Only left join is implemented, keeping the index and columns of the\n    original object.\noverwrite : bool, default True\n    How to handle non-NA values for overlapping keys:\n\n    * True: overwrite original DataFrame's values\n      with values from `other`.\n    * False: only update values that are NA in\n      the original DataFrame.\n\nfilter_func : callable(1d-array) -> bool 1d-array, optional\n    Can choose to replace values other than NA. Return True for values\n    that should be updated.\nerrors : {'raise', 'ignore'}, default 'ignore'\n    If 'raise', will raise a ValueError if the DataFrame and `other`\n    both contain non-NA data in the same place.\n\nReturns\n-------\nNone : method directly changes calling object\n\nRaises\n------\nValueError\n    * When `errors='raise'` and there's overlapping non-NA data.\n    * When `errors` is not either `'ignore'` or `'raise'`\nNotImplementedError\n    * If `join != 'left'`\n\nSee Also\n--------\ndict.update : Similar method for dictionaries.\nDataFrame.merge : For column(s)-on-column(s) operations.\n\nExamples\n--------\n>>> df = pd.DataFrame({'A': [1, 2, 3],\n...                    'B': [400, 500, 600]})\n>>> new_df = pd.DataFrame({'B': [4, 5, 6],\n...                        'C': [7, 8, 9]})\n>>> df.update(new_df)\n>>> df\n   A  B\n0  1  4\n1  2  5\n2  3  6\n\nThe DataFrame's length does not increase as a result of the update,\nonly values at matching index/column labels are updated.\n\n>>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n...                    'B': ['x', 'y', 'z']})\n>>> new_df = pd.DataFrame({'B': ['d', 'e', 'f', 'g', 'h', 'i']})\n>>> df.update(new_df)\n>>> df\n   A  B\n0  a  d\n1  b  e\n2  c  f\n\nFor Series, its name attribute must be set.\n\n>>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n...                    'B': ['x', 'y', 'z']})\n>>> new_column = pd.Series(['d', 'e'], name='B', index=[0, 2])\n>>> df.update(new_column)\n>>> df\n   A  B\n0  a  d\n1  b  y\n2  c  e\n>>> df = pd.DataFrame({'A': ['a', 'b', 'c'],\n...                    'B': ['x', 'y', 'z']})\n>>> new_df = pd.DataFrame({'B': ['d', 'e']}, index=[1, 2])\n>>> df.update(new_df)\n>>> df\n   A  B\n0  a  x\n1  b  d\n2  c  e\n\nIf `other` contains NaNs the corresponding values are not updated\nin the original dataframe.\n\n>>> df = pd.DataFrame({'A': [1, 2, 3],\n...                    'B': [400, 500, 600]})\n>>> new_df = pd.DataFrame({'B': [4, np.nan, 6]})\n>>> df.update(new_df)\n>>> df\n   A      B\n0  1    4.0\n1  2  500.0\n2  3    6.0", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.value_counts", "Docstring": "Return a Series containing counts of unique rows in the DataFrame.\n\n.. versionadded:: 1.1.0\n\nParameters\n----------\nsubset : list-like, optional\n    Columns to use when counting unique combinations.\nnormalize : bool, default False\n    Return proportions rather than frequencies.\nsort : bool, default True\n    Sort by frequencies.\nascending : bool, default False\n    Sort in ascending order.\ndropna : bool, default True\n    Don\u2019t include counts of rows that contain NA values.\n\n    .. versionadded:: 1.3.0\n\nReturns\n-------\nSeries\n\nSee Also\n--------\nSeries.value_counts: Equivalent method on Series.\n\nNotes\n-----\nThe returned Series will have a MultiIndex with one level per input\ncolumn. By default, rows that contain any NA values are omitted from\nthe result. By default, the resulting Series will be in descending\norder so that the first element is the most frequently-occurring row.\n\nExamples\n--------\n>>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6],\n...                    'num_wings': [2, 0, 0, 0]},\n...                   index=['falcon', 'dog', 'cat', 'ant'])\n>>> df\n        num_legs  num_wings\nfalcon         2          2\ndog            4          0\ncat            4          0\nant            6          0\n\n>>> df.value_counts()\nnum_legs  num_wings\n4         0            2\n2         2            1\n6         0            1\ndtype: int64\n\n>>> df.value_counts(sort=False)\nnum_legs  num_wings\n2         2            1\n4         0            2\n6         0            1\ndtype: int64\n\n>>> df.value_counts(ascending=True)\nnum_legs  num_wings\n2         2            1\n6         0            1\n4         0            2\ndtype: int64\n\n>>> df.value_counts(normalize=True)\nnum_legs  num_wings\n4         0            0.50\n2         2            0.25\n6         0            0.25\ndtype: float64\n\nWith `dropna` set to `False` we can also count rows with NA values.\n\n>>> df = pd.DataFrame({'first_name': ['John', 'Anne', 'John', 'Beth'],\n...                    'middle_name': ['Smith', pd.NA, pd.NA, 'Louise']})\n>>> df\n  first_name middle_name\n0       John       Smith\n1       Anne        <NA>\n2       John        <NA>\n3       Beth      Louise\n\n>>> df.value_counts()\nfirst_name  middle_name\nBeth        Louise         1\nJohn        Smith          1\ndtype: int64\n\n>>> df.value_counts(dropna=False)\nfirst_name  middle_name\nAnne        NaN            1\nBeth        Louise         1\nJohn        Smith          1\n            NaN            1\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.values", "Docstring": "Return a Numpy representation of the DataFrame.\n\n.. warning::\n\n   We recommend using :meth:`DataFrame.to_numpy` instead.\n\nOnly the values in the DataFrame will be returned, the axes labels\nwill be removed.\n\nReturns\n-------\nnumpy.ndarray\n    The values of the DataFrame.\n\nSee Also\n--------\nDataFrame.to_numpy : Recommended alternative to this method.\nDataFrame.index : Retrieve the index labels.\nDataFrame.columns : Retrieving the column names.\n\nNotes\n-----\nThe dtype will be a lower-common-denominator dtype (implicit\nupcasting); that is to say if the dtypes (even of numeric types)\nare mixed, the one that accommodates all will be chosen. Use this\nwith care if you are not dealing with the blocks.\n\ne.g. If the dtypes are float16 and float32, dtype will be upcast to\nfloat32.  If dtypes are int32 and uint8, dtype will be upcast to\nint32. By :func:`numpy.find_common_type` convention, mixing int64\nand uint64 will result in a float64 dtype.\n\nExamples\n--------\nA DataFrame where all columns are the same type (e.g., int64) results\nin an array of the same type.\n\n>>> df = pd.DataFrame({'age':    [ 3,  29],\n...                    'height': [94, 170],\n...                    'weight': [31, 115]})\n>>> df\n   age  height  weight\n0    3      94      31\n1   29     170     115\n>>> df.dtypes\nage       int64\nheight    int64\nweight    int64\ndtype: object\n>>> df.values\narray([[  3,  94,  31],\n       [ 29, 170, 115]])\n\nA DataFrame with mixed type columns(e.g., str/object, int64, float32)\nresults in an ndarray of the broadest type that accommodates these\nmixed types (e.g., object).\n\n>>> df2 = pd.DataFrame([('parrot',   24.0, 'second'),\n...                     ('lion',     80.5, 1),\n...                     ('monkey', np.nan, None)],\n...                   columns=('name', 'max_speed', 'rank'))\n>>> df2.dtypes\nname          object\nmax_speed    float64\nrank          object\ndtype: object\n>>> df2.values\narray([['parrot', 24.0, 'second'],\n       ['lion', 80.5, 1],\n       ['monkey', nan, None]], dtype=object)", "Library": "Pandas"}
{"API_Name": "pandas.DataFrame.where", "Docstring": "Replace values where the condition is False.\n\nParameters\n----------\ncond : bool Series/DataFrame, array-like, or callable\n    Where `cond` is True, keep the original value. Where\n    False, replace with corresponding value from `other`.\n    If `cond` is callable, it is computed on the Series/DataFrame and\n    should return boolean Series/DataFrame or array. The callable must\n    not change input Series/DataFrame (though pandas doesn't check it).\nother : scalar, Series/DataFrame, or callable\n    Entries where `cond` is False are replaced with\n    corresponding value from `other`.\n    If other is callable, it is computed on the Series/DataFrame and\n    should return scalar or Series/DataFrame. The callable must not\n    change input Series/DataFrame (though pandas doesn't check it).\ninplace : bool, default False\n    Whether to perform the operation in place on the data.\naxis : int, default None\n    Alignment axis if needed. For `Series` this parameter is\n    unused and defaults to 0.\nlevel : int, default None\n    Alignment level if needed.\nerrors : str, {'raise', 'ignore'}, default 'raise'\n    Note that currently this parameter won't affect\n    the results and will always coerce to a suitable dtype.\n\n    - 'raise' : allow exceptions to be raised.\n    - 'ignore' : suppress exceptions. On error return original object.\n\n    .. deprecated:: 1.5.0\n       This argument had no effect.\n\ntry_cast : bool, default None\n    Try to cast the result back to the input type (if possible).\n\n    .. deprecated:: 1.3.0\n        Manually cast back if necessary.\n\nReturns\n-------\nSame type as caller or None if ``inplace=True``.\n\nSee Also\n--------\n:func:`DataFrame.mask` : Return an object of same shape as\n    self.\n\nNotes\n-----\nThe where method is an application of the if-then idiom. For each\nelement in the calling DataFrame, if ``cond`` is ``True`` the\nelement is used; otherwise the corresponding element from the DataFrame\n``other`` is used. If the axis of ``other`` does not align with axis of\n``cond`` Series/DataFrame, the misaligned index positions will be filled with\nFalse.\n\nThe signature for :func:`DataFrame.where` differs from\n:func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to\n``np.where(m, df1, df2)``.\n\nFor further details and examples see the ``where`` documentation in\n:ref:`indexing <indexing.where_mask>`.\n\nThe dtype of the object takes precedence. The fill value is casted to\nthe object's dtype, if this can be done losslessly.\n\nExamples\n--------\n>>> s = pd.Series(range(5))\n>>> s.where(s > 0)\n0    NaN\n1    1.0\n2    2.0\n3    3.0\n4    4.0\ndtype: float64\n>>> s.mask(s > 0)\n0    0.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n>>> s = pd.Series(range(5))\n>>> t = pd.Series([True, False])\n>>> s.where(t, 99)\n0     0\n1    99\n2    99\n3    99\n4    99\ndtype: int64\n>>> s.mask(t, 99)\n0    99\n1     1\n2    99\n3    99\n4    99\ndtype: int64\n\n>>> s.where(s > 1, 10)\n0    10\n1    10\n2    2\n3    3\n4    4\ndtype: int64\n>>> s.mask(s > 1, 10)\n0     0\n1     1\n2    10\n3    10\n4    10\ndtype: int64\n\n>>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B'])\n>>> df\n   A  B\n0  0  1\n1  2  3\n2  4  5\n3  6  7\n4  8  9\n>>> m = df % 3 == 0\n>>> df.where(m, -df)\n   A  B\n0  0 -1\n1 -2  3\n2 -4 -5\n3  6 -7\n4 -8  9\n>>> df.where(m, -df) == np.where(m, df, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True\n>>> df.where(m, -df) == df.mask(~m, -df)\n      A     B\n0  True  True\n1  True  True\n2  True  True\n3  True  True\n4  True  True", "Library": "Pandas"}
{"API_Name": "pandas.Index.get_level_values", "Docstring": "Return an Index of values for requested level.\n\nThis is primarily useful to get an individual level of values from a\nMultiIndex, but is provided on Index as well for compatibility.\n\nParameters\n----------\nlevel : int or str\n    It is either the integer position or the name of the level.\n\nReturns\n-------\nIndex\n    Calling object, as there is only one level in the Index.\n\nSee Also\n--------\nMultiIndex.get_level_values : Get values for a level of a MultiIndex.\n\nNotes\n-----\nFor Index, level should be 0, since there are no multiple levels.\n\nExamples\n--------\n>>> idx = pd.Index(list('abc'))\n>>> idx\nIndex(['a', 'b', 'c'], dtype='object')\n\nGet level values by supplying `level` as integer:\n\n>>> idx.get_level_values(0)\nIndex(['a', 'b', 'c'], dtype='object')", "Library": "Pandas"}
{"API_Name": "pandas.Index.map", "Docstring": "Map values using an input mapping or function.\n\nParameters\n----------\nmapper : function, dict, or Series\n    Mapping correspondence.\nna_action : {None, 'ignore'}\n    If 'ignore', propagate NA values, without passing them to the\n    mapping correspondence.\n\nReturns\n-------\napplied : Union[Index, MultiIndex], inferred\n    The output of the mapping function applied to the index.\n    If the function returns a tuple with more than one element\n    a MultiIndex will be returned.", "Library": "Pandas"}
{"API_Name": "pandas.DatetimeIndex.strftime", "Docstring": "Convert to Index using specified date_format. Return an Index of formatted strings specified by date_format, which supports the same string format as the python standard library. Details of the string format can be found in `python string format doc <https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior>`__. Formats supported by the C `strftime` API but not by the python string format doc (such as `\"%R\"`, `\"%r\"`) are not officially supported and should be preferably replaced with their supported equivalents (such as `\"%H:%M\"`, `\"%I:%M:%S %p\"`). Note that `PeriodIndex` support additional directives, detailed in `Period.strftime`. Parameters ---------- date_format : str Date format string (e.g. \"%Y-%m-%d\"). Returns ------- ndarray[object] NumPy ndarray of formatted strings. See Also -------- to_datetime : Convert the given argument to datetime. DatetimeIndex.normalize : Return DatetimeIndex with times to midnight. DatetimeIndex.round : Round the DatetimeIndex to the specified freq. DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq. Timestamp.strftime : Format a single Timestamp. Period.strftime : Format a single Period. Examples -------- >>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"), ...                     periods=3, freq='s') >>> rng.strftime('%B %d, %Y, %r') Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM', 'March 10, 2018, 09:00:02 AM'], dtype='object')", "Library": "Pandas"}
{"API_Name": "pandas.Index.tolist", "Docstring": "Return a list of the values.\n\nThese are each a scalar type, which is a Python scalar\n(for str, int, float) or a pandas scalar\n(for Timestamp/Timedelta/Interval/Period)\n\nReturns\n-------\nlist\n\nSee Also\n--------\nnumpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n    nested list of Python scalars.", "Library": "Pandas"}
{"API_Name": "pandas.MultiIndex.get_loc", "Docstring": "Get location for a label or a tuple of labels.\n\nThe location is returned as an integer/slice or boolean\nmask.\n\nParameters\n----------\nkey : label or tuple of labels (one for each level)\nmethod : None\n\nReturns\n-------\nloc : int, slice object or boolean mask\n    If the key is past the lexsort depth, the return may be a\n    boolean mask array, otherwise it is always a slice or int.\n\nSee Also\n--------\nIndex.get_loc : The get_loc method for (single-level) index.\nMultiIndex.slice_locs : Get slice location given start label(s) and\n                        end label(s).\nMultiIndex.get_locs : Get location for a label/slice/list/mask or a\n                      sequence of such.\n\nNotes\n-----\nThe key cannot be a slice, list of same-level labels, a boolean mask,\nor a sequence of such. If you want to use those, use\n:meth:`MultiIndex.get_locs` instead.\n\nExamples\n--------\n>>> mi = pd.MultiIndex.from_arrays([list('abb'), list('def')])\n\n>>> mi.get_loc('b')\nslice(1, 3, None)\n\n>>> mi.get_loc(('b', 'e'))\n1", "Library": "Pandas"}
{"API_Name": "pandas.Series.argsort", "Docstring": "Return the integer indices that would sort the Series values.\n\nOverride ndarray.argsort. Argsorts the value, omitting NA/null values,\nand places the result in the same locations as the non-NA values.\n\nParameters\n----------\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\nkind : {'mergesort', 'quicksort', 'heapsort', 'stable'}, default 'quicksort'\n    Choice of sorting algorithm. See :func:`numpy.sort` for more\n    information. 'mergesort' and 'stable' are the only stable algorithms.\norder : None\n    Has no effect but is accepted for compatibility with numpy.\n\nReturns\n-------\nSeries[np.intp]\n    Positions of values within the sort order with -1 indicating\n    nan values.\n\nSee Also\n--------\nnumpy.ndarray.argsort : Returns the indices that would sort this array.", "Library": "Pandas"}
{"API_Name": "pandas.Series.astype", "Docstring": "Cast a pandas object to a specified dtype ``dtype``.\n\nParameters\n----------\ndtype : data type, or dict of column name -> data type\n    Use a numpy.dtype or Python type to cast entire pandas object to\n    the same type. Alternatively, use {col: dtype, ...}, where col is a\n    column label and dtype is a numpy.dtype or Python type to cast one\n    or more of the DataFrame's columns to column-specific types.\ncopy : bool, default True\n    Return a copy when ``copy=True`` (be very careful setting\n    ``copy=False`` as changes to values then may propagate to other\n    pandas objects).\nerrors : {'raise', 'ignore'}, default 'raise'\n    Control raising of exceptions on invalid data for provided dtype.\n\n    - ``raise`` : allow exceptions to be raised\n    - ``ignore`` : suppress exceptions. On error return original object.\n\nReturns\n-------\ncasted : same type as caller\n\nSee Also\n--------\nto_datetime : Convert argument to datetime.\nto_timedelta : Convert argument to timedelta.\nto_numeric : Convert argument to a numeric type.\nnumpy.ndarray.astype : Cast a numpy array to a specified type.\n\nNotes\n-----\n.. deprecated:: 1.3.0\n\n    Using ``astype`` to convert from timezone-naive dtype to\n    timezone-aware dtype is deprecated and will raise in a\n    future version.  Use :meth:`Series.dt.tz_localize` instead.\n\nExamples\n--------\nCreate a DataFrame:\n\n>>> d = {'col1': [1, 2], 'col2': [3, 4]}\n>>> df = pd.DataFrame(data=d)\n>>> df.dtypes\ncol1    int64\ncol2    int64\ndtype: object\n\nCast all columns to int32:\n\n>>> df.astype('int32').dtypes\ncol1    int32\ncol2    int32\ndtype: object\n\nCast col1 to int32 using a dictionary:\n\n>>> df.astype({'col1': 'int32'}).dtypes\ncol1    int32\ncol2    int64\ndtype: object\n\nCreate a series:\n\n>>> ser = pd.Series([1, 2], dtype='int32')\n>>> ser\n0    1\n1    2\ndtype: int32\n>>> ser.astype('int64')\n0    1\n1    2\ndtype: int64\n\nConvert to categorical type:\n\n>>> ser.astype('category')\n0    1\n1    2\ndtype: category\nCategories (2, int64): [1, 2]\n\nConvert to ordered categorical type with custom ordering:\n\n>>> from pandas.api.types import CategoricalDtype\n>>> cat_dtype = CategoricalDtype(\n...     categories=[2, 1], ordered=True)\n>>> ser.astype(cat_dtype)\n0    1\n1    2\ndtype: category\nCategories (2, int64): [2 < 1]\n\nNote that using ``copy=False`` and changing data on a new\npandas object may propagate changes:\n\n>>> s1 = pd.Series([1, 2])\n>>> s2 = s1.astype('int64', copy=False)\n>>> s2[0] = 10\n>>> s1  # note that s1[0] has changed too\n0    10\n1     2\ndtype: int64\n\nCreate a series of dates:\n\n>>> ser_date = pd.Series(pd.date_range('20200101', periods=3))\n>>> ser_date\n0   2020-01-01\n1   2020-01-02\n2   2020-01-03\ndtype: datetime64[ns]", "Library": "Pandas"}
{"API_Name": "pandas.Series.copy", "Docstring": "Make a copy of this object's indices and data.\n\nWhen ``deep=True`` (default), a new object will be created with a\ncopy of the calling object's data and indices. Modifications to\nthe data or indices of the copy will not be reflected in the\noriginal object (see notes below).\n\nWhen ``deep=False``, a new object will be created without copying\nthe calling object's data or index (only references to the data\nand index are copied). Any changes to the data of the original\nwill be reflected in the shallow copy (and vice versa).\n\nParameters\n----------\ndeep : bool, default True\n    Make a deep copy, including a copy of the data and the indices.\n    With ``deep=False`` neither the indices nor the data are copied.\n\nReturns\n-------\ncopy : Series or DataFrame\n    Object type matches caller.\n\nNotes\n-----\nWhen ``deep=True``, data is copied but actual Python objects\nwill not be copied recursively, only the reference to the object.\nThis is in contrast to `copy.deepcopy` in the Standard Library,\nwhich recursively copies object data (see examples below).\n\nWhile ``Index`` objects are copied when ``deep=True``, the underlying\nnumpy array is not copied for performance reasons. Since ``Index`` is\nimmutable, the underlying data can be safely shared and a copy\nis not needed.\n\nSince pandas is not thread safe, see the\n:ref:`gotchas <gotchas.thread-safety>` when copying in a threading\nenvironment.\n\nExamples\n--------\n>>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n>>> s\na    1\nb    2\ndtype: int64\n\n>>> s_copy = s.copy()\n>>> s_copy\na    1\nb    2\ndtype: int64\n\n**Shallow copy versus default (deep) copy:**\n\n>>> s = pd.Series([1, 2], index=[\"a\", \"b\"])\n>>> deep = s.copy()\n>>> shallow = s.copy(deep=False)\n\nShallow copy shares data and index with original.\n\n>>> s is shallow\nFalse\n>>> s.values is shallow.values and s.index is shallow.index\nTrue\n\nDeep copy has own copy of data and index.\n\n>>> s is deep\nFalse\n>>> s.values is deep.values or s.index is deep.index\nFalse\n\nUpdates to the data shared by shallow copy and original is reflected\nin both; deep copy remains unchanged.\n\n>>> s[0] = 3\n>>> shallow[1] = 4\n>>> s\na    3\nb    4\ndtype: int64\n>>> shallow\na    3\nb    4\ndtype: int64\n>>> deep\na    1\nb    2\ndtype: int64\n\nNote that when copying an object containing Python objects, a deep copy\nwill copy the data, but will not do so recursively. Updating a nested\ndata object will be reflected in the deep copy.\n\n>>> s = pd.Series([[1, 2], [3, 4]])\n>>> deep = s.copy()\n>>> s[0][0] = 10\n>>> s\n0    [10, 2]\n1     [3, 4]\ndtype: object\n>>> deep\n0    [10, 2]\n1     [3, 4]\ndtype: object", "Library": "Pandas"}
{"API_Name": "pandas.Series.corr", "Docstring": "Compute correlation with `other` Series, excluding missing values.\n\nThe two `Series` objects are not required to be the same length and will be\naligned internally before the correlation function is applied.\n\nParameters\n----------\nother : Series\n    Series with which to compute the correlation.\nmethod : {'pearson', 'kendall', 'spearman'} or callable\n    Method used to compute correlation:\n\n    - pearson : Standard correlation coefficient\n    - kendall : Kendall Tau correlation coefficient\n    - spearman : Spearman rank correlation\n    - callable: Callable with input two 1d ndarrays and returning a float.\n\n    .. warning::\n        Note that the returned matrix from corr will have 1 along the\n        diagonals and will be symmetric regardless of the callable's\n        behavior.\nmin_periods : int, optional\n    Minimum number of observations needed to have a valid result.\n\nReturns\n-------\nfloat\n    Correlation with other.\n\nSee Also\n--------\nDataFrame.corr : Compute pairwise correlation between columns.\nDataFrame.corrwith : Compute pairwise correlation with another\n    DataFrame or Series.\n\nNotes\n-----\nPearson, Kendall and Spearman correlation are currently computed using pairwise complete observations.\n\n* `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`_\n* `Kendall rank correlation coefficient <https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient>`_\n* `Spearman's rank correlation coefficient <https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient>`_\n\nExamples\n--------\n>>> def histogram_intersection(a, b):\n...     v = np.minimum(a, b).sum().round(decimals=1)\n...     return v\n>>> s1 = pd.Series([.2, .0, .6, .2])\n>>> s2 = pd.Series([.3, .6, .0, .1])\n>>> s1.corr(s2, method=histogram_intersection)\n0.3", "Library": "Pandas"}
{"API_Name": "pandas.Series.cumsum", "Docstring": "Return cumulative sum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nsum.\n\nParameters\n----------\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    The index or the name of the axis. 0 is equivalent to None or 'index'.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\n*args, **kwargs\n    Additional keywords have no effect but might be accepted for\n    compatibility with NumPy.\n\nReturns\n-------\nscalar or Series\n    Return cumulative sum of scalar or Series.\n\nSee Also\n--------\ncore.window.expanding.Expanding.sum : Similar functionality\n    but ignores ``NaN`` values.\nSeries.sum : Return the sum over\n    Series axis.\nSeries.cummax : Return cumulative maximum over Series axis.\nSeries.cummin : Return cumulative minimum over Series axis.\nSeries.cumsum : Return cumulative sum over Series axis.\nSeries.cumprod : Return cumulative product over Series axis.\n\nExamples\n--------\n**Series**\n\n>>> s = pd.Series([2, np.nan, 5, -1, 0])\n>>> s\n0    2.0\n1    NaN\n2    5.0\n3   -1.0\n4    0.0\ndtype: float64\n\nBy default, NA values are ignored.\n\n>>> s.cumsum()\n0    2.0\n1    NaN\n2    7.0\n3    6.0\n4    6.0\ndtype: float64\n\nTo include NA values in the operation, use ``skipna=False``\n\n>>> s.cumsum(skipna=False)\n0    2.0\n1    NaN\n2    NaN\n3    NaN\n4    NaN\ndtype: float64\n\n**DataFrame**\n\n>>> df = pd.DataFrame([[2.0, 1.0],\n...                    [3.0, np.nan],\n...                    [1.0, 0.0]],\n...                    columns=list('AB'))\n>>> df\n     A    B\n0  2.0  1.0\n1  3.0  NaN\n2  1.0  0.0\n\nBy default, iterates over rows and finds the sum\nin each column. This is equivalent to ``axis=None`` or ``axis='index'``.\n\n>>> df.cumsum()\n     A    B\n0  2.0  1.0\n1  5.0  NaN\n2  6.0  1.0\n\nTo iterate over columns and find the sum in each row,\nuse ``axis=1``\n\n>>> df.cumsum(axis=1)\n     A    B\n0  2.0  3.0\n1  3.0  NaN\n2  1.0  1.0", "Library": "Pandas"}
{"API_Name": "pandas.Series.dropna", "Docstring": "Return a new Series with missing values removed.\n\nSee the :ref:`User Guide <missing_data>` for more on which values are\nconsidered missing, and how to work with missing data.\n\nParameters\n----------\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\ninplace : bool, default False\n    If True, do operation inplace and return None.\nhow : str, optional\n    Not in use. Kept for compatibility.\n\nReturns\n-------\nSeries or None\n    Series with NA entries dropped from it or None if ``inplace=True``.\n\nSee Also\n--------\nSeries.isna: Indicate missing values.\nSeries.notna : Indicate existing (non-missing) values.\nSeries.fillna : Replace missing values.\nDataFrame.dropna : Drop rows or columns which contain NA values.\nIndex.dropna : Drop missing indices.\n\nExamples\n--------\n>>> ser = pd.Series([1., 2., np.nan])\n>>> ser\n0    1.0\n1    2.0\n2    NaN\ndtype: float64\n\nDrop NA values from a Series.\n\n>>> ser.dropna()\n0    1.0\n1    2.0\ndtype: float64\n\nKeep the Series with valid entries in the same variable.\n\n>>> ser.dropna(inplace=True)\n>>> ser\n0    1.0\n1    2.0\ndtype: float64\n\nEmpty strings are not considered NA values. ``None`` is considered an\nNA value.\n\n>>> ser = pd.Series([np.NaN, 2, pd.NaT, '', None, 'I stay'])\n>>> ser\n0       NaN\n1         2\n2       NaT\n3\n4      None\n5    I stay\ndtype: object\n>>> ser.dropna()\n1         2\n3\n5    I stay\ndtype: object", "Library": "Pandas"}
{"API_Name": "pandas.Series.dt.strftime", "Docstring": "Convert to Index using specified date_format.\n\nReturn an Index of formatted strings specified by date_format, which\nsupports the same string format as the python standard library. Details\nof the string format can be found in `python string format\ndoc <https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior>`__.\n\nFormats supported by the C `strftime` API but not by the python string format\ndoc (such as `\"%R\"`, `\"%r\"`) are not officially supported and should be\npreferably replaced with their supported equivalents (such as `\"%H:%M\"`,\n`\"%I:%M:%S %p\"`).\n\nNote that `PeriodIndex` support additional directives, detailed in\n`Period.strftime`.\n\nParameters\n----------\ndate_format : str\n    Date format string (e.g. \"%Y-%m-%d\").\n\nReturns\n-------\nndarray[object]\n    NumPy ndarray of formatted strings.\n\nSee Also\n--------\nto_datetime : Convert the given argument to datetime.\nDatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\nDatetimeIndex.round : Round the DatetimeIndex to the specified freq.\nDatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\nTimestamp.strftime : Format a single Timestamp.\nPeriod.strftime : Format a single Period.\n\nExamples\n--------\n>>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n...                     periods=3, freq='s')\n>>> rng.strftime('%B %d, %Y, %r')\nIndex(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n       'March 10, 2018, 09:00:02 AM'],\n      dtype='object')", "Library": "Pandas"}
{"API_Name": "pandas.Series.equals", "Docstring": "Test whether two objects contain the same elements.\n\nThis function allows two Series or DataFrames to be compared against\neach other to see if they have the same shape and elements. NaNs in\nthe same location are considered equal.\n\nThe row/column index do not need to have the same type, as long\nas the values are considered equal. Corresponding columns must be of\nthe same dtype.\n\nParameters\n----------\nother : Series or DataFrame\n    The other Series or DataFrame to be compared with the first.\n\nReturns\n-------\nbool\n    True if all elements are the same in both objects, False\n    otherwise.\n\nSee Also\n--------\nSeries.eq : Compare two Series objects of the same length\n    and return a Series where each element is True if the element\n    in each Series is equal, False otherwise.\nDataFrame.eq : Compare two DataFrame objects of the same shape and\n    return a DataFrame where each element is True if the respective\n    element in each DataFrame is equal, False otherwise.\ntesting.assert_series_equal : Raises an AssertionError if left and\n    right are not equal. Provides an easy interface to ignore\n    inequality in dtypes, indexes and precision among others.\ntesting.assert_frame_equal : Like assert_series_equal, but targets\n    DataFrames.\nnumpy.array_equal : Return True if two arrays have the same shape\n    and elements, False otherwise.\n\nExamples\n--------\n>>> df = pd.DataFrame({1: [10], 2: [20]})\n>>> df\n    1   2\n0  10  20\n\nDataFrames df and exactly_equal have the same types and values for\ntheir elements and column labels, which will return True.\n\n>>> exactly_equal = pd.DataFrame({1: [10], 2: [20]})\n>>> exactly_equal\n    1   2\n0  10  20\n>>> df.equals(exactly_equal)\nTrue\n\nDataFrames df and different_column_type have the same element\ntypes and values, but have different types for the column labels,\nwhich will still return True.\n\n>>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]})\n>>> different_column_type\n   1.0  2.0\n0   10   20\n>>> df.equals(different_column_type)\nTrue\n\nDataFrames df and different_data_type have different types for the\nsame values for their elements, and will return False even though\ntheir column labels are the same values and types.\n\n>>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]})\n>>> different_data_type\n      1     2\n0  10.0  20.0\n>>> df.equals(different_data_type)\nFalse", "Library": "Pandas"}
{"API_Name": "pandas.Series.fillna", "Docstring": "Fill NA/NaN values using the specified method.\n\nParameters\n----------\nvalue : scalar, dict, Series, or DataFrame\n    Value to use to fill holes (e.g. 0), alternately a\n    dict/Series/DataFrame of values specifying which value to use for\n    each index (for a Series) or column (for a DataFrame).  Values not\n    in the dict/Series/DataFrame will not be filled. This value cannot\n    be a list.\nmethod : {'backfill', 'bfill', 'pad', 'ffill', None}, default None\n    Method to use for filling holes in reindexed Series\n    pad / ffill: propagate last valid observation forward to next valid\n    backfill / bfill: use next valid observation to fill gap.\naxis : {0 or 'index'}\n    Axis along which to fill missing values. For `Series`\n    this parameter is unused and defaults to 0.\ninplace : bool, default False\n    If True, fill in-place. Note: this will modify any\n    other views on this object (e.g., a no-copy slice for a column in a\n    DataFrame).\nlimit : int, default None\n    If method is specified, this is the maximum number of consecutive\n    NaN values to forward/backward fill. In other words, if there is\n    a gap with more than this number of consecutive NaNs, it will only\n    be partially filled. If method is not specified, this is the\n    maximum number of entries along the entire axis where NaNs will be\n    filled. Must be greater than 0 if not None.\ndowncast : dict, default is None\n    A dict of item->dtype of what to downcast if possible,\n    or the string 'infer' which will try to downcast to an appropriate\n    equal type (e.g. float64 to int64 if possible).\n\nReturns\n-------\nSeries or None\n    Object with missing values filled or None if ``inplace=True``.\n\nSee Also\n--------\ninterpolate : Fill NaN values using interpolation.\nreindex : Conform object to new index.\nasfreq : Convert TimeSeries to specified frequency.\n\nExamples\n--------\n>>> df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n...                    [3, 4, np.nan, 1],\n...                    [np.nan, np.nan, np.nan, np.nan],\n...                    [np.nan, 3, np.nan, 4]],\n...                   columns=list(\"ABCD\"))\n>>> df\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  NaN  NaN NaN  NaN\n3  NaN  3.0 NaN  4.0\n\nReplace all NaN elements with 0s.\n\n>>> df.fillna(0)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  0.0\n3  0.0  3.0  0.0  4.0\n\nWe can also propagate non-null values forward or backward.\n\n>>> df.fillna(method=\"ffill\")\n     A    B   C    D\n0  NaN  2.0 NaN  0.0\n1  3.0  4.0 NaN  1.0\n2  3.0  4.0 NaN  1.0\n3  3.0  3.0 NaN  4.0\n\nReplace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1,\n2, and 3 respectively.\n\n>>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n>>> df.fillna(value=values)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  2.0  1.0\n2  0.0  1.0  2.0  3.0\n3  0.0  3.0  2.0  4.0\n\nOnly replace the first NaN element.\n\n>>> df.fillna(value=values, limit=1)\n     A    B    C    D\n0  0.0  2.0  2.0  0.0\n1  3.0  4.0  NaN  1.0\n2  NaN  1.0  NaN  3.0\n3  NaN  3.0  NaN  4.0\n\nWhen filling using a DataFrame, replacement happens along\nthe same column names and same indices\n\n>>> df2 = pd.DataFrame(np.zeros((4, 4)), columns=list(\"ABCE\"))\n>>> df.fillna(df2)\n     A    B    C    D\n0  0.0  2.0  0.0  0.0\n1  3.0  4.0  0.0  1.0\n2  0.0  0.0  0.0  NaN\n3  0.0  3.0  0.0  4.0\n\nNote that column D is not affected since it is not present in df2.", "Library": "Pandas"}
{"API_Name": "pandas.Series.groupby", "Docstring": "Group Series using a mapper or by a Series of columns.\n\nA groupby operation involves some combination of splitting the\nobject, applying a function, and combining the results. This can be\nused to group large amounts of data and compute operations on these\ngroups.\n\nParameters\n----------\nby : mapping, function, label, or list of labels\n    Used to determine the groups for the groupby.\n    If ``by`` is a function, it's called on each value of the object's\n    index. If a dict or Series is passed, the Series or dict VALUES\n    will be used to determine the groups (the Series' values are first\n    aligned; see ``.align()`` method). If a list or ndarray of length\n    equal to the selected axis is passed (see the `groupby user guide\n    <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups>`_),\n    the values are used as-is to determine the groups. A label or list\n    of labels may be passed to group by the columns in ``self``.\n    Notice that a tuple is interpreted as a (single) key.\naxis : {0 or 'index', 1 or 'columns'}, default 0\n    Split along rows (0) or columns (1). For `Series` this parameter\n    is unused and defaults to 0.\nlevel : int, level name, or sequence of such, default None\n    If the axis is a MultiIndex (hierarchical), group by a particular\n    level or levels. Do not specify both ``by`` and ``level``.\nas_index : bool, default True\n    For aggregated output, return object with group labels as the\n    index. Only relevant for DataFrame input. as_index=False is\n    effectively \"SQL-style\" grouped output.\nsort : bool, default True\n    Sort group keys. Get better performance by turning this off.\n    Note this does not influence the order of observations within each\n    group. Groupby preserves the order of rows within each group.\ngroup_keys : bool, optional\n    When calling apply and the ``by`` argument produces a like-indexed\n    (i.e. :ref:`a transform <groupby.transform>`) result, add group keys to\n    index to identify pieces. By default group keys are not included\n    when the result's index (and column) labels match the inputs, and\n    are included otherwise. This argument has no effect if the result produced\n    is not like-indexed with respect to the input.\n\n    .. versionchanged:: 1.5.0\n\n       Warns that `group_keys` will no longer be ignored when the\n       result from ``apply`` is a like-indexed Series or DataFrame.\n       Specify ``group_keys`` explicitly to include the group keys or\n       not.\nsqueeze : bool, default False\n    Reduce the dimensionality of the return type if possible,\n    otherwise return a consistent type.\n\n    .. deprecated:: 1.1.0\n\nobserved : bool, default False\n    This only applies if any of the groupers are Categoricals.\n    If True: only show observed values for categorical groupers.\n    If False: show all values for categorical groupers.\ndropna : bool, default True\n    If True, and if group keys contain NA values, NA values together\n    with row/column will be dropped.\n    If False, NA values will also be treated as the key in groups.\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nSeriesGroupBy\n    Returns a groupby object that contains information about the groups.\n\nSee Also\n--------\nresample : Convenience method for frequency conversion and resampling\n    of time series.\n\nNotes\n-----\nSee the `user guide\n<https://pandas.pydata.org/pandas-docs/stable/groupby.html>`__ for more\ndetailed usage and examples, including splitting an object into groups,\niterating through groups, selecting a group, aggregation, and more.\n\nExamples\n--------\n>>> ser = pd.Series([390., 350., 30., 20.],\n...                 index=['Falcon', 'Falcon', 'Parrot', 'Parrot'], name=\"Max Speed\")\n>>> ser\nFalcon    390.0\nFalcon    350.0\nParrot     30.0\nParrot     20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby([\"a\", \"b\", \"a\", \"b\"]).mean()\na    210.0\nb    185.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(ser > 100).mean()\nMax Speed\nFalse     25.0\nTrue     370.0\nName: Max Speed, dtype: float64\n\n**Grouping by Indexes**\n\nWe can groupby different levels of a hierarchical index\nusing the `level` parameter:\n\n>>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n...           ['Captive', 'Wild', 'Captive', 'Wild']]\n>>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n>>> ser = pd.Series([390., 350., 30., 20.], index=index, name=\"Max Speed\")\n>>> ser\nAnimal  Type\nFalcon  Captive    390.0\n        Wild       350.0\nParrot  Captive     30.0\n        Wild        20.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=0).mean()\nAnimal\nFalcon    370.0\nParrot     25.0\nName: Max Speed, dtype: float64\n>>> ser.groupby(level=\"Type\").mean()\nType\nCaptive    210.0\nWild       185.0\nName: Max Speed, dtype: float64\n\nWe can also choose to include `NA` in group keys or not by defining\n`dropna` parameter, the default setting is `True`.\n\n>>> ser = pd.Series([1, 2, 3, 3], index=[\"a\", 'a', 'b', np.nan])\n>>> ser.groupby(level=0).sum()\na    3\nb    3\ndtype: int64\n\n>>> ser.groupby(level=0, dropna=False).sum()\na    3\nb    3\nNaN  3\ndtype: int64\n\n>>> arrays = ['Falcon', 'Falcon', 'Parrot', 'Parrot']\n>>> ser = pd.Series([390., 350., 30., 20.], index=arrays, name=\"Max Speed\")\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan]).mean()\na    210.0\nb    350.0\nName: Max Speed, dtype: float64\n\n>>> ser.groupby([\"a\", \"b\", \"a\", np.nan], dropna=False).mean()\na    210.0\nb    350.0\nNaN   20.0\nName: Max Speed, dtype: float64", "Library": "Pandas"}
{"API_Name": "pandas.Series.gt", "Docstring": "Return Greater than of series and other, element-wise (binary operator `gt`).\n\nEquivalent to ``series > other``, but with support to substitute a fill_value for\nmissing data in either one of the inputs.\n\nParameters\n----------\nother : Series or scalar value\nlevel : int or name\n    Broadcast across a level, matching Index values on the\n    passed MultiIndex level.\nfill_value : None or float value, default None (NaN)\n    Fill existing missing (NaN) values, and any new element needed for\n    successful Series alignment, with this value before computation.\n    If data in both corresponding Series locations is missing\n    the result of filling (at that location) will be missing.\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\n\nReturns\n-------\nSeries\n    The result of the operation.\n\nExamples\n--------\n>>> a = pd.Series([1, 1, 1, np.nan, 1], index=['a', 'b', 'c', 'd', 'e'])\n>>> a\na    1.0\nb    1.0\nc    1.0\nd    NaN\ne    1.0\ndtype: float64\n>>> b = pd.Series([0, 1, 2, np.nan, 1], index=['a', 'b', 'c', 'd', 'f'])\n>>> b\na    0.0\nb    1.0\nc    2.0\nd    NaN\nf    1.0\ndtype: float64\n>>> a.gt(b, fill_value=0)\na     True\nb    False\nc    False\nd    False\ne     True\nf    False\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.Series.idxmax", "Docstring": "Return the row label of the maximum value.\n\nIf multiple values equal the maximum, the first row label with that\nvalue is returned.\n\nParameters\n----------\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\nskipna : bool, default True\n    Exclude NA/null values. If the entire Series is NA, the result\n    will be NA.\n*args, **kwargs\n    Additional arguments and keywords have no effect but might be\n    accepted for compatibility with NumPy.\n\nReturns\n-------\nIndex\n    Label of the maximum value.\n\nRaises\n------\nValueError\n    If the Series is empty.\n\nSee Also\n--------\nnumpy.argmax : Return indices of the maximum values\n    along the given axis.\nDataFrame.idxmax : Return index of first occurrence of maximum\n    over requested axis.\nSeries.idxmin : Return index *label* of the first occurrence\n    of minimum of values.\n\nNotes\n-----\nThis method is the Series version of ``ndarray.argmax``. This method\nreturns the label of the maximum, while ``ndarray.argmax`` returns\nthe position. To get the position, use ``series.values.argmax()``.\n\nExamples\n--------\n>>> s = pd.Series(data=[1, None, 4, 3, 4],\n...               index=['A', 'B', 'C', 'D', 'E'])\n>>> s\nA    1.0\nB    NaN\nC    4.0\nD    3.0\nE    4.0\ndtype: float64\n\n>>> s.idxmax()\n'C'\n\nIf `skipna` is False and there is an NA value in the data,\nthe function returns ``nan``.\n\n>>> s.idxmax(skipna=False)\nnan", "Library": "Pandas"}
{"API_Name": "pandas.Series.iloc", "Docstring": "Purely integer-location based indexing for selection by position.\n\n``.iloc[]`` is primarily integer position based (from ``0`` to\n``length-1`` of the axis), but may also be used with a boolean\narray.\n\nAllowed inputs are:\n\n- An integer, e.g. ``5``.\n- A list or array of integers, e.g. ``[4, 3, 0]``.\n- A slice object with ints, e.g. ``1:7``.\n- A boolean array.\n- A ``callable`` function with one argument (the calling Series or\n  DataFrame) and that returns valid output for indexing (one of the above).\n  This is useful in method chains, when you don't have a reference to the\n  calling object, but would like to base your selection on some value.\n- A tuple of row and column indexes. The tuple elements consist of one of the\n  above inputs, e.g. ``(0, 1)``.\n\n``.iloc`` will raise ``IndexError`` if a requested indexer is\nout-of-bounds, except *slice* indexers which allow out-of-bounds\nindexing (this conforms with python/numpy *slice* semantics).\n\nSee more at :ref:`Selection by Position <indexing.integer>`.\n\nSee Also\n--------\nDataFrame.iat : Fast integer location scalar accessor.\nDataFrame.loc : Purely label-location based indexer for selection by label.\nSeries.iloc : Purely integer-location based indexing for\n               selection by position.\n\nExamples\n--------\n>>> mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4},\n...           {'a': 100, 'b': 200, 'c': 300, 'd': 400},\n...           {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }]\n>>> df = pd.DataFrame(mydict)\n>>> df\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\n\n**Indexing just the rows**\n\nWith a scalar integer.\n\n>>> type(df.iloc[0])\n<class 'pandas.core.series.Series'>\n>>> df.iloc[0]\na    1\nb    2\nc    3\nd    4\nName: 0, dtype: int64\n\nWith a list of integers.\n\n>>> df.iloc[[0]]\n   a  b  c  d\n0  1  2  3  4\n>>> type(df.iloc[[0]])\n<class 'pandas.core.frame.DataFrame'>\n\n>>> df.iloc[[0, 1]]\n     a    b    c    d\n0    1    2    3    4\n1  100  200  300  400\n\nWith a `slice` object.\n\n>>> df.iloc[:3]\n      a     b     c     d\n0     1     2     3     4\n1   100   200   300   400\n2  1000  2000  3000  4000\n\nWith a boolean mask the same length as the index.\n\n>>> df.iloc[[True, False, True]]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\n\nWith a callable, useful in method chains. The `x` passed\nto the ``lambda`` is the DataFrame being sliced. This selects\nthe rows whose index label even.\n\n>>> df.iloc[lambda x: x.index % 2 == 0]\n      a     b     c     d\n0     1     2     3     4\n2  1000  2000  3000  4000\n\n**Indexing both axes**\n\nYou can mix the indexer types for the index and columns. Use ``:`` to\nselect the entire axis.\n\nWith scalar integers.\n\n>>> df.iloc[0, 1]\n2\n\nWith lists of integers.\n\n>>> df.iloc[[0, 2], [1, 3]]\n      b     d\n0     2     4\n2  2000  4000\n\nWith `slice` objects.\n\n>>> df.iloc[1:3, 0:3]\n      a     b     c\n1   100   200   300\n2  1000  2000  3000\n\nWith a boolean array whose length matches the columns.\n\n>>> df.iloc[:, [True, False, True, False]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000\n\nWith a callable function that expects the Series or DataFrame.\n\n>>> df.iloc[:, lambda df: [0, 2]]\n      a     c\n0     1     3\n1   100   300\n2  1000  3000", "Library": "Pandas"}
{"API_Name": "pandas.Series.index", "Docstring": "The index (axis labels) of the Series.", "Library": "Pandas"}
{"API_Name": "pandas.Series.isin", "Docstring": "Whether elements in Series are contained in `values`.\n\nReturn a boolean Series showing whether each element in the Series\nmatches an element in the passed sequence of `values` exactly.\n\nParameters\n----------\nvalues : set or list-like\n    The sequence of values to test. Passing in a single string will\n    raise a ``TypeError``. Instead, turn a single string into a\n    list of one element.\n\nReturns\n-------\nSeries\n    Series of booleans indicating if each element is in values.\n\nRaises\n------\nTypeError\n  * If `values` is a string\n\nSee Also\n--------\nDataFrame.isin : Equivalent method on DataFrame.\n\nExamples\n--------\n>>> s = pd.Series(['lama', 'cow', 'lama', 'beetle', 'lama',\n...                'hippo'], name='animal')\n>>> s.isin(['cow', 'lama'])\n0     True\n1     True\n2     True\n3    False\n4     True\n5    False\nName: animal, dtype: bool\n\nTo invert the boolean values, use the ``~`` operator:\n\n>>> ~s.isin(['cow', 'lama'])\n0    False\n1    False\n2    False\n3     True\n4    False\n5     True\nName: animal, dtype: bool\n\nPassing a single string as ``s.isin('lama')`` will raise an error. Use\na list of one element instead:\n\n>>> s.isin(['lama'])\n0     True\n1    False\n2     True\n3    False\n4     True\n5    False\nName: animal, dtype: bool\n\nStrings and integers are distinct and are therefore not comparable:\n\n>>> pd.Series([1]).isin(['1'])\n0    False\ndtype: bool\n>>> pd.Series([1.1]).isin(['1.1'])\n0    False\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.Series.isna", "Docstring": "Detect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA.\nNA values, such as None or :attr:`numpy.NaN`, gets mapped to True\nvalues.\nEverything else gets mapped to False values. Characters such as empty\nstrings ``''`` or :attr:`numpy.inf` are not considered NA values\n(unless you set ``pandas.options.mode.use_inf_as_na = True``).\n\nReturns\n-------\nSeries\n    Mask of bool values for each element in Series that\n    indicates whether an element is an NA value.\n\nSee Also\n--------\nSeries.isnull : Alias of isna.\nSeries.notna : Boolean inverse of isna.\nSeries.dropna : Omit axes labels with missing values.\nisna : Top-level isna.\n\nExamples\n--------\nShow which entries in a DataFrame are NA.\n\n>>> df = pd.DataFrame(dict(age=[5, 6, np.NaN],\n...                    born=[pd.NaT, pd.Timestamp('1939-05-27'),\n...                          pd.Timestamp('1940-04-25')],\n...                    name=['Alfred', 'Batman', ''],\n...                    toy=[None, 'Batmobile', 'Joker']))\n>>> df\n   age       born    name        toy\n0  5.0        NaT  Alfred       None\n1  6.0 1939-05-27  Batman  Batmobile\n2  NaN 1940-04-25              Joker\n\n>>> df.isna()\n     age   born   name    toy\n0  False   True  False   True\n1  False  False  False  False\n2   True  False  False  False\n\nShow which entries in a Series are NA.\n\n>>> ser = pd.Series([5, 6, np.NaN])\n>>> ser\n0    5.0\n1    6.0\n2    NaN\ndtype: float64\n\n>>> ser.isna()\n0    False\n1    False\n2     True\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.Series.map", "Docstring": "Map values of Series according to an input mapping or function.\n\nUsed for substituting each value in a Series with another value,\nthat may be derived from a function, a ``dict`` or\na :class:`Series`.\n\nParameters\n----------\narg : function, collections.abc.Mapping subclass or Series\n    Mapping correspondence.\nna_action : {None, 'ignore'}, default None\n    If 'ignore', propagate NaN values, without passing them to the\n    mapping correspondence.\n\nReturns\n-------\nSeries\n    Same index as caller.\n\nSee Also\n--------\nSeries.apply : For applying more complex functions on a Series.\nDataFrame.apply : Apply a function row-/column-wise.\nDataFrame.applymap : Apply a function elementwise on a whole DataFrame.\n\nNotes\n-----\nWhen ``arg`` is a dictionary, values in Series that are not in the\ndictionary (as keys) are converted to ``NaN``. However, if the\ndictionary is a ``dict`` subclass that defines ``__missing__`` (i.e.\nprovides a method for default values), then this default is used\nrather than ``NaN``.\n\nExamples\n--------\n>>> s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n>>> s\n0      cat\n1      dog\n2      NaN\n3   rabbit\ndtype: object\n\n``map`` accepts a ``dict`` or a ``Series``. Values that are not found\nin the ``dict`` are converted to ``NaN``, unless the dict has a default\nvalue (e.g. ``defaultdict``):\n\n>>> s.map({'cat': 'kitten', 'dog': 'puppy'})\n0   kitten\n1    puppy\n2      NaN\n3      NaN\ndtype: object\n\nIt also accepts a function:\n\n>>> s.map('I am a {}'.format)\n0       I am a cat\n1       I am a dog\n2       I am a nan\n3    I am a rabbit\ndtype: object\n\nTo avoid applying the function to missing values (and keep them as\n``NaN``) ``na_action='ignore'`` can be used:\n\n>>> s.map('I am a {}'.format, na_action='ignore')\n0     I am a cat\n1     I am a dog\n2            NaN\n3  I am a rabbit\ndtype: object", "Library": "Pandas"}
{"API_Name": "pandas.Series.max", "Docstring": "Return the maximum of the values over the requested axis.\n\nIf you want the *index* of the maximum, use ``idxmax``. This is the equivalent of the ``numpy.ndarray`` method ``argmax``.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Series (if level specified)\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.max()\n8", "Library": "Pandas"}
{"API_Name": "pandas.Series.mean", "Docstring": "Return the mean of the values over the requested axis.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Series (if level specified)", "Library": "Pandas"}
{"API_Name": "pandas.Series.min", "Docstring": "Return the minimum of the values over the requested axis.\n\nIf you want the *index* of the minimum, use ``idxmin``. This is the equivalent of the ``numpy.ndarray`` method ``argmin``.\n\nParameters\n----------\naxis : {index (0)}\n    Axis for the function to be applied on.\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values when computing the result.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\n**kwargs\n    Additional keyword arguments to be passed to the function.\n\nReturns\n-------\nscalar or Series (if level specified)\n\nSee Also\n--------\nSeries.sum : Return the sum.\nSeries.min : Return the minimum.\nSeries.max : Return the maximum.\nSeries.idxmin : Return the index of the minimum.\nSeries.idxmax : Return the index of the maximum.\nDataFrame.sum : Return the sum over the requested axis.\nDataFrame.min : Return the minimum over the requested axis.\nDataFrame.max : Return the maximum over the requested axis.\nDataFrame.idxmin : Return the index of the minimum over the requested axis.\nDataFrame.idxmax : Return the index of the maximum over the requested axis.\n\nExamples\n--------\n>>> idx = pd.MultiIndex.from_arrays([\n...     ['warm', 'warm', 'cold', 'cold'],\n...     ['dog', 'falcon', 'fish', 'spider']],\n...     names=['blooded', 'animal'])\n>>> s = pd.Series([4, 2, 0, 8], name='legs', index=idx)\n>>> s\nblooded  animal\nwarm     dog       4\n         falcon    2\ncold     fish      0\n         spider    8\nName: legs, dtype: int64\n\n>>> s.min()\n0", "Library": "Pandas"}
{"API_Name": "pandas.Series.nlargest", "Docstring": "Return the largest `n` elements.\n\nParameters\n----------\nn : int, default 5\n    Return this many descending sorted values.\nkeep : {'first', 'last', 'all'}, default 'first'\n    When there are duplicate values that cannot all fit in a\n    Series of `n` elements:\n\n    - ``first`` : return the first `n` occurrences in order\n      of appearance.\n    - ``last`` : return the last `n` occurrences in reverse\n      order of appearance.\n    - ``all`` : keep all occurrences. This can result in a Series of\n      size larger than `n`.\n\nReturns\n-------\nSeries\n    The `n` largest values in the Series, sorted in decreasing order.\n\nSee Also\n--------\nSeries.nsmallest: Get the `n` smallest elements.\nSeries.sort_values: Sort Series by values.\nSeries.head: Return the first `n` rows.\n\nNotes\n-----\nFaster than ``.sort_values(ascending=False).head(n)`` for small `n`\nrelative to the size of the ``Series`` object.\n\nExamples\n--------\n>>> countries_population = {\"Italy\": 59000000, \"France\": 65000000,\n...                         \"Malta\": 434000, \"Maldives\": 434000,\n...                         \"Brunei\": 434000, \"Iceland\": 337000,\n...                         \"Nauru\": 11300, \"Tuvalu\": 11300,\n...                         \"Anguilla\": 11300, \"Montserrat\": 5200}\n>>> s = pd.Series(countries_population)\n>>> s\nItaly       59000000\nFrance      65000000\nMalta         434000\nMaldives      434000\nBrunei        434000\nIceland       337000\nNauru          11300\nTuvalu         11300\nAnguilla       11300\nMontserrat      5200\ndtype: int64\n\nThe `n` largest elements where ``n=5`` by default.\n\n>>> s.nlargest()\nFrance      65000000\nItaly       59000000\nMalta         434000\nMaldives      434000\nBrunei        434000\ndtype: int64\n\nThe `n` largest elements where ``n=3``. Default `keep` value is 'first'\nso Malta will be kept.\n\n>>> s.nlargest(3)\nFrance    65000000\nItaly     59000000\nMalta       434000\ndtype: int64\n\nThe `n` largest elements where ``n=3`` and keeping the last duplicates.\nBrunei will be kept since it is the last with value 434000 based on\nthe index order.\n\n>>> s.nlargest(3, keep='last')\nFrance      65000000\nItaly       59000000\nBrunei        434000\ndtype: int64\n\nThe `n` largest elements where ``n=3`` with all duplicates kept. Note\nthat the returned Series has five elements due to the three duplicates.\n\n>>> s.nlargest(3, keep='all')\nFrance      65000000\nItaly       59000000\nMalta         434000\nMaldives      434000\nBrunei        434000\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.Series.quantile", "Docstring": "Return value at the given quantile.\n\nParameters\n----------\nq : float or array-like, default 0.5 (50% quantile)\n    The quantile(s) to compute, which can lie in range: 0 <= q <= 1.\ninterpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}\n    This optional parameter specifies the interpolation method to use,\n    when the desired quantile lies between two data points `i` and `j`:\n\n        * linear: `i + (j - i) * fraction`, where `fraction` is the\n          fractional part of the index surrounded by `i` and `j`.\n        * lower: `i`.\n        * higher: `j`.\n        * nearest: `i` or `j` whichever is nearest.\n        * midpoint: (`i` + `j`) / 2.\n\nReturns\n-------\nfloat or Series\n    If ``q`` is an array, a Series will be returned where the\n    index is ``q`` and the values are the quantiles, otherwise\n    a float will be returned.\n\nSee Also\n--------\ncore.window.Rolling.quantile : Calculate the rolling quantile.\nnumpy.percentile : Returns the q-th percentile(s) of the array elements.\n\nExamples\n--------\n>>> s = pd.Series([1, 2, 3, 4])\n>>> s.quantile(.5)\n2.5\n>>> s.quantile([.25, .5, .75])\n0.25    1.75\n0.50    2.50\n0.75    3.25\ndtype: float64", "Library": "Pandas"}
{"API_Name": "pandas.Series.rename", "Docstring": "Alter Series index labels or name.\n\nFunction / dict values must be unique (1-to-1). Labels not contained in\na dict / Series will be left as-is. Extra labels listed don't throw an\nerror.\n\nAlternatively, change ``Series.name`` with a scalar value.\n\nSee the :ref:`user guide <basics.rename>` for more.\n\nParameters\n----------\nindex : scalar, hashable sequence, dict-like or function optional\n    Functions or dict-like are transformations to apply to\n    the index.\n    Scalar or hashable sequence-like will alter the ``Series.name``\n    attribute.\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\ncopy : bool, default True\n    Also copy underlying data.\ninplace : bool, default False\n    Whether to return a new Series. If True the value of copy is ignored.\nlevel : int or level name, default None\n    In case of MultiIndex, only rename labels in the specified level.\nerrors : {'ignore', 'raise'}, default 'ignore'\n    If 'raise', raise `KeyError` when a `dict-like mapper` or\n    `index` contains labels that are not present in the index being transformed.\n    If 'ignore', existing keys will be renamed and extra keys will be ignored.\n\nReturns\n-------\nSeries or None\n    Series with index labels or name altered or None if ``inplace=True``.\n\nSee Also\n--------\nDataFrame.rename : Corresponding DataFrame method.\nSeries.rename_axis : Set the name of the axis.\n\nExamples\n--------\n>>> s = pd.Series([1, 2, 3])\n>>> s\n0    1\n1    2\n2    3\ndtype: int64\n>>> s.rename(\"my_name\")  # scalar, changes Series.name\n0    1\n1    2\n2    3\nName: my_name, dtype: int64\n>>> s.rename(lambda x: x ** 2)  # function, changes labels\n0    1\n1    2\n4    3\ndtype: int64\n>>> s.rename({1: 3, 2: 5})  # mapping, changes labels\n0    1\n3    2\n5    3\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.Series.reset_index", "Docstring": "Generate a new DataFrame or Series with the index reset.\n\nThis is useful when the index needs to be treated as a column, or\nwhen the index is meaningless and needs to be reset to the default\nbefore another operation.\n\nParameters\n----------\nlevel : int, str, tuple, or list, default optional\n    For a Series with a MultiIndex, only remove the specified levels\n    from the index. Removes all levels by default.\ndrop : bool, default False\n    Just reset the index, without inserting it as a column in\n    the new DataFrame.\nname : object, optional\n    The name to use for the column containing the original Series\n    values. Uses ``self.name`` by default. This argument is ignored\n    when `drop` is True.\ninplace : bool, default False\n    Modify the Series in place (do not create a new object).\nallow_duplicates : bool, default False\n    Allow duplicate column labels to be created.\n\n    .. versionadded:: 1.5.0\n\nReturns\n-------\nSeries or DataFrame or None\n    When `drop` is False (the default), a DataFrame is returned.\n    The newly created columns will come first in the DataFrame,\n    followed by the original Series values.\n    When `drop` is True, a `Series` is returned.\n    In either case, if ``inplace=True``, no value is returned.\n\nSee Also\n--------\nDataFrame.reset_index: Analogous function for DataFrame.\n\nExamples\n--------\n>>> s = pd.Series([1, 2, 3, 4], name='foo',\n...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\nGenerate a DataFrame with default index.\n\n>>> s.reset_index()\n  idx  foo\n0   a    1\n1   b    2\n2   c    3\n3   d    4\n\nTo specify the name of the new column use `name`.\n\n>>> s.reset_index(name='values')\n  idx  values\n0   a       1\n1   b       2\n2   c       3\n3   d       4\n\nTo generate a new Series with the default set `drop` to True.\n\n>>> s.reset_index(drop=True)\n0    1\n1    2\n2    3\n3    4\nName: foo, dtype: int64\n\nTo update the Series in place, without generating a new one\nset `inplace` to True. Note that it also requires ``drop=True``.\n\n>>> s.reset_index(inplace=True, drop=True)\n>>> s\n0    1\n1    2\n2    3\n3    4\nName: foo, dtype: int64\n\nThe `level` parameter is interesting for Series with a multi-level\nindex.\n\n>>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n...           np.array(['one', 'two', 'one', 'two'])]\n>>> s2 = pd.Series(\n...     range(4), name='foo',\n...     index=pd.MultiIndex.from_arrays(arrays,\n...                                     names=['a', 'b']))\n\nTo remove a specific level from the Index, use `level`.\n\n>>> s2.reset_index(level='a')\n       a  foo\nb\none  bar    0\ntwo  bar    1\none  baz    2\ntwo  baz    3\n\nIf `level` is not set, all levels are removed from the Index.\n\n>>> s2.reset_index()\n     a    b  foo\n0  bar  one    0\n1  bar  two    1\n2  baz  one    2\n3  baz  two    3", "Library": "Pandas"}
{"API_Name": "pandas.Series.shift", "Docstring": "Shift index by desired number of periods with an optional time `freq`.\n\nWhen `freq` is not passed, shift the index without realigning the data.\nIf `freq` is passed (in this case, the index must be date or datetime,\nor it will raise a `NotImplementedError`), the index will be\nincreased using the periods and the `freq`. `freq` can be inferred\nwhen specified as \"infer\" as long as either freq or inferred_freq\nattribute is set in the index.\n\nParameters\n----------\nperiods : int\n    Number of periods to shift. Can be positive or negative.\nfreq : DateOffset, tseries.offsets, timedelta, or str, optional\n    Offset to use from the tseries module or time rule (e.g. 'EOM').\n    If `freq` is specified then the index values are shifted but the\n    data is not realigned. That is, use `freq` if you would like to\n    extend the index when shifting and preserve the original data.\n    If `freq` is specified as \"infer\" then it will be inferred from\n    the freq or inferred_freq attributes of the index. If neither of\n    those attributes exist, a ValueError is thrown.\naxis : {0 or 'index', 1 or 'columns', None}, default None\n    Shift direction. For `Series` this parameter is unused and defaults to 0.\nfill_value : object, optional\n    The scalar value to use for newly introduced missing values.\n    the default depends on the dtype of `self`.\n    For numeric data, ``np.nan`` is used.\n    For datetime, timedelta, or period data, etc. :attr:`NaT` is used.\n    For extension dtypes, ``self.dtype.na_value`` is used.\n\n    .. versionchanged:: 1.1.0\n\nReturns\n-------\nSeries\n    Copy of input object, shifted.\n\nSee Also\n--------\nIndex.shift : Shift values of Index.\nDatetimeIndex.shift : Shift values of DatetimeIndex.\nPeriodIndex.shift : Shift values of PeriodIndex.\ntshift : Shift the time index, using the index's frequency if\n    available.\n\nExamples\n--------\n>>> df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45],\n...                    \"Col2\": [13, 23, 18, 33, 48],\n...                    \"Col3\": [17, 27, 22, 37, 52]},\n...                   index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))\n>>> df\n            Col1  Col2  Col3\n2020-01-01    10    13    17\n2020-01-02    20    23    27\n2020-01-03    15    18    22\n2020-01-04    30    33    37\n2020-01-05    45    48    52\n\n>>> df.shift(periods=3)\n            Col1  Col2  Col3\n2020-01-01   NaN   NaN   NaN\n2020-01-02   NaN   NaN   NaN\n2020-01-03   NaN   NaN   NaN\n2020-01-04  10.0  13.0  17.0\n2020-01-05  20.0  23.0  27.0\n\n>>> df.shift(periods=1, axis=\"columns\")\n            Col1  Col2  Col3\n2020-01-01   NaN    10    13\n2020-01-02   NaN    20    23\n2020-01-03   NaN    15    18\n2020-01-04   NaN    30    33\n2020-01-05   NaN    45    48\n\n>>> df.shift(periods=3, fill_value=0)\n            Col1  Col2  Col3\n2020-01-01     0     0     0\n2020-01-02     0     0     0\n2020-01-03     0     0     0\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n\n>>> df.shift(periods=3, freq=\"D\")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52\n\n>>> df.shift(periods=3, freq=\"infer\")\n            Col1  Col2  Col3\n2020-01-04    10    13    17\n2020-01-05    20    23    27\n2020-01-06    15    18    22\n2020-01-07    30    33    37\n2020-01-08    45    48    52", "Library": "Pandas"}
{"API_Name": "pandas.Series.sort_values", "Docstring": "Sort by the values.\n\nSort a Series in ascending or descending order by some\ncriterion.\n\nParameters\n----------\naxis : {0 or 'index'}\n    Unused. Parameter needed for compatibility with DataFrame.\nascending : bool or list of bools, default True\n    If True, sort values in ascending order, otherwise descending.\ninplace : bool, default False\n    If True, perform operation in-place.\nkind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n    Choice of sorting algorithm. See also :func:`numpy.sort` for more\n    information. 'mergesort' and 'stable' are the only stable  algorithms.\nna_position : {'first' or 'last'}, default 'last'\n    Argument 'first' puts NaNs at the beginning, 'last' puts NaNs at\n    the end.\nignore_index : bool, default False\n    If True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\n    .. versionadded:: 1.0.0\n\nkey : callable, optional\n    If not None, apply the key function to the series values\n    before sorting. This is similar to the `key` argument in the\n    builtin :meth:`sorted` function, with the notable difference that\n    this `key` function should be *vectorized*. It should expect a\n    ``Series`` and return an array-like.\n\n    .. versionadded:: 1.1.0\n\nReturns\n-------\nSeries or None\n    Series ordered by values or None if ``inplace=True``.\n\nSee Also\n--------\nSeries.sort_index : Sort by the Series indices.\nDataFrame.sort_values : Sort DataFrame by the values along either axis.\nDataFrame.sort_index : Sort DataFrame by indices.\n\nExamples\n--------\n>>> s = pd.Series([np.nan, 1, 3, 10, 5])\n>>> s\n0     NaN\n1     1.0\n2     3.0\n3     10.0\n4     5.0\ndtype: float64\n\nSort values ascending order (default behaviour)\n\n>>> s.sort_values(ascending=True)\n1     1.0\n2     3.0\n4     5.0\n3    10.0\n0     NaN\ndtype: float64\n\nSort values descending order\n\n>>> s.sort_values(ascending=False)\n3    10.0\n4     5.0\n2     3.0\n1     1.0\n0     NaN\ndtype: float64\n\nSort values inplace\n\n>>> s.sort_values(ascending=False, inplace=True)\n>>> s\n3    10.0\n4     5.0\n2     3.0\n1     1.0\n0     NaN\ndtype: float64\n\nSort values putting NAs first\n\n>>> s.sort_values(na_position='first')\n0     NaN\n1     1.0\n2     3.0\n4     5.0\n3    10.0\ndtype: float64\n\nSort a series of strings\n\n>>> s = pd.Series(['z', 'b', 'd', 'a', 'c'])\n>>> s\n0    z\n1    b\n2    d\n3    a\n4    c\ndtype: object\n\n>>> s.sort_values()\n3    a\n1    b\n4    c\n2    d\n0    z\ndtype: object\n\nSort using a key function. Your `key` function will be\ngiven the ``Series`` of values and should return an array-like.\n\n>>> s = pd.Series(['a', 'B', 'c', 'D', 'e'])\n>>> s.sort_values()\n1    B\n3    D\n0    a\n2    c\n4    e\ndtype: object\n>>> s.sort_values(key=lambda x: x.str.lower())\n0    a\n1    B\n2    c\n3    D\n4    e\ndtype: object\n\nNumPy ufuncs work well here. For example, we can\nsort by the ``sin`` of the value\n\n>>> s = pd.Series([-4, -2, 0, 2, 4])\n>>> s.sort_values(key=np.sin)\n1   -2\n4    4\n2    0\n0   -4\n3    2\ndtype: int64\n\nMore complicated user-defined functions can be used,\nas long as they expect a Series and return an array-like\n\n>>> s.sort_values(key=lambda x: (np.tan(x.cumsum())))\n0   -4\n3    2\n4    4\n1   -2\n2    0\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.Series.std", "Docstring": "Return sample standard deviation over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.\n\nParameters\n----------\naxis : {index (0)}\n    For `Series` this parameter is unused and defaults to 0.\nskipna : bool, default True\n    Exclude NA/null values. If an entire row/column is NA, the result\n    will be NA.\nlevel : int or level name, default None\n    If the axis is a MultiIndex (hierarchical), count along a\n    particular level, collapsing into a scalar.\n\n    .. deprecated:: 1.3.0\n        The level keyword is deprecated. Use groupby instead.\nddof : int, default 1\n    Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n    where N represents the number of elements.\nnumeric_only : bool, default None\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data. Not implemented for Series.\n\n    .. deprecated:: 1.5.0\n        Specifying ``numeric_only=None`` is deprecated. The default value will be\n        ``False`` in a future version of pandas.\n\nReturns\n-------\nscalar or Series (if level specified) \n\nNotes\n-----\nTo have the same behaviour as `numpy.std`, use `ddof=0` (instead of the\ndefault `ddof=1`)\n\nExamples\n--------\n>>> df = pd.DataFrame({'person_id': [0, 1, 2, 3],\n...                   'age': [21, 25, 62, 43],\n...                   'height': [1.61, 1.87, 1.49, 2.01]}\n...                  ).set_index('person_id')\n>>> df\n           age  height\nperson_id\n0           21    1.61\n1           25    1.87\n2           62    1.49\n3           43    2.01\n\nThe standard deviation of the columns can be found as follows:\n\n>>> df.std()\nage       18.786076\nheight     0.237417\n\nAlternatively, `ddof=0` can be set to normalize by N instead of N-1:\n\n>>> df.std(ddof=0)\nage       16.269219\nheight     0.205609", "Library": "Pandas"}
{"API_Name": "pandas.Series.str.endswith", "Docstring": "Test if the end of each string element matches a pattern.\n\nEquivalent to :meth:`str.endswith`.\n\nParameters\n----------\npat : str or tuple[str, ...]\n    Character sequence or tuple of strings. Regular expressions are not\n    accepted.\nna : object, default NaN\n    Object shown if element tested is not a string. The default depends\n    on dtype of the array. For object-dtype, ``numpy.nan`` is used.\n    For ``StringDtype``, ``pandas.NA`` is used.\n\nReturns\n-------\nSeries or Index of bool\n    A Series of booleans indicating whether the given pattern matches\n    the end of each string element.\n\nSee Also\n--------\nstr.endswith : Python standard library string method.\nSeries.str.startswith : Same as endswith, but tests the start of string.\nSeries.str.contains : Tests if string element contains a pattern.\n\nExamples\n--------\n>>> s = pd.Series(['bat', 'bear', 'caT', np.nan])\n>>> s\n0     bat\n1    bear\n2     caT\n3     NaN\ndtype: object\n\n>>> s.str.endswith('t')\n0     True\n1    False\n2    False\n3      NaN\ndtype: object\n\n>>> s.str.endswith(('t', 'T'))\n0     True\n1    False\n2     True\n3      NaN\ndtype: object\n\nSpecifying `na` to be `False` instead of `NaN`.\n\n>>> s.str.endswith('t', na=False)\n0     True\n1    False\n2    False\n3    False\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.Series.str.get", "Docstring": "Extract element from each component at specified position or with specified key.\n\nExtract element from lists, tuples, dict, or strings in each element in the\nSeries/Index.\n\nParameters\n----------\ni : int or hashable dict label\n    Position or key of element to extract.\n\nReturns\n-------\nSeries or Index\n\nExamples\n--------\n>>> s = pd.Series([\"String\",\n...               (1, 2, 3),\n...               [\"a\", \"b\", \"c\"],\n...               123,\n...               -456,\n...               {1: \"Hello\", \"2\": \"World\"}])\n>>> s\n0                        String\n1                     (1, 2, 3)\n2                     [a, b, c]\n3                           123\n4                          -456\n5    {1: 'Hello', '2': 'World'}\ndtype: object\n\n>>> s.str.get(1)\n0        t\n1        2\n2        b\n3      NaN\n4      NaN\n5    Hello\ndtype: object\n\n>>> s.str.get(-1)\n0      g\n1      3\n2      c\n3    NaN\n4    NaN\n5    None\ndtype: object\n\nReturn element with given key\n\n>>> s = pd.Series([{\"name\": \"Hello\", \"value\": \"World\"},\n...               {\"name\": \"Goodbye\", \"value\": \"Planet\"}])\n>>> s.str.get('name')\n0      Hello\n1    Goodbye\ndtype: object", "Library": "Pandas"}
{"API_Name": "pandas.Series.str.isdigit", "Docstring": "Check whether all characters in each string are digits.\n\nThis is equivalent to running the Python string method\n:meth:`str.isdigit` for each element of the Series/Index. If a string\nhas zero characters, ``False`` is returned for that check.\n\nReturns\n-------\nSeries or Index of bool\n    Series or Index of boolean values with the same length as the original\n    Series/Index.\n\nSee Also\n--------\nSeries.str.isalpha : Check whether all characters are alphabetic.\nSeries.str.isnumeric : Check whether all characters are numeric.\nSeries.str.isalnum : Check whether all characters are alphanumeric.\nSeries.str.isdigit : Check whether all characters are digits.\nSeries.str.isdecimal : Check whether all characters are decimal.\nSeries.str.isspace : Check whether all characters are whitespace.\nSeries.str.islower : Check whether all characters are lowercase.\nSeries.str.isupper : Check whether all characters are uppercase.\nSeries.str.istitle : Check whether all characters are titlecase.\n\nExamples\n--------\n**Checks for Alphabetic and Numeric Characters**\n\n>>> s1 = pd.Series(['one', 'one1', '1', ''])\n\n>>> s1.str.isalpha()\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n>>> s1.str.isnumeric()\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\n>>> s1.str.isalnum()\n0     True\n1     True\n2     True\n3    False\ndtype: bool\n\nNote that checks against characters mixed with any additional punctuation\nor whitespace will evaluate to false for an alphanumeric check.\n\n>>> s2 = pd.Series(['A B', '1.5', '3,000'])\n>>> s2.str.isalnum()\n0    False\n1    False\n2    False\ndtype: bool\n\n**More Detailed Checks for Numeric Characters**\n\nThere are several different but overlapping sets of numeric characters that\ncan be checked for.\n\n>>> s3 = pd.Series(['23', '\u00b3', '\u2155', ''])\n\nThe ``s3.str.isdecimal`` method checks for characters used to form numbers\nin base 10.\n\n>>> s3.str.isdecimal()\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\nThe ``s.str.isdigit`` method is the same as ``s3.str.isdecimal`` but also\nincludes special digits, like superscripted and subscripted digits in\nunicode.\n\n>>> s3.str.isdigit()\n0     True\n1     True\n2    False\n3    False\ndtype: bool\n\nThe ``s.str.isnumeric`` method is the same as ``s3.str.isdigit`` but also\nincludes other characters that can represent quantities such as unicode\nfractions.\n\n>>> s3.str.isnumeric()\n0     True\n1     True\n2     True\n3    False\ndtype: bool\n\n**Checks for Whitespace**\n\n>>> s4 = pd.Series([' ', '\\t\\r\\n ', ''])\n>>> s4.str.isspace()\n0     True\n1     True\n2    False\ndtype: bool\n\n**Checks for Character Case**\n\n>>> s5 = pd.Series(['leopard', 'Golden Eagle', 'SNAKE', ''])\n\n>>> s5.str.islower()\n0     True\n1    False\n2    False\n3    False\ndtype: bool\n\n>>> s5.str.isupper()\n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\nThe ``s5.str.istitle`` method checks for whether all words are in title\ncase (whether only the first letter of each word is capitalized). Words are\nassumed to be as any sequence of non-numeric characters separated by\nwhitespace characters.\n\n>>> s5.str.istitle()\n0    False\n1     True\n2    False\n3    False\ndtype: bool", "Library": "Pandas"}
{"API_Name": "pandas.Series.str.split", "Docstring": "Split strings around given separator/delimiter.\n\nSplits the string in the Series/Index from the beginning,\nat the specified delimiter string.\n\nParameters\n----------\npat : str or compiled regex, optional\n    String or regular expression to split on.\n    If not specified, split on whitespace.\nn : int, default -1 (all)\n    Limit number of splits in output.\n    ``None``, 0 and -1 will be interpreted as return all splits.\nexpand : bool, default False\n    Expand the split strings into separate columns.\n\n    - If ``True``, return DataFrame/MultiIndex expanding dimensionality.\n    - If ``False``, return Series/Index, containing lists of strings.\n\nregex : bool, default None\n    Determines if the passed-in pattern is a regular expression:\n\n    - If ``True``, assumes the passed-in pattern is a regular expression\n    - If ``False``, treats the pattern as a literal string.\n    - If ``None`` and `pat` length is 1, treats `pat` as a literal string.\n    - If ``None`` and `pat` length is not 1, treats `pat` as a regular expression.\n    - Cannot be set to False if `pat` is a compiled regex\n\n    .. versionadded:: 1.4.0\n\nReturns\n-------\nSeries, Index, DataFrame or MultiIndex\n    Type matches caller unless ``expand=True`` (see Notes).\n\n                  Raises\n                  ------\n                  ValueError\n                      * if `regex` is False and `pat` is a compiled regex\n\nSee Also\n--------\nSeries.str.split : Split strings around given separator/delimiter.\nSeries.str.rsplit : Splits string around given separator/delimiter,\n    starting from the right.\nSeries.str.join : Join lists contained as elements in the Series/Index\n    with passed delimiter.\nstr.split : Standard library version for split.\nstr.rsplit : Standard library version for rsplit.\n\nNotes\n-----\nThe handling of the `n` keyword depends on the number of found splits:\n\n- If found splits > `n`,  make first `n` splits only\n- If found splits <= `n`, make all splits\n- If for a certain row the number of found splits < `n`,\n  append `None` for padding up to `n` if ``expand=True``\n\nIf using ``expand=True``, Series and Index callers return DataFrame and\nMultiIndex objects, respectively.\n\nUse of `regex =False` with a `pat` as a compiled regex will raise an error.\n\nExamples\n--------\n>>> s = pd.Series(\n...     [\n...         \"this is a regular sentence\",\n...         \"https://docs.python.org/3/tutorial/index.html\",\n...         np.nan\n...     ]\n... )\n>>> s\n0                       this is a regular sentence\n1    https://docs.python.org/3/tutorial/index.html\n2                                              NaN\ndtype: object\n\nIn the default setting, the string is split by whitespace.\n\n>>> s.str.split()\n0                   [this, is, a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object\n\nWithout the `n` parameter, the outputs of `rsplit` and `split`\nare identical.\n\n>>> s.str.rsplit()\n0                   [this, is, a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object\n\nThe `n` parameter can be used to limit the number of splits on the\ndelimiter. The outputs of `split` and `rsplit` are different.\n\n>>> s.str.split(n=2)\n0                     [this, is, a regular sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object\n\n>>> s.str.rsplit(n=2)\n0                     [this is a, regular, sentence]\n1    [https://docs.python.org/3/tutorial/index.html]\n2                                                NaN\ndtype: object\n\nThe `pat` parameter can be used to split by other characters.\n\n>>> s.str.split(pat=\"/\")\n0                         [this is a regular sentence]\n1    [https:, , docs.python.org, 3, tutorial, index...\n2                                                  NaN\ndtype: object\n\nWhen using ``expand=True``, the split elements will expand out into\nseparate columns. If NaN is present, it is propagated throughout\nthe columns during the split.\n\n>>> s.str.split(expand=True)\n                                               0     1     2        3         4\n0                                           this    is     a  regular  sentence\n1  https://docs.python.org/3/tutorial/index.html  None  None     None      None\n2                                            NaN   NaN   NaN      NaN       NaN\n\nFor slightly more complex use cases like splitting the html document name\nfrom a url, a combination of parameter settings can be used.\n\n>>> s.str.rsplit(\"/\", n=1, expand=True)\n                                    0           1\n0          this is a regular sentence        None\n1  https://docs.python.org/3/tutorial  index.html\n2                                 NaN         NaN\n\nRemember to escape special characters when explicitly using regular expressions.\n\n>>> s = pd.Series([\"foo and bar plus baz\"])\n>>> s.str.split(r\"and|plus\", expand=True)\n    0   1   2\n0 foo bar baz\n\nRegular expressions can be used to handle urls or file names.\nWhen `pat` is a string and ``regex=None`` (the default), the given `pat` is compiled\nas a regex only if ``len(pat) != 1``.\n\n>>> s = pd.Series(['foojpgbar.jpg'])\n>>> s.str.split(r\".\", expand=True)\n           0    1\n0  foojpgbar  jpg\n\n>>> s.str.split(r\"\\.jpg\", expand=True)\n           0 1\n0  foojpgbar\n\nWhen ``regex=True``, `pat` is interpreted as a regex\n\n>>> s.str.split(r\"\\.jpg\", regex=True, expand=True)\n           0 1\n0  foojpgbar\n\nA compiled regex can be passed as `pat`\n\n>>> import re\n>>> s.str.split(re.compile(r\"\\.jpg\"), expand=True)\n           0 1\n0  foojpgbar\n\nWhen ``regex=False``, `pat` is interpreted as the string itself\n\n>>> s.str.split(r\"\\.jpg\", regex=False, expand=True)\n               0\n0  foojpgbar.jpg", "Library": "Pandas"}
{"API_Name": "pandas.Series.to_frame", "Docstring": "Convert Series to DataFrame.\n\nParameters\n----------\nname : object, optional\n    The passed name should substitute for the series name (if it has\n    one).\n\nReturns\n-------\nDataFrame\n    DataFrame representation of Series.\n\nExamples\n--------\n>>> s = pd.Series([\"a\", \"b\", \"c\"],\n...               name=\"vals\")\n>>> s.to_frame()\n  vals\n0    a\n1    b\n2    c", "Library": "Pandas"}
{"API_Name": "pandas.Series.tolist", "Docstring": "Return a list of the values.\n\nThese are each a scalar type, which is a Python scalar\n(for str, int, float) or a pandas scalar\n(for Timestamp/Timedelta/Interval/Period)\n\nReturns\n-------\nlist\n\nSee Also\n--------\nnumpy.ndarray.tolist : Return the array as an a.ndim-levels deep\n    nested list of Python scalars.", "Library": "Pandas"}
{"API_Name": "pandas.Series.value_counts", "Docstring": "Return a Series containing counts of unique values.\n\nThe resulting object will be in descending order so that the\nfirst element is the most frequently-occurring element.\nExcludes NA values by default.\n\nParameters\n----------\nnormalize : bool, default False\n    If True then the object returned will contain the relative\n    frequencies of the unique values.\nsort : bool, default True\n    Sort by frequencies.\nascending : bool, default False\n    Sort in ascending order.\nbins : int, optional\n    Rather than count values, group them into half-open bins,\n    a convenience for ``pd.cut``, only works with numeric data.\ndropna : bool, default True\n    Don't include counts of NaN.\n\nReturns\n-------\nSeries\n\nSee Also\n--------\nSeries.count: Number of non-NA elements in a Series.\nDataFrame.count: Number of non-NA elements in a DataFrame.\nDataFrame.value_counts: Equivalent method on DataFrames.\n\nExamples\n--------\n>>> index = pd.Index([3, 1, 2, 3, 4, np.nan])\n>>> index.value_counts()\n3.0    2\n1.0    1\n2.0    1\n4.0    1\ndtype: int64\n\nWith `normalize` set to `True`, returns the relative frequency by\ndividing all values by the sum of values.\n\n>>> s = pd.Series([3, 1, 2, 3, 4, np.nan])\n>>> s.value_counts(normalize=True)\n3.0    0.4\n1.0    0.2\n2.0    0.2\n4.0    0.2\ndtype: float64\n\n**bins**\n\nBins can be useful for going from a continuous variable to a\ncategorical variable; instead of counting unique\napparitions of values, divide the index in the specified\nnumber of half-open bins.\n\n>>> s.value_counts(bins=3)\n(0.996, 2.0]    2\n(2.0, 3.0]      2\n(3.0, 4.0]      1\ndtype: int64\n\n**dropna**\n\nWith `dropna` set to `False` we can also see NaN index values.\n\n>>> s.value_counts(dropna=False)\n3.0    2\n1.0    1\n2.0    1\n4.0    1\nNaN    1\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.Series.values", "Docstring": "Return Series as ndarray or ndarray-like depending on the dtype.\n\n.. warning::\n\n   We recommend using :attr:`Series.array` or\n   :meth:`Series.to_numpy`, depending on whether you need\n   a reference to the underlying data or a NumPy array.\n\nReturns\n-------\nnumpy.ndarray or ndarray-like\n\nSee Also\n--------\nSeries.array : Reference to the underlying data.\nSeries.to_numpy : A NumPy array representing the underlying data.\n\nExamples\n--------\n>>> pd.Series([1, 2, 3]).values\narray([1, 2, 3])\n\n>>> pd.Series(list('aabc')).values\narray(['a', 'a', 'b', 'c'], dtype=object)\n\n>>> pd.Series(list('aabc')).astype('category').values\n['a', 'a', 'b', 'c']\nCategories (3, object): ['a', 'b', 'c']\n\nTimezone aware datetime data is converted to UTC:\n\n>>> pd.Series(pd.date_range('20130101', periods=3,\n...                         tz='US/Eastern')).values\narray(['2013-01-01T05:00:00.000000000',\n       '2013-01-02T05:00:00.000000000',\n       '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.DataFrameGroupBy.agg", "Docstring": "Aggregate using one or more operations over the specified axis.\n\nParameters\n----------\nfunc : function, str, list or dict\n    Function to use for aggregating the data. If a function, must either\n    work when passed a DataFrame or when passed to DataFrame.apply.\n\n    Accepted combinations are:\n\n    - function\n    - string function name\n    - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n    - dict of axis labels -> functions, function names or list of such.\n\n    Can also accept a Numba JIT function with\n    ``engine='numba'`` specified. Only passing a single function is supported\n    with this engine.\n\n    If the ``'numba'`` engine is chosen, the function must be\n    a user defined function with ``values`` and ``index`` as the\n    first and second arguments respectively in the function signature.\n    Each group's index will be passed to the user defined function\n    and optionally available for use.\n\n    .. versionchanged:: 1.1.0\n*args\n    Positional arguments to pass to func.\nengine : str, default None\n    * ``'cython'`` : Runs the function through C-extensions from cython.\n    * ``'numba'`` : Runs the function through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or globally setting ``compute.use_numba``\n\n    .. versionadded:: 1.1.0\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be\n      applied to the function\n\n    .. versionadded:: 1.1.0\n**kwargs\n    Keyword arguments to be passed into func.\n\nReturns\n-------\nDataFrame\n\nSee Also\n--------\nDataFrame.groupby.apply : Apply function func group-wise\n    and combine the results together.\nDataFrame.groupby.transform : Aggregate using one or more\n    operations over the specified axis.\nDataFrame.aggregate : Transforms the Series on each group\n    based on the given function.\n\nNotes\n-----\nWhen using ``engine='numba'``, there will be no \"fall back\" behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\nfor more details.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    see the examples below.\n\nExamples\n--------\n>>> df = pd.DataFrame(\n...     {\n...         \"A\": [1, 1, 2, 2],\n...         \"B\": [1, 2, 3, 4],\n...         \"C\": [0.362838, 0.227877, 1.267767, -0.562860],\n...     }\n... )\n\n>>> df\n   A  B         C\n0  1  1  0.362838\n1  1  2  0.227877\n2  2  3  1.267767\n3  2  4 -0.562860\n\nThe aggregation is for each column.\n\n>>> df.groupby('A').agg('min')\n   B         C\nA\n1  1  0.227877\n2  3 -0.562860\n\nMultiple aggregations\n\n>>> df.groupby('A').agg(['min', 'max'])\n    B             C\n  min max       min       max\nA\n1   1   2  0.227877  0.362838\n2   3   4 -0.562860  1.267767\n\nSelect a column for aggregation\n\n>>> df.groupby('A').B.agg(['min', 'max'])\n   min  max\nA\n1    1    2\n2    3    4\n\nUser-defined function for aggregation\n\n>>> df.groupby('A').agg(lambda x: sum(x) + 2)\n    B          C\nA\n1       5       2.590715\n2       9       2.704907\n\nDifferent aggregations per column\n\n>>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n    B             C\n  min max       sum\nA\n1   1   2  0.590715\n2   3   4  0.704907\n\nTo control the output names with different aggregations per column,\npandas supports \"named aggregation\"\n\n>>> df.groupby(\"A\").agg(\n...     b_min=pd.NamedAgg(column=\"B\", aggfunc=\"min\"),\n...     c_sum=pd.NamedAgg(column=\"C\", aggfunc=\"sum\"))\n   b_min     c_sum\nA\n1      1  0.590715\n2      3  0.704907\n\n- The keywords are the *output* column names\n- The values are tuples whose first element is the column to select\n  and the second element is the aggregation to apply to that column.\n  Pandas provides the ``pandas.NamedAgg`` namedtuple with the fields\n  ``['column', 'aggfunc']`` to make it clearer what the arguments are.\n  As usual, the aggregation can be a callable or a string alias.\n\nSee :ref:`groupby.aggregate.named` for more.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the aggregating function.\n\n>>> df.groupby(\"A\")[[\"B\"]].agg(lambda x: x.astype(float).min())\n      B\nA\n1   1.0\n2   3.0", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.DataFrameGroupBy.apply", "Docstring": "Apply function ``func`` group-wise and combine the results together.\n\nThe function passed to ``apply`` must take a dataframe as its first\nargument and return a DataFrame, Series or scalar. ``apply`` will\nthen take care of combining the results back together into a single\ndataframe or series. ``apply`` is therefore a highly flexible\ngrouping method.\n\nWhile ``apply`` is a very flexible method, its downside is that\nusing it can be quite a bit slower than using more specific methods\nlike ``agg`` or ``transform``. Pandas offers a wide range of method that will\nbe much faster than using ``apply`` for their specific purposes, so try to\nuse them before reaching for ``apply``.\n\nParameters\n----------\nfunc : callable\n    A callable that takes a dataframe as its first argument, and\n    returns a dataframe, a series or a scalar. In addition the\n    callable may take positional and keyword arguments.\nargs, kwargs : tuple and dict\n    Optional positional and keyword arguments to pass to ``func``.\n\nReturns\n-------\napplied : Series or DataFrame\n\nSee Also\n--------\npipe : Apply function to the full GroupBy object instead of to each\n    group.\naggregate : Apply aggregate function to the GroupBy object.\ntransform : Apply function column-by-column to the GroupBy object.\nSeries.apply : Apply a function to a Series.\nDataFrame.apply : Apply a function to each row or column of a DataFrame.\n\nNotes\n-----\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    see the examples below.\n\nFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\nfor more details.\n\nExamples\n--------\n\n>>> df = pd.DataFrame({'A': 'a a b'.split(),\n...                    'B': [1,2,3],\n...                    'C': [4,6,5]})\n>>> g1 = df.groupby('A', group_keys=False)\n>>> g2 = df.groupby('A', group_keys=True)\n\nNotice that ``g1`` have ``g2`` have two groups, ``a`` and ``b``, and only\ndiffer in their ``group_keys`` argument. Calling `apply` in various ways,\nwe can get different grouping results:\n\nExample 1: below the function passed to `apply` takes a DataFrame as\nits argument and returns a DataFrame. `apply` combines the result for\neach group together into a new DataFrame:\n\n>>> g1[['B', 'C']].apply(lambda x: x / x.sum())\n          B    C\n0  0.333333  0.4\n1  0.666667  0.6\n2  1.000000  1.0\n\nIn the above, the groups are not part of the index. We can have them included\nby using ``g2`` where ``group_keys=True``:\n\n>>> g2[['B', 'C']].apply(lambda x: x / x.sum())\n            B    C\nA\na 0  0.333333  0.4\n  1  0.666667  0.6\nb 2  1.000000  1.0\n\nExample 2: The function passed to `apply` takes a DataFrame as\nits argument and returns a Series.  `apply` combines the result for\neach group together into a new DataFrame.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``.\n\n>>> g1[['B', 'C']].apply(lambda x: x.astype(float).max() - x.min())\n     B    C\nA\na  1.0  2.0\nb  0.0  0.0\n\n>>> g2[['B', 'C']].apply(lambda x: x.astype(float).max() - x.min())\n     B    C\nA\na  1.0  2.0\nb  0.0  0.0\n\nThe ``group_keys`` argument has no effect here because the result is not\nlike-indexed (i.e. :ref:`a transform <groupby.transform>`) when compared\nto the input.\n\nExample 3: The function passed to `apply` takes a DataFrame as\nits argument and returns a scalar. `apply` combines the result for\neach group together into a Series, including setting the index as\nappropriate:\n\n>>> g1.apply(lambda x: x.C.max() - x.B.min())\nA\na    5\nb    2\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.generic.DataFrameGroupBy.agg", "Docstring": "Aggregate using one or more operations over the specified axis.\n\nParameters\n----------\nfunc : function, str, list or dict\n    Function to use for aggregating the data. If a function, must either\n    work when passed a DataFrame or when passed to DataFrame.apply.\n\n    Accepted combinations are:\n\n    - function\n    - string function name\n    - list of functions and/or function names, e.g. ``[np.sum, 'mean']``\n    - dict of axis labels -> functions, function names or list of such.\n\n    Can also accept a Numba JIT function with\n    ``engine='numba'`` specified. Only passing a single function is supported\n    with this engine.\n\n    If the ``'numba'`` engine is chosen, the function must be\n    a user defined function with ``values`` and ``index`` as the\n    first and second arguments respectively in the function signature.\n    Each group's index will be passed to the user defined function\n    and optionally available for use.\n\n    .. versionchanged:: 1.1.0\n*args\n    Positional arguments to pass to func.\nengine : str, default None\n    * ``'cython'`` : Runs the function through C-extensions from cython.\n    * ``'numba'`` : Runs the function through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or globally setting ``compute.use_numba``\n\n    .. versionadded:: 1.1.0\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be\n      applied to the function\n\n    .. versionadded:: 1.1.0\n**kwargs\n    Keyword arguments to be passed into func.\n\nReturns\n-------\nDataFrame\n\nSee Also\n--------\nDataFrame.groupby.apply : Apply function func group-wise\n    and combine the results together.\nDataFrame.groupby.transform : Aggregate using one or more\n    operations over the specified axis.\nDataFrame.aggregate : Transforms the Series on each group\n    based on the given function.\n\nNotes\n-----\nWhen using ``engine='numba'``, there will be no \"fall back\" behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\nfor more details.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    see the examples below.\n\nExamples\n--------\n>>> df = pd.DataFrame(\n...     {\n...         \"A\": [1, 1, 2, 2],\n...         \"B\": [1, 2, 3, 4],\n...         \"C\": [0.362838, 0.227877, 1.267767, -0.562860],\n...     }\n... )\n\n>>> df\n   A  B         C\n0  1  1  0.362838\n1  1  2  0.227877\n2  2  3  1.267767\n3  2  4 -0.562860\n\nThe aggregation is for each column.\n\n>>> df.groupby('A').agg('min')\n   B         C\nA\n1  1  0.227877\n2  3 -0.562860\n\nMultiple aggregations\n\n>>> df.groupby('A').agg(['min', 'max'])\n    B             C\n  min max       min       max\nA\n1   1   2  0.227877  0.362838\n2   3   4 -0.562860  1.267767\n\nSelect a column for aggregation\n\n>>> df.groupby('A').B.agg(['min', 'max'])\n   min  max\nA\n1    1    2\n2    3    4\n\nUser-defined function for aggregation\n\n>>> df.groupby('A').agg(lambda x: sum(x) + 2)\n    B          C\nA\n1       5       2.590715\n2       9       2.704907\n\nDifferent aggregations per column\n\n>>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'})\n    B             C\n  min max       sum\nA\n1   1   2  0.590715\n2   3   4  0.704907\n\nTo control the output names with different aggregations per column,\npandas supports \"named aggregation\"\n\n>>> df.groupby(\"A\").agg(\n...     b_min=pd.NamedAgg(column=\"B\", aggfunc=\"min\"),\n...     c_sum=pd.NamedAgg(column=\"C\", aggfunc=\"sum\"))\n   b_min     c_sum\nA\n1      1  0.590715\n2      3  0.704907\n\n- The keywords are the *output* column names\n- The values are tuples whose first element is the column to select\n  and the second element is the aggregation to apply to that column.\n  Pandas provides the ``pandas.NamedAgg`` namedtuple with the fields\n  ``['column', 'aggfunc']`` to make it clearer what the arguments are.\n  As usual, the aggregation can be a callable or a string alias.\n\nSee :ref:`groupby.aggregate.named` for more.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the aggregating function.\n\n>>> df.groupby(\"A\")[[\"B\"]].agg(lambda x: x.astype(float).min())\n      B\nA\n1   1.0\n2   3.0", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.generic.DataFrameGroupBy.mean", "Docstring": "Compute mean of groups, excluding missing values.\n\nParameters\n----------\nnumeric_only : bool, default True\n    Include only float, int, boolean columns. If None, will attempt to use\n    everything, then use only numeric data.\n\nengine : str, default None\n    * ``'cython'`` : Runs the operation through C-extensions from cython.\n    * ``'numba'`` : Runs the operation through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or globally setting\n      ``compute.use_numba``\n\n    .. versionadded:: 1.4.0\n\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{{'nopython': True, 'nogil': False, 'parallel': False}}``\n\n    .. versionadded:: 1.4.0\n\nReturns\n-------\npandas.Series or pandas.DataFrame\n\nSee Also\n--------\nSeries.groupby : Apply a function groupby to a Series.\nDataFrame.groupby : Apply a function groupby\n    to each row or column of a DataFrame.\n\nExamples\n--------\n>>> df = pd.DataFrame({'A': [1, 1, 2, 1, 2],\n...                    'B': [np.nan, 2, 3, 4, 5],\n...                    'C': [1, 2, 1, 1, 2]}, columns=['A', 'B', 'C'])\n\nGroupby one column and return the mean of the remaining columns in\neach group.\n\n>>> df.groupby('A').mean()\n     B         C\nA\n1  3.0  1.333333\n2  4.0  1.500000\n\nGroupby two columns and return the mean of the remaining column.\n\n>>> df.groupby(['A', 'B']).mean()\n         C\nA B\n1 2.0  2.0\n  4.0  1.0\n2 3.0  1.0\n  5.0  2.0\n\nGroupby one column and return the mean of only particular column in\nthe group.\n\n>>> df.groupby('A')['B'].mean()\nA\n1    3.0\n2    4.0\nName: B, dtype: float64", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.generic.DataFrameGroupBy.transform", "Docstring": "Call function producing a same-indexed DataFrame on each group.\n\nReturns a DataFrame having the same indexes as the original object\nfilled with the transformed values.\n\nParameters\n----------\nf : function\n    Function to apply to each group. See the Notes section below for requirements.\n\n    Can also accept a Numba JIT function with\n    ``engine='numba'`` specified.\n\n    If the ``'numba'`` engine is chosen, the function must be\n    a user defined function with ``values`` and ``index`` as the\n    first and second arguments respectively in the function signature.\n    Each group's index will be passed to the user defined function\n    and optionally available for use.\n\n    .. versionchanged:: 1.1.0\n*args\n    Positional arguments to pass to func.\nengine : str, default None\n    * ``'cython'`` : Runs the function through C-extensions from cython.\n    * ``'numba'`` : Runs the function through JIT compiled code from numba.\n    * ``None`` : Defaults to ``'cython'`` or the global setting ``compute.use_numba``\n\n    .. versionadded:: 1.1.0\nengine_kwargs : dict, default None\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n      and ``parallel`` dictionary keys. The values must either be ``True`` or\n      ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n      ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be\n      applied to the function\n\n    .. versionadded:: 1.1.0\n**kwargs\n    Keyword arguments to be passed into func.\n\nReturns\n-------\nDataFrame\n\nSee Also\n--------\nDataFrame.groupby.apply : Apply function ``func`` group-wise and combine\n    the results together.\nDataFrame.groupby.aggregate : Aggregate using one or more\n    operations over the specified axis.\nDataFrame.transform : Call ``func`` on self producing a DataFrame with the\n    same axis shape as self.\n\nNotes\n-----\nEach group is endowed the attribute 'name' in case you need to know\nwhich group you are working on.\n\nThe current implementation imposes three requirements on f:\n\n* f must return a value that either has the same shape as the input\n  subframe or can be broadcast to the shape of the input subframe.\n  For example, if `f` returns a scalar it will be broadcast to have the\n  same shape as the input subframe.\n* if this is a DataFrame, f must support application column-by-column\n  in the subframe. If f also supports application to the entire subframe,\n  then a fast path is used starting from the second chunk.\n* f must not mutate groups. Mutation is not supported and may\n  produce unexpected results. See :ref:`gotchas.udf-mutation` for more details.\n\nWhen using ``engine='numba'``, there will be no \"fall back\" behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    see the examples below.\n\n.. deprecated:: 1.5.0\n\n    When using ``.transform`` on a grouped DataFrame and the transformation function\n    returns a DataFrame, currently pandas does not align the result's index\n    with the input's index. This behavior is deprecated and alignment will\n    be performed in a future version of pandas. You can apply ``.to_numpy()`` to the\n    result of the transformation function to avoid alignment.\n\nExamples\n--------\n\n>>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n...                           'foo', 'bar'],\n...                    'B' : ['one', 'one', 'two', 'three',\n...                           'two', 'two'],\n...                    'C' : [1, 5, 5, 2, 5, 5],\n...                    'D' : [2.0, 5., 8., 1., 2., 9.]})\n>>> grouped = df.groupby('A')[['C', 'D']]\n>>> grouped.transform(lambda x: (x - x.mean()) / x.std())\n          C         D\n0 -1.154701 -0.577350\n1  0.577350  0.000000\n2  0.577350  1.154701\n3 -1.154701 -1.000000\n4  0.577350 -0.577350\n5  0.577350  1.000000\n\nBroadcast result of the transformation\n\n>>> grouped.transform(lambda x: x.max() - x.min())\n     C    D\n0  4.0  6.0\n1  3.0  8.0\n2  4.0  6.0\n3  3.0  8.0\n4  4.0  6.0\n5  3.0  8.0\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    for example:\n\n>>> grouped.transform(lambda x: x.astype(int).max())\n   C  D\n0  5  8\n1  5  9\n2  5  8\n3  5  9\n4  5  8\n5  5  9", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.GroupBy.agg", "Docstring": "Aggregate using one or more operations over the specified axis. Parameters ---------- func : function, str, list, dict or None Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are: - function - string function name - list of functions and/or function names, e.g. ``[np.sum, 'mean']`` - dict of axis labels -> functions, function names or list of such. - None, in which case ``**kwargs`` are used with Named Aggregation. Here the output has one column for each element in ``**kwargs``. The name of the column is keyword, whereas the value determines the aggregation used to compute the values in the column. Can also accept a Numba JIT function with ``engine='numba'`` specified. Only passing a single function is supported with this engine. If the ``'numba'`` engine is chosen, the function must be a user defined function with ``values`` and ``index`` as the first and second arguments respectively in the function signature. Each group's index will be passed to the user defined function and optionally available for use. *args Positional arguments to pass to func. engine : str, default None * ``'cython'`` : Runs the function through C-extensions from cython. * ``'numba'`` : Runs the function through JIT compiled code from numba. * ``None`` : Defaults to ``'cython'`` or globally setting ``compute.use_numba`` engine_kwargs : dict, default None * For ``'cython'`` engine, there are no accepted ``engine_kwargs`` * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil`` and ``parallel`` dictionary keys. The values must either be ``True`` or ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be applied to the function **kwargs * If ``func`` is None, ``**kwargs`` are used to define the output names and aggregations via Named Aggregation. See ``func`` entry. * Otherwise, keyword arguments to be passed into func. Returns ------- DataFrame See Also -------- DataFrame.groupby.apply : Apply function func group-wise and combine the results together. DataFrame.groupby.transform : Transforms the Series on each group based on the given function. DataFrame.aggregate : Aggregate using one or more operations over the specified axis. Notes ----- When using ``engine='numba'``, there will be no \"fall back\" behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. Functions that mutate the passed object can produce unexpected behavior or errors and are not supported. See :ref:`gotchas.udf-mutation` for more details. .. versionchanged:: 1.3.0 The resulting dtype will reflect the return value of the passed ``func``, see the examples below. Examples -------- >>> df = pd.DataFrame( ...     { ...         \"A\": [1, 1, 2, 2], ...         \"B\": [1, 2, 3, 4], ...         \"C\": [0.362838, 0.227877, 1.267767, -0.562860], ...     } ... ) >>> df A  B         C 0  1  1  0.362838 1  1  2  0.227877 2  2  3  1.267767 3  2  4 -0.562860 The aggregation is for each column. >>> df.groupby('A').agg('min') B         C A 1  1  0.227877 2  3 -0.562860 Multiple aggregations >>> df.groupby('A').agg(['min', 'max']) B             C min max       min       max A 1   1   2  0.227877  0.362838 2   3   4 -0.562860  1.267767 Select a column for aggregation >>> df.groupby('A').B.agg(['min', 'max']) min  max A 1    1    2 2    3    4 User-defined function for aggregation >>> df.groupby('A').agg(lambda x: sum(x) + 2) B\t       C A 1\t5\t2.590715 2\t9\t2.704907 Different aggregations per column >>> df.groupby('A').agg({'B': ['min', 'max'], 'C': 'sum'}) B             C min max       sum A 1   1   2  0.590715 2   3   4  0.704907 To control the output names with different aggregations per column, pandas supports \"named aggregation\" >>> df.groupby(\"A\").agg( ...     b_min=pd.NamedAgg(column=\"B\", aggfunc=\"min\"), ...     c_sum=pd.NamedAgg(column=\"C\", aggfunc=\"sum\")) b_min     c_sum A 1      1  0.590715 2      3  0.704907 - The keywords are the *output* column names - The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. Pandas provides the ``pandas.NamedAgg`` namedtuple with the fields ``['column', 'aggfunc']`` to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias. See :ref:`groupby.aggregate.named` for more. .. versionchanged:: 1.3.0 The resulting dtype will reflect the return value of the aggregating function. >>> df.groupby(\"A\")[[\"B\"]].agg(lambda x: x.astype(float).min()) B A 1   1.0 2   3.0", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.GroupBy.apply", "Docstring": "Apply function ``func`` group-wise and combine the results together.\n\nThe function passed to ``apply`` must take a dataframe as its first\nargument and return a DataFrame, Series or scalar. ``apply`` will\nthen take care of combining the results back together into a single\ndataframe or series. ``apply`` is therefore a highly flexible\ngrouping method.\n\nWhile ``apply`` is a very flexible method, its downside is that\nusing it can be quite a bit slower than using more specific methods\nlike ``agg`` or ``transform``. Pandas offers a wide range of method that will\nbe much faster than using ``apply`` for their specific purposes, so try to\nuse them before reaching for ``apply``.\n\nParameters\n----------\nfunc : callable\n    A callable that takes a dataframe as its first argument, and\n    returns a dataframe, a series or a scalar. In addition the\n    callable may take positional and keyword arguments.\nargs, kwargs : tuple and dict\n    Optional positional and keyword arguments to pass to ``func``.\n\nReturns\n-------\napplied : Series or DataFrame\n\nSee Also\n--------\npipe : Apply function to the full GroupBy object instead of to each\n    group.\naggregate : Apply aggregate function to the GroupBy object.\ntransform : Apply function column-by-column to the GroupBy object.\nSeries.apply : Apply a function to a Series.\nDataFrame.apply : Apply a function to each row or column of a DataFrame.\n\nNotes\n-----\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``,\n    see the examples below.\n\nFunctions that mutate the passed object can produce unexpected\nbehavior or errors and are not supported. See :ref:`gotchas.udf-mutation`\nfor more details.\n\nExamples\n--------\n\n>>> df = pd.DataFrame({'A': 'a a b'.split(),\n...                    'B': [1,2,3],\n...                    'C': [4,6,5]})\n>>> g1 = df.groupby('A', group_keys=False)\n>>> g2 = df.groupby('A', group_keys=True)\n\nNotice that ``g1`` have ``g2`` have two groups, ``a`` and ``b``, and only\ndiffer in their ``group_keys`` argument. Calling `apply` in various ways,\nwe can get different grouping results:\n\nExample 1: below the function passed to `apply` takes a DataFrame as\nits argument and returns a DataFrame. `apply` combines the result for\neach group together into a new DataFrame:\n\n>>> g1[['B', 'C']].apply(lambda x: x / x.sum())\n          B    C\n0  0.333333  0.4\n1  0.666667  0.6\n2  1.000000  1.0\n\nIn the above, the groups are not part of the index. We can have them included\nby using ``g2`` where ``group_keys=True``:\n\n>>> g2[['B', 'C']].apply(lambda x: x / x.sum())\n            B    C\nA\na 0  0.333333  0.4\n  1  0.666667  0.6\nb 2  1.000000  1.0\n\nExample 2: The function passed to `apply` takes a DataFrame as\nits argument and returns a Series.  `apply` combines the result for\neach group together into a new DataFrame.\n\n.. versionchanged:: 1.3.0\n\n    The resulting dtype will reflect the return value of the passed ``func``.\n\n>>> g1[['B', 'C']].apply(lambda x: x.astype(float).max() - x.min())\n     B    C\nA\na  1.0  2.0\nb  0.0  0.0\n\n>>> g2[['B', 'C']].apply(lambda x: x.astype(float).max() - x.min())\n     B    C\nA\na  1.0  2.0\nb  0.0  0.0\n\nThe ``group_keys`` argument has no effect here because the result is not\nlike-indexed (i.e. :ref:`a transform <groupby.transform>`) when compared\nto the input.\n\nExample 3: The function passed to `apply` takes a DataFrame as\nits argument and returns a scalar. `apply` combines the result for\neach group together into a Series, including setting the index as\nappropriate:\n\n>>> g1.apply(lambda x: x.C.max() - x.B.min())\nA\na    5\nb    2\ndtype: int64", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.GroupBy.cumsum", "Docstring": "Cumulative sum for each group.\n\nReturns\n-------\nSeries or DataFrame\n\nSee Also\n--------\nSeries.groupby : Apply a function groupby to a Series.\nDataFrame.groupby : Apply a function groupby\n    to each row or column of a DataFrame.", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.GroupBy.idxmin", "Docstring": "Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Parameters ---------- axis : {{0 or 'index', 1 or 'columns'}}, default None The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. If axis is not provided, grouper's axis is used. .. versionchanged:: 2.0.0 .. deprecated:: 2.1.0 For axis=1, operate on the underlying object instead. Otherwise the axis keyword is not necessary. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. numeric_only : bool, default False Include only `float`, `int` or `boolean` data. .. versionadded:: 1.5.0 Returns ------- Series Indexes of minima along the specified axis. Raises ------ ValueError * If the row/column is empty See Also -------- Series.idxmin : Return index of the minimum element. Notes ----- This method is the DataFrame version of ``ndarray.argmin``. Examples -------- Consider a dataset containing food consumption in Argentina. >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ...                    'co2_emissions': [37.2, 19.66, 1712]}, ...                    index=['Pork', 'Wheat Products', 'Beef']) >>> df consumption  co2_emissions Pork                  10.51         37.20 Wheat Products       103.11         19.66 Beef                  55.48       1712.00 By default, it returns the index for the minimum value in each column. >>> df.idxmin() consumption                Pork co2_emissions    Wheat Products dtype: object To return the index for the minimum value in each row, use ``axis=\"columns\"``. >>> df.idxmin(axis=\"columns\") Pork                consumption Wheat Products    co2_emissions Beef                consumption dtype: object", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.GroupBy.rank", "Docstring": "Provide the rank of values within each group.\n\nParameters\n----------\nmethod : {'average', 'min', 'max', 'first', 'dense'}, default 'average'\n    * average: average rank of group.\n    * min: lowest rank in group.\n    * max: highest rank in group.\n    * first: ranks assigned in order they appear in the array.\n    * dense: like 'min', but rank always increases by 1 between groups.\nascending : bool, default True\n    False for ranks by high (1) to low (N).\nna_option : {'keep', 'top', 'bottom'}, default 'keep'\n    * keep: leave NA values where they are.\n    * top: smallest rank if ascending.\n    * bottom: smallest rank if descending.\npct : bool, default False\n    Compute percentage rank of data within each group.\naxis : int, default 0\n    The axis of the object over which to compute the rank.\n\nReturns\n-------\nDataFrame with ranking of values within each group\n\nSee Also\n--------\nSeries.groupby : Apply a function groupby to a Series.\nDataFrame.groupby : Apply a function groupby\n    to each row or column of a DataFrame.\n\nExamples\n--------\n>>> df = pd.DataFrame(\n...     {\n...         \"group\": [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n...         \"value\": [2, 4, 2, 3, 5, 1, 2, 4, 1, 5],\n...     }\n... )\n>>> df\n  group  value\n0     a      2\n1     a      4\n2     a      2\n3     a      3\n4     a      5\n5     b      1\n6     b      2\n7     b      4\n8     b      1\n9     b      5\n>>> for method in ['average', 'min', 'max', 'dense', 'first']:\n...     df[f'{method}_rank'] = df.groupby('group')['value'].rank(method)\n>>> df\n  group  value  average_rank  min_rank  max_rank  dense_rank  first_rank\n0     a      2           1.5       1.0       2.0         1.0         1.0\n1     a      4           4.0       4.0       4.0         3.0         4.0\n2     a      2           1.5       1.0       2.0         1.0         2.0\n3     a      3           3.0       3.0       3.0         2.0         3.0\n4     a      5           5.0       5.0       5.0         4.0         5.0\n5     b      1           1.5       1.0       2.0         1.0         1.0\n6     b      2           3.0       3.0       3.0         2.0         3.0\n7     b      4           4.0       4.0       4.0         3.0         4.0\n8     b      1           1.5       1.0       2.0         1.0         2.0\n9     b      5           5.0       5.0       5.0         4.0         5.0", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.GroupBy.size", "Docstring": "Compute group sizes.\n\nReturns\n-------\nDataFrame or Series\n    Number of rows in each group as a Series if as_index is True\n    or a DataFrame if as_index is False.\n\nSee Also\n--------\nSeries.groupby : Apply a function groupby to a Series.\nDataFrame.groupby : Apply a function groupby\n    to each row or column of a DataFrame.", "Library": "Pandas"}
{"API_Name": "pandas.core.groupby.DataFrameGroupBy.transform", "Docstring": "Call function producing a same-indexed DataFrame on each group. Returns a DataFrame having the same indexes as the original object filled with the transformed values. Parameters ---------- f : function, str Function to apply to each group. See the Notes section below for requirements. Accepted inputs are: - String - Python function - Numba JIT function with ``engine='numba'`` specified. Only passing a single function is supported with this engine. If the ``'numba'`` engine is chosen, the function must be a user defined function with ``values`` and ``index`` as the first and second arguments respectively in the function signature. Each group's index will be passed to the user defined function and optionally available for use. If a string is chosen, then it needs to be the name of the groupby method you want to use. *args Positional arguments to pass to func. engine : str, default None * ``'cython'`` : Runs the function through C-extensions from cython. * ``'numba'`` : Runs the function through JIT compiled code from numba. * ``None`` : Defaults to ``'cython'`` or the global setting ``compute.use_numba`` engine_kwargs : dict, default None * For ``'cython'`` engine, there are no accepted ``engine_kwargs`` * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil`` and ``parallel`` dictionary keys. The values must either be ``True`` or ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be applied to the function **kwargs Keyword arguments to be passed into func. Returns ------- DataFrame See Also -------- DataFrame.groupby.apply : Apply function ``func`` group-wise and combine the results together. DataFrame.groupby.aggregate : Aggregate using one or more operations over the specified axis. DataFrame.transform : Call ``func`` on self producing a DataFrame with the same axis shape as self. Notes ----- Each group is endowed the attribute 'name' in case you need to know which group you are working on. The current implementation imposes three requirements on f: * f must return a value that either has the same shape as the input subframe or can be broadcast to the shape of the input subframe. For example, if `f` returns a scalar it will be broadcast to have the same shape as the input subframe. * if this is a DataFrame, f must support application column-by-column in the subframe. If f also supports application to the entire subframe, then a fast path is used starting from the second chunk. * f must not mutate groups. Mutation is not supported and may produce unexpected results. See :ref:`gotchas.udf-mutation` for more details. When using ``engine='numba'``, there will be no \"fall back\" behavior internally. The group data and group index will be passed as numpy arrays to the JITed user defined function, and no alternative execution attempts will be tried. .. versionchanged:: 1.3.0 The resulting dtype will reflect the return value of the passed ``func``, see the examples below. .. versionchanged:: 2.0.0 When using ``.transform`` on a grouped DataFrame and the transformation function returns a DataFrame, pandas now aligns the result's index with the input's index. You can call ``.to_numpy()`` on the result of the transformation function to avoid alignment. Examples -------- >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar', ...                           'foo', 'bar'], ...                    'B' : ['one', 'one', 'two', 'three', ...                           'two', 'two'], ...                    'C' : [1, 5, 5, 2, 5, 5], ...                    'D' : [2.0, 5., 8., 1., 2., 9.]}) >>> grouped = df.groupby('A')[['C', 'D']] >>> grouped.transform(lambda x: (x - x.mean()) / x.std()) C         D 0 -1.154701 -0.577350 1  0.577350  0.000000 2  0.577350  1.154701 3 -1.154701 -1.000000 4  0.577350 -0.577350 5  0.577350  1.000000 Broadcast result of the transformation >>> grouped.transform(lambda x: x.max() - x.min()) C    D 0  4.0  6.0 1  3.0  8.0 2  4.0  6.0 3  3.0  8.0 4  4.0  6.0 5  3.0  8.0 >>> grouped.transform(\"mean\") C    D 0  3.666667  4.0 1  4.000000  5.0 2  3.666667  4.0 3  4.000000  5.0 4  3.666667  4.0 5  4.000000  5.0 .. versionchanged:: 1.3.0 The resulting dtype will reflect the return value of the passed ``func``, for example: >>> grouped.transform(lambda x: x.astype(int).max()) C  D 0  5  8 1  5  9 2  5  8 3  5  9 4  5  8 5  5  9", "Library": "Pandas"}
{"API_Name": "pandas.core.indexes.accessors.CombinedDatetimelikeProperties.strftime", "Docstring": "Convert to Index using specified date_format.\n\nReturn an Index of formatted strings specified by date_format, which\nsupports the same string format as the python standard library. Details\nof the string format can be found in `python string format\ndoc <https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior>`__.\n\nFormats supported by the C `strftime` API but not by the python string format\ndoc (such as `\"%R\"`, `\"%r\"`) are not officially supported and should be\npreferably replaced with their supported equivalents (such as `\"%H:%M\"`,\n`\"%I:%M:%S %p\"`).\n\nNote that `PeriodIndex` support additional directives, detailed in\n`Period.strftime`.\n\nParameters\n----------\ndate_format : str\n    Date format string (e.g. \"%Y-%m-%d\").\n\nReturns\n-------\nndarray[object]\n    NumPy ndarray of formatted strings.\n\nSee Also\n--------\nto_datetime : Convert the given argument to datetime.\nDatetimeIndex.normalize : Return DatetimeIndex with times to midnight.\nDatetimeIndex.round : Round the DatetimeIndex to the specified freq.\nDatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.\nTimestamp.strftime : Format a single Timestamp.\nPeriod.strftime : Format a single Period.\n\nExamples\n--------\n>>> rng = pd.date_range(pd.Timestamp(\"2018-03-10 09:00\"),\n...                     periods=3, freq='s')\n>>> rng.strftime('%B %d, %Y, %r')\nIndex(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',\n       'March 10, 2018, 09:00:02 AM'],\n      dtype='object')", "Library": "Pandas"}
{"API_Name": "pandas.core.indexes.accessors.CombinedDatetimelikeProperties.tz_localize", "Docstring": "Localize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\nThis method takes a time zone (tz) naive Datetime Array/Index object\nand makes this time zone aware. It does not move the time to another\ntime zone.\n\nThis method can also be used to do the inverse -- to create a time\nzone unaware object from an aware object. To that end, pass `tz=None`.\n\nParameters\n----------\ntz : str, pytz.timezone, dateutil.tz.tzfile or None\n    Time zone to convert timestamps to. Passing ``None`` will\n    remove the time zone information preserving local time.\nambiguous : 'infer', 'NaT', bool array, default 'raise'\n    When clocks moved backward due to DST, ambiguous times may arise.\n    For example in Central European Time (UTC+01), when going from\n    03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at\n    00:30:00 UTC and at 01:30:00 UTC. In such a situation, the\n    `ambiguous` parameter dictates how ambiguous times should be\n    handled.\n\n    - 'infer' will attempt to infer fall dst-transition hours based on\n      order\n    - bool-ndarray where True signifies a DST time, False signifies a\n      non-DST time (note that this flag is only applicable for\n      ambiguous times)\n    - 'NaT' will return NaT where there are ambiguous times\n    - 'raise' will raise an AmbiguousTimeError if there are ambiguous\n      times.\n\nnonexistent : 'shift_forward', 'shift_backward, 'NaT', timedelta, default 'raise'\n    A nonexistent time does not exist in a particular timezone\n    where clocks moved forward due to DST.\n\n    - 'shift_forward' will shift the nonexistent time forward to the\n      closest existing time\n    - 'shift_backward' will shift the nonexistent time backward to the\n      closest existing time\n    - 'NaT' will return NaT where there are nonexistent times\n    - timedelta objects will shift nonexistent times by the timedelta\n    - 'raise' will raise an NonExistentTimeError if there are\n      nonexistent times.\n\nReturns\n-------\nSame type as self\n    Array/Index converted to the specified time zone.\n\nRaises\n------\nTypeError\n    If the Datetime Array/Index is tz-aware and tz is not None.\n\nSee Also\n--------\nDatetimeIndex.tz_convert : Convert tz-aware DatetimeIndex from\n    one time zone to another.\n\nExamples\n--------\n>>> tz_naive = pd.date_range('2018-03-01 09:00', periods=3)\n>>> tz_naive\nDatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n               '2018-03-03 09:00:00'],\n              dtype='datetime64[ns]', freq='D')\n\nLocalize DatetimeIndex in US/Eastern time zone:\n\n>>> tz_aware = tz_naive.tz_localize(tz='US/Eastern')\n>>> tz_aware\nDatetimeIndex(['2018-03-01 09:00:00-05:00',\n               '2018-03-02 09:00:00-05:00',\n               '2018-03-03 09:00:00-05:00'],\n              dtype='datetime64[ns, US/Eastern]', freq=None)\n\nWith the ``tz=None``, we can remove the time zone information\nwhile keeping the local time (not converted to UTC):\n\n>>> tz_aware.tz_localize(None)\nDatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',\n               '2018-03-03 09:00:00'],\n              dtype='datetime64[ns]', freq=None)\n\nBe careful with DST changes. When there is sequential data, pandas can\ninfer the DST time:\n\n>>> s = pd.to_datetime(pd.Series(['2018-10-28 01:30:00',\n...                               '2018-10-28 02:00:00',\n...                               '2018-10-28 02:30:00',\n...                               '2018-10-28 02:00:00',\n...                               '2018-10-28 02:30:00',\n...                               '2018-10-28 03:00:00',\n...                               '2018-10-28 03:30:00']))\n>>> s.dt.tz_localize('CET', ambiguous='infer')\n0   2018-10-28 01:30:00+02:00\n1   2018-10-28 02:00:00+02:00\n2   2018-10-28 02:30:00+02:00\n3   2018-10-28 02:00:00+01:00\n4   2018-10-28 02:30:00+01:00\n5   2018-10-28 03:00:00+01:00\n6   2018-10-28 03:30:00+01:00\ndtype: datetime64[ns, CET]\n\nIn some cases, inferring the DST is impossible. In such cases, you can\npass an ndarray to the ambiguous parameter to set the DST explicitly\n\n>>> s = pd.to_datetime(pd.Series(['2018-10-28 01:20:00',\n...                               '2018-10-28 02:36:00',\n...                               '2018-10-28 03:46:00']))\n>>> s.dt.tz_localize('CET', ambiguous=np.array([True, True, False]))\n0   2018-10-28 01:20:00+02:00\n1   2018-10-28 02:36:00+02:00\n2   2018-10-28 03:46:00+01:00\ndtype: datetime64[ns, CET]\n\nIf the DST transition causes nonexistent times, you can shift these\ndates forward or backwards with a timedelta object or `'shift_forward'`\nor `'shift_backwards'`.\n\n>>> s = pd.to_datetime(pd.Series(['2015-03-29 02:30:00',\n...                               '2015-03-29 03:30:00']))\n>>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_forward')\n0   2015-03-29 03:00:00+02:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]\n\n>>> s.dt.tz_localize('Europe/Warsaw', nonexistent='shift_backward')\n0   2015-03-29 01:59:59.999999999+01:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]\n\n>>> s.dt.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H'))\n0   2015-03-29 03:30:00+02:00\n1   2015-03-29 03:30:00+02:00\ndtype: datetime64[ns, Europe/Warsaw]", "Library": "Pandas"}
{"API_Name": "pandas.core.series.Series.reset_index", "Docstring": "Generate a new DataFrame or Series with the index reset.\n\nThis is useful when the index needs to be treated as a column, or\nwhen the index is meaningless and needs to be reset to the default\nbefore another operation.\n\nParameters\n----------\nlevel : int, str, tuple, or list, default optional\n    For a Series with a MultiIndex, only remove the specified levels\n    from the index. Removes all levels by default.\ndrop : bool, default False\n    Just reset the index, without inserting it as a column in\n    the new DataFrame.\nname : object, optional\n    The name to use for the column containing the original Series\n    values. Uses ``self.name`` by default. This argument is ignored\n    when `drop` is True.\ninplace : bool, default False\n    Modify the Series in place (do not create a new object).\nallow_duplicates : bool, default False\n    Allow duplicate column labels to be created.\n\n    .. versionadded:: 1.5.0\n\nReturns\n-------\nSeries or DataFrame or None\n    When `drop` is False (the default), a DataFrame is returned.\n    The newly created columns will come first in the DataFrame,\n    followed by the original Series values.\n    When `drop` is True, a `Series` is returned.\n    In either case, if ``inplace=True``, no value is returned.\n\nSee Also\n--------\nDataFrame.reset_index: Analogous function for DataFrame.\n\nExamples\n--------\n>>> s = pd.Series([1, 2, 3, 4], name='foo',\n...               index=pd.Index(['a', 'b', 'c', 'd'], name='idx'))\n\nGenerate a DataFrame with default index.\n\n>>> s.reset_index()\n  idx  foo\n0   a    1\n1   b    2\n2   c    3\n3   d    4\n\nTo specify the name of the new column use `name`.\n\n>>> s.reset_index(name='values')\n  idx  values\n0   a       1\n1   b       2\n2   c       3\n3   d       4\n\nTo generate a new Series with the default set `drop` to True.\n\n>>> s.reset_index(drop=True)\n0    1\n1    2\n2    3\n3    4\nName: foo, dtype: int64\n\nTo update the Series in place, without generating a new one\nset `inplace` to True. Note that it also requires ``drop=True``.\n\n>>> s.reset_index(inplace=True, drop=True)\n>>> s\n0    1\n1    2\n2    3\n3    4\nName: foo, dtype: int64\n\nThe `level` parameter is interesting for Series with a multi-level\nindex.\n\n>>> arrays = [np.array(['bar', 'bar', 'baz', 'baz']),\n...           np.array(['one', 'two', 'one', 'two'])]\n>>> s2 = pd.Series(\n...     range(4), name='foo',\n...     index=pd.MultiIndex.from_arrays(arrays,\n...                                     names=['a', 'b']))\n\nTo remove a specific level from the Index, use `level`.\n\n>>> s2.reset_index(level='a')\n       a  foo\nb\none  bar    0\ntwo  bar    1\none  baz    2\ntwo  baz    3\n\nIf `level` is not set, all levels are removed from the Index.\n\n>>> s2.reset_index()\n     a    b  foo\n0  bar  one    0\n1  bar  two    1\n2  baz  one    2\n3  baz  two    3", "Library": "Pandas"}
{"API_Name": "pandas.core.series.Series.unstack", "Docstring": "Unstack, also known as pivot, Series with MultiIndex to produce DataFrame.\n\nParameters\n----------\nlevel : int, str, or list of these, default last level\n    Level(s) to unstack, can pass level name.\nfill_value : scalar value, default None\n    Value to use when replacing NaN values.\n\nReturns\n-------\nDataFrame\n    Unstacked Series.\n\nNotes\n-----\nReference :ref:`the user guide <reshaping.stacking>` for more examples.\n\nExamples\n--------\n>>> s = pd.Series([1, 2, 3, 4],\n...               index=pd.MultiIndex.from_product([['one', 'two'],\n...                                                 ['a', 'b']]))\n>>> s\none  a    1\n     b    2\ntwo  a    3\n     b    4\ndtype: int64\n\n>>> s.unstack(level=-1)\n     a  b\none  1  2\ntwo  3  4\n\n>>> s.unstack(level=0)\n   one  two\na    1    3\nb    2    4", "Library": "Pandas"}
{"API_Name": "optim.SGD", "Docstring": "Implements stochastic gradient descent (optionally with momentum).\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n            \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n        &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n        \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n        &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n        &\\hspace{10mm}\\textbf{else}                                                          \\\\\n        &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n        &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n        &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n        &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n        &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n        &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nNesterov momentum is based on the formula from\n`On the importance of initialization and momentum in deep learning`__.\n\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float, optional): learning rate (default: 1e-3)\n    momentum (float, optional): momentum factor (default: 0)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    dampening (float, optional): dampening for momentum (default: 0)\n    nesterov (bool, optional): enables Nesterov momentum (default: False)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation. Thus, if the user has not specified BOTH flags\n          (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. For example, if the user specifies\n          True for fused but nothing for foreach, we will run the fused implementation. If\n          the user specifies False for foreach but nothing for fused (or False for fused but\n          nothing for foreach), we will run the for-loop implementation. If the user specifies\n          True for both foreach and fused, we will prioritize fused over foreach, as it is\n          typically faster. We attempt to use the fastest, so the hierarchy goes fused ->\n          foreach -> for-loop. HOWEVER, since the fused implementation is relatively new,\n          we want to give it sufficient bake-in time, so we default to foreach and NOT\n          fused when the user has not specified either flag.\n\n\nExample:\n    >>> # xdoctest: +SKIP\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    >>> optimizer.zero_grad()\n    >>> loss_fn(model(input), target).backward()\n    >>> optimizer.step()\n\n__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n\n.. note::\n    The implementation of SGD with Momentum/Nesterov subtly differs from\n    Sutskever et al. and implementations in some other frameworks.\n\n    Considering the specific case of Momentum, the update can be written as\n\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n        \\end{aligned}\n\n    where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n    parameters, gradient, velocity, and momentum respectively.\n\n    This is in contrast to Sutskever et al. and\n    other frameworks which employ an update of the form\n\n    .. math::\n        \\begin{aligned}\n            v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n            p_{t+1} & = p_{t} - v_{t+1}.\n        \\end{aligned}\n\n    The Nesterov version is analogously modified.\n\n    Moreover, the initial value of the momentum buffer is set to the\n    gradient value at the first step. This is in contrast to some other\n    frameworks that initialize it to all zeros.", "Library": "PyTorch"}
{"API_Name": "torch.allclose", "Docstring": "allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) -> bool\n\nThis function checks if :attr:`input` and :attr:`other` satisfy the condition:\n\n.. math::\n    \\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert\n\nelementwise, for all elements of :attr:`input` and :attr:`other`. The behaviour of this function is analogous to\n`numpy.allclose <https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html>`_\n\nArgs:\n    input (Tensor): first tensor to compare\n    other (Tensor): second tensor to compare\n    atol (float, optional): absolute tolerance. Default: 1e-08\n    rtol (float, optional): relative tolerance. Default: 1e-05\n    equal_nan (bool, optional): if ``True``, then two ``NaN`` s will be considered equal. Default: ``False``\n\nExample::\n\n    >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\n    False\n    >>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\n    True\n    >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\n    False\n    >>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\n    True", "Library": "PyTorch"}
{"API_Name": "torch.arange", "Docstring": "arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n\nReturns a 1-D tensor of size :math:`\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil`\nwith values from the interval ``[start, end)`` taken with common difference\n:attr:`step` beginning from `start`.\n\nNote that non-integer :attr:`step` is subject to floating point rounding errors when\ncomparing against :attr:`end`; to avoid inconsistency, we advise subtracting a small epsilon from :attr:`end`\nin such cases.\n\n.. math::\n    \\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step}\n\nArgs:\n    start (Number): the starting value for the set of points. Default: ``0``.\n    end (Number): the ending value for the set of points\n    step (Number): the gap between each pair of adjacent points. Default: ``1``.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`). If `dtype` is not given, infer the data type from the other input\n        arguments. If any of `start`, `end`, or `stop` are floating-point, the\n        `dtype` is inferred to be the default dtype, see\n        :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to\n        be `torch.int64`.\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n\nExample::\n\n    >>> torch.arange(5)\n    tensor([ 0,  1,  2,  3,  4])\n    >>> torch.arange(1, 4)\n    tensor([ 1,  2,  3])\n    >>> torch.arange(1, 2.5, 0.5)\n    tensor([ 1.0000,  1.5000,  2.0000])", "Library": "PyTorch"}
{"API_Name": "torch.as_tensor", "Docstring": "as_tensor(data, dtype=None, device=None) -> Tensor\n\nConverts :attr:`data` into a tensor, sharing data and preserving autograd\nhistory if possible.\n\nIf :attr:`data` is already a tensor with the requested dtype and device\nthen :attr:`data` itself is returned, but if :attr:`data` is a\ntensor with a different dtype or device then it's copied as if using\n`data.to(dtype=dtype, device=device)`.\n\nIf :attr:`data` is a NumPy array (an ndarray) with the same dtype and device then a\ntensor is constructed using :func:`torch.from_numpy`.\n\n.. seealso::\n\n    :func:`torch.tensor` never shares its data and creates a new \"leaf tensor\" (see :doc:`/notes/autograd`).\n\n\nArgs:\n    data (array_like): Initial data for the tensor. Can be a list, tuple,\n        NumPy ``ndarray``, scalar, and other types.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, infers data type from :attr:`data`.\n    device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor\n        then the device of data is used. If None and data is not a tensor then\n        the result tensor is constructed on the current device.\n\n\nExample::\n\n    >>> a = numpy.array([1, 2, 3])\n    >>> t = torch.as_tensor(a)\n    >>> t\n    tensor([ 1,  2,  3])\n    >>> t[0] = -1\n    >>> a\n    array([-1,  2,  3])\n\n    >>> a = numpy.array([1, 2, 3])\n    >>> t = torch.as_tensor(a, device=torch.device('cuda'))\n    >>> t\n    tensor([ 1,  2,  3])\n    >>> t[0] = -1\n    >>> a\n    array([1,  2,  3])", "Library": "PyTorch"}
{"API_Name": "torch.atanh", "Docstring": "atanh(input, *, out=None) -> Tensor\n\nReturns a new tensor with the inverse hyperbolic tangent of the elements of :attr:`input`.\n\nNote:\n    The domain of the inverse hyperbolic tangent is `(-1, 1)` and values outside this range\n    will be mapped to ``NaN``, except for the values `1` and `-1` for which the output is\n    mapped to `+/-INF` respectively.\n\n.. math::\n    \\text{out}_{i} = \\tanh^{-1}(\\text{input}_{i})\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword arguments:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> a = torch.randn(4).uniform_(-1, 1)\n    >>> a\n    tensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n    >>> torch.atanh(a)\n    tensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])", "Library": "PyTorch"}
{"API_Name": "torch.bitwise_xor", "Docstring": "bitwise_xor(input, other, *, out=None) -> Tensor\n\nComputes the bitwise XOR of :attr:`input` and :attr:`other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR.\n\nArgs:\n    input: the first input tensor\n    other: the second input tensor\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\n    tensor([-2, -2,  0], dtype=torch.int8)\n    >>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\n    tensor([ True, False, False])", "Library": "PyTorch"}
{"API_Name": "torch.bmm", "Docstring": "bmm(input, mat2, *, out=None) -> Tensor\n\nPerforms a batch matrix-matrix product of matrices stored in :attr:`input`\nand :attr:`mat2`.\n\n:attr:`input` and :attr:`mat2` must be 3-D tensors each containing\nthe same number of matrices.\n\nIf :attr:`input` is a :math:`(b \\times n \\times m)` tensor, :attr:`mat2` is a\n:math:`(b \\times m \\times p)` tensor, :attr:`out` will be a\n:math:`(b \\times n \\times p)` tensor.\n\n.. math::\n    \\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i\n\nThis operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nOn certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n.. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n          For broadcasting matrix products, see :func:`torch.matmul`.\n\nArgs:\n    input (Tensor): the first batch of matrices to be multiplied\n    mat2 (Tensor): the second batch of matrices to be multiplied\n\nKeyword Args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> input = torch.randn(10, 3, 4)\n    >>> mat2 = torch.randn(10, 4, 5)\n    >>> res = torch.bmm(input, mat2)\n    >>> res.size()\n    torch.Size([10, 3, 5])", "Library": "PyTorch"}
{"API_Name": "torch.cat", "Docstring": "cat(tensors, dim=0, *, out=None) -> Tensor\n\nConcatenates the given sequence of :attr:`seq` tensors in the given dimension.\nAll tensors must either have the same shape (except in the concatenating\ndimension) or be a 1-D empty tensor with size ``(0,)``.\n\n:func:`torch.cat` can be seen as an inverse operation for :func:`torch.split`\nand :func:`torch.chunk`.\n\n:func:`torch.cat` can be best understood via examples.\n\n.. seealso::\n\n    :func:`torch.stack` concatenates the given sequence along a new dimension.\n\nArgs:\n    tensors (sequence of Tensors): any python sequence of tensors of the same type.\n        Non-empty tensors provided must have the same shape, except in the\n        cat dimension.\n    dim (int, optional): the dimension over which the tensors are concatenated\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> x = torch.randn(2, 3)\n    >>> x\n    tensor([[ 0.6580, -1.0969, -0.4614],\n            [-0.1034, -0.5790,  0.1497]])\n    >>> torch.cat((x, x, x), 0)\n    tensor([[ 0.6580, -1.0969, -0.4614],\n            [-0.1034, -0.5790,  0.1497],\n            [ 0.6580, -1.0969, -0.4614],\n            [-0.1034, -0.5790,  0.1497],\n            [ 0.6580, -1.0969, -0.4614],\n            [-0.1034, -0.5790,  0.1497]])\n    >>> torch.cat((x, x, x), 1)\n    tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n             -1.0969, -0.4614],\n            [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n             -0.5790,  0.1497]])", "Library": "PyTorch"}
{"API_Name": "torch.complex", "Docstring": "complex(real, imag, *, out=None) -> Tensor\n\nConstructs a complex tensor with its real part equal to :attr:`real` and its\nimaginary part equal to :attr:`imag`.\n\nArgs:\n    real (Tensor): The real part of the complex tensor. Must be half, float or double.\n    imag (Tensor): The imaginary part of the complex tensor. Must be same dtype\n        as :attr:`real`.\n\nKeyword args:\n    out (Tensor): If the inputs are ``torch.float32``, must be\n        ``torch.complex64``. If the inputs are ``torch.float64``, must be\n        ``torch.complex128``.\n\nExample::\n\n    >>> real = torch.tensor([1, 2], dtype=torch.float32)\n    >>> imag = torch.tensor([3, 4], dtype=torch.float32)\n    >>> z = torch.complex(real, imag)\n    >>> z\n    tensor([(1.+3.j), (2.+4.j)])\n    >>> z.dtype\n    torch.complex64", "Library": "PyTorch"}
{"API_Name": "torch.cos", "Docstring": "cos(input, *, out=None) -> Tensor\n\nReturns a new tensor with the cosine  of the elements of :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\cos(\\text{input}_{i})\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> a = torch.randn(4)\n    >>> a\n    tensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n    >>> torch.cos(a)\n    tensor([ 0.1395,  0.2957,  0.6553,  0.5574])", "Library": "PyTorch"}
{"API_Name": "torch.cuda.is_available", "Docstring": "Return a bool indicating if CUDA is currently available.", "Library": "PyTorch"}
{"API_Name": "torch.diag", "Docstring": "diag(input, diagonal=0, *, out=None) -> Tensor\n\n- If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor\n  with the elements of :attr:`input` as the diagonal.\n- If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with\n  the diagonal elements of :attr:`input`.\n\nThe argument :attr:`diagonal` controls which diagonal to consider:\n\n- If :attr:`diagonal` = 0, it is the main diagonal.\n- If :attr:`diagonal` > 0, it is above the main diagonal.\n- If :attr:`diagonal` < 0, it is below the main diagonal.\n\nArgs:\n    input (Tensor): the input tensor.\n    diagonal (int, optional): the diagonal to consider\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\n.. seealso::\n\n        :func:`torch.diagonal` always returns the diagonal of its input.\n\n        :func:`torch.diagflat` always constructs a tensor with diagonal elements\n        specified by the input.\n\nExamples:\n\nGet the square matrix where the input vector is the diagonal::\n\n    >>> a = torch.randn(3)\n    >>> a\n    tensor([ 0.5950,-0.0872, 2.3298])\n    >>> torch.diag(a)\n    tensor([[ 0.5950, 0.0000, 0.0000],\n            [ 0.0000,-0.0872, 0.0000],\n            [ 0.0000, 0.0000, 2.3298]])\n    >>> torch.diag(a, 1)\n    tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n            [ 0.0000, 0.0000,-0.0872, 0.0000],\n            [ 0.0000, 0.0000, 0.0000, 2.3298],\n            [ 0.0000, 0.0000, 0.0000, 0.0000]])\n\nGet the k-th diagonal of a given matrix::\n\n    >>> a = torch.randn(3, 3)\n    >>> a\n    tensor([[-0.4264, 0.0255,-0.1064],\n            [ 0.8795,-0.2429, 0.1374],\n            [ 0.1029,-0.6482,-1.6300]])\n    >>> torch.diag(a, 0)\n    tensor([-0.4264,-0.2429,-1.6300])\n    >>> torch.diag(a, 1)\n    tensor([ 0.0255, 0.1374])", "Library": "PyTorch"}
{"API_Name": "torch.dist", "Docstring": "dist(input, other, p=2) -> Tensor\n\nReturns the p-norm of (:attr:`input` - :attr:`other`)\n\nThe shapes of :attr:`input` and :attr:`other` must be\n:ref:`broadcastable <broadcasting-semantics>`.\n\nArgs:\n    input (Tensor): the input tensor.\n    other (Tensor): the Right-hand-side input tensor\n    p (float, optional): the norm to be computed\n\nExample::\n\n    >>> x = torch.randn(4)\n    >>> x\n    tensor([-1.5393, -0.8675,  0.5916,  1.6321])\n    >>> y = torch.randn(4)\n    >>> y\n    tensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n    >>> torch.dist(x, y, 3.5)\n    tensor(1.6727)\n    >>> torch.dist(x, y, 3)\n    tensor(1.6973)\n    >>> torch.dist(x, y, 0)\n    tensor(4.)\n    >>> torch.dist(x, y, 1)\n    tensor(2.6537)", "Library": "PyTorch"}
{"API_Name": "torch.empty", "Docstring": "empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) -> Tensor\n\nReturns a tensor filled with uninitialized data. The shape of the tensor is\ndefined by the variable argument :attr:`size`.\n\n.. note::\n    If :func:`torch.use_deterministic_algorithms()` and\n    :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to\n    ``True``, the output tensor is initialized to prevent any possible\n    nondeterministic behavior from using the data as an input to an operation.\n    Floating point and complex tensors are filled with NaN, and integer tensors\n    are filled with the maximum value.\n\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor.\n        Can be a variable number of arguments or a collection like a list or tuple.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.contiguous_format``.\n\nExample::\n\n    >>> torch.empty((2,3), dtype=torch.int64)\n    tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n            [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])", "Library": "PyTorch"}
{"API_Name": "torch.exp", "Docstring": "exp(input, *, out=None) -> Tensor\n\nReturns a new tensor with the exponential of the elements\nof the input tensor :attr:`input`.\n\n.. math::\n    y_{i} = e^{x_{i}}\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> torch.exp(torch.tensor([0, math.log(2.)]))\n    tensor([ 1.,  2.])", "Library": "PyTorch"}
{"API_Name": "torch.fill_diagonal_", "Docstring": "fill_diagonal_(fill_value, wrap=False) -> Tensor\n\nFill the main diagonal of a tensor that has at least 2-dimensions.\nWhen dims>2, all dimensions of input must be of equal length.\nThis function modifies the input tensor in-place, and returns the input tensor.\n\nArguments:\n    fill_value (Scalar): the fill value\n    wrap (bool): the diagonal 'wrapped' after N columns for tall matrices.\n\nExample::\n\n    >>> a = torch.zeros(3, 3)\n    >>> a.fill_diagonal_(5)\n    tensor([[5., 0., 0.],\n            [0., 5., 0.],\n            [0., 0., 5.]])\n    >>> b = torch.zeros(7, 3)\n    >>> b.fill_diagonal_(5)\n    tensor([[5., 0., 0.],\n            [0., 5., 0.],\n            [0., 0., 5.],\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 0.],\n            [0., 0., 0.]])\n    >>> c = torch.zeros(7, 3)\n    >>> c.fill_diagonal_(5, wrap=True)\n    tensor([[5., 0., 0.],\n            [0., 5., 0.],\n            [0., 0., 5.],\n            [0., 0., 0.],\n            [5., 0., 0.],\n            [0., 5., 0.],\n            [0., 0., 5.]])", "Library": "PyTorch"}
{"API_Name": "torch.flatten", "Docstring": "flatten(input, start_dim=0, end_dim=-1) -> Tensor\n\nFlattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim`\nare passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened.\nThe order of elements in :attr:`input` is unchanged.\n\nUnlike NumPy's flatten, which always copies input's data, this function may return the original object, a view,\nor copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can\nbe viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the\nflattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned.\n\n.. note::\n    Flattening a zero-dimensional tensor will return a one-dimensional view.\n\nArgs:\n    input (Tensor): the input tensor.\n    start_dim (int): the first dim to flatten\n    end_dim (int): the last dim to flatten\n\nExample::\n\n    >>> t = torch.tensor([[[1, 2],\n    ...                    [3, 4]],\n    ...                   [[5, 6],\n    ...                    [7, 8]]])\n    >>> torch.flatten(t)\n    tensor([1, 2, 3, 4, 5, 6, 7, 8])\n    >>> torch.flatten(t, start_dim=1)\n    tensor([[1, 2, 3, 4],\n            [5, 6, 7, 8]])", "Library": "PyTorch"}
{"API_Name": "torch.from_numpy", "Docstring": "from_numpy(ndarray) -> Tensor\n\nCreates a :class:`Tensor` from a :class:`numpy.ndarray`.\n\nThe returned tensor and :attr:`ndarray` share the same memory. Modifications to\nthe tensor will be reflected in the :attr:`ndarray` and vice versa. The returned\ntensor is not resizable.\n\nIt currently accepts :attr:`ndarray` with dtypes of ``numpy.float64``,\n``numpy.float32``, ``numpy.float16``, ``numpy.complex64``, ``numpy.complex128``,\n``numpy.int64``, ``numpy.int32``, ``numpy.int16``, ``numpy.int8``, ``numpy.uint8``,\nand ``bool``.\n\n.. warning::\n    Writing to a tensor created from a read-only NumPy array is not supported and will result in undefined behavior.\n\nExample::\n\n    >>> a = numpy.array([1, 2, 3])\n    >>> t = torch.from_numpy(a)\n    >>> t\n    tensor([ 1,  2,  3])\n    >>> t[0] = -1\n    >>> a\n    array([-1,  2,  3])", "Library": "PyTorch"}
{"API_Name": "torch.gather", "Docstring": "gather(input, dim, index, *, sparse_grad=False, out=None) -> Tensor\n\nGathers values along an axis specified by `dim`.\n\nFor a 3-D tensor the output is specified by::\n\n    out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n    out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n    out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n\n:attr:`input` and :attr:`index` must have the same number of dimensions.\nIt is also required that ``index.size(d) <= input.size(d)`` for all\ndimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`.\nNote that ``input`` and ``index`` do not broadcast against each other.\n\nArgs:\n    input (Tensor): the source tensor\n    dim (int): the axis along which to index\n    index (LongTensor): the indices of elements to gather\n\nKeyword arguments:\n    sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor.\n    out (Tensor, optional): the destination tensor\n\nExample::\n\n    >>> t = torch.tensor([[1, 2], [3, 4]])\n    >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\n    tensor([[ 1,  1],\n            [ 4,  3]])", "Library": "PyTorch"}
{"API_Name": "torch.index_add_", "Docstring": "index_add_(dim, index, source, *, alpha=1) -> Tensor\n\nAccumulate the elements of :attr:`alpha` times ``source`` into the :attr:`self`\ntensor by adding to the indices in the order given in :attr:`index`. For example,\nif ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\\ th row of\n``source`` is subtracted from the ``j``\\ th row of :attr:`self`.\n\nThe :attr:`dim`\\ th dimension of ``source`` must have the same size as the\nlength of :attr:`index` (which must be a vector), and all other dimensions must\nmatch :attr:`self`, or an error will be raised.\n\nFor a 3-D tensor the output is given as::\n\n    self[index[i], :, :] += alpha * src[i, :, :]  # if dim == 0\n    self[:, index[i], :] += alpha * src[:, i, :]  # if dim == 1\n    self[:, :, index[i]] += alpha * src[:, :, i]  # if dim == 2\n\nNote:\n    This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n\nArgs:\n    dim (int): dimension along which to index\n    index (Tensor): indices of ``source`` to select from,\n            should have dtype either `torch.int64` or `torch.int32`\n    source (Tensor): the tensor containing values to add\n\nKeyword args:\n    alpha (Number): the scalar multiplier for ``source``\n\nExample::\n\n    >>> x = torch.ones(5, 3)\n    >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n    >>> index = torch.tensor([0, 4, 2])\n    >>> x.index_add_(0, index, t)\n    tensor([[  2.,   3.,   4.],\n            [  1.,   1.,   1.],\n            [  8.,   9.,  10.],\n            [  1.,   1.,   1.],\n            [  5.,   6.,   7.]])\n    >>> x.index_add_(0, index, t, alpha=-1)\n    tensor([[  1.,   1.,   1.],\n            [  1.,   1.,   1.],\n            [  1.,   1.,   1.],\n            [  1.,   1.,   1.],\n            [  1.,   1.,   1.]])", "Library": "PyTorch"}
{"API_Name": "torch.isin", "Docstring": "isin(elements, test_elements, *, assume_unique=False, invert=False) -> Tensor\n\nTests if each element of :attr:`elements` is in :attr:`test_elements`. Returns\na boolean tensor of the same shape as :attr:`elements` that is True for elements\nin :attr:`test_elements` and False otherwise.\n\n.. note::\n    One of :attr:`elements` or :attr:`test_elements` can be a scalar, but not both.\n\nArgs:\n    elements (Tensor or Scalar): Input elements\n    test_elements (Tensor or Scalar): Values against which to test for each input element\n    assume_unique (bool, optional): If True, assumes both :attr:`elements` and\n        :attr:`test_elements` contain unique elements, which can speed up the\n        calculation. Default: False\n    invert (bool, optional): If True, inverts the boolean return tensor, resulting in True\n        values for elements *not* in :attr:`test_elements`. Default: False\n\nReturns:\n    A boolean tensor of the same shape as :attr:`elements` that is True for elements in\n    :attr:`test_elements` and False otherwise\n\nExample:\n    >>> torch.isin(torch.tensor([[1, 2], [3, 4]]), torch.tensor([2, 3]))\n    tensor([[False,  True],\n            [ True, False]])", "Library": "PyTorch"}
{"API_Name": "torch.linalg.eigvals", "Docstring": "linalg.eigvals(A, *, out=None) -> Tensor\n\nComputes the eigenvalues of a square matrix.\n\nLetting :math:`\\mathbb{K}` be :math:`\\mathbb{R}` or :math:`\\mathbb{C}`,\nthe **eigenvalues** of a square matrix :math:`A \\in \\mathbb{K}^{n \\times n}` are defined\nas the roots (counted with multiplicity) of the polynomial `p` of degree `n` given by\n\n.. math::\n\n    p(\\lambda) = \\operatorname{det}(A - \\lambda \\mathrm{I}_n)\\mathrlap{\\qquad \\lambda \\in \\mathbb{C}}\n\nwhere :math:`\\mathrm{I}_n` is the `n`-dimensional identity matrix.\n\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\n\nThe returned eigenvalues are not guaranteed to be in any specific order.\n\n.. note:: The eigenvalues of a real matrix may be complex, as the roots of a real polynomial may be complex.\n\n          The eigenvalues of a matrix are always well-defined, even when the matrix is not diagonalizable.\n\n\n.. note:: When inputs are on a CUDA device, this function synchronizes that device with the CPU.\n\n\n.. seealso::\n\n        :func:`torch.linalg.eig` computes the full eigenvalue decomposition.\n\nArgs:\n    A (Tensor): tensor of shape `(*, n, n)` where `*` is zero or more batch dimensions.\n\nKeyword args:\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\n\nReturns:\n    A complex-valued tensor containing the eigenvalues even when :attr:`A` is real.\n\nExamples::\n\n    >>> A = torch.randn(2, 2, dtype=torch.complex128)\n    >>> L = torch.linalg.eigvals(A)\n    >>> L\n    tensor([ 1.1226+0.5738j, -0.7537-0.1286j], dtype=torch.complex128)\n\n    >>> torch.dist(L, torch.linalg.eig(A).eigenvalues)\n    tensor(2.4576e-07)", "Library": "PyTorch"}
{"API_Name": "torch.load", "Docstring": "load(f, map_location=None, pickle_module=pickle, *, weights_only=False, mmap=None, **pickle_load_args)\n\nLoads an object saved with :func:`torch.save` from a file.\n\n:func:`torch.load` uses Python's unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn't have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using the :attr:`map_location` argument.\n\nIf :attr:`map_location` is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to :attr:`map_location`. The builtin location tags are ``'cpu'``\nfor CPU tensors and ``'cuda:device_id'`` (e.g. ``'cuda:2'``) for CUDA tensors.\n:attr:`map_location` should return either ``None`` or a storage. If\n:attr:`map_location` returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, :func:`torch.load` will\nfall back to the default behavior, as if :attr:`map_location` wasn't specified.\n\nIf :attr:`map_location` is a :class:`torch.device` object or a string containing\na device tag, it indicates the location where all tensors should be loaded.\n\nOtherwise, if :attr:`map_location` is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values).\n\nUser extensions can register their own location tags and tagging and\ndeserialization methods using :func:`torch.serialization.register_package`.\n\nArgs:\n    f: a file-like object (has to implement :meth:`read`, :meth:`readline`, :meth:`tell`, and :meth:`seek`),\n        or a string or os.PathLike object containing a file name\n    map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage\n        locations\n    pickle_module: module used for unpickling metadata and objects (has to\n        match the :attr:`pickle_module` used to serialize file)\n    weights_only: Indicates whether unpickler should be restricted to\n        loading only tensors, primitive types, dictionaries\n        and any types added via :func:`torch.serialization.add_safe_globals`.\n    mmap: Indicates whether the file should be mmaped rather than loading all the storages into memory.\n        Typically, tensor storages in the file will first be moved from disk to CPU memory, after which they\n        are moved to the location that they were tagged with when saving, or specified by ``map_location``. This\n        second step is a no-op if the final location is CPU. When the ``mmap`` flag is set, instead of copying the\n        tensor storages from disk to CPU memory in the first step, ``f`` is mmaped.\n    pickle_load_args: (Python 3 only) optional keyword arguments passed over to\n        :func:`pickle_module.load` and :func:`pickle_module.Unpickler`, e.g.,\n        :attr:`errors=...`.\n\n.. warning::\n    :func:`torch.load()` unless `weights_only` parameter is set to `True`,\n    uses ``pickle`` module implicitly, which is known to be insecure.\n    It is possible to construct malicious pickle data which will execute arbitrary code\n    during unpickling. Never load data that could have come from an untrusted\n    source in an unsafe mode, or that could have been tampered with. **Only load data you trust**.\n\n.. note::\n    When you call :func:`torch.load()` on a file which contains GPU tensors, those tensors\n    will be loaded to GPU by default. You can call ``torch.load(.., map_location='cpu')``\n    and then :meth:`load_state_dict` to avoid GPU RAM surge when loading a model checkpoint.\n\n.. note::\n    By default, we decode byte strings as ``utf-8``.  This is to avoid a common error\n    case ``UnicodeDecodeError: 'ascii' codec can't decode byte 0x...``\n    when loading files saved by Python 2 in Python 3.  If this default\n    is incorrect, you may use an extra :attr:`encoding` keyword argument to specify how\n    these objects should be loaded, e.g., :attr:`encoding='latin1'` decodes them\n    to strings using ``latin1`` encoding, and :attr:`encoding='bytes'` keeps them\n    as byte arrays which can be decoded later with ``byte_array.decode(...)``.\n\nExample:\n    >>> # xdoctest: +SKIP(\"undefined filepaths\")\n    >>> torch.load('tensors.pt', weights_only=True)\n    # Load all tensors onto the CPU\n    >>> torch.load('tensors.pt', map_location=torch.device('cpu'), weights_only=True)\n    # Load all tensors onto the CPU, using a function\n    >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage, weights_only=True)\n    # Load all tensors onto GPU 1\n    >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1), weights_only=True)\n    # Map tensors from GPU 1 to GPU 0\n    >>> torch.load('tensors.pt', map_location={'cuda:1': 'cuda:0'}, weights_only=True)\n    # Load tensor from io.BytesIO object\n    # Loading from a buffer setting weights_only=False, warning this can be unsafe\n    >>> with open('tensor.pt', 'rb') as f:\n    ...     buffer = io.BytesIO(f.read())\n    >>> torch.load(buffer, weights_only=False)\n    # Load a module with 'ascii' encoding for unpickling\n    # Loading from a module setting weights_only=False, warning this can be unsafe\n    >>> torch.load('module.pt', encoding='ascii', weights_only=False)", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.log_softmax", "Docstring": "Apply a softmax followed by a logarithm. While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly. See :class:`~torch.nn.LogSoftmax` for more details. Args: input (Tensor): input dim (int): A dimension along which log_softmax will be computed. dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. If specified, the input tensor is cast to :attr:`dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Library": "PyTorch"}
{"API_Name": "torch.manual_seed", "Docstring": "Sets the seed for generating random numbers on all devices. Returns a\n`torch.Generator` object.\n\nArgs:\n    seed (int): The desired seed. Value must be within the inclusive range\n        `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\n        is raised. Negative inputs are remapped to positive values with the formula\n        `0xffff_ffff_ffff_ffff + seed`.", "Library": "PyTorch"}
{"API_Name": "torch.matmul", "Docstring": "matmul(input, other, *, out=None) -> Tensor\n\nMatrix product of two tensors.\n\nThe behavior depends on the dimensionality of the tensors as follows:\n\n- If both tensors are 1-dimensional, the dot product (scalar) is returned.\n- If both arguments are 2-dimensional, the matrix-matrix product is returned.\n- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n  a 1 is prepended to its dimension for the purpose of the matrix multiply.\n  After the matrix multiply, the prepended dimension is removed.\n- If the first argument is 2-dimensional and the second argument is 1-dimensional,\n  the matrix-vector product is returned.\n- If both arguments are at least 1-dimensional and at least one argument is\n  N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n  argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n  batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n  1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n  The non-matrix (i.e. batch) dimensions are :ref:`broadcasted <broadcasting-semantics>` (and thus\n  must be broadcastable).  For example, if :attr:`input` is a\n  :math:`(j \\times 1 \\times n \\times n)` tensor and :attr:`other` is a :math:`(k \\times n \\times n)`\n  tensor, :attr:`out` will be a :math:`(j \\times k \\times n \\times n)` tensor.\n\n  Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n  are broadcastable, and not the matrix dimensions. For example, if :attr:`input` is a\n  :math:`(j \\times 1 \\times n \\times m)` tensor and :attr:`other` is a :math:`(k \\times m \\times p)`\n  tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n  matrix dimensions) are different. :attr:`out` will be a :math:`(j \\times k \\times n \\times p)` tensor.\n\nThis operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. In particular the\nmatrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions\nas :func:`torch.mm`\n\n\n.. warning::\n    Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,\n    or may not have autograd support. If you notice missing functionality please\n    open a feature request.\n\nThis operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nOn certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n.. note::\n\n    The 1-dimensional dot product version of this function does not support an :attr:`out` parameter.\n\nArguments:\n    input (Tensor): the first tensor to be multiplied\n    other (Tensor): the second tensor to be multiplied\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> # vector x vector\n    >>> tensor1 = torch.randn(3)\n    >>> tensor2 = torch.randn(3)\n    >>> torch.matmul(tensor1, tensor2).size()\n    torch.Size([])\n    >>> # matrix x vector\n    >>> tensor1 = torch.randn(3, 4)\n    >>> tensor2 = torch.randn(4)\n    >>> torch.matmul(tensor1, tensor2).size()\n    torch.Size([3])\n    >>> # batched matrix x broadcasted vector\n    >>> tensor1 = torch.randn(10, 3, 4)\n    >>> tensor2 = torch.randn(4)\n    >>> torch.matmul(tensor1, tensor2).size()\n    torch.Size([10, 3])\n    >>> # batched matrix x batched matrix\n    >>> tensor1 = torch.randn(10, 3, 4)\n    >>> tensor2 = torch.randn(10, 4, 5)\n    >>> torch.matmul(tensor1, tensor2).size()\n    torch.Size([10, 3, 5])\n    >>> # batched matrix x broadcasted matrix\n    >>> tensor1 = torch.randn(10, 3, 4)\n    >>> tensor2 = torch.randn(4, 5)\n    >>> torch.matmul(tensor1, tensor2).size()\n    torch.Size([10, 3, 5])", "Library": "PyTorch"}
{"API_Name": "torch.max", "Docstring": "max(input) -> Tensor\n\nReturns the maximum value of all elements in the ``input`` tensor.\n\n.. warning::\n    This function produces deterministic (sub)gradients unlike ``max(dim=0)``\n\nArgs:\n    input (Tensor): the input tensor.\n\nExample::\n\n    >>> a = torch.randn(1, 3)\n    >>> a\n    tensor([[ 0.6763,  0.7445, -2.2369]])\n    >>> torch.max(a)\n    tensor(0.7445)\n\n.. function:: max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n   :noindex:\n\nReturns a namedtuple ``(values, indices)`` where ``values`` is the maximum\nvalue of each row of the :attr:`input` tensor in the given dimension\n:attr:`dim`. And ``indices`` is the index location of each maximum value found\n(argmax).\n\nIf ``keepdim`` is ``True``, the output tensors are of the same size\nas ``input`` except in the dimension ``dim`` where they are of size 1.\nOtherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting\nin the output tensors having 1 fewer dimension than ``input``.\n\n.. note:: If there are multiple maximal values in a reduced row then\n          the indices of the first maximal value are returned.\n\nArgs:\n    input (Tensor): the input tensor.\n    dim (int): the dimension to reduce.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.\n\nKeyword args:\n    out (tuple, optional): the result tuple of two output tensors (max, max_indices)\n\nExample::\n\n    >>> a = torch.randn(4, 4)\n    >>> a\n    tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n            [ 1.1949, -1.1127, -2.2379, -0.6702],\n            [ 1.5717, -0.9207,  0.1297, -1.8768],\n            [-0.6172,  1.0036, -0.6060, -0.2432]])\n    >>> torch.max(a, 1)\n    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n\n.. function:: max(input, other, *, out=None) -> Tensor\n   :noindex:\n\nSee :func:`torch.maximum`.", "Library": "PyTorch"}
{"API_Name": "torch.mean", "Docstring": "mean(input, *, dtype=None) -> Tensor\n\nReturns the mean value of all elements in the :attr:`input` tensor. Input must be floating point or complex.\n\nArgs:\n    input (Tensor):\n      the input tensor, either of floating point or complex dtype\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n\nExample::\n\n    >>> a = torch.randn(1, 3)\n    >>> a\n    tensor([[ 0.2294, -0.5481,  1.3288]])\n    >>> torch.mean(a)\n    tensor(0.3367)\n\n.. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -> Tensor\n   :noindex:\n\nReturns the mean value of each row of the :attr:`input` tensor in the given\ndimension :attr:`dim`. If :attr:`dim` is a list of dimensions,\nreduce over all of them.\n\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 (or ``len(dim)``) fewer dimension(s).\n\n\nArgs:\n    input (Tensor): the input tensor.\n    dim (int or tuple of ints): the dimension or dimensions to reduce.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n    out (Tensor, optional): the output tensor.\n\n.. seealso::\n\n    :func:`torch.nanmean` computes the mean value of `non-NaN` elements.\n\nExample::\n\n    >>> a = torch.randn(4, 4)\n    >>> a\n    tensor([[-0.3841,  0.6320,  0.4254, -0.7384],\n            [-0.9644,  1.0131, -0.6549, -1.4279],\n            [-0.2951, -1.3350, -0.7694,  0.5600],\n            [ 1.0842, -0.9580,  0.3623,  0.2343]])\n    >>> torch.mean(a, 1)\n    tensor([-0.0163, -0.5085, -0.4599,  0.1807])\n    >>> torch.mean(a, 1, True)\n    tensor([[-0.0163],\n            [-0.5085],\n            [-0.4599],\n            [ 0.1807]])", "Library": "PyTorch"}
{"API_Name": "torch.nn.AvgPool2d", "Docstring": "Applies a 2D average pooling over an input signal composed of several input planes.\n\nIn the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,\noutput :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`\ncan be precisely described as:\n\n.. math::\n\n    out(N_i, C_j, h, w)  = \\frac{1}{kH * kW} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1}\n                           input(N_i, C_j, stride[0] \\times h + m, stride[1] \\times w + n)\n\nIf :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides\nfor :attr:`padding` number of points.\n\nNote:\n    When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n    or the input. Sliding windows that would start in the right padded region are ignored.\n\nThe parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can either be:\n\n    - a single ``int`` -- in which case the same value is used for the height and width dimension\n    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n      and the second `int` for the width dimension\n\nArgs:\n    kernel_size: the size of the window\n    stride: the stride of the window. Default value is :attr:`kernel_size`\n    padding: implicit zero padding to be added on both sides\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n    count_include_pad: when True, will include the zero-padding in the averaging calculation\n    divisor_override: if specified, it will be used as divisor, otherwise size of the pooling region will be used.\n\n\nShape:\n    - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`.\n    - Output: :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, where\n\n      .. math::\n          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -\n            \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\n      .. math::\n          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -\n            \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\n      Per the note above, if ``ceil_mode`` is True and :math:`(H_{out} - 1)\\times \\text{stride}[0]\\geq H_{in}\n      + \\text{padding}[0]`, we skip the last window as it would start in the bottom padded region,\n      resulting in :math:`H_{out}` being reduced by one.\n\n      The same applies for :math:`W_{out}`.\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> m = nn.AvgPool2d(3, stride=2)\n    >>> # pool of non-square window\n    >>> m = nn.AvgPool2d((3, 2), stride=(2, 1))\n    >>> input = torch.randn(20, 16, 50, 32)\n    >>> output = m(input)", "Library": "PyTorch"}
{"API_Name": "torch.nn.BatchNorm1d", "Docstring": "Applies Batch Normalization over a 2D or 3D input.\n\nMethod described in the paper\n`Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n\n.. math::\n\n    y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\nof size `C` (where `C` is the number of features or channels of the input). By default, the\nelements of :math:`\\gamma` are set to 1 and the elements of :math:`\\beta` are set to 0.\nAt train time in the forward pass, the standard-deviation is calculated via the biased estimator,\nequivalent to ``torch.var(input, unbiased=False)``. However, the value stored in the\nmoving average of the standard-deviation is calculated via the unbiased  estimator, equivalent to\n``torch.var(input, unbiased=True)``.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default :attr:`momentum`\nof 0.1.\n\nIf :attr:`track_running_stats` is set to ``False``, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.\n\n.. note::\n    This :attr:`momentum` argument is different from one used in optimizer\n    classes and the conventional notion of momentum. Mathematically, the\n    update rule for running statistics here is\n    :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n    where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n    new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing statistics\non `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.\n\nArgs:\n    num_features: number of features or channels :math:`C` of the input\n    eps: a value added to the denominator for numerical stability.\n        Default: 1e-5\n    momentum: the value used for the running_mean and running_var\n        computation. Can be set to ``None`` for cumulative moving average\n        (i.e. simple average). Default: 0.1\n    affine: a boolean value that when set to ``True``, this module has\n        learnable affine parameters. Default: ``True``\n    track_running_stats: a boolean value that when set to ``True``, this\n        module tracks the running mean and variance, and when set to ``False``,\n        this module does not track such statistics, and initializes statistics\n        buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n        When these buffers are ``None``, this module always uses batch statistics.\n        in both training and eval modes. Default: ``True``\n\nShape:\n    - Input: :math:`(N, C)` or :math:`(N, C, L)`, where :math:`N` is the batch size,\n      :math:`C` is the number of features or channels, and :math:`L` is the sequence length\n    - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\nExamples::\n\n    >>> # With Learnable Parameters\n    >>> m = nn.BatchNorm1d(100)\n    >>> # Without Learnable Parameters\n    >>> m = nn.BatchNorm1d(100, affine=False)\n    >>> input = torch.randn(20, 100)\n    >>> output = m(input)", "Library": "PyTorch"}
{"API_Name": "torch.nn.BatchNorm2d", "Docstring": "Applies Batch Normalization over a 4D input.\n\n4D is a mini-batch of 2D inputs\nwith additional channel dimension. Method described in the paper\n`Batch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n\n.. math::\n\n    y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n\nThe mean and standard-deviation are calculated per-dimension over\nthe mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\nof size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\nto 1 and the elements of :math:`\\beta` are set to 0. At train time in the forward pass, the\nstandard-deviation is calculated via the biased estimator, equivalent to\n``torch.var(input, unbiased=False)``. However, the value stored in the moving average of the\nstandard-deviation is calculated via the unbiased  estimator, equivalent to\n``torch.var(input, unbiased=True)``.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default :attr:`momentum`\nof 0.1.\n\nIf :attr:`track_running_stats` is set to ``False``, this layer then does not\nkeep running estimates, and batch statistics are instead used during\nevaluation time as well.\n\n.. note::\n    This :attr:`momentum` argument is different from one used in optimizer\n    classes and the conventional notion of momentum. Mathematically, the\n    update rule for running statistics here is\n    :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n    where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n    new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing statistics\non `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.\n\nArgs:\n    num_features: :math:`C` from an expected input of size\n        :math:`(N, C, H, W)`\n    eps: a value added to the denominator for numerical stability.\n        Default: 1e-5\n    momentum: the value used for the running_mean and running_var\n        computation. Can be set to ``None`` for cumulative moving average\n        (i.e. simple average). Default: 0.1\n    affine: a boolean value that when set to ``True``, this module has\n        learnable affine parameters. Default: ``True``\n    track_running_stats: a boolean value that when set to ``True``, this\n        module tracks the running mean and variance, and when set to ``False``,\n        this module does not track such statistics, and initializes statistics\n        buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n        When these buffers are ``None``, this module always uses batch statistics.\n        in both training and eval modes. Default: ``True``\n\nShape:\n    - Input: :math:`(N, C, H, W)`\n    - Output: :math:`(N, C, H, W)` (same shape as input)\n\nExamples::\n\n    >>> # With Learnable Parameters\n    >>> m = nn.BatchNorm2d(100)\n    >>> # Without Learnable Parameters\n    >>> m = nn.BatchNorm2d(100, affine=False)\n    >>> input = torch.randn(20, 100, 35, 45)\n    >>> output = m(input)", "Library": "PyTorch"}
{"API_Name": "torch.nn.BCELoss", "Docstring": "Creates a criterion that measures the Binary Cross Entropy between the target and\nthe input probabilities:\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],\n\nwhere :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then\n\n.. math::\n    \\ell(x, y) = \\begin{cases}\n        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nThis is used for measuring the error of a reconstruction in for example\nan auto-encoder. Note that the targets :math:`y` should be numbers\nbetween 0 and 1.\n\nNotice that if :math:`x_n` is either 0 or 1, one of the log terms would be\nmathematically undefined in the above loss equation. PyTorch chooses to set\n:math:`\\log (0) = -\\infty`, since :math:`\\lim_{x\\to 0} \\log (x) = -\\infty`.\nHowever, an infinite term in the loss equation is not desirable for several reasons.\n\nFor one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be\nmultiplying 0 with infinity. Secondly, if we have an infinite loss value, then\nwe would also have an infinite term in our gradient, since\n:math:`\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty`.\nThis would make BCELoss's backward method nonlinear with respect to :math:`x_n`,\nand using it for things like linear regression would not be straight-forward.\n\nOur solution is that BCELoss clamps its log function outputs to be greater than\nor equal to -100. This way, we can always have a finite loss value and a linear\nbackward method.\n\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to the loss\n        of each batch element. If given, has to be a Tensor of size `nbatch`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same\n      shape as input.\n\nExamples::\n\n    >>> m = nn.Sigmoid()\n    >>> loss = nn.BCELoss()\n    >>> input = torch.randn(3, 2, requires_grad=True)\n    >>> target = torch.rand(3, 2, requires_grad=False)\n    >>> output = loss(m(input), target)\n    >>> output.backward()", "Library": "PyTorch"}
{"API_Name": "torch.nn.Conv2d", "Docstring": "Applies a 2D convolution over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n:math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\ncan be precisely described as:\n\n.. math::\n    \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n    \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n\n\nwhere :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n:math:`N` is a batch size, :math:`C` denotes a number of channels,\n:math:`H` is a height of input planes in pixels, and :math:`W` is\nwidth in pixels.\n\n\nThis module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nOn certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n* :attr:`stride` controls the stride for the cross-correlation, a single\n  number or a tuple.\n\n* :attr:`padding` controls the amount of padding applied to the input. It\n  can be either a string {'valid', 'same'} or an int / a tuple of ints giving the\n  amount of implicit padding applied on both sides.\n\n* :attr:`dilation` controls the spacing between the kernel points; also\n  known as the \\u00e0 trous algorithm. It is harder to describe, but this `link`_\n  has a nice visualization of what :attr:`dilation` does.\n\n* :attr:`groups` controls the connections between inputs and outputs.\n  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n  :attr:`groups`. For example,\n\n    * At groups=1, all inputs are convolved to all outputs.\n    * At groups=2, the operation becomes equivalent to having two conv\n      layers side by side, each seeing half the input channels\n      and producing half the output channels, and both subsequently\n      concatenated.\n    * At groups= :attr:`in_channels`, each input channel is convolved with\n      its own set of filters (of size\n      :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n\nThe parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n\n    - a single ``int`` -- in which case the same value is used for the height and width dimension\n    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n      and the second `int` for the width dimension\n\nNote:\n    When `groups == in_channels` and `out_channels == K * in_channels`,\n    where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n\n    In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n    a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n    :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n\nNote:\n    In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n\nNote:\n    ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n    the input so the output has the shape as the input. However, this mode\n    doesn't support any stride values other than 1.\n\nNote:\n    This module supports complex data types i.e. ``complex32, complex64, complex128``.\n\nArgs:\n    in_channels (int): Number of channels in the input image\n    out_channels (int): Number of channels produced by the convolution\n    kernel_size (int or tuple): Size of the convolving kernel\n    stride (int or tuple, optional): Stride of the convolution. Default: 1\n    padding (int, tuple or str, optional): Padding added to all four sides of\n        the input. Default: 0\n    padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n        ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n    groups (int, optional): Number of blocked connections from input\n        channels to output channels. Default: 1\n    bias (bool, optional): If ``True``, adds a learnable bias to the\n        output. Default: ``True``\n\n\nShape:\n    - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n\n      .. math::\n          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\n      .. math::\n          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nAttributes:\n    weight (Tensor): the learnable weights of the module of shape\n        :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n        :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n        The values of these weights are sampled from\n        :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n    bias (Tensor):   the learnable bias of the module of shape\n        (out_channels). If :attr:`bias` is ``True``,\n        then the values of these weights are\n        sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n\nExamples:\n\n    >>> # With square kernels and equal stride\n    >>> m = nn.Conv2d(16, 33, 3, stride=2)\n    >>> # non-square kernels and unequal stride and with padding\n    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n    >>> # non-square kernels and unequal stride and with padding and dilation\n    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n    >>> input = torch.randn(20, 16, 50, 100)\n    >>> output = m(input)\n\n.. _cross-correlation:\n    https://en.wikipedia.org/wiki/Cross-correlation\n\n.. _link:\n    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md", "Library": "PyTorch"}
{"API_Name": "torch.nn.ConvTranspose2d", "Docstring": "Applies a 2D transposed convolution operator over an input image\ncomposed of several input planes.\n\nThis module can be seen as the gradient of Conv2d with respect to its input.\nIt is also known as a fractionally-strided convolution or\na deconvolution (although it is not an actual deconvolution operation as it does\nnot compute a true inverse of convolution). For more information, see the visualizations\n`here`_ and the `Deconvolutional Networks`_ paper.\n\nThis module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nOn certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\n* :attr:`stride` controls the stride for the cross-correlation.\n\n* :attr:`padding` controls the amount of implicit zero padding on both\n  sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note\n  below for details.\n\n* :attr:`output_padding` controls the additional size added to one side\n  of the output shape. See note below for details.\n\n* :attr:`dilation` controls the spacing between the kernel points; also known as the \\u00e0 trous algorithm.\n  It is harder to describe, but the link `here`_ has a nice visualization of what :attr:`dilation` does.\n\n* :attr:`groups` controls the connections between inputs and outputs.\n  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n  :attr:`groups`. For example,\n\n    * At groups=1, all inputs are convolved to all outputs.\n    * At groups=2, the operation becomes equivalent to having two conv\n      layers side by side, each seeing half the input channels\n      and producing half the output channels, and both subsequently\n      concatenated.\n    * At groups= :attr:`in_channels`, each input channel is convolved with\n      its own set of filters (of size\n      :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n\nThe parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`\ncan either be:\n\n    - a single ``int`` -- in which case the same value is used for the height and width dimensions\n    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n      and the second `int` for the width dimension\n\nNote:\n    The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``\n    amount of zero padding to both sizes of the input. This is set so that\n    when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`\n    are initialized with same parameters, they are inverses of each other in\n    regard to the input and output shapes. However, when ``stride > 1``,\n    :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output\n    shape. :attr:`output_padding` is provided to resolve this ambiguity by\n    effectively increasing the calculated output shape on one side. Note\n    that :attr:`output_padding` is only used to find output shape, but does\n    not actually add zero-padding to output.\n\nNote:\n    In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n\nArgs:\n    in_channels (int): Number of channels in the input image\n    out_channels (int): Number of channels produced by the convolution\n    kernel_size (int or tuple): Size of the convolving kernel\n    stride (int or tuple, optional): Stride of the convolution. Default: 1\n    padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding\n        will be added to both sides of each dimension in the input. Default: 0\n    output_padding (int or tuple, optional): Additional size added to one side\n        of each dimension in the output shape. Default: 0\n    groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n    bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n\n\nShape:\n    - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n\n    .. math::\n          H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n                    \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n    .. math::\n          W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n                    \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\nAttributes:\n    weight (Tensor): the learnable weights of the module of shape\n                     :math:`(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},`\n                     :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n                     The values of these weights are sampled from\n                     :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                     :math:`k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n    bias (Tensor):   the learnable bias of the module of shape (out_channels)\n                     If :attr:`bias` is ``True``, then the values of these weights are\n                     sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                     :math:`k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n\nExamples::\n\n    >>> # With square kernels and equal stride\n    >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n    >>> # non-square kernels and unequal stride and with padding\n    >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n    >>> input = torch.randn(20, 16, 50, 100)\n    >>> output = m(input)\n    >>> # exact output size can be also specified as an argument\n    >>> input = torch.randn(1, 16, 12, 12)\n    >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n    >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n    >>> h = downsample(input)\n    >>> h.size()\n    torch.Size([1, 16, 6, 6])\n    >>> output = upsample(h, output_size=input.size())\n    >>> output.size()\n    torch.Size([1, 16, 12, 12])\n\n.. _`here`:\n    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n\n.. _`Deconvolutional Networks`:\n    https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf", "Library": "PyTorch"}
{"API_Name": "torch.nn.CrossEntropyLoss", "Docstring": "This criterion computes the cross entropy loss between input logits\nand target.\n\nIt is useful when training a classification problem with `C` classes.\nIf provided, the optional argument :attr:`weight` should be a 1D `Tensor`\nassigning weight to each of the classes.\nThis is particularly useful when you have an unbalanced training set.\n\nThe `input` is expected to contain the unnormalized logits for each class (which do `not` need\nto be positive or sum to 1, in general).\n`input` has to be a Tensor of size :math:`(C)` for unbatched input,\n:math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n`K`-dimensional case. The last being useful for higher dimension inputs, such\nas computing cross entropy loss per-pixel for 2D images.\n\nThe `target` that this criterion expects should contain either:\n\n- Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n  `ignore_index` is specified, this loss also accepts this class index (this index\n  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n  set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n  Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`\n  on an input, followed by :class:`~torch.nn.NLLLoss`.\n\n- Probabilities for each class; useful when labels beyond a single class per minibatch item\n  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n\n  .. math::\n      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n\n  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n\n  .. math::\n      \\ell(x, y) = \\begin{cases}\n          \\frac{\\sum_{n=1}^N l_n}{N}, &\n           \\text{if reduction} = \\text{`mean';}\\\\\n            \\sum_{n=1}^N l_n,  &\n            \\text{if reduction} = \\text{`sum'.}\n        \\end{cases}\n\n.. note::\n    The performance of this criterion is generally better when `target` contains class\n    indices, as this allows for optimized computation. Consider providing `target` as\n    class probabilities only when a single class label per minibatch item is too restrictive.\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to each class.\n        If given, has to be a Tensor of size `C` and floating point dtype\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    ignore_index (int, optional): Specifies a target value that is ignored\n        and does not contribute to the input gradient. When :attr:`size_average` is\n        ``True``, the loss is averaged over non-ignored targets. Note that\n        :attr:`ignore_index` is only applicable when the target contains class indices.\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n        be applied, ``'mean'``: the weighted mean of the output is taken,\n        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in\n        the meantime, specifying either of those two args will override\n        :attr:`reduction`. Default: ``'mean'``\n    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n        become a mixture of the original ground truth and a uniform distribution as described in\n        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n\nShape:\n    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of `K`-dimensional loss.\n    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n      If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n    - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n\n\n    where:\n\n    .. math::\n        \\begin{aligned}\n            C ={} & \\text{number of classes} \\\\\n            N ={} & \\text{batch size} \\\\\n        \\end{aligned}\n\nExamples::\n\n    >>> # Example of target with class indices\n    >>> loss = nn.CrossEntropyLoss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n    >>> output = loss(input, target)\n    >>> output.backward()\n    >>>\n    >>> # Example of target with class probabilities\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5).softmax(dim=1)\n    >>> output = loss(input, target)\n    >>> output.backward()", "Library": "PyTorch"}
{"API_Name": "torch.nn.Dropout", "Docstring": "During training, randomly zeroes some of the elements of the input tensor with probability :attr:`p`.\n\nThe zeroed elements are chosen independently for each forward call and are sampled from a Bernoulli distribution.\n\nEach channel will be zeroed out independently on every forward call.\n\nThis has proven to be an effective technique for regularization and\npreventing the co-adaptation of neurons as described in the paper\n`Improving neural networks by preventing co-adaptation of feature\ndetectors`_ .\n\nFurthermore, the outputs are scaled by a factor of :math:`\\frac{1}{1-p}` during\ntraining. This means that during evaluation the module simply computes an\nidentity function.\n\nArgs:\n    p: probability of an element to be zeroed. Default: 0.5\n    inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`. Input can be of any shape\n    - Output: :math:`(*)`. Output is of the same shape as input\n\nExamples::\n\n    >>> m = nn.Dropout(p=0.2)\n    >>> input = torch.randn(20, 16)\n    >>> output = m(input)\n\n.. _Improving neural networks by preventing co-adaptation of feature\n    detectors: https://arxiv.org/abs/1207.0580", "Library": "PyTorch"}
{"API_Name": "torch.nn.Dropout2d", "Docstring": "Randomly zero out entire channels.\n\nA channel is a 2D feature map,\ne.g., the :math:`j`-th channel of the :math:`i`-th sample in the\nbatched input is a 2D tensor :math:`\\text{input}[i, j]`.\n\nEach channel will be zeroed out independently on every forward call with\nprobability :attr:`p` using samples from a Bernoulli distribution.\n\nUsually the input comes from :class:`nn.Conv2d` modules.\n\nAs described in the paper\n`Efficient Object Localization Using Convolutional Networks`_ ,\nif adjacent pixels within feature maps are strongly correlated\n(as is normally the case in early convolution layers) then i.i.d. dropout\nwill not regularize the activations and will otherwise just result\nin an effective learning rate decrease.\n\nIn this case, :func:`nn.Dropout2d` will help promote independence between\nfeature maps and should be used instead.\n\nArgs:\n    p (float, optional): probability of an element to be zero-ed.\n    inplace (bool, optional): If set to ``True``, will do this operation\n        in-place\n\n.. warning ::\n    Due to historical reasons, this class will perform 1D channel-wise dropout\n    for 3D inputs (as done by :class:`nn.Dropout1d`). Thus, it currently does NOT\n    support inputs without a batch dimension of shape :math:`(C, H, W)`. This\n    behavior will change in a future release to interpret 3D inputs as no-batch-dim\n    inputs. To maintain the old behavior, switch to :class:`nn.Dropout1d`.\n\nShape:\n    - Input: :math:`(N, C, H, W)` or :math:`(N, C, L)`.\n    - Output: :math:`(N, C, H, W)` or :math:`(N, C, L)` (same shape as input).\n\nExamples::\n\n    >>> m = nn.Dropout2d(p=0.2)\n    >>> input = torch.randn(20, 16, 32, 32)\n    >>> output = m(input)\n\n.. _Efficient Object Localization Using Convolutional Networks:\n   https://arxiv.org/abs/1411.4280", "Library": "PyTorch"}
{"API_Name": "torch.nn.Embedding", "Docstring": "A simple lookup table that stores embeddings of a fixed dictionary and size.\n\nThis module is often used to store word embeddings and retrieve them using indices.\nThe input to the module is a list of indices, and the output is the corresponding\nword embeddings.\n\nArgs:\n    num_embeddings (int): size of the dictionary of embeddings\n    embedding_dim (int): the size of each embedding vector\n    padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                 therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                 i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n                                 the embedding vector at :attr:`padding_idx` will default to all zeros,\n                                 but can be updated to another value to be used as the padding vector.\n    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                is renormalized to have norm :attr:`max_norm`.\n    norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n    scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                            the words in the mini-batch. Default ``False``.\n    sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n                             See Notes for more details regarding sparse gradients.\n\nAttributes:\n    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n                     initialized from :math:`\\mathcal{N}(0, 1)`\n\nShape:\n    - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n    - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n\n.. note::\n    Keep in mind that only a limited number of optimizers support\n    sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n    :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n\n.. note::\n    When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n    :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n    modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n    calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n    :attr:`max_norm` is not ``None``. For example::\n\n        n, d, m = 3, 5, 7\n        embedding = nn.Embedding(n, d, max_norm=True)\n        W = torch.randn((m, d), requires_grad=True)\n        idx = torch.tensor([1, 2])\n        a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n        b = embedding(idx) @ W.t()  # modifies weight in-place\n        out = (a.unsqueeze(0) + b.unsqueeze(1))\n        loss = out.sigmoid().prod()\n        loss.backward()\n\nExamples::\n\n    >>> # an Embedding module containing 10 tensors of size 3\n    >>> embedding = nn.Embedding(10, 3)\n    >>> # a batch of 2 samples of 4 indices each\n    >>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n    >>> embedding(input)\n    tensor([[[-0.0251, -1.6902,  0.7172],\n             [-0.6431,  0.0748,  0.6969],\n             [ 1.4970,  1.3448, -0.9685],\n             [-0.3677, -2.7265, -0.1685]],\n\n            [[ 1.4970,  1.3448, -0.9685],\n             [ 0.4362, -0.4004,  0.9400],\n             [-0.6431,  0.0748,  0.6969],\n             [ 0.9124, -2.3616,  1.1151]]])\n\n\n    >>> # example with padding_idx\n    >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n    >>> input = torch.LongTensor([[0, 2, 0, 5]])\n    >>> embedding(input)\n    tensor([[[ 0.0000,  0.0000,  0.0000],\n             [ 0.1535, -2.0309,  0.9315],\n             [ 0.0000,  0.0000,  0.0000],\n             [-0.1655,  0.9897,  0.0635]]])\n\n    >>> # example of changing `pad` vector\n    >>> padding_idx = 0\n    >>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n    >>> embedding.weight\n    Parameter containing:\n    tensor([[ 0.0000,  0.0000,  0.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n    >>> with torch.no_grad():\n    ...     embedding.weight[padding_idx] = torch.ones(3)\n    >>> embedding.weight\n    Parameter containing:\n    tensor([[ 1.0000,  1.0000,  1.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)", "Library": "PyTorch"}
{"API_Name": "torch.nn.EmbeddingBag", "Docstring": "Compute sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.\n\nFor bags of constant length, no :attr:`per_sample_weights`, no indices equal to :attr:`padding_idx`,\nand with 2D inputs, this class\n\n    * with ``mode=\"sum\"`` is equivalent to :class:`~torch.nn.Embedding` followed by ``torch.sum(dim=1)``,\n    * with ``mode=\"mean\"`` is equivalent to :class:`~torch.nn.Embedding` followed by ``torch.mean(dim=1)``,\n    * with ``mode=\"max\"`` is equivalent to :class:`~torch.nn.Embedding` followed by ``torch.max(dim=1)``.\n\nHowever, :class:`~torch.nn.EmbeddingBag` is much more time and memory efficient than using a chain of these\noperations.\n\nEmbeddingBag also supports per-sample weights as an argument to the forward\npass. This scales the output of the Embedding before performing a weighted\nreduction as specified by ``mode``. If :attr:`per_sample_weights` is passed, the\nonly supported ``mode`` is ``\"sum\"``, which computes a weighted sum according to\n:attr:`per_sample_weights`.\n\nArgs:\n    num_embeddings (int): size of the dictionary of embeddings\n    embedding_dim (int): the size of each embedding vector\n    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                is renormalized to have norm :attr:`max_norm`.\n    norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n    scale_grad_by_freq (bool, optional): if given, this will scale gradients by the inverse of frequency of\n                                            the words in the mini-batch. Default ``False``.\n                                            Note: this option is not supported when ``mode=\"max\"``.\n    mode (str, optional): ``\"sum\"``, ``\"mean\"`` or ``\"max\"``. Specifies the way to reduce the bag.\n                             ``\"sum\"`` computes the weighted sum, taking :attr:`per_sample_weights`\n                             into consideration. ``\"mean\"`` computes the average of the values\n                             in the bag, ``\"max\"`` computes the max value over each bag.\n                             Default: ``\"mean\"``\n    sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor. See\n                             Notes for more details regarding sparse gradients. Note: this option is not\n                             supported when ``mode=\"max\"``.\n    include_last_offset (bool, optional): if ``True``, :attr:`offsets` has one additional element, where the last element\n                                  is equivalent to the size of `indices`. This matches the CSR format.\n    padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the\n                                 gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated\n                                 during training, i.e. it remains as a fixed \"pad\". For a newly constructed\n                                 EmbeddingBag, the embedding vector at :attr:`padding_idx` will default to all\n                                 zeros, but can be updated to another value to be used as the padding vector.\n                                 Note that the embedding vector at :attr:`padding_idx` is excluded from the\n                                 reduction.\n\nAttributes:\n    weight (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`\n                     initialized from :math:`\\mathcal{N}(0, 1)`.\n\nExamples::\n\n    >>> # an EmbeddingBag module containing 10 tensors of size 3\n    >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\n    >>> # a batch of 2 samples of 4 indices each\n    >>> input = torch.tensor([1, 2, 4, 5, 4, 3, 2, 9], dtype=torch.long)\n    >>> offsets = torch.tensor([0, 4], dtype=torch.long)\n    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n    >>> embedding_sum(input, offsets)\n    tensor([[-0.8861, -5.4350, -0.0523],\n            [ 1.1306, -2.5798, -1.0044]])\n\n    >>> # Example with padding_idx\n    >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum', padding_idx=2)\n    >>> input = torch.tensor([2, 2, 2, 2, 4, 3, 2, 9], dtype=torch.long)\n    >>> offsets = torch.tensor([0, 4], dtype=torch.long)\n    >>> embedding_sum(input, offsets)\n    tensor([[ 0.0000,  0.0000,  0.0000],\n            [-0.7082,  3.2145, -2.6251]])\n\n    >>> # An EmbeddingBag can be loaded from an Embedding like so\n    >>> embedding = nn.Embedding(10, 3, padding_idx=2)\n    >>> embedding_sum = nn.EmbeddingBag.from_pretrained(\n            embedding.weight,\n            padding_idx=embedding.padding_idx,\n            mode='sum')", "Library": "PyTorch"}
{"API_Name": "torch.nn.Flatten", "Docstring": "Flattens a contiguous range of dims into a tensor.\n\nFor use with :class:`~nn.Sequential`, see :meth:`torch.flatten` for details.\n\nShape:\n    - Input: :math:`(*, S_{\\text{start}},..., S_{i}, ..., S_{\\text{end}}, *)`,'\n      where :math:`S_{i}` is the size at dimension :math:`i` and :math:`*` means any\n      number of dimensions including none.\n    - Output: :math:`(*, \\prod_{i=\\text{start}}^{\\text{end}} S_{i}, *)`.\n\nArgs:\n    start_dim: first dim to flatten (default = 1).\n    end_dim: last dim to flatten (default = -1).\n\nExamples::\n    >>> input = torch.randn(32, 1, 5, 5)\n    >>> # With default parameters\n    >>> m = nn.Flatten()\n    >>> output = m(input)\n    >>> output.size()\n    torch.Size([32, 25])\n    >>> # With non-default parameters\n    >>> m = nn.Flatten(0, 2)\n    >>> output = m(input)\n    >>> output.size()\n    torch.Size([160, 5])", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.avg_pool1d", "Docstring": "avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -> Tensor\n\nApplies a 1D average pooling over an input signal composed of several\ninput planes.\n\nSee :class:`~torch.nn.AvgPool1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    kernel_size: the size of the window. Can be a single number or a\n      tuple `(kW,)`\n    stride: the stride of the window. Can be a single number or a tuple\n      `(sW,)`. Default: :attr:`kernel_size`\n    padding: implicit zero paddings on both sides of the input. Can be a\n      single number or a tuple `(padW,)`. Default: 0\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the\n        output shape. Default: ``False``\n    count_include_pad: when True, will include the zero-padding in the\n        averaging calculation. Default: ``True``\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n    >>> F.avg_pool1d(input, kernel_size=3, stride=2)\n    tensor([[[ 2.,  4.,  6.]]])", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.binary_cross_entropy", "Docstring": "Measure Binary Cross Entropy between the target and input probabilities.\n\nSee :class:`~torch.nn.BCELoss` for details.\n\nArgs:\n    input: Tensor of arbitrary shape as probabilities.\n    target: Tensor of the same shape as input with values between 0 and 1.\n    weight (Tensor, optional): a manual rescaling weight\n            if provided it's repeated to match input tensor shape\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when reduce is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nExamples::\n\n    >>> input = torch.randn(3, 2, requires_grad=True)\n    >>> target = torch.rand(3, 2, requires_grad=False)\n    >>> loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n    >>> loss.backward()", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.cosine_similarity", "Docstring": "cosine_similarity(x1, x2, dim=1, eps=1e-8) -> Tensor\n\nReturns cosine similarity between ``x1`` and ``x2``, computed along dim. ``x1`` and ``x2`` must be broadcastable\nto a common shape. ``dim`` refers to the dimension in this common shape. Dimension ``dim`` of the output is\nsqueezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 fewer dimension.\n\n.. math ::\n    \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2, \\epsilon) \\cdot \\max(\\Vert x_2 \\Vert _2, \\epsilon)}\n\nSupports :ref:`type promotion <type-promotion-doc>`.\n\nArgs:\n    x1 (Tensor): First input.\n    x2 (Tensor): Second input.\n    dim (int, optional): Dimension along which cosine similarity is computed. Default: 1\n    eps (float, optional): Small value to avoid division by zero.\n        Default: 1e-8\n\nExample::\n\n    >>> input1 = torch.randn(100, 128)\n    >>> input2 = torch.randn(100, 128)\n    >>> output = F.cosine_similarity(input1, input2)\n    >>> print(output)", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.dropout", "Docstring": "During training, randomly zeroes some elements of the input tensor with probability :attr:`p`.\n\nUses samples from a Bernoulli distribution.\n\nSee :class:`~torch.nn.Dropout` for details.\n\nArgs:\n    p: probability of an element to be zeroed. Default: 0.5\n    training: apply dropout if is ``True``. Default: ``True``\n    inplace: If set to ``True``, will do this operation in-place. Default: ``False``", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.leaky_relu", "Docstring": "leaky_relu(input, negative_slope=0.01, inplace=False) -> Tensor\n\nApplies element-wise,\n:math:`\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)`\n\nSee :class:`~torch.nn.LeakyReLU` for more details.", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.linear", "Docstring": "linear(input, weight, bias=None) -> Tensor\n\nApplies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n\nThis operation supports 2-D :attr:`weight` with :ref:`sparse layout<sparse-docs>`\n\n\n.. warning::\n    Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,\n    or may not have autograd support. If you notice missing functionality please\n    open a feature request.\n\nThis operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nShape:\n\n    - Input: :math:`(*, in\\_features)` where `*` means any number of\n      additional dimensions, including none\n    - Weight: :math:`(out\\_features, in\\_features)` or :math:`(in\\_features)`\n    - Bias: :math:`(out\\_features)` or :math:`()`\n    - Output: :math:`(*, out\\_features)` or :math:`(*)`, based on the shape of the weight", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.log_softmax", "Docstring": "Apply a softmax followed by a logarithm.\n\nWhile mathematically equivalent to log(softmax(x)), doing these two\noperations separately is slower and numerically unstable. This function\nuses an alternative formulation to compute the output and gradient correctly.\n\nSee :class:`~torch.nn.LogSoftmax` for more details.\n\nArgs:\n    input (Tensor): input\n    dim (int): A dimension along which log_softmax will be computed.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n      If specified, the input tensor is cast to :attr:`dtype` before the operation\n      is performed. This is useful for preventing data type overflows. Default: None.", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.max_pool2d", "Docstring": "max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)\n\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\n.. note::\n    The order of :attr:`ceil_mode` and :attr:`return_indices` is different from\n    what seen in :class:`~torch.nn.MaxPool2d`, and will change in a future release.\n\nSee :class:`~torch.nn.MaxPool2d` for details.\n\nArgs:\n    input: input tensor :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`, minibatch dim optional.\n    kernel_size: size of the pooling region. Can be a single number or a\n        tuple `(kH, kW)`\n    stride: stride of the pooling operation. Can be a single number or a\n        tuple `(sH, sW)`. Default: :attr:`kernel_size`\n    padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2.\n    dilation: The stride between elements within a sliding window, must be > 0.\n    ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This\n               ensures that every element in the input tensor is covered by a sliding window.\n    return_indices: If ``True``, will return the argmax along with the max values.\n                    Useful for :class:`torch.nn.functional.max_unpool2d` later", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.one_hot", "Docstring": "one_hot(tensor, num_classes=-1) -> LongTensor\n\nTakes LongTensor with index values of shape ``(*)`` and returns a tensor\nof shape ``(*, num_classes)`` that have zeros everywhere except where the\nindex of last dimension matches the corresponding value of the input tensor,\nin which case it will be 1.\n\nSee also `One-hot on Wikipedia`_ .\n\n.. _One-hot on Wikipedia:\n    https://en.wikipedia.org/wiki/One-hot\n\nArguments:\n    tensor (LongTensor): class values of any shape.\n    num_classes (int):  Total number of classes. If set to -1, the number\n        of classes will be inferred as one greater than the largest class\n        value in the input tensor.\n\nReturns:\n    LongTensor that has one more dimension with 1 values at the\n    index of last dimension indicated by the input, and 0 everywhere\n    else.\n\nExamples:\n    >>> F.one_hot(torch.arange(0, 5) % 3)\n    tensor([[1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1, 0, 0],\n            [0, 1, 0]])\n    >>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\n    tensor([[1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0],\n            [0, 0, 1, 0, 0],\n            [1, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0]])\n    >>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\n    tensor([[[1, 0, 0],\n             [0, 1, 0]],\n            [[0, 0, 1],\n             [1, 0, 0]],\n            [[0, 1, 0],\n             [0, 0, 1]]])", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.pad", "Docstring": "pad(input, pad, mode=\"constant\", value=None) -> Tensor\n\nPads tensor.\n\nPadding size:\n    The padding size by which to pad some dimensions of :attr:`input`\n    are described starting from the last dimension and moving forward.\n    :math:`\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor` dimensions\n    of ``input`` will be padded.\n    For example, to pad only the last dimension of the input tensor, then\n    :attr:`pad` has the form\n    :math:`(\\text{padding\\_left}, \\text{padding\\_right})`;\n    to pad the last 2 dimensions of the input tensor, then use\n    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n    :math:`\\text{padding\\_top}, \\text{padding\\_bottom})`;\n    to pad the last 3 dimensions, use\n    :math:`(\\text{padding\\_left}, \\text{padding\\_right},`\n    :math:`\\text{padding\\_top}, \\text{padding\\_bottom}`\n    :math:`\\text{padding\\_front}, \\text{padding\\_back})`.\n\nPadding mode:\n    See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,\n    :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`\n    for concrete examples on how each of the padding modes works. Constant\n    padding is implemented for arbitrary dimensions. Circular, replicate and\n    reflection padding are implemented for padding the last 3 dimensions of a\n    4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,\n    or the last dimension of a 2D or 3D input tensor.\n\nNote:\n    When using the CUDA backend, this operation may induce nondeterministic\n    behaviour in its backward pass that is not easily switched off.\n    Please see the notes on :doc:`/notes/randomness` for background.\n\nArgs:\n    input (Tensor): N-dimensional tensor\n    pad (tuple): m-elements tuple, where\n        :math:`\\frac{m}{2} \\leq` input dimensions and :math:`m` is even.\n    mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n        Default: ``'constant'``\n    value: fill value for ``'constant'`` padding. Default: ``0``\n\nExamples::\n\n    >>> t4d = torch.empty(3, 3, 4, 2)\n    >>> p1d = (1, 1) # pad last dim by 1 on each side\n    >>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n    >>> print(out.size())\n    torch.Size([3, 3, 4, 4])\n    >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n    >>> out = F.pad(t4d, p2d, \"constant\", 0)\n    >>> print(out.size())\n    torch.Size([3, 3, 8, 4])\n    >>> t4d = torch.empty(3, 3, 4, 2)\n    >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n    >>> out = F.pad(t4d, p3d, \"constant\", 0)\n    >>> print(out.size())\n    torch.Size([3, 9, 7, 3])", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.pairwise_distance", "Docstring": "pairwise_distance(x1, x2, p=2.0, eps=1e-6, keepdim=False) -> Tensor\n\nSee :class:`torch.nn.PairwiseDistance` for details", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.relu", "Docstring": "relu(input, inplace=False) -> Tensor\n\nApplies the rectified linear unit function element-wise. See\n:class:`~torch.nn.ReLU` for more details.", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.relu6", "Docstring": "relu6(input, inplace=False) -> Tensor\n\nApplies the element-wise function :math:`\\text{ReLU6}(x) = \\min(\\max(0,x), 6)`.\n\nSee :class:`~torch.nn.ReLU6` for more details.", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.sigmoid", "Docstring": "sigmoid(input) -> Tensor\n\nApplies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\n\nSee :class:`~torch.nn.Sigmoid` for more details.", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.softmax", "Docstring": "Apply a softmax function.\n\nSoftmax is defined as:\n\n:math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n\nIt is applied to all slices along dim, and will re-scale them so that the elements\nlie in the range `[0, 1]` and sum to 1.\n\nSee :class:`~torch.nn.Softmax` for more details.\n\nArgs:\n    input (Tensor): input\n    dim (int): A dimension along which softmax will be computed.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n      If specified, the input tensor is casted to :attr:`dtype` before the operation\n      is performed. This is useful for preventing data type overflows. Default: None.\n\n.. note::\n    This function doesn't work directly with NLLLoss,\n    which expects the Log to be computed between the Softmax and itself.\n    Use log_softmax instead (it's faster and has better numerical properties).", "Library": "PyTorch"}
{"API_Name": "torch.nn.functional.tanh", "Docstring": "tanh(input) -> Tensor\n\nApplies element-wise,\n:math:`\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}`\n\nSee :class:`~torch.nn.Tanh` for more details.", "Library": "PyTorch"}
{"API_Name": "torch.nn.GaussianNLLLoss", "Docstring": "Gaussian negative log likelihood loss.\n\nThe targets are treated as samples from Gaussian distributions with\nexpectations and variances predicted by the neural network. For a\n``target`` tensor modelled as having Gaussian distribution with a tensor\nof expectations ``input`` and a tensor of positive variances ``var`` the loss is:\n\n.. math::\n    \\text{loss} = \\frac{1}{2}\\left(\\log\\left(\\text{max}\\left(\\text{var},\n    \\ \\text{eps}\\right)\\right) + \\frac{\\left(\\text{input} - \\text{target}\\right)^2}\n    {\\text{max}\\left(\\text{var}, \\ \\text{eps}\\right)}\\right) + \\text{const.}\n\nwhere :attr:`eps` is used for stability. By default, the constant term of\nthe loss function is omitted unless :attr:`full` is ``True``. If ``var`` is not the same\nsize as ``input`` (due to a homoscedastic assumption), it must either have a final dimension\nof 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.\n\nArgs:\n    full (bool, optional): include the constant term in the loss\n        calculation. Default: ``False``.\n    eps (float, optional): value used to clamp ``var`` (see note below), for\n        stability. Default: 1e-6.\n    reduction (str, optional): specifies the reduction to apply to the\n        output:``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n        will be applied, ``'mean'``: the output is the average of all batch\n        member losses, ``'sum'``: the output is the sum of all batch member\n        losses. Default: ``'mean'``.\n\nShape:\n    - Input: :math:`(N, *)` or :math:`(*)` where :math:`*` means any number of additional\n      dimensions\n    - Target: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input\n      but with one dimension equal to 1 (to allow for broadcasting)\n    - Var: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input but\n      with one dimension equal to 1, or same shape as the input but with one fewer\n      dimension (to allow for broadcasting)\n    - Output: scalar if :attr:`reduction` is ``'mean'`` (default) or\n      ``'sum'``. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same\n      shape as the input\n\nExamples::\n    >>> loss = nn.GaussianNLLLoss()\n    >>> input = torch.randn(5, 2, requires_grad=True)\n    >>> target = torch.randn(5, 2)\n    >>> var = torch.ones(5, 2, requires_grad=True)  # heteroscedastic\n    >>> output = loss(input, target, var)\n    >>> output.backward()\n\n    >>> loss = nn.GaussianNLLLoss()\n    >>> input = torch.randn(5, 2, requires_grad=True)\n    >>> target = torch.randn(5, 2)\n    >>> var = torch.ones(5, 1, requires_grad=True)  # homoscedastic\n    >>> output = loss(input, target, var)\n    >>> output.backward()\n\nNote:\n    The clamping of ``var`` is ignored with respect to autograd, and so the\n    gradients are unaffected by it.\n\nReference:\n    Nix, D. A. and Weigend, A. S., \"Estimating the mean and variance of the\n    target probability distribution\", Proceedings of 1994 IEEE International\n    Conference on Neural Networks (ICNN'94), Orlando, FL, USA, 1994, pp. 55-60\n    vol.1, doi: 10.1109/ICNN.1994.374138.", "Library": "PyTorch"}
{"API_Name": "torch.nn.GRU", "Docstring": "__init__(input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,device=None,dtype=None)\n\nApply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\nFor each element in the input sequence, each layer computes the following\nfunction:\n\n.. math::\n    \\begin{array}{ll}\n        r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n        z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n        n_t = \\tanh(W_{in} x_t + b_{in} + r_t \\odot (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n        h_t = (1 - z_t) \\odot n_t + z_t \\odot h_{(t-1)}\n    \\end{array}\n\nwhere :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the input\nat time `t`, :math:`h_{(t-1)}` is the hidden state of the layer\nat time `t-1` or the initial hidden state at time `0`, and :math:`r_t`,\n:math:`z_t`, :math:`n_t` are the reset, update, and new gates, respectively.\n:math:`\\sigma` is the sigmoid function, and :math:`\\odot` is the Hadamard product.\n\nIn a multilayer GRU, the input :math:`x^{(l)}_t` of the :math:`l` -th layer\n(:math:`l \\ge 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by\ndropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random\nvariable which is :math:`0` with probability :attr:`dropout`.\n\nArgs:\n    input_size: The number of expected features in the input `x`\n    hidden_size: The number of features in the hidden state `h`\n    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n        would mean stacking two GRUs together to form a `stacked GRU`,\n        with the second GRU taking in outputs of the first GRU and\n        computing the final results. Default: 1\n    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n        Default: ``True``\n    batch_first: If ``True``, then the input and output tensors are provided\n        as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n        Note that this does not apply to hidden or cell states. See the\n        Inputs/Outputs sections below for details.  Default: ``False``\n    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n        GRU layer except the last layer, with dropout probability equal to\n        :attr:`dropout`. Default: 0\n    bidirectional: If ``True``, becomes a bidirectional GRU. Default: ``False``\n\nInputs: input, h_0\n    * **input**: tensor of shape :math:`(L, H_{in})` for unbatched input,\n      :math:`(L, N, H_{in})` when ``batch_first=False`` or\n      :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n      the input sequence.  The input can also be a packed variable length sequence.\n      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n    * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n      :math:`(D * \\text{num\\_layers}, N, H_{out})`\n      containing the initial hidden state for the input sequence. Defaults to zeros if not provided.\n\n    where:\n\n    .. math::\n        \\begin{aligned}\n            N ={} & \\text{batch size} \\\\\n            L ={} & \\text{sequence length} \\\\\n            D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n            H_{in} ={} & \\text{input\\_size} \\\\\n            H_{out} ={} & \\text{hidden\\_size}\n        \\end{aligned}\n\nOutputs: output, h_n\n    * **output**: tensor of shape :math:`(L, D * H_{out})` for unbatched input,\n      :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n      :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n      `(h_t)` from the last layer of the GRU, for each `t`. If a\n      :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n      will also be a packed sequence.\n    * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n      :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the final hidden state\n      for the input sequence.\n\nAttributes:\n    weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n        (W_ir|W_iz|W_in), of shape `(3*hidden_size, input_size)` for `k = 0`.\n        Otherwise, the shape is `(3*hidden_size, num_directions * hidden_size)`\n    weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n        (W_hr|W_hz|W_hn), of shape `(3*hidden_size, hidden_size)`\n    bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n        (b_ir|b_iz|b_in), of shape `(3*hidden_size)`\n    bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n        (b_hr|b_hz|b_hn), of shape `(3*hidden_size)`\n\n.. note::\n    All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n    where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n\n.. note::\n    For bidirectional GRUs, forward and backward are directions 0 and 1 respectively.\n    Example of splitting the output layers when ``batch_first=False``:\n    ``output.view(seq_len, batch, num_directions, hidden_size)``.\n\n.. note::\n    ``batch_first`` argument is ignored for unbatched inputs.\n\n.. note::\n    The calculation of new gate :math:`n_t` subtly differs from the original paper and other frameworks.\n    In the original implementation, the Hadamard product :math:`(\\odot)` between :math:`r_t` and the\n    previous hidden state :math:`h_{(t-1)}` is done before the multiplication with the weight matrix\n    `W` and addition of bias:\n\n    .. math::\n        \\begin{aligned}\n            n_t = \\tanh(W_{in} x_t + b_{in} + W_{hn} ( r_t \\odot h_{(t-1)} ) + b_{hn})\n        \\end{aligned}\n\n    This is in contrast to PyTorch implementation, which is done after :math:`W_{hn} h_{(t-1)}`\n\n    .. math::\n        \\begin{aligned}\n            n_t = \\tanh(W_{in} x_t + b_{in} + r_t \\odot (W_{hn} h_{(t-1)}+ b_{hn}))\n        \\end{aligned}\n\n    This implementation differs on purpose for efficiency.\n\n.. include:: ../cudnn_persistent_rnn.rst\n\nExamples::\n\n    >>> rnn = nn.GRU(10, 20, 2)\n    >>> input = torch.randn(5, 3, 10)\n    >>> h0 = torch.randn(2, 3, 20)\n    >>> output, hn = rnn(input, h0)", "Library": "PyTorch"}
{"API_Name": "torch.nn.HingeEmbeddingLoss", "Docstring": "Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`\n(containing 1 or -1).\nThis is usually used for measuring whether two inputs are similar or\ndissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically\nused for learning nonlinear embeddings or semi-supervised learning.\n\nThe loss function for :math:`n`-th sample in the mini-batch is\n\n.. math::\n    l_n = \\begin{cases}\n        x_n, & \\text{if}\\; y_n = 1,\\\\\n        \\max \\{0, margin - x_n\\}, & \\text{if}\\; y_n = -1,\n    \\end{cases}\n\nand the total loss functions is\n\n.. math::\n    \\ell(x, y) = \\begin{cases}\n        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nwhere :math:`L = \\{l_1,\\dots,l_N\\}^\\top`.\n\nArgs:\n    margin (float, optional): Has a default value of `1`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation\n      operates over all the elements.\n    - Target: :math:`(*)`, same shape as the input\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input", "Library": "PyTorch"}
{"API_Name": "torch.nn.HuberLoss", "Docstring": "Creates a criterion that uses a squared term if the absolute\nelement-wise error falls below delta and a delta-scaled L1 term otherwise.\nThis loss combines advantages of both :class:`L1Loss` and :class:`MSELoss`; the\ndelta-scaled L1 region makes the loss less sensitive to outliers than :class:`MSELoss`,\nwhile the L2 region provides smoothness over :class:`L1Loss` near 0. See\n`Huber loss <https://en.wikipedia.org/wiki/Huber_loss>`_ for more information.\n\nFor a batch of size :math:`N`, the unreduced loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1, ..., l_N\\}^T\n\nwith\n\n.. math::\n    l_n = \\begin{cases}\n    0.5 (x_n - y_n)^2, & \\text{if } |x_n - y_n| < delta \\\\\n    delta * (|x_n - y_n| - 0.5 * delta), & \\text{otherwise }\n    \\end{cases}\n\nIf `reduction` is not `none`, then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\n.. note::\n    When delta is set to 1, this loss is equivalent to :class:`SmoothL1Loss`.\n    In general, this loss differs from :class:`SmoothL1Loss` by a factor of delta (AKA beta\n    in Smooth L1).\n    See :class:`SmoothL1Loss` for additional discussion on the differences in behavior\n    between the two losses.\n\nArgs:\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Default: ``'mean'``\n    delta (float, optional): Specifies the threshold at which to change between delta-scaled L1 and L2 loss.\n        The value must be positive.  Default: 1.0\n\nShape:\n    - Input: :math:`(*)` where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same shape as the input.", "Library": "PyTorch"}
{"API_Name": "torch.nn.init.calculate_gain", "Docstring": "Return the recommended gain value for the given nonlinearity function.\n\nThe values are as follows:\n\n================= ====================================================\nnonlinearity      gain\n================= ====================================================\nLinear / Identity :math:`1`\nConv{1,2,3}D      :math:`1`\nSigmoid           :math:`1`\nTanh              :math:`\\frac{5}{3}`\nReLU              :math:`\\sqrt{2}`\nLeaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\nSELU              :math:`\\frac{3}{4}`\n================= ====================================================\n\n.. warning::\n    In order to implement `Self-Normalizing Neural Networks`_ ,\n    you should use ``nonlinearity='linear'`` instead of ``nonlinearity='selu'``.\n    This gives the initial weights a variance of ``1 / N``,\n    which is necessary to induce a stable fixed point in the forward pass.\n    In contrast, the default gain for ``SELU`` sacrifices the normalization\n    effect for more stable gradient flow in rectangular layers.\n\nArgs:\n    nonlinearity: the non-linear function (`nn.functional` name)\n    param: optional parameter for the non-linear function\n\nExamples:\n    >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n\n.. _Self-Normalizing Neural Networks: https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html", "Library": "PyTorch"}
{"API_Name": "torch.nn.init.kaiming_uniform_", "Docstring": "Fill the input `Tensor` with values using a Kaiming uniform distribution.\n\nThe method is described in `Delving deep into rectifiers: Surpassing\nhuman-level performance on ImageNet classification` - He, K. et al. (2015).\nThe resulting tensor will have values sampled from\n:math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where\n\n.. math::\n    \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}\n\nAlso known as He initialization.\n\nArgs:\n    tensor: an n-dimensional `torch.Tensor`\n    a: the negative slope of the rectifier used after this layer (only\n        used with ``'leaky_relu'``)\n    mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n        preserves the magnitude of the variance of the weights in the\n        forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n        backwards pass.\n    nonlinearity: the non-linear function (`nn.functional` name),\n        recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n    generator: the torch Generator to sample from (default: None)\n\nExamples:\n    >>> w = torch.empty(3, 5)\n    >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')", "Library": "PyTorch"}
{"API_Name": "torch.nn.init.orthogonal_", "Docstring": "Fill the input `Tensor` with a (semi) orthogonal matrix.\n\nDescribed in `Exact solutions to the nonlinear dynamics of learning in deep\nlinear neural networks` - Saxe, A. et al. (2013). The input tensor must have\nat least 2 dimensions, and for tensors with more than 2 dimensions the\ntrailing dimensions are flattened.\n\nArgs:\n    tensor: an n-dimensional `torch.Tensor`, where :math:`n \\geq 2`\n    gain: optional scaling factor\n    generator: the torch Generator to sample from (default: None)\n\nExamples:\n    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK)\n    >>> w = torch.empty(3, 5)\n    >>> nn.init.orthogonal_(w)", "Library": "PyTorch"}
{"API_Name": "torch.nn.init.xavier_uniform_", "Docstring": "Fill the input `Tensor` with values using a Xavier uniform distribution.\n\nThe method is described in `Understanding the difficulty of training\ndeep feedforward neural networks` - Glorot, X. & Bengio, Y. (2010).\nThe resulting tensor will have values sampled from\n:math:`\\mathcal{U}(-a, a)` where\n\n.. math::\n    a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}\n\nAlso known as Glorot initialization.\n\nArgs:\n    tensor: an n-dimensional `torch.Tensor`\n    gain: an optional scaling factor\n    generator: the torch Generator to sample from (default: None)\n\nExamples:\n    >>> w = torch.empty(3, 5)\n    >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))", "Library": "PyTorch"}
{"API_Name": "torch.nn.KLDivLoss", "Docstring": "The Kullback-Leibler divergence loss.\n\nFor tensors of the same shape :math:`y_{\\text{pred}},\\ y_{\\text{true}}`,\nwhere :math:`y_{\\text{pred}}` is the :attr:`input` and :math:`y_{\\text{true}}` is the\n:attr:`target`, we define the **pointwise KL-divergence** as\n\n.. math::\n\n    L(y_{\\text{pred}},\\ y_{\\text{true}})\n        = y_{\\text{true}} \\cdot \\log \\frac{y_{\\text{true}}}{y_{\\text{pred}}}\n        = y_{\\text{true}} \\cdot (\\log y_{\\text{true}} - \\log y_{\\text{pred}})\n\nTo avoid underflow issues when computing this quantity, this loss expects the argument\n:attr:`input` in the log-space. The argument :attr:`target` may also be provided in the\nlog-space if :attr:`log_target`\\ `= True`.\n\nTo summarise, this function is roughly equivalent to computing\n\n.. code-block:: python\n\n    if not log_target: # default\n        loss_pointwise = target * (target.log() - input)\n    else:\n        loss_pointwise = target.exp() * (target - input)\n\nand then reducing this result depending on the argument :attr:`reduction` as\n\n.. code-block:: python\n\n    if reduction == \"mean\":  # default\n        loss = loss_pointwise.mean()\n    elif reduction == \"batchmean\":  # mathematically correct\n        loss = loss_pointwise.sum() / input.size(0)\n    elif reduction == \"sum\":\n        loss = loss_pointwise.sum()\n    else:  # reduction == \"none\"\n        loss = loss_pointwise\n\n.. note::\n    As all the other losses in PyTorch, this function expects the first argument,\n    :attr:`input`, to be the output of the model (e.g. the neural network)\n    and the second, :attr:`target`, to be the observations in the dataset.\n    This differs from the standard mathematical notation :math:`KL(P\\ ||\\ Q)` where\n    :math:`P` denotes the distribution of the observations and :math:`Q` denotes the model.\n\n.. warning::\n    :attr:`reduction`\\ `= \"mean\"` doesn't return the true KL divergence value, please use\n    :attr:`reduction`\\ `= \"batchmean\"` which aligns with the mathematical definition.\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to `False`, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is `False`. Default: `True`\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is `False`, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: `True`\n    reduction (str, optional): Specifies the reduction to apply to the output. Default: `\"mean\"`\n    log_target (bool, optional): Specifies whether `target` is the log space. Default: `False`\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar by default. If :attr:`reduction` is `'none'`, then :math:`(*)`,\n      same shape as the input.\n\nExamples::\n    >>> kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n    >>> # input should be a distribution in the log space\n    >>> input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)\n    >>> # Sample a batch of distributions. Usually this would come from the dataset\n    >>> target = F.softmax(torch.rand(3, 5), dim=1)\n    >>> output = kl_loss(input, target)\n\n    >>> kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n    >>> log_target = F.log_softmax(torch.rand(3, 5), dim=1)\n    >>> output = kl_loss(input, log_target)", "Library": "PyTorch"}
{"API_Name": "torch.nn.Linear", "Docstring": "Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n\nThis module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n\nOn certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n\nArgs:\n    in_features: size of each input sample\n    out_features: size of each output sample\n    bias: If set to ``False``, the layer will not learn an additive bias.\n        Default: ``True``\n\nShape:\n    - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n      dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n    - Output: :math:`(*, H_{out})` where all but the last dimension\n      are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n\nAttributes:\n    weight: the learnable weights of the module of shape\n        :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n        initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n        :math:`k = \\frac{1}{\\text{in\\_features}}`\n    bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n            If :attr:`bias` is ``True``, the values are initialized from\n            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\n\nExamples::\n\n    >>> m = nn.Linear(20, 30)\n    >>> input = torch.randn(128, 20)\n    >>> output = m(input)\n    >>> print(output.size())\n    torch.Size([128, 30])", "Library": "PyTorch"}
{"API_Name": "torch.nn.LogSoftmax", "Docstring": "Applies the :math:`\\log(\\text{Softmax}(x))` function to an n-dimensional input Tensor.\n\nThe LogSoftmax formulation can be simplified as:\n\n.. math::\n    \\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)\n\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\n\nArgs:\n    dim (int): A dimension along which LogSoftmax will be computed.\n\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [-inf, 0)\n\nExamples::\n\n    >>> m = nn.LogSoftmax(dim=1)\n    >>> input = torch.randn(2, 3)\n    >>> output = m(input)", "Library": "PyTorch"}
{"API_Name": "torch.nn.LSTM", "Docstring": "__init__(input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,proj_size=0,device=None,dtype=None)\n\nApply a multi-layer long short-term memory (LSTM) RNN to an input sequence.\nFor each element in the input sequence, each layer computes the following\nfunction:\n\n.. math::\n    \\begin{array}{ll} \\\\\n        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n        h_t = o_t \\odot \\tanh(c_t) \\\\\n    \\end{array}\n\nwhere :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\nstate at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{t-1}`\nis the hidden state of the layer at time `t-1` or the initial hidden\nstate at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,\n:math:`o_t` are the input, forget, cell, and output gates, respectively.\n:math:`\\sigma` is the sigmoid function, and :math:`\\odot` is the Hadamard product.\n\nIn a multilayer LSTM, the input :math:`x^{(l)}_t` of the :math:`l` -th layer\n(:math:`l \\ge 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by\ndropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random\nvariable which is :math:`0` with probability :attr:`dropout`.\n\nIf ``proj_size > 0`` is specified, LSTM with projections will be used. This changes\nthe LSTM cell in the following way. First, the dimension of :math:`h_t` will be changed from\n``hidden_size`` to ``proj_size`` (dimensions of :math:`W_{hi}` will be changed accordingly).\nSecond, the output hidden state of each layer will be multiplied by a learnable projection\nmatrix: :math:`h_t = W_{hr}h_t`. Note that as a consequence of this, the output\nof LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact\ndimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.\n\nArgs:\n    input_size: The number of expected features in the input `x`\n    hidden_size: The number of features in the hidden state `h`\n    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n        would mean stacking two LSTMs together to form a `stacked LSTM`,\n        with the second LSTM taking in outputs of the first LSTM and\n        computing the final results. Default: 1\n    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n        Default: ``True``\n    batch_first: If ``True``, then the input and output tensors are provided\n        as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n        Note that this does not apply to hidden or cell states. See the\n        Inputs/Outputs sections below for details.  Default: ``False``\n    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n        LSTM layer except the last layer, with dropout probability equal to\n        :attr:`dropout`. Default: 0\n    bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n    proj_size: If ``> 0``, will use LSTM with projections of corresponding size. Default: 0\n\nInputs: input, (h_0, c_0)\n    * **input**: tensor of shape :math:`(L, H_{in})` for unbatched input,\n      :math:`(L, N, H_{in})` when ``batch_first=False`` or\n      :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n      the input sequence.  The input can also be a packed variable length sequence.\n      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n    * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n      initial hidden state for each element in the input sequence.\n      Defaults to zeros if (h_0, c_0) is not provided.\n    * **c_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{cell})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n      initial cell state for each element in the input sequence.\n      Defaults to zeros if (h_0, c_0) is not provided.\n\n    where:\n\n    .. math::\n        \\begin{aligned}\n            N ={} & \\text{batch size} \\\\\n            L ={} & \\text{sequence length} \\\\\n            D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n            H_{in} ={} & \\text{input\\_size} \\\\\n            H_{cell} ={} & \\text{hidden\\_size} \\\\\n            H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n        \\end{aligned}\n\nOutputs: output, (h_n, c_n)\n    * **output**: tensor of shape :math:`(L, D * H_{out})` for unbatched input,\n      :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n      :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n      `(h_t)` from the last layer of the LSTM, for each `t`. If a\n      :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n      will also be a packed sequence. When ``bidirectional=True``, `output` will contain\n      a concatenation of the forward and reverse hidden states at each time step in the sequence.\n    * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n      final hidden state for each element in the sequence. When ``bidirectional=True``,\n      `h_n` will contain a concatenation of the final forward and reverse hidden states, respectively.\n    * **c_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{cell})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n      final cell state for each element in the sequence. When ``bidirectional=True``,\n      `c_n` will contain a concatenation of the final forward and reverse cell states, respectively.\n\nAttributes:\n    weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n        `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size, input_size)` for `k = 0`.\n        Otherwise, the shape is `(4*hidden_size, num_directions * hidden_size)`. If\n        ``proj_size > 0`` was specified, the shape will be\n        `(4*hidden_size, num_directions * proj_size)` for `k > 0`\n    weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n        `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size, hidden_size)`. If ``proj_size > 0``\n        was specified, the shape will be `(4*hidden_size, proj_size)`.\n    bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n        `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n    bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n        `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n    weight_hr_l[k] : the learnable projection weights of the :math:`\\text{k}^{th}` layer\n        of shape `(proj_size, hidden_size)`. Only present when ``proj_size > 0`` was\n        specified.\n    weight_ih_l[k]_reverse: Analogous to `weight_ih_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    weight_hh_l[k]_reverse:  Analogous to `weight_hh_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    bias_ih_l[k]_reverse:  Analogous to `bias_ih_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    bias_hh_l[k]_reverse:  Analogous to `bias_hh_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    weight_hr_l[k]_reverse:  Analogous to `weight_hr_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True`` and ``proj_size > 0`` was specified.\n\n.. note::\n    All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n    where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n\n.. note::\n    For bidirectional LSTMs, forward and backward are directions 0 and 1 respectively.\n    Example of splitting the output layers when ``batch_first=False``:\n    ``output.view(seq_len, batch, num_directions, hidden_size)``.\n\n.. note::\n    For bidirectional LSTMs, `h_n` is not equivalent to the last element of `output`; the\n    former contains the final forward and reverse hidden states, while the latter contains the\n    final forward hidden state and the initial reverse hidden state.\n\n.. note::\n    ``batch_first`` argument is ignored for unbatched inputs.\n\n.. note::\n    ``proj_size`` should be smaller than ``hidden_size``.\n\n.. include:: ../cudnn_rnn_determinism.rst\n\n.. include:: ../cudnn_persistent_rnn.rst\n\nExamples::\n\n    >>> rnn = nn.LSTM(10, 20, 2)\n    >>> input = torch.randn(5, 3, 10)\n    >>> h0 = torch.randn(2, 3, 20)\n    >>> c0 = torch.randn(2, 3, 20)\n    >>> output, (hn, cn) = rnn(input, (h0, c0))", "Library": "PyTorch"}
{"API_Name": "torch.nn.L1Loss", "Docstring": "Creates a criterion that measures the mean absolute error (MAE) between each element in\nthe input :math:`x` and target :math:`y`.\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = \\left| x_n - y_n \\right|,\n\nwhere :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\n:math:`x` and :math:`y` are tensors of arbitrary shapes with a total\nof :math:`n` elements each.\n\nThe sum operation still operates over all the elements, and divides by :math:`n`.\n\nThe division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.\n\nSupports real-valued and complex-valued inputs.\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar. If :attr:`reduction` is ``'none'``, then\n      :math:`(*)`, same shape as the input.\n\nExamples::\n\n    >>> loss = nn.L1Loss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5)\n    >>> output = loss(input, target)\n    >>> output.backward()", "Library": "PyTorch"}
{"API_Name": "torch.nn.MarginRankingLoss", "Docstring": "Creates a criterion that measures the loss given\ninputs :math:`x1`, :math:`x2`, two 1D mini-batch or 0D `Tensors`,\nand a label 1D mini-batch or 0D `Tensor` :math:`y` (containing 1 or -1).\n\nIf :math:`y = 1` then it assumed the first input should be ranked higher\n(have a larger value) than the second input, and vice-versa for :math:`y = -1`.\n\nThe loss function for each pair of samples in the mini-batch is:\n\n.. math::\n    \\text{loss}(x1, x2, y) = \\max(0, -y * (x1 - x2) + \\text{margin})\n\nArgs:\n    margin (float, optional): Has a default value of :math:`0`.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input1: :math:`(N)` or :math:`()` where `N` is the batch size.\n    - Input2: :math:`(N)` or :math:`()`, same shape as the Input1.\n    - Target: :math:`(N)` or :math:`()`, same shape as the inputs.\n    - Output: scalar. If :attr:`reduction` is ``'none'`` and Input size is not :math:`()`, then :math:`(N)`.\n\nExamples::\n\n    >>> loss = nn.MarginRankingLoss()\n    >>> input1 = torch.randn(3, requires_grad=True)\n    >>> input2 = torch.randn(3, requires_grad=True)\n    >>> target = torch.randn(3).sign()\n    >>> output = loss(input1, input2, target)\n    >>> output.backward()", "Library": "PyTorch"}
{"API_Name": "torch.nn.MaxPool2d", "Docstring": "Applies a 2D max pooling over an input signal composed of several input planes.\n\nIn the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,\noutput :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`\ncan be precisely described as:\n\n.. math::\n    \\begin{aligned}\n        out(N_i, C_j, h, w) ={} & \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\\n                                & \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m,\n                                               \\text{stride[1]} \\times w + n)\n    \\end{aligned}\n\nIf :attr:`padding` is non-zero, then the input is implicitly padded with negative infinity on both sides\nfor :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.\nIt is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n\nNote:\n    When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding\n    or the input. Sliding windows that would start in the right padded region are ignored.\n\nThe parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n\n    - a single ``int`` -- in which case the same value is used for the height and width dimension\n    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n      and the second `int` for the width dimension\n\nArgs:\n    kernel_size: the size of the window to take a max over\n    stride: the stride of the window. Default value is :attr:`kernel_size`\n    padding: Implicit negative infinity padding to be added on both sides\n    dilation: a parameter that controls the stride of elements in the window\n    return_indices: if ``True``, will return the max indices along with the outputs.\n                    Useful for :class:`torch.nn.MaxUnpool2d` later\n    ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape\n\nShape:\n    - Input: :math:`(N, C, H_{in}, W_{in})` or :math:`(C, H_{in}, W_{in})`\n    - Output: :math:`(N, C, H_{out}, W_{out})` or :math:`(C, H_{out}, W_{out})`, where\n\n      .. math::\n          H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\n                \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor\n\n      .. math::\n          W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\n                \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor\n\nExamples::\n\n    >>> # pool of square window of size=3, stride=2\n    >>> m = nn.MaxPool2d(3, stride=2)\n    >>> # pool of non-square window\n    >>> m = nn.MaxPool2d((3, 2), stride=(2, 1))\n    >>> input = torch.randn(20, 16, 50, 32)\n    >>> output = m(input)\n\n.. _link:\n    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md", "Library": "PyTorch"}
{"API_Name": "torch.nn.Module", "Docstring": "Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will have their\nparameters converted too when you call :meth:`to`, etc.\n\n.. note::\n    As per the example above, an ``__init__()`` call to the parent class\n    must be made before assignment on the child.\n\n:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool", "Library": "PyTorch"}
{"API_Name": "torch.nn.Module.parameters", "Docstring": "Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    recurse (bool): if True, then yields parameters of this module\n        and all submodules. Otherwise, yields only parameters that\n        are direct members of this module.\n\nYields:\n    Parameter: module parameter\n\nExample::\n\n    >>> # xdoctest: +SKIP(\"undefined vars\")\n    >>> for param in model.parameters():\n    >>>     print(type(param), param.size())\n    <class 'torch.Tensor'> (20L,)\n    <class 'torch.Tensor'> (20L, 1L, 5L, 5L)", "Library": "PyTorch"}
{"API_Name": "torch.nn.Module.to", "Docstring": "Move and/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n   :noindex:\n\n.. function:: to(dtype, non_blocking=False)\n   :noindex:\n\n.. function:: to(tensor, non_blocking=False)\n   :noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n   :noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\n    This method modifies the module in-place.\n\nArgs:\n    device (:class:`torch.device`): the desired device of the parameters\n        and buffers in this module\n    dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n        the parameters and buffers in this module\n    tensor (torch.Tensor): Tensor whose dtype and device are the desired\n        dtype and device for all parameters and buffers in this module\n    memory_format (:class:`torch.memory_format`): the desired memory\n        format for 4D parameters and buffers in this module (keyword\n        only argument)\n\nReturns:\n    Module: self\n\nExamples::\n\n    >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n    >>> linear = nn.Linear(2, 2)\n    >>> linear.weight\n    Parameter containing:\n    tensor([[ 0.1913, -0.3420],\n            [-0.5113, -0.2325]])\n    >>> linear.to(torch.double)\n    Linear(in_features=2, out_features=2, bias=True)\n    >>> linear.weight\n    Parameter containing:\n    tensor([[ 0.1913, -0.3420],\n            [-0.5113, -0.2325]], dtype=torch.float64)\n    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n    >>> gpu1 = torch.device(\"cuda:1\")\n    >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n    Linear(in_features=2, out_features=2, bias=True)\n    >>> linear.weight\n    Parameter containing:\n    tensor([[ 0.1914, -0.3420],\n            [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n    >>> cpu = torch.device(\"cpu\")\n    >>> linear.to(cpu)\n    Linear(in_features=2, out_features=2, bias=True)\n    >>> linear.weight\n    Parameter containing:\n    tensor([[ 0.1914, -0.3420],\n            [-0.5112, -0.2324]], dtype=torch.float16)\n\n    >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n    >>> linear.weight\n    Parameter containing:\n    tensor([[ 0.3741+0.j,  0.2382+0.j],\n            [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n    >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n    tensor([[0.6122+0.j, 0.1150+0.j],\n            [0.6122+0.j, 0.1150+0.j],\n            [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)", "Library": "PyTorch"}
{"API_Name": "torch.nn.MSELoss", "Docstring": "Creates a criterion that measures the mean squared error (squared L2 norm) between\neach element in the input :math:`x` and target :math:`y`.\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = \\left( x_n - y_n \\right)^2,\n\nwhere :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then:\n\n.. math::\n    \\ell(x, y) =\n    \\begin{cases}\n        \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n        \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\n:math:`x` and :math:`y` are tensors of arbitrary shapes with a total\nof :math:`n` elements each.\n\nThe mean operation still operates over all the elements, and divides by :math:`n`.\n\nThe division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.\n\nArgs:\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n\nExamples::\n\n    >>> loss = nn.MSELoss()\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> target = torch.randn(3, 5)\n    >>> output = loss(input, target)\n    >>> output.backward()", "Library": "PyTorch"}
{"API_Name": "torch.nn.NLLLoss", "Docstring": "The negative log likelihood loss. It is useful to train a classification\nproblem with `C` classes.\n\nIf provided, the optional argument :attr:`weight` should be a 1D Tensor assigning\nweight to each of the classes. This is particularly useful when you have an\nunbalanced training set.\n\nThe `input` given through a forward call is expected to contain\nlog-probabilities of each class. `input` has to be a Tensor of size either\n:math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`\nwith :math:`K \\geq 1` for the `K`-dimensional case. The latter is useful for\nhigher dimension inputs, such as computing NLL loss per-pixel for 2D images.\n\nObtaining log-probabilities in a neural network is easily achieved by\nadding a  `LogSoftmax`  layer in the last layer of your network.\nYou may use `CrossEntropyLoss` instead, if you prefer not to add an extra\nlayer.\n\nThe `target` that this loss expects should be a class index in the range :math:`[0, C-1]`\nwhere `C = number of classes`; if `ignore_index` is specified, this loss also accepts\nthis class index (this index may not necessarily be in the class range).\n\nThe unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n\n.. math::\n    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n    l_n = - w_{y_n} x_{n,y_n}, \\quad\n    w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},\n\nwhere :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and\n:math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n(default ``'mean'``), then\n\n.. math::\n    \\ell(x, y) = \\begin{cases}\n        \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\n        \\text{if reduction} = \\text{`mean';}\\\\\n        \\sum_{n=1}^N l_n,  &\n        \\text{if reduction} = \\text{`sum'.}\n    \\end{cases}\n\nArgs:\n    weight (Tensor, optional): a manual rescaling weight given to each\n        class. If given, it has to be a Tensor of size `C`. Otherwise, it is\n        treated as if having all ones.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``None``\n    ignore_index (int, optional): Specifies a target value that is ignored\n        and does not contribute to the input gradient. When\n        :attr:`size_average` is ``True``, the loss is averaged over\n        non-ignored targets.\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``None``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n        be applied, ``'mean'``: the weighted mean of the output is taken,\n        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in\n        the meantime, specifying either of those two args will override\n        :attr:`reduction`. Default: ``'mean'``\n\nShape::\n    - Input: :math:`(N, C)` or :math:`(C)`, where `C = number of classes`, `N = batch size`, or\n      :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n      in the case of `K`-dimensional loss.\n    - Target: :math:`(N)` or :math:`()`, where each value is\n      :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\n      :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\n      K-dimensional loss.\n    - Output: If :attr:`reduction` is ``'none'``, shape :math:`(N)` or\n      :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of K-dimensional loss.\n      Otherwise, scalar.\n\nExamples::\n\n    >>> log_softmax = nn.LogSoftmax(dim=1)\n    >>> loss_fn = nn.NLLLoss()\n    >>> # input to NLLLoss is of size N x C = 3 x 5\n    >>> input = torch.randn(3, 5, requires_grad=True)\n    >>> # each element in target must have 0 <= value < C\n    >>> target = torch.tensor([1, 0, 4])\n    >>> loss = loss_fn(log_softmax(input), target)\n    >>> loss.backward()\n    >>>\n    >>>\n    >>> # 2D loss example (used, for example, with image inputs)\n    >>> N, C = 5, 4\n    >>> loss_fn = nn.NLLLoss()\n    >>> data = torch.randn(N, 16, 10, 10)\n    >>> conv = nn.Conv2d(16, C, (3, 3))\n    >>> log_softmax = nn.LogSoftmax(dim=1)\n    >>> # output of conv forward is of shape [N, C, 8, 8]\n    >>> output = log_softmax(conv(data))\n    >>> # each element in target must have 0 <= value < C\n    >>> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n    >>> # input to NLLLoss is of size N x C x height (8) x width (8)\n    >>> loss = loss_fn(output, target)\n    >>> loss.backward()", "Library": "PyTorch"}
{"API_Name": "torch.nn.Parameter", "Docstring": "A kind of Tensor that is to be considered a module parameter.\n\nParameters are :class:`~torch.Tensor` subclasses, that have a\nvery special property when used with :class:`Module` s - when they're\nassigned as Module attributes they are automatically added to the list of\nits parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\nAssigning a Tensor doesn't have such effect. This is because one might\nwant to cache some temporary state, like last hidden state of the RNN, in\nthe model. If there was no such class as :class:`Parameter`, these\ntemporaries would get registered too.\n\nArgs:\n    data (Tensor): parameter tensor.\n    requires_grad (bool, optional): if the parameter requires gradient. Note that\n        the torch.no_grad() context does NOT affect the default behavior of\n        Parameter creation--the Parameter will still have `requires_grad=True` in\n        :class:`~no_grad` mode. See :ref:`locally-disable-grad-doc` for more\n        details. Default: `True`", "Library": "PyTorch"}
{"API_Name": "torch.nn.parallel.data_parallel", "Docstring": "Evaluate module(input) in parallel across the GPUs given in device_ids.\n\nThis is the functional version of the DataParallel module.\n\nArgs:\n    module (Module): the module to evaluate in parallel\n    inputs (Tensor): inputs to the module\n    device_ids (list of int or torch.device): GPU ids on which to replicate module\n    output_device (list of int or torch.device): GPU location of the output  Use -1 to indicate the CPU.\n        (default: device_ids[0])\nReturns:\n    a Tensor containing the result of module(input) located on\n    output_device", "Library": "PyTorch"}
{"API_Name": "torch.nn.PixelShuffle", "Docstring": "Rearrange elements in a tensor according to an upscaling factor.\n\nRearranges elements in a tensor of shape :math:`(*, C \\times r^2, H, W)`\nto a tensor of shape :math:`(*, C, H \\times r, W \\times r)`, where r is an upscale factor.\n\nThis is useful for implementing efficient sub-pixel convolution\nwith a stride of :math:`1/r`.\n\nSee the paper:\n`Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network`_\nby Shi et al. (2016) for more details.\n\nArgs:\n    upscale_factor (int): factor to increase spatial resolution by\n\nShape:\n    - Input: :math:`(*, C_{in}, H_{in}, W_{in})`, where * is zero or more batch dimensions\n    - Output: :math:`(*, C_{out}, H_{out}, W_{out})`, where\n\n.. math::\n    C_{out} = C_{in} \\div \\text{upscale\\_factor}^2\n\n.. math::\n    H_{out} = H_{in} \\times \\text{upscale\\_factor}\n\n.. math::\n    W_{out} = W_{in} \\times \\text{upscale\\_factor}\n\nExamples::\n\n    >>> pixel_shuffle = nn.PixelShuffle(3)\n    >>> input = torch.randn(1, 9, 4, 4)\n    >>> output = pixel_shuffle(input)\n    >>> print(output.size())\n    torch.Size([1, 1, 12, 12])\n\n.. _Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network:\n    https://arxiv.org/abs/1609.05158", "Library": "PyTorch"}
{"API_Name": "torch.nn.PoissonNLLLoss", "Docstring": "Negative log likelihood loss with Poisson distribution of target.\n\nThe loss can be described as:\n\n.. math::\n    \\text{target} \\sim \\mathrm{Poisson}(\\text{input})\n\n    \\text{loss}(\\text{input}, \\text{target}) = \\text{input} - \\text{target} * \\log(\\text{input})\n                                + \\log(\\text{target!})\n\nThe last term can be omitted or approximated with Stirling formula. The\napproximation is used for target values more than 1. For targets less or\nequal to 1 zeros are added to the loss.\n\nArgs:\n    log_input (bool, optional): if ``True`` the loss is computed as\n        :math:`\\exp(\\text{input}) - \\text{target}*\\text{input}`, if ``False`` the loss is\n        :math:`\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})`.\n    full (bool, optional): whether to compute full loss, i. e. to add the\n        Stirling approximation term\n\n        .. math::\n            \\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    eps (float, optional): Small value to avoid evaluation of :math:`\\log(0)` when\n        :attr:`log_input = False`. Default: 1e-8\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nExamples::\n\n    >>> loss = nn.PoissonNLLLoss()\n    >>> log_input = torch.randn(5, 2, requires_grad=True)\n    >>> target = torch.randn(5, 2)\n    >>> output = loss(log_input, target)\n    >>> output.backward()\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Target: :math:`(*)`, same shape as the input.\n    - Output: scalar by default. If :attr:`reduction` is ``'none'``, then :math:`(*)`,\n      the same shape as the input.", "Library": "PyTorch"}
{"API_Name": "torch.nn.ReLU", "Docstring": "Applies the rectified linear unit function element-wise.\n\n:math:`\\text{ReLU}(x) = (x)^+ = \\max(0, x)`\n\nArgs:\n    inplace: can optionally do the operation in-place. Default: ``False``\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/ReLU.png\n\nExamples::\n\n    >>> m = nn.ReLU()\n    >>> input = torch.randn(2)\n    >>> output = m(input)\n\n\n  An implementation of CReLU - https://arxiv.org/abs/1603.05201\n\n    >>> m = nn.ReLU()\n    >>> input = torch.randn(2).unsqueeze(0)\n    >>> output = torch.cat((m(input), m(-input)))", "Library": "PyTorch"}
{"API_Name": "torch.nn.Sequential", "Docstring": "A sequential container.\n\nModules will be added to it in the order they are passed in the\nconstructor. Alternatively, an ``OrderedDict`` of modules can be\npassed in. The ``forward()`` method of ``Sequential`` accepts any\ninput and forwards it to the first module it contains. It then\n\"chains\" outputs to inputs sequentially for each subsequent module,\nfinally returning the output of the last module.\n\nThe value a ``Sequential`` provides over manually calling a sequence\nof modules is that it allows treating the whole container as a\nsingle module, such that performing a transformation on the\n``Sequential`` applies to each of the modules it stores (which are\neach a registered submodule of the ``Sequential``).\n\nWhat's the difference between a ``Sequential`` and a\n:class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it\nsounds like--a list for storing ``Module`` s! On the other hand,\nthe layers in a ``Sequential`` are connected in a cascading way.\n\nExample::\n\n    # Using Sequential to create a small model. When `model` is run,\n    # input will first be passed to `Conv2d(1,20,5)`. The output of\n    # `Conv2d(1,20,5)` will be used as the input to the first\n    # `ReLU`; the output of the first `ReLU` will become the input\n    # for `Conv2d(20,64,5)`. Finally, the output of\n    # `Conv2d(20,64,5)` will be used as input to the second `ReLU`\n    model = nn.Sequential(\n              nn.Conv2d(1,20,5),\n              nn.ReLU(),\n              nn.Conv2d(20,64,5),\n              nn.ReLU()\n            )\n\n    # Using Sequential with OrderedDict. This is functionally the\n    # same as the above code\n    model = nn.Sequential(OrderedDict([\n              ('conv1', nn.Conv2d(1,20,5)),\n              ('relu1', nn.ReLU()),\n              ('conv2', nn.Conv2d(20,64,5)),\n              ('relu2', nn.ReLU())\n            ]))", "Library": "PyTorch"}
{"API_Name": "torch.nn.Sigmoid", "Docstring": "Applies the Sigmoid function element-wise.\n\n.. math::\n    \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Sigmoid.png\n\nExamples::\n\n    >>> m = nn.Sigmoid()\n    >>> input = torch.randn(2)\n    >>> output = m(input)", "Library": "PyTorch"}
{"API_Name": "torch.nn.Softmax", "Docstring": "Applies the Softmax function to an n-dimensional input Tensor.\n\nRescales them so that the elements of the n-dimensional output Tensor\nlie in the range [0,1] and sum to 1.\n\nSoftmax is defined as:\n\n.. math::\n    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n\nWhen the input Tensor is a sparse tensor then the unspecified\nvalues are treated as ``-inf``.\n\nShape:\n    - Input: :math:`(*)` where `*` means, any number of additional\n      dimensions\n    - Output: :math:`(*)`, same shape as the input\n\nReturns:\n    a Tensor of the same dimension and shape as the input with\n    values in the range [0, 1]\n\nArgs:\n    dim (int): A dimension along which Softmax will be computed (so every slice\n        along dim will sum to 1).\n\n.. note::\n    This module doesn't work directly with NLLLoss,\n    which expects the Log to be computed between the Softmax and itself.\n    Use `LogSoftmax` instead (it's faster and has better numerical properties).\n\nExamples::\n\n    >>> m = nn.Softmax(dim=1)\n    >>> input = torch.randn(2, 3)\n    >>> output = m(input)", "Library": "PyTorch"}
{"API_Name": "torch.nn.Tanh", "Docstring": "Applies the Hyperbolic Tangent (Tanh) function element-wise.\n\nTanh is defined as:\n\n.. math::\n    \\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}\n\nShape:\n    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n    - Output: :math:`(*)`, same shape as the input.\n\n.. image:: ../scripts/activation_images/Tanh.png\n\nExamples::\n\n    >>> m = nn.Tanh()\n    >>> input = torch.randn(2)\n    >>> output = m(input)", "Library": "PyTorch"}
{"API_Name": "torch.nn.TripletMarginLoss", "Docstring": "Creates a criterion that measures the triplet loss given an input\ntensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.\nThis is used for measuring a relative similarity between samples. A triplet\nis composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative\nexamples` respectively). The shapes of all input tensors should be\n:math:`(N, D)`.\n\nThe distance swap is described in detail in the paper `Learning shallow\nconvolutional feature descriptors with triplet losses`_ by\nV. Balntas, E. Riba et al.\n\nThe loss function for each sample in the mini-batch is:\n\n.. math::\n    L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}\n\n\nwhere\n\n.. math::\n    d(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p\n\nThe norm is calculated using the specified p value and a small constant :math:`\\varepsilon` is\nadded for numerical stability.\n\nSee also :class:`~torch.nn.TripletMarginWithDistanceLoss`, which computes the\ntriplet margin loss for input tensors using a custom distance function.\n\nArgs:\n    margin (float, optional): Default: :math:`1`.\n    p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.\n    eps (float, optional): Small constant for numerical stability. Default: :math:`1e-6`.\n    swap (bool, optional): The distance swap is described in detail in the paper\n        `Learning shallow convolutional feature descriptors with triplet losses` by\n        V. Balntas, E. Riba et al. Default: ``False``.\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when :attr:`reduce` is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n\nShape:\n    - Input: :math:`(N, D)` or :math:`(D)` where :math:`D` is the vector dimension.\n    - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``'none'`` and\n      input shape is :math:`(N, D)`; a scalar otherwise.\n\nExamples::\n\n>>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n>>> anchor = torch.randn(100, 128, requires_grad=True)\n>>> positive = torch.randn(100, 128, requires_grad=True)\n>>> negative = torch.randn(100, 128, requires_grad=True)\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n\n.. _Learning shallow convolutional feature descriptors with triplet losses:\n    http://www.bmva.org/bmvc/2016/papers/paper119/index.html", "Library": "PyTorch"}
{"API_Name": "torch.nn.utils.rnn.pad_sequence", "Docstring": "Pad a list of variable length Tensors with :attr:`padding_value`.\n\n``pad_sequence`` stacks a list of Tensors along a new dimension, and pads them\nto equal length. :attr:`sequences` can be list of sequences with size ``L x *``,\nwhere `L` is length of the sequence and ``*`` is any number of dimensions\n(including 0). If :attr:`batch_first` is ``False``, the output is of size\n``T x B x *``, and ``B x T x *`` otherwise, where ``B`` is the batch size\n(the number of elements in :attr:`sequences`), ``T`` is the length of the longest\nsequence.\n\nExample:\n    >>> from torch.nn.utils.rnn import pad_sequence\n    >>> a = torch.ones(25, 300)\n    >>> b = torch.ones(22, 300)\n    >>> c = torch.ones(15, 300)\n    >>> pad_sequence([a, b, c]).size()\n    torch.Size([25, 3, 300])\n\nNote:\n    This function returns a Tensor of size ``T x B x *`` or ``B x T x *``\n    where `T` is the length of the longest sequence. This function assumes\n    trailing dimensions and type of all the Tensors in sequences are same.\n\nArgs:\n    sequences (list[Tensor]): list of variable length sequences.\n    batch_first (bool, optional): if ``True``, the output will be in ``B x T x *``\n        format, ``T x B x *`` otherwise.\n    padding_value (float, optional): value for padded elements. Default: 0.\n\nReturns:\n    Tensor of size ``T x B x *`` if :attr:`batch_first` is ``False``.\n    Tensor of size ``B x T x *`` otherwise", "Library": "PyTorch"}
{"API_Name": "torch.no_grad", "Docstring": "Context-manager that disables gradient calculation.\n\nDisabling gradient calculation is useful for inference, when you are sure\nthat you will not call :meth:`Tensor.backward()`. It will reduce memory\nconsumption for computations that would otherwise have `requires_grad=True`.\n\nIn this mode, the result of every computation will have\n`requires_grad=False`, even when the inputs have `requires_grad=True`.\nThere is an exception! All factory functions, or functions that create\na new Tensor and take a requires_grad kwarg, will NOT be affected by\nthis mode.\n\nThis context manager is thread local; it will not affect computation\nin other threads.\n\nAlso functions as a decorator.\n\n.. note::\n    No-grad is one of several mechanisms that can enable or\n    disable gradients locally see :ref:`locally-disable-grad-doc` for\n    more information on how they compare.\n\n.. note::\n    This API does not apply to :ref:`forward-mode AD <forward-mode-ad>`.\n    If you want to disable forward AD for a computation, you can unpack\n    your dual tensors.\n\nExample::\n    >>> # xdoctest: +SKIP\n    >>> x = torch.tensor([1.], requires_grad=True)\n    >>> with torch.no_grad():\n    ...     y = x * 2\n    >>> y.requires_grad\n    False\n    >>> @torch.no_grad()\n    ... def doubler(x):\n    ...     return x * 2\n    >>> z = doubler(x)\n    >>> z.requires_grad\n    False\n    >>> @torch.no_grad\n    ... def tripler(x):\n    ...     return x * 3\n    >>> z = tripler(x)\n    >>> z.requires_grad\n    False\n    >>> # factory function exception\n    >>> with torch.no_grad():\n    ...     a = torch.nn.Parameter(torch.rand(10))\n    >>> a.requires_grad\n    True", "Library": "PyTorch"}
{"API_Name": "torch.norm", "Docstring": "Returns the matrix norm or vector norm of a given tensor.\n\n.. warning::\n\n    torch.norm is deprecated and may be removed in a future PyTorch release.\n    Its documentation and behavior may be incorrect, and it is no longer\n    actively maintained.\n\n    Use :func:`torch.linalg.vector_norm` when computing vector norms and\n    :func:`torch.linalg.matrix_norm` when computing matrix norms.\n    For a function with a similar behavior as this one see :func:`torch.linalg.norm`.\n    Note, however, the signature for these functions is slightly different than the\n    signature for ``torch.norm``.\n\nArgs:\n    input (Tensor): The input tensor. Its data type must be either a floating\n        point or complex type. For complex inputs, the norm is calculated using the\n        absolute value of each element. If the input is complex and neither\n        :attr:`dtype` nor :attr:`out` is specified, the result's data type will\n        be the corresponding floating point type (e.g. float if :attr:`input` is\n        complexfloat).\n\n    p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``\n        The following norms can be calculated:\n\n        ======  ==============  ==========================\n        ord     matrix norm     vector norm\n        ======  ==============  ==========================\n        'fro'   Frobenius norm  --\n        'nuc'   nuclear norm    --\n        Number  --              sum(abs(x)**ord)**(1./ord)\n        ======  ==============  ==========================\n\n        The vector norm can be calculated across any number of dimensions.\n        The corresponding dimensions of :attr:`input` are flattened into\n        one dimension, and the norm is calculated on the flattened\n        dimension.\n\n        Frobenius norm produces the same result as ``p=2`` in all cases\n        except when :attr:`dim` is a list of three or more dims, in which\n        case Frobenius norm throws an error.\n\n        Nuclear norm can only be calculated across exactly two dimensions.\n\n    dim (int, tuple of ints, list of ints, optional):\n        Specifies which dimension or dimensions of :attr:`input` to\n        calculate the norm across. If :attr:`dim` is ``None``, the norm will\n        be calculated across all dimensions of :attr:`input`. If the norm\n        type indicated by :attr:`p` does not support the specified number of\n        dimensions, an error will occur.\n    keepdim (bool, optional): whether the output tensors have :attr:`dim`\n        retained or not. Ignored if :attr:`dim` = ``None`` and\n        :attr:`out` = ``None``. Default: ``False``\n    out (Tensor, optional): the output tensor. Ignored if\n        :attr:`dim` = ``None`` and :attr:`out` = ``None``.\n    dtype (:class:`torch.dtype`, optional): the desired data type of\n        returned tensor. If specified, the input tensor is casted to\n        :attr:`dtype` while performing the operation. Default: None.\n\n.. note::\n    Even though ``p='fro'`` supports any number of dimensions, the true\n    mathematical definition of Frobenius norm only applies to tensors with\n    exactly two dimensions. :func:`torch.linalg.matrix_norm` with ``ord='fro'``\n    aligns with the mathematical definition, since it can only be applied across\n    exactly two dimensions.\n\nExample::\n\n    >>> import torch\n    >>> a = torch.arange(9, dtype= torch.float) - 4\n    >>> b = a.reshape((3, 3))\n    >>> torch.norm(a)\n    tensor(7.7460)\n    >>> torch.norm(b)\n    tensor(7.7460)\n    >>> torch.norm(a, float('inf'))\n    tensor(4.)\n    >>> torch.norm(b, float('inf'))\n    tensor(4.)\n    >>> c = torch.tensor([[ 1, 2, 3], [-1, 1, 4]] , dtype=torch.float)\n    >>> torch.norm(c, dim=0)\n    tensor([1.4142, 2.2361, 5.0000])\n    >>> torch.norm(c, dim=1)\n    tensor([3.7417, 4.2426])\n    >>> torch.norm(c, p=1, dim=1)\n    tensor([6., 6.])\n    >>> d = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)\n    >>> torch.norm(d, dim=(1, 2))\n    tensor([ 3.7417, 11.2250])\n    >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n    (tensor(3.7417), tensor(11.2250))", "Library": "PyTorch"}
{"API_Name": "torch.normal", "Docstring": "normal(mean, std, *, generator=None, out=None) -> Tensor\n\nReturns a tensor of random numbers drawn from separate normal distributions\nwhose mean and standard deviation are given.\n\nThe :attr:`mean` is a tensor with the mean of\neach output element's normal distribution\n\nThe :attr:`std` is a tensor with the standard deviation of\neach output element's normal distribution\n\nThe shapes of :attr:`mean` and :attr:`std` don't need to match, but the\ntotal number of elements in each tensor need to be the same.\n\n.. note:: When the shapes do not match, the shape of :attr:`mean`\n          is used as the shape for the returned output tensor\n\n.. note:: When :attr:`std` is a CUDA tensor, this function synchronizes\n          its device with the CPU.\n\nArgs:\n    mean (Tensor): the tensor of per-element means\n    std (Tensor): the tensor of per-element standard deviations\n\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n    tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n              8.0505,   8.1408,   9.0563,  10.0566])\n\n.. function:: normal(mean=0.0, std, *, out=None) -> Tensor\n   :noindex:\n\nSimilar to the function above, but the means are shared among all drawn\nelements.\n\nArgs:\n    mean (float, optional): the mean for all distributions\n    std (Tensor): the tensor of per-element standard deviations\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\n    tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\n\n.. function:: normal(mean, std=1.0, *, out=None) -> Tensor\n   :noindex:\n\nSimilar to the function above, but the standard deviations are shared among\nall drawn elements.\n\nArgs:\n    mean (Tensor): the tensor of per-element means\n    std (float, optional): the standard deviation for all distributions\n\nKeyword args:\n    out (Tensor, optional): the output tensor\n\nExample::\n\n    >>> torch.normal(mean=torch.arange(1., 6.))\n    tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\n\n.. function:: normal(mean, std, size, *, out=None) -> Tensor\n   :noindex:\n\nSimilar to the function above, but the means and standard deviations are shared\namong all drawn elements. The resulting tensor has size given by :attr:`size`.\n\nArgs:\n    mean (float): the mean for all distributions\n    std (float): the standard deviation for all distributions\n    size (int...): a sequence of integers defining the shape of the output tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> torch.normal(2, 3, size=(1, 4))\n    tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])", "Library": "PyTorch"}
{"API_Name": "torch.ones", "Docstring": "ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n\nReturns a tensor filled with the scalar value `1`, with the shape defined\nby the variable argument :attr:`size`.\n\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor.\n        Can be a variable number of arguments or a collection like a list or tuple.\n\nKeyword arguments:\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n\nExample::\n\n    >>> torch.ones(2, 3)\n    tensor([[ 1.,  1.,  1.],\n            [ 1.,  1.,  1.]])\n\n    >>> torch.ones(5)\n    tensor([ 1.,  1.,  1.,  1.,  1.])", "Library": "PyTorch"}
{"API_Name": "torch.optim.Adam", "Docstring": "Implements Adam algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n            \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n        &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\n            \\:\\textit{maximize}                                                              \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max},\n            \\widehat{v_t})                                                                   \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.\n\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n        is not yet supported for all our implementations. Please use a float\n        LR if you are not also specifying fused=True or capturable=True.\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n        algorithm from the paper `On the Convergence of Adam and Beyond`_\n        (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a CUDA graph. Passing True can impair ungraphed performance,\n        so if you don't intend to graph capture this instance, leave it False\n        (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation. Thus, if the user has not specified BOTH flags\n          (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. For example, if the user specifies\n          True for fused but nothing for foreach, we will run the fused implementation. If\n          the user specifies False for foreach but nothing for fused (or False for fused but\n          nothing for foreach), we will run the for-loop implementation. If the user specifies\n          True for both foreach and fused, we will prioritize fused over foreach, as it is\n          typically faster. We attempt to use the fastest, so the hierarchy goes fused ->\n          foreach -> for-loop. HOWEVER, since the fused implementation is relatively new,\n          we want to give it sufficient bake-in time, so we default to foreach and NOT\n          fused when the user has not specified either flag.\n.. _Adam\\: A Method for Stochastic Optimization:\n    https://arxiv.org/abs/1412.6980\n.. _On the Convergence of Adam and Beyond:\n    https://openreview.net/forum?id=ryQu7f-RZ", "Library": "PyTorch"}
{"API_Name": "torch.optim.AdamW", "Docstring": "Implements AdamW algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{(lr)}, \\: \\beta_1, \\beta_2\n            \\text{(betas)}, \\: \\theta_0 \\text{(params)}, \\: f(\\theta) \\text{(objective)},\n            \\: \\epsilon \\text{ (epsilon)}                                                    \\\\\n        &\\hspace{13mm}      \\lambda \\text{(weight decay)},  \\: \\textit{amsgrad},\n            \\: \\textit{maximize}                                                             \\\\\n        &\\textbf{initialize} : m_0 \\leftarrow 0 \\text{ (first moment)}, v_0 \\leftarrow 0\n            \\text{ ( second moment)}, \\: \\widehat{v_0}^{max}\\leftarrow 0              \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\lambda \\theta_{t-1}         \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max},\n            \\widehat{v_t})                                                                   \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_t - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to `Decoupled Weight Decay Regularization`_.\n\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n        is not yet supported for all our implementations. Please use a float\n        LR if you are not also specifying fused=True or capturable=True.\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n        algorithm from the paper `On the Convergence of Adam and Beyond`_\n        (default: False)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a CUDA graph. Passing True can impair ungraphed performance,\n        so if you don't intend to graph capture this instance, leave it False\n        (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation. Thus, if the user has not specified BOTH flags\n          (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. For example, if the user specifies\n          True for fused but nothing for foreach, we will run the fused implementation. If\n          the user specifies False for foreach but nothing for fused (or False for fused but\n          nothing for foreach), we will run the for-loop implementation. If the user specifies\n          True for both foreach and fused, we will prioritize fused over foreach, as it is\n          typically faster. We attempt to use the fastest, so the hierarchy goes fused ->\n          foreach -> for-loop. HOWEVER, since the fused implementation is relatively new,\n          we want to give it sufficient bake-in time, so we default to foreach and NOT\n          fused when the user has not specified either flag.\n.. _Decoupled Weight Decay Regularization:\n    https://arxiv.org/abs/1711.05101\n.. _On the Convergence of Adam and Beyond:\n    https://openreview.net/forum?id=ryQu7f-RZ", "Library": "PyTorch"}
{"API_Name": "torch.optim.RMSprop", "Docstring": "Implements RMSprop algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\alpha \\text{ (alpha)},\\: \\gamma \\text{ (lr)},\n            \\: \\theta_0 \\text{ (params)}, \\: f(\\theta) \\text{ (objective)}                   \\\\\n        &\\hspace{13mm}   \\lambda \\text{ (weight decay)},\\: \\mu \\text{ (momentum)},\\: centered\\\\\n        &\\textbf{initialize} : v_0 \\leftarrow 0 \\text{ (square average)}, \\:\n            \\textbf{b}_0 \\leftarrow 0 \\text{ (buffer)}, \\: g^{ave}_0 \\leftarrow 0     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm}if \\: \\lambda \\neq 0                                                    \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\alpha v_{t-1} + (1 - \\alpha) g^2_t\n            \\hspace{8mm}                                                                     \\\\\n        &\\hspace{5mm} \\tilde{v_t} \\leftarrow v_t                                             \\\\\n        &\\hspace{5mm}if \\: centered                                                          \\\\\n        &\\hspace{10mm} g^{ave}_t \\leftarrow g^{ave}_{t-1} \\alpha + (1-\\alpha) g_t            \\\\\n        &\\hspace{10mm} \\tilde{v_t} \\leftarrow \\tilde{v_t} -  \\big(g^{ave}_{t} \\big)^2        \\\\\n        &\\hspace{5mm}if \\: \\mu > 0                                                           \\\\\n        &\\hspace{10mm} \\textbf{b}_t\\leftarrow \\mu \\textbf{b}_{t-1} +\n            g_t/ \\big(\\sqrt{\\tilde{v_t}} +  \\epsilon \\big)                                   \\\\\n        &\\hspace{10mm} \\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\textbf{b}_t                \\\\\n        &\\hspace{5mm} else                                                                   \\\\\n        &\\hspace{10mm}\\theta_t      \\leftarrow   \\theta_{t-1} -\n            \\gamma  g_t/ \\big(\\sqrt{\\tilde{v_t}} + \\epsilon \\big)  \\hspace{3mm}              \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to\n`lecture notes <https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_ by G. Hinton.\nand centered version `Generating Sequences\nWith Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\nThe implementation here takes the square root of the gradient average before\nadding epsilon (note that TensorFlow interchanges these two operations). The effective\nlearning rate is thus :math:`\\gamma/(\\sqrt{v} + \\epsilon)` where :math:`\\gamma`\nis the scheduled learning rate and :math:`v` is the weighted moving average\nof the squared gradient.\n\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float, optional): learning rate (default: 1e-2)\n    momentum (float, optional): momentum factor (default: 0)\n    alpha (float, optional): smoothing constant (default: 0.99)\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    centered (bool, optional) : if ``True``, compute the centered RMSProp,\n        the gradient is normalized by an estimation of its variance\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a CUDA graph. Passing True can impair ungraphed performance,\n        so if you don't intend to graph capture this instance, leave it False\n        (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)", "Library": "PyTorch"}
{"API_Name": "torch.optim.Rprop", "Docstring": "Implements the resilient backpropagation algorithm.\n\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\theta_0 \\in \\mathbf{R}^d \\text{ (params)},f(\\theta)\n            \\text{ (objective)},                                                             \\\\\n        &\\hspace{13mm}      \\eta_{+/-} \\text{ (etaplus, etaminus)}, \\Gamma_{max/min}\n            \\text{ (step sizes)}                                                             \\\\\n        &\\textbf{initialize} :   g^0_{prev} \\leftarrow 0,\n            \\: \\eta_0 \\leftarrow \\text{lr (learning rate)}                                   \\\\\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n        &\\hspace{5mm} \\textbf{for} \\text{  } i = 0, 1, \\ldots, d-1 \\: \\mathbf{do}            \\\\\n        &\\hspace{10mm}  \\textbf{if} \\:   g^i_{prev} g^i_t  > 0                               \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{min}(\\eta^i_{t-1} \\eta_{+},\n            \\Gamma_{max})                                                                    \\\\\n        &\\hspace{10mm}  \\textbf{else if}  \\:  g^i_{prev} g^i_t < 0                           \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{max}(\\eta^i_{t-1} \\eta_{-},\n            \\Gamma_{min})                                                                    \\\\\n        &\\hspace{15mm}  g^i_t \\leftarrow 0                                                   \\\\\n        &\\hspace{10mm}  \\textbf{else}  \\:                                                    \\\\\n        &\\hspace{15mm}  \\eta^i_t \\leftarrow \\eta^i_{t-1}                                     \\\\\n        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1}- \\eta_t \\mathrm{sign}(g_t)             \\\\\n        &\\hspace{5mm}g_{prev} \\leftarrow  g_t                                                \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\n\nFor further details regarding the algorithm we refer to the paper\n`A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm\n<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1417>`_.\n\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float, optional): learning rate (default: 1e-2)\n    etas (Tuple[float, float], optional): pair of (etaminus, etaplus), that\n        are multiplicative increase and decrease factors\n        (default: (0.5, 1.2))\n    step_sizes (Tuple[float, float], optional): a pair of minimal and\n        maximal allowed step sizes (default: (1e-6, 50))\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a CUDA graph. Passing True can impair ungraphed performance,\n        so if you don't intend to graph capture this instance, leave it False\n        (default: False)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)", "Library": "PyTorch"}
{"API_Name": "torch.permute", "Docstring": "permute(input, dims) -> Tensor\n\nReturns a view of the original tensor :attr:`input` with its dimensions permuted.\n\nArgs:\n    input (Tensor): the input tensor.\n    dims (tuple of int): The desired ordering of dimensions\n\nExample:\n    >>> x = torch.randn(2, 3, 5)\n    >>> x.size()\n    torch.Size([2, 3, 5])\n    >>> torch.permute(x, (2, 0, 1)).size()\n    torch.Size([5, 2, 3])", "Library": "PyTorch"}
{"API_Name": "torch.prod", "Docstring": "prod(input, *, dtype=None) -> Tensor\n\nReturns the product of all elements in the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n\nExample::\n\n    >>> a = torch.randn(1, 3)\n    >>> a\n    tensor([[-0.8020,  0.5428, -1.5854]])\n    >>> torch.prod(a)\n    tensor(0.6902)\n\n.. function:: prod(input, dim, keepdim=False, *, dtype=None) -> Tensor\n   :noindex:\n\nReturns the product of each row of the :attr:`input` tensor in the given\ndimension :attr:`dim`.\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\nthe output tensor having 1 fewer dimension than :attr:`input`.\n\nArgs:\n    input (Tensor): the input tensor.\n    dim (int): the dimension to reduce.\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n\nExample::\n\n    >>> a = torch.randn(4, 2)\n    >>> a\n    tensor([[ 0.5261, -0.3837],\n            [ 1.1857, -0.2498],\n            [-1.1646,  0.0705],\n            [ 1.1131, -1.0629]])\n    >>> torch.prod(a, 1)\n    tensor([-0.2018, -0.2962, -0.0821, -1.1831])", "Library": "PyTorch"}
{"API_Name": "torch.rand", "Docstring": "rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n\nReturns a tensor filled with random numbers from a uniform distribution\non the interval :math:`[0, 1)`\n\nThe shape of the tensor is defined by the variable argument :attr:`size`.\n\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor.\n        Can be a variable number of arguments or a collection like a list or tuple.\n\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n\nExample::\n\n    >>> torch.rand(4)\n    tensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n    >>> torch.rand(2, 3)\n    tensor([[ 0.8237,  0.5781,  0.6879],\n            [ 0.3816,  0.7249,  0.0998]])", "Library": "PyTorch"}
{"API_Name": "torch.randint", "Docstring": "randint(low=0, high, size, \\*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n\nReturns a tensor filled with random integers generated uniformly\nbetween :attr:`low` (inclusive) and :attr:`high` (exclusive).\n\nThe shape of the tensor is defined by the variable argument :attr:`size`.\n\n.. note::\n    With the global dtype default (``torch.float32``), this function returns\n    a tensor with dtype ``torch.int64``.\n\nArgs:\n    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0.\n    high (int): One above the highest integer to be drawn from the distribution.\n    size (tuple): a tuple defining the shape of the output tensor.\n\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n    dtype (`torch.dtype`, optional) - the desired data type of returned tensor. Default: if ``None``,\n        this function returns a tensor with dtype ``torch.int64``.\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n\nExample::\n\n    >>> torch.randint(3, 5, (3,))\n    tensor([4, 3, 4])\n\n\n    >>> torch.randint(10, (2, 2))\n    tensor([[0, 2],\n            [5, 5]])\n\n\n    >>> torch.randint(3, 10, (2, 2))\n    tensor([[4, 5],\n            [6, 7]])", "Library": "PyTorch"}
{"API_Name": "torch.randn", "Docstring": "randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n\n\nReturns a tensor filled with random numbers from a normal distribution\nwith mean `0` and variance `1` (also called the standard normal\ndistribution).\n\n.. math::\n    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n\nFor complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and\nunit variance as\n\n.. math::\n    \\text{out}_{i} \\sim \\mathcal{CN}(0, 1)\n\nThis is equivalent to separately sampling the real :math:`(\\operatorname{Re})` and imaginary\n:math:`(\\operatorname{Im})` part of :math:`\\text{out}_i` as\n\n.. math::\n    \\operatorname{Re}(\\text{out}_{i}) \\sim \\mathcal{N}(0, \\frac{1}{2}),\\quad\n    \\operatorname{Im}(\\text{out}_{i}) \\sim \\mathcal{N}(0, \\frac{1}{2})\n\nThe shape of the tensor is defined by the variable argument :attr:`size`.\n\n\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor.\n        Can be a variable number of arguments or a collection like a list or tuple.\n\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n\nExample::\n\n    >>> torch.randn(4)\n    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n    >>> torch.randn(2, 3)\n    tensor([[ 1.5954,  2.8929, -1.0923],\n            [ 1.1719, -0.4709, -0.1996]])\n\n.. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution", "Library": "PyTorch"}
{"API_Name": "torch.randn_like", "Docstring": "randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor\n\nReturns a tensor with the same size as :attr:`input` that is filled with\nrandom numbers from a normal distribution with mean 0 and variance 1. Please refer to :func:`torch.randn` for the\nsampling process of complex dtypes. ``torch.randn_like(input)`` is equivalent to\n``torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.\n\nArgs:\n    input (Tensor): the size of :attr:`input` will determine size of the output tensor.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n        Default: if ``None``, defaults to the dtype of :attr:`input`.\n    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.\n        Default: if ``None``, defaults to the layout of :attr:`input`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, defaults to the device of :attr:`input`.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.", "Library": "PyTorch"}
{"API_Name": "torch.repeat", "Docstring": "repeat(*sizes) -> Tensor\n\nRepeats this tensor along the specified dimensions.\n\nUnlike :meth:`~Tensor.expand`, this function copies the tensor's data.\n\n.. warning::\n\n    :meth:`~Tensor.repeat` behaves differently from\n    `numpy.repeat <https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html>`_,\n    but is more similar to\n    `numpy.tile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html>`_.\n    For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`.\n\nArgs:\n    sizes (torch.Size or int...): The number of times to repeat this tensor along each\n        dimension\n\nExample::\n\n    >>> x = torch.tensor([1, 2, 3])\n    >>> x.repeat(4, 2)\n    tensor([[ 1,  2,  3,  1,  2,  3],\n            [ 1,  2,  3,  1,  2,  3],\n            [ 1,  2,  3,  1,  2,  3],\n            [ 1,  2,  3,  1,  2,  3]])\n    >>> x.repeat(4, 2, 1).size()\n    torch.Size([4, 2, 3])", "Library": "PyTorch"}
{"API_Name": "torch.save", "Docstring": "save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\n\nSaves an object to a disk file.\n\nSee also: :ref:`saving-loading-tensors`\n\nArgs:\n    obj: saved object\n    f: a file-like object (has to implement write and flush) or a string or\n       os.PathLike object containing a file name\n    pickle_module: module used for pickling metadata and objects\n    pickle_protocol: can be specified to override the default protocol\n\n.. note::\n    A common PyTorch convention is to save tensors using .pt file extension.\n\n.. note::\n    PyTorch preserves storage sharing across serialization. See\n    :ref:`preserve-storage-sharing` for more details.\n\n.. note::\n    The 1.6 release of PyTorch switched ``torch.save`` to use a new\n    zipfile-based file format. ``torch.load`` still retains the ability to\n    load files in the old format. If for any reason you want ``torch.save``\n    to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.\n\nExample:\n    >>> # xdoctest: +SKIP(\"makes cwd dirty\")\n    >>> # Save to file\n    >>> x = torch.tensor([0, 1, 2, 3, 4])\n    >>> torch.save(x, 'tensor.pt')\n    >>> # Save to io.BytesIO buffer\n    >>> buffer = io.BytesIO()\n    >>> torch.save(x, buffer)", "Library": "PyTorch"}
{"API_Name": "torch.scatter_add_", "Docstring": "scatter_add_(dim, index, src) -> Tensor\n\nAdds all values from the tensor :attr:`src` into :attr:`self` at the indices\nspecified in the :attr:`index` tensor in a similar fashion as\n:meth:`~torch.Tensor.scatter_`. For each value in :attr:`src`, it is added to\nan index in :attr:`self` which is specified by its index in :attr:`src`\nfor ``dimension != dim`` and by the corresponding value in :attr:`index` for\n``dimension = dim``.\n\nFor a 3-D tensor, :attr:`self` is updated as::\n\n    self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n    self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n    self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n\n:attr:`self`, :attr:`index` and :attr:`src` should have same number of\ndimensions. It is also required that ``index.size(d) <= src.size(d)`` for all\ndimensions ``d``, and that ``index.size(d) <= self.size(d)`` for all dimensions\n``d != dim``. Note that ``index`` and ``src`` do not broadcast.\n\nNote:\n    This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n\n.. note::\n\n    The backward pass is implemented only for ``src.shape == index.shape``.\n\nArgs:\n    dim (int): the axis along which to index\n    index (LongTensor): the indices of elements to scatter and add, can be\n        either empty or of the same dimensionality as ``src``. When empty, the\n        operation returns ``self`` unchanged.\n    src (Tensor): the source elements to scatter and add\n\nExample::\n\n    >>> src = torch.ones((2, 5))\n    >>> index = torch.tensor([[0, 1, 2, 0, 0]])\n    >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n    tensor([[1., 0., 0., 1., 1.],\n            [0., 1., 0., 0., 0.],\n            [0., 0., 1., 0., 0.]])\n    >>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n    >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n    tensor([[2., 0., 0., 1., 1.],\n            [0., 2., 0., 0., 0.],\n            [0., 0., 2., 1., 1.]])", "Library": "PyTorch"}
{"API_Name": "torch.sigmoid", "Docstring": "sigmoid(input, *, out=None) -> Tensor\n\nAlias for :func:`torch.special.expit`.", "Library": "PyTorch"}
{"API_Name": "torch.sin", "Docstring": "sin(input, *, out=None) -> Tensor\n\nReturns a new tensor with the sine of the elements of :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\sin(\\text{input}_{i})\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> a = torch.randn(4)\n    >>> a\n    tensor([-0.5461,  0.1347, -2.7266, -0.2746])\n    >>> torch.sin(a)\n    tensor([-0.5194,  0.1343, -0.4032, -0.2711])", "Library": "PyTorch"}
{"API_Name": "torch.softmax", "Docstring": "softmax(input, dim, *, dtype=None) -> Tensor\n\nAlias for :func:`torch.nn.functional.softmax`.", "Library": "PyTorch"}
{"API_Name": "torch.sort", "Docstring": "sort(input, dim=-1, descending=False, stable=False, *, out=None) -> (Tensor, LongTensor)\n\nSorts the elements of the :attr:`input` tensor along a given dimension\nin ascending order by value.\n\nIf :attr:`dim` is not given, the last dimension of the `input` is chosen.\n\nIf :attr:`descending` is ``True`` then the elements are sorted in descending\norder by value.\n\nIf :attr:`stable` is ``True`` then the sorting routine becomes stable, preserving\nthe order of equivalent elements.\n\nA namedtuple of (values, indices) is returned, where the `values` are the\nsorted values and `indices` are the indices of the elements in the original\n`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor.\n    dim (int, optional): the dimension to sort along\n    descending (bool, optional): controls the sorting order (ascending or descending)\n    stable (bool, optional): makes the sorting routine stable, which guarantees that the order\n       of equivalent elements is preserved.\n\nKeyword args:\n    out (tuple, optional): the output tuple of (`Tensor`, `LongTensor`) that can\n        be optionally given to be used as output buffers\n\nExample::\n\n    >>> x = torch.randn(3, 4)\n    >>> sorted, indices = torch.sort(x)\n    >>> sorted\n    tensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n            [-0.5793,  0.0061,  0.6058,  0.9497],\n            [-0.5071,  0.3343,  0.9553,  1.0960]])\n    >>> indices\n    tensor([[ 1,  0,  2,  3],\n            [ 3,  1,  0,  2],\n            [ 0,  3,  1,  2]])\n\n    >>> sorted, indices = torch.sort(x, 0)\n    >>> sorted\n    tensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n            [ 0.0608,  0.0061,  0.9497,  0.3343],\n            [ 0.6058,  0.9553,  1.0960,  2.3332]])\n    >>> indices\n    tensor([[ 2,  0,  0,  1],\n            [ 0,  1,  1,  2],\n            [ 1,  2,  2,  0]])\n    >>> x = torch.tensor([0, 1] * 9)\n    >>> x.sort()\n    torch.return_types.sort(\n        values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n        indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))\n    >>> x.sort(stable=True)\n    torch.return_types.sort(\n        values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n        indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))", "Library": "PyTorch"}
{"API_Name": "torch.split", "Docstring": "Splits the tensor into chunks. Each chunk is a view of the original tensor.\n\nIf :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will\nbe split into equally sized chunks (if possible). Last chunk will be smaller if\nthe tensor size along the given dimension :attr:`dim` is not divisible by\n:attr:`split_size`.\n\nIf :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split\ninto ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according\nto :attr:`split_size_or_sections`.\n\nArgs:\n    tensor (Tensor): tensor to split.\n    split_size_or_sections (int) or (list(int)): size of a single chunk or\n        list of sizes for each chunk\n    dim (int): dimension along which to split the tensor.\n\nExample::\n\n    >>> a = torch.arange(10).reshape(5, 2)\n    >>> a\n    tensor([[0, 1],\n            [2, 3],\n            [4, 5],\n            [6, 7],\n            [8, 9]])\n    >>> torch.split(a, 2)\n    (tensor([[0, 1],\n             [2, 3]]),\n     tensor([[4, 5],\n             [6, 7]]),\n     tensor([[8, 9]]))\n    >>> torch.split(a, [1, 4])\n    (tensor([[0, 1]]),\n     tensor([[2, 3],\n             [4, 5],\n             [6, 7],\n             [8, 9]]))", "Library": "PyTorch"}
{"API_Name": "torch.squeeze", "Docstring": "squeeze(input, dim=None) -> Tensor\n\nReturns a tensor with all specified dimensions of :attr:`input` of size `1` removed.\n\nFor example, if `input` is of shape:\n:math:`(A \\times 1 \\times B \\times C \\times 1 \\times D)` then the `input.squeeze()`\nwill be of shape: :math:`(A \\times B \\times C \\times D)`.\n\nWhen :attr:`dim` is given, a squeeze operation is done only in the given\ndimension(s). If `input` is of shape: :math:`(A \\times 1 \\times B)`,\n``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)``\nwill squeeze the tensor to the shape :math:`(A \\times B)`.\n\n.. note:: The returned tensor shares the storage with the input tensor,\n          so changing the contents of one will change the contents of the other.\n\n.. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)`\n          will also remove the batch dimension, which can lead to unexpected\n          errors. Consider specifying only the dims you wish to be squeezed.\n\nArgs:\n    input (Tensor): the input tensor.\n    dim (int or tuple of ints, optional): if given, the input will be squeezed\n           only in the specified dimensions.\n\n        .. versionchanged:: 2.0\n           :attr:`dim` now accepts tuples of dimensions.\n\nExample::\n\n    >>> x = torch.zeros(2, 1, 2, 1, 2)\n    >>> x.size()\n    torch.Size([2, 1, 2, 1, 2])\n    >>> y = torch.squeeze(x)\n    >>> y.size()\n    torch.Size([2, 2, 2])\n    >>> y = torch.squeeze(x, 0)\n    >>> y.size()\n    torch.Size([2, 1, 2, 1, 2])\n    >>> y = torch.squeeze(x, 1)\n    >>> y.size()\n    torch.Size([2, 2, 1, 2])\n    >>> y = torch.squeeze(x, (1, 2, 3))\n    torch.Size([2, 2, 2])", "Library": "PyTorch"}
{"API_Name": "torch.stack", "Docstring": "stack(tensors, dim=0, *, out=None) -> Tensor\n\nConcatenates a sequence of tensors along a new dimension.\n\nAll tensors need to be of the same size.\n\n.. seealso::\n\n    :func:`torch.cat` concatenates the given sequence along an existing dimension.\n\nArguments:\n    tensors (sequence of Tensors): sequence of tensors to concatenate\n    dim (int, optional): dimension to insert. Has to be between 0 and the number\n        of dimensions of concatenated tensors (inclusive). Default: 0\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> x = torch.randn(2, 3)\n    >>> x\n    tensor([[ 0.3367,  0.1288,  0.2345],\n            [ 0.2303, -1.1229, -0.1863]])\n    >>> x = torch.stack((x, x)) # same as torch.stack((x, x), dim=0)\n    >>> x\n    tensor([[[ 0.3367,  0.1288,  0.2345],\n             [ 0.2303, -1.1229, -0.1863]],\n\n            [[ 0.3367,  0.1288,  0.2345],\n             [ 0.2303, -1.1229, -0.1863]]])\n    >>> x.size()\n    torch.Size([2, 2, 3])\n    >>> x = torch.stack((x, x), dim=1)\n    tensor([[[ 0.3367,  0.1288,  0.2345],\n             [ 0.3367,  0.1288,  0.2345]],\n\n            [[ 0.2303, -1.1229, -0.1863],\n             [ 0.2303, -1.1229, -0.1863]]])\n    >>> x = torch.stack((x, x), dim=2)\n    tensor([[[ 0.3367,  0.3367],\n             [ 0.1288,  0.1288],\n             [ 0.2345,  0.2345]],\n\n            [[ 0.2303,  0.2303],\n             [-1.1229, -1.1229],\n             [-0.1863, -0.1863]]])\n    >>> x = torch.stack((x, x), dim=-1)\n    tensor([[[ 0.3367,  0.3367],\n             [ 0.1288,  0.1288],\n             [ 0.2345,  0.2345]],\n\n            [[ 0.2303,  0.2303],\n             [-1.1229, -1.1229],\n             [-0.1863, -0.1863]]])", "Library": "PyTorch"}
{"API_Name": "torch.sum", "Docstring": "sum(input, *, dtype=None) -> Tensor\n\nReturns the sum of all elements in the :attr:`input` tensor.\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n\nExample::\n\n    >>> a = torch.randn(1, 3)\n    >>> a\n    tensor([[ 0.1133, -0.9567,  0.2958]])\n    >>> torch.sum(a)\n    tensor(-0.5475)\n\n.. function:: sum(input, dim, keepdim=False, *, dtype=None) -> Tensor\n   :noindex:\n\nReturns the sum of each row of the :attr:`input` tensor in the given\ndimension :attr:`dim`. If :attr:`dim` is a list of dimensions,\nreduce over all of them.\n\n\nIf :attr:`keepdim` is ``True``, the output tensor is of the same size\nas :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.\nOtherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the\noutput tensor having 1 (or ``len(dim)``) fewer dimension(s).\n\n\nArgs:\n    input (Tensor): the input tensor.\n    \n    dim (int or tuple of ints, optional): the dimension or dimensions to reduce.\n        If ``None``, all dimensions are reduced.\n\n    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        If specified, the input tensor is casted to :attr:`dtype` before the operation\n        is performed. This is useful for preventing data type overflows. Default: None.\n\nExample::\n\n    >>> a = torch.randn(4, 4)\n    >>> a\n    tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n            [-0.2993,  0.9138,  0.9337, -1.6864],\n            [ 0.1132,  0.7892, -0.1003,  0.5688],\n            [ 0.3637, -0.9906, -0.4752, -1.5197]])\n    >>> torch.sum(a, 1)\n    tensor([-0.4598, -0.1381,  1.3708, -2.6217])\n    >>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n    >>> torch.sum(b, (2, 1))\n    tensor([  435.,  1335.,  2235.,  3135.])", "Library": "PyTorch"}
{"API_Name": "torch.tanh", "Docstring": "tanh(input, *, out=None) -> Tensor\n\nReturns a new tensor with the hyperbolic tangent of the elements\nof :attr:`input`.\n\n.. math::\n    \\text{out}_{i} = \\tanh(\\text{input}_{i})\n\nArgs:\n    input (Tensor): the input tensor.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nExample::\n\n    >>> a = torch.randn(4)\n    >>> a\n    tensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n    >>> torch.tanh(a)\n    tensor([ 0.7156, -0.6218,  0.8257,  0.2553])", "Library": "PyTorch"}
{"API_Name": "torch.tensor", "Docstring": "tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n\nConstructs a tensor with no autograd history (also known as a \"leaf tensor\", see :doc:`/notes/autograd`) by copying :attr:`data`.\n\n.. warning::\n\n    When working with tensors prefer using :func:`torch.Tensor.clone`,\n    :func:`torch.Tensor.detach`, and :func:`torch.Tensor.requires_grad_` for\n    readability. Letting `t` be a tensor, ``torch.tensor(t)`` is equivalent to\n    ``t.clone().detach()``, and ``torch.tensor(t, requires_grad=True)``\n    is equivalent to ``t.clone().detach().requires_grad_(True)``.\n\n.. seealso::\n\n    :func:`torch.as_tensor` preserves autograd history and avoids copies where possible.\n    :func:`torch.from_numpy` creates a tensor that shares storage with a NumPy array.\n\nArgs:\n    data (array_like): Initial data for the tensor. Can be a list, tuple,\n        NumPy ``ndarray``, scalar, and other types.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, infers data type from :attr:`data`.\n    device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor\n        then the device of data is used. If None and data is not a tensor then\n        the result tensor is constructed on the current device.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    pin_memory (bool, optional): If set, returned tensor would be allocated in\n        the pinned memory. Works only for CPU tensors. Default: ``False``.\n\n\nExample::\n\n    >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n    tensor([[ 0.1000,  1.2000],\n            [ 2.2000,  3.1000],\n            [ 4.9000,  5.2000]])\n\n    >>> torch.tensor([0, 1])  # Type inference on data\n    tensor([ 0,  1])\n\n    >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n    ...              dtype=torch.float64,\n    ...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device\n    tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n\n    >>> torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor\n    tensor(3.1416)\n\n    >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n    tensor([])", "Library": "PyTorch"}
{"API_Name": "torch.tensor.backward", "Docstring": "Computes the gradient of current tensor wrt graph leaves.\n\nThe graph is differentiated using the chain rule. If the tensor is\nnon-scalar (i.e. its data has more than one element) and requires\ngradient, the function additionally requires specifying a ``gradient``.\nIt should be a tensor of matching type and shape, that represents\nthe gradient of the differentiated function w.r.t. ``self``.\n\nThis function accumulates gradients in the leaves - you might need to zero\n``.grad`` attributes or set them to ``None`` before calling it.\nSee :ref:`Default gradient layouts<default-grad-layouts>`\nfor details on the memory layout of accumulated gradients.\n\n.. note::\n\n    If you run any forward ops, create ``gradient``, and/or call ``backward``\n    in a user-specified CUDA stream context, see\n    :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n\n.. note::\n\n    When ``inputs`` are provided and a given input is not a leaf,\n    the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n    It is an implementation detail on which the user should not rely.\n    See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n\nArgs:\n    gradient (Tensor, optional): The gradient of the function\n        being differentiated w.r.t. ``self``.\n        This argument can be omitted if ``self`` is a scalar.\n    retain_graph (bool, optional): If ``False``, the graph used to compute\n        the grads will be freed. Note that in nearly all cases setting\n        this option to True is not needed and often can be worked around\n        in a much more efficient way. Defaults to the value of\n        ``create_graph``.\n    create_graph (bool, optional): If ``True``, graph of the derivative will\n        be constructed, allowing to compute higher order derivative\n        products. Defaults to ``False``.\n    inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be\n        accumulated into ``.grad``. All other tensors will be ignored. If not\n        provided, the gradient is accumulated into all the leaf Tensors that were\n        used to compute the :attr:`tensors`.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.argsort", "Docstring": "argsort(dim=-1, descending=False) -> LongTensor\n\nSee :func:`torch.argsort`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.bool", "Docstring": "bool(memory_format=torch.preserve_format) -> Tensor\n\n``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.\n\nArgs:\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.clone", "Docstring": "clone(*, memory_format=torch.preserve_format) -> Tensor\n\nSee :func:`torch.clone`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.contiguous", "Docstring": "contiguous(memory_format=torch.contiguous_format) -> Tensor\n\nReturns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If\n:attr:`self` tensor is already in the specified memory format, this function returns the\n:attr:`self` tensor.\n\nArgs:\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.contiguous_format``.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.detach", "Docstring": "Returns a new Tensor, detached from the current graph.\n\nThe result will never require gradient.\n\nThis method also affects forward mode AD gradients and the result will never\nhave forward mode AD gradients.\n\n.. note::\n\n  Returned Tensor shares the same storage with the original one.\n  In-place modifications on either of them will be seen, and may trigger\n  errors in correctness checks.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.exp", "Docstring": "exp() -> Tensor\n\nSee :func:`torch.exp`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.expand", "Docstring": "expand(*sizes) -> Tensor\n\nReturns a new view of the :attr:`self` tensor with singleton dimensions expanded\nto a larger size.\n\nPassing -1 as the size for a dimension means not changing the size of\nthat dimension.\n\nTensor can be also expanded to a larger number of dimensions, and the\nnew ones will be appended at the front. For the new dimensions, the\nsize cannot be set to -1.\n\nExpanding a tensor does not allocate new memory, but only creates a\nnew view on the existing tensor where a dimension of size one is\nexpanded to a larger size by setting the ``stride`` to 0. Any dimension\nof size 1 can be expanded to an arbitrary value without allocating new\nmemory.\n\nArgs:\n    *sizes (torch.Size or int...): the desired expanded size\n\n.. warning::\n\n    More than one element of an expanded tensor may refer to a single\n    memory location. As a result, in-place operations (especially ones that\n    are vectorized) may result in incorrect behavior. If you need to write\n    to the tensors, please clone them first.\n\nExample::\n\n    >>> x = torch.tensor([[1], [2], [3]])\n    >>> x.size()\n    torch.Size([3, 1])\n    >>> x.expand(3, 4)\n    tensor([[ 1,  1,  1,  1],\n            [ 2,  2,  2,  2],\n            [ 3,  3,  3,  3]])\n    >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n    tensor([[ 1,  1,  1,  1],\n            [ 2,  2,  2,  2],\n            [ 3,  3,  3,  3]])", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.fill_", "Docstring": "fill_(value) -> Tensor\n\nFills :attr:`self` tensor with the specified value.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.flatten", "Docstring": "flatten(start_dim=0, end_dim=-1) -> Tensor\n\nSee :func:`torch.flatten`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.float", "Docstring": "float(memory_format=torch.preserve_format) -> Tensor\n\n``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.\n\nArgs:\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.item", "Docstring": "item() -> number\n\nReturns the value of this tensor as a standard Python number. This only works\nfor tensors with one element. For other cases, see :meth:`~Tensor.tolist`.\n\nThis operation is not differentiable.\n\nExample::\n\n    >>> x = torch.tensor([1.0])\n    >>> x.item()\n    1.0", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.long", "Docstring": "long(memory_format=torch.preserve_format) -> Tensor\n\n``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.\n\nArgs:\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.max", "Docstring": "max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n\nSee :func:`torch.max`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.norm", "Docstring": "See :func:`torch.norm`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.numpy", "Docstring": "numpy(*, force=False) -> numpy.ndarray\n\nReturns the tensor as a NumPy :class:`ndarray`.\n\nIf :attr:`force` is ``False`` (the default), the conversion\nis performed only if the tensor is on the CPU, does not require grad,\ndoes not have its conjugate bit set, and is a dtype and layout that\nNumPy supports. The returned ndarray and the tensor will share their\nstorage, so changes to the tensor will be reflected in the ndarray\nand vice versa.\n\nIf :attr:`force` is ``True`` this is equivalent to\ncalling ``t.detach().cpu().resolve_conj().resolve_neg().numpy()``.\nIf the tensor isn't on the CPU or the conjugate or negative bit is set,\nthe tensor won't share its storage with the returned ndarray.\nSetting :attr:`force` to ``True`` can be a useful shorthand.\n\nArgs:\n    force (bool): if ``True``, the ndarray may be a copy of the tensor\n               instead of always sharing memory, defaults to ``False``.", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.numel", "Docstring": "numel() -> int\n\nSee :func:`torch.numel`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.pow", "Docstring": "pow(exponent) -> Tensor\n\nSee :func:`torch.pow`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.reshape", "Docstring": "reshape(*shape) -> Tensor\n\nReturns a tensor with the same data and number of elements as :attr:`self`\nbut with the specified shape. This method returns a view if :attr:`shape` is\ncompatible with the current shape. See :meth:`torch.Tensor.view` on when it is\npossible to return a view.\n\nSee :func:`torch.reshape`\n\nArgs:\n    shape (tuple of ints or int...): the desired shape", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.scatter_", "Docstring": "scatter_(dim, index, src, *, reduce=None) -> Tensor\n\nWrites all values from the tensor :attr:`src` into :attr:`self` at the indices\nspecified in the :attr:`index` tensor. For each value in :attr:`src`, its output\nindex is specified by its index in :attr:`src` for ``dimension != dim`` and by\nthe corresponding value in :attr:`index` for ``dimension = dim``.\n\nFor a 3-D tensor, :attr:`self` is updated as::\n\n    self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n    self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n    self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n\nThis is the reverse operation of the manner described in :meth:`~Tensor.gather`.\n\n:attr:`self`, :attr:`index` and :attr:`src` (if it is a Tensor) should all have\nthe same number of dimensions. It is also required that\n``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\nNote that ``index`` and ``src`` do not broadcast.\n\nMoreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be\nbetween ``0`` and ``self.size(dim) - 1`` inclusive.\n\n.. warning::\n\n    When indices are not unique, the behavior is non-deterministic (one of the\n    values from ``src`` will be picked arbitrarily) and the gradient will be\n    incorrect (it will be propagated to all locations in the source that\n    correspond to the same index)!\n\n.. note::\n\n    The backward pass is implemented only for ``src.shape == index.shape``.\n\nAdditionally accepts an optional :attr:`reduce` argument that allows\nspecification of an optional reduction operation, which is applied to all\nvalues in the tensor :attr:`src` into :attr:`self` at the indices\nspecified in the :attr:`index`. For each value in :attr:`src`, the reduction\noperation is applied to an index in :attr:`self` which is specified by\nits index in :attr:`src` for ``dimension != dim`` and by the corresponding\nvalue in :attr:`index` for ``dimension = dim``.\n\nGiven a 3-D tensor and reduction using the multiplication operation, :attr:`self`\nis updated as::\n\n    self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\n    self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\n    self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n\nReducing with the addition operation is the same as using\n:meth:`~torch.Tensor.scatter_add_`.\n\n.. warning::\n    The reduce argument with Tensor ``src`` is deprecated and will be removed in\n    a future PyTorch release. Please use :meth:`~torch.Tensor.scatter_reduce_`\n    instead for more reduction options.\n\nArgs:\n    dim (int): the axis along which to index\n    index (LongTensor): the indices of elements to scatter, can be either empty\n        or of the same dimensionality as ``src``. When empty, the operation\n        returns ``self`` unchanged.\n    src (Tensor): the source element(s) to scatter.\n\nKeyword args:\n    reduce (str, optional): reduction operation to apply, can be either\n        ``'add'`` or ``'multiply'``.\n\nExample::\n\n    >>> src = torch.arange(1, 11).reshape((2, 5))\n    >>> src\n    tensor([[ 1,  2,  3,  4,  5],\n            [ 6,  7,  8,  9, 10]])\n    >>> index = torch.tensor([[0, 1, 2, 0]])\n    >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n    tensor([[1, 0, 0, 4, 0],\n            [0, 2, 0, 0, 0],\n            [0, 0, 3, 0, 0]])\n    >>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n    >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n    tensor([[1, 2, 3, 0, 0],\n            [6, 7, 0, 0, 8],\n            [0, 0, 0, 0, 0]])\n\n    >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n    ...            1.23, reduce='multiply')\n    tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n            [2.0000, 2.0000, 2.0000, 2.4600]])\n    >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n    ...            1.23, reduce='add')\n    tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n            [2.0000, 2.0000, 2.0000, 3.2300]])\n\n.. function:: scatter_(dim, index, value, *, reduce=None) -> Tensor:\n   :noindex:\n\nWrites the value from :attr:`value` into :attr:`self` at the indices\nspecified in the :attr:`index` tensor.  This operation is equivalent to the previous version,\nwith the :attr:`src` tensor filled entirely with :attr:`value`.\n\nArgs:\n    dim (int): the axis along which to index\n    index (LongTensor): the indices of elements to scatter, can be either empty\n        or of the same dimensionality as ``src``. When empty, the operation\n        returns ``self`` unchanged.\n    value (Scalar): the value to scatter.\n\nKeyword args:\n    reduce (str, optional): reduction operation to apply, can be either\n        ``'add'`` or ``'multiply'``.\n\nExample::\n\n    >>> index = torch.tensor([[0, 1]])\n    >>> value = 2\n    >>> torch.zeros(3, 5).scatter_(0, index, value)\n    tensor([[2., 0., 0., 0., 0.],\n            [0., 2., 0., 0., 0.],\n            [0., 0., 0., 0., 0.]])", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.size", "Docstring": "size(dim=None) -> torch.Size or int\n\nReturns the size of the :attr:`self` tensor. If ``dim`` is not specified,\nthe returned value is a :class:`torch.Size`, a subclass of :class:`tuple`.\nIf ``dim`` is specified, returns an int holding the size of that dimension.\n\nArgs:\n  dim (int, optional): The dimension for which to retrieve the size.\n\nExample::\n\n    >>> t = torch.empty(3, 4, 5)\n    >>> t.size()\n    torch.Size([3, 4, 5])\n    >>> t.size(dim=1)\n    4", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.squeeze", "Docstring": "squeeze(dim=None) -> Tensor\n\nSee :func:`torch.squeeze`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.sqrt", "Docstring": "sqrt() -> Tensor\n\nSee :func:`torch.sqrt`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.sum", "Docstring": "sum(dim=None, keepdim=False, dtype=None) -> Tensor\n\nSee :func:`torch.sum`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.topk", "Docstring": "topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n\nSee :func:`torch.topk`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.transpose", "Docstring": "transpose(dim0, dim1) -> Tensor\n\nSee :func:`torch.transpose`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.uniform_", "Docstring": "uniform_(from=0, to=1, *, generator=None) -> Tensor\n\nFills :attr:`self` tensor with numbers sampled from the continuous uniform\ndistribution:\n\n.. math::\n    f(x) = \\dfrac{1}{\\text{to} - \\text{from}}", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.unique", "Docstring": "Returns the unique elements of the input tensor.\n\nSee :func:`torch.unique`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.unsqueeze", "Docstring": "unsqueeze(dim) -> Tensor\n\nSee :func:`torch.unsqueeze`", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.view", "Docstring": "view(*shape) -> Tensor\n\nReturns a new tensor with the same data as the :attr:`self` tensor but of a\ndifferent :attr:`shape`.\n\nThe returned tensor shares the same data and must have the same number\nof elements, but may have a different size. For a tensor to be viewed, the new\nview size must be compatible with its original size and stride, i.e., each new\nview dimension must either be a subspace of an original dimension, or only span\nacross original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\ncontiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n\n.. math::\n\n  \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n\nOtherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\nwithout copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n:meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\nreturns a view if the shapes are compatible, and copies (equivalent to calling\n:meth:`contiguous`) otherwise.\n\nArgs:\n    shape (torch.Size or int...): the desired size\n\nExample::\n\n    >>> x = torch.randn(4, 4)\n    >>> x.size()\n    torch.Size([4, 4])\n    >>> y = x.view(16)\n    >>> y.size()\n    torch.Size([16])\n    >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n    >>> z.size()\n    torch.Size([2, 8])\n\n    >>> a = torch.randn(1, 2, 3, 4)\n    >>> a.size()\n    torch.Size([1, 2, 3, 4])\n    >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n    >>> b.size()\n    torch.Size([1, 3, 2, 4])\n    >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n    >>> c.size()\n    torch.Size([1, 3, 2, 4])\n    >>> torch.equal(b, c)\n    False\n\n\n.. method:: view(dtype) -> Tensor\n   :noindex:\n\nReturns a new tensor with the same data as the :attr:`self` tensor but of a\ndifferent :attr:`dtype`.\n\nIf the element size of :attr:`dtype` is different than that of ``self.dtype``,\nthen the size of the last dimension of the output will be scaled\nproportionally.  For instance, if :attr:`dtype` element size is twice that of\n``self.dtype``, then each pair of elements in the last dimension of\n:attr:`self` will be combined, and the size of the last dimension of the output\nwill be half that of :attr:`self`. If :attr:`dtype` element size is half that\nof ``self.dtype``, then each element in the last dimension of :attr:`self` will\nbe split in two, and the size of the last dimension of the output will be\ndouble that of :attr:`self`. For this to be possible, the following conditions\nmust be true:\n\n    * ``self.dim()`` must be greater than 0.\n    * ``self.stride(-1)`` must be 1.\n\nAdditionally, if the element size of :attr:`dtype` is greater than that of\n``self.dtype``, the following conditions must be true as well:\n\n    * ``self.size(-1)`` must be divisible by the ratio between the element\n      sizes of the dtypes.\n    * ``self.storage_offset()`` must be divisible by the ratio between the\n      element sizes of the dtypes.\n    * The strides of all dimensions, except the last dimension, must be\n      divisible by the ratio between the element sizes of the dtypes.\n\nIf any of the above conditions are not met, an error is thrown.\n\n.. warning::\n\n    This overload is not supported by TorchScript, and using it in a Torchscript\n    program will cause undefined behavior.\n\n\nArgs:\n    dtype (:class:`torch.dtype`): the desired dtype\n\nExample::\n\n    >>> x = torch.randn(4, 4)\n    >>> x\n    tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n            [-0.1520,  0.7472,  0.5617, -0.8649],\n            [-2.4724, -0.0334, -0.2976, -0.8499],\n            [-0.2109,  1.9913, -0.9607, -0.6123]])\n    >>> x.dtype\n    torch.float32\n\n    >>> y = x.view(torch.int32)\n    >>> y\n    tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n            [-1105482831,  1061112040,  1057999968, -1084397505],\n            [-1071760287, -1123489973, -1097310419, -1084649136],\n            [-1101533110,  1073668768, -1082790149, -1088634448]],\n        dtype=torch.int32)\n    >>> y[0, 0] = 1000000000\n    >>> x\n    tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n            [-0.1520,  0.7472,  0.5617, -0.8649],\n            [-2.4724, -0.0334, -0.2976, -0.8499],\n            [-0.2109,  1.9913, -0.9607, -0.6123]])\n\n    >>> x.view(torch.cfloat)\n    tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n            [-0.1520+0.7472j,  0.5617-0.8649j],\n            [-2.4724-0.0334j, -0.2976-0.8499j],\n            [-0.2109+1.9913j, -0.9607-0.6123j]])\n    >>> x.view(torch.cfloat).size()\n    torch.Size([4, 2])\n\n    >>> x.view(torch.uint8)\n    tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n               8, 191],\n            [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n              93, 191],\n            [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n              89, 191],\n            [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n              28, 191]], dtype=torch.uint8)\n    >>> x.view(torch.uint8).size()\n    torch.Size([4, 16])", "Library": "PyTorch"}
{"API_Name": "torch.Tensor.zero_", "Docstring": "zero_() -> Tensor\n\nFills :attr:`self` tensor with zeros.", "Library": "PyTorch"}
{"API_Name": "torch.topk", "Docstring": "topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)\n\nReturns the :attr:`k` largest elements of the given :attr:`input` tensor along\na given dimension.\n\nIf :attr:`dim` is not given, the last dimension of the `input` is chosen.\n\nIf :attr:`largest` is ``False`` then the `k` smallest elements are returned.\n\nA namedtuple of `(values, indices)` is returned with the `values` and\n`indices` of the largest `k` elements of each row of the `input` tensor in the\ngiven dimension `dim`.\n\nThe boolean option :attr:`sorted` if ``True``, will make sure that the returned\n`k` elements are themselves sorted\n\nArgs:\n    input (Tensor): the input tensor.\n    k (int): the k in \"top-k\"\n    dim (int, optional): the dimension to sort along\n    largest (bool, optional): controls whether to return largest or\n           smallest elements\n    sorted (bool, optional): controls whether to return the elements\n           in sorted order\n\nKeyword args:\n    out (tuple, optional): the output tuple of (Tensor, LongTensor) that can be\n        optionally given to be used as output buffers\n\nExample::\n\n    >>> x = torch.arange(1., 6.)\n    >>> x\n    tensor([ 1.,  2.,  3.,  4.,  5.])\n    >>> torch.topk(x, 3)\n    torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))", "Library": "PyTorch"}
{"API_Name": "torch.trace", "Docstring": "trace(input) -> Tensor\n\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\n\nExample::\n\n    >>> x = torch.arange(1., 10.).view(3, 3)\n    >>> x\n    tensor([[ 1.,  2.,  3.],\n            [ 4.,  5.,  6.],\n            [ 7.,  8.,  9.]])\n    >>> torch.trace(x)\n    tensor(15.)", "Library": "PyTorch"}
{"API_Name": "torch.triu_indices", "Docstring": "triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) -> Tensor\n\nReturns the indices of the upper triangular part of a :attr:`row` by\n:attr:`col` matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns.\n\nThe upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.\n\nThe argument :attr:`offset` controls which diagonal to consider. If\n:attr:`offset` = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n:math:`\\lbrace (i, i) \\rbrace` for :math:`i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]`\nwhere :math:`d_{1}, d_{2}` are the dimensions of the matrix.\n\n.. note::\n    When running on CUDA, ``row * col`` must be less than :math:`2^{59}` to\n    prevent overflow during calculation.\n\nArgs:\n    row (``int``): number of rows in the 2-D matrix.\n    col (``int``): number of columns in the 2-D matrix.\n    offset (``int``): diagonal offset from the main diagonal.\n        Default: if not provided, 0.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, ``torch.long``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    layout (:class:`torch.layout`, optional): currently only support ``torch.strided``.\n\nExample::\n\n    >>> a = torch.triu_indices(3, 3)\n    >>> a\n    tensor([[0, 0, 0, 1, 1, 2],\n            [0, 1, 2, 1, 2, 2]])\n\n    >>> a = torch.triu_indices(4, 3, -1)\n    >>> a\n    tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n            [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n\n    >>> a = torch.triu_indices(4, 3, 1)\n    >>> a\n    tensor([[0, 0, 1],\n            [1, 2, 2]])", "Library": "PyTorch"}
{"API_Name": "torch.unbind", "Docstring": "unbind(input, dim=0) -> seq\n\nRemoves a tensor dimension.\n\nReturns a tuple of all slices along a given dimension, already without it.\n\nArguments:\n    input (Tensor): the tensor to unbind\n    dim (int): dimension to remove\n\nExample::\n\n    >>> torch.unbind(torch.tensor([[1, 2, 3],\n    >>>                            [4, 5, 6],\n    >>>                            [7, 8, 9]]))\n    (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))", "Library": "PyTorch"}
{"API_Name": "torch.unsqueeze", "Docstring": "unsqueeze(input, dim) -> Tensor\n\nReturns a new tensor with a dimension of size one inserted at the\nspecified position.\n\nThe returned tensor shares the same underlying data with this tensor.\n\nA :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)``\ncan be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze`\napplied at :attr:`dim` = ``dim + input.dim() + 1``.\n\nArgs:\n    input (Tensor): the input tensor.\n    dim (int): the index at which to insert the singleton dimension\n\nExample::\n\n    >>> x = torch.tensor([1, 2, 3, 4])\n    >>> torch.unsqueeze(x, 0)\n    tensor([[ 1,  2,  3,  4]])\n    >>> torch.unsqueeze(x, 1)\n    tensor([[ 1],\n            [ 2],\n            [ 3],\n            [ 4]])", "Library": "PyTorch"}
{"API_Name": "torch.utils.data.DataLoader", "Docstring": "Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n\nThe :class:`~torch.utils.data.DataLoader` supports both map-style and\niterable-style datasets with single- or multi-process loading, customizing\nloading order and optional automatic batching (collation) and memory pinning.\n\nSee :py:mod:`torch.utils.data` documentation page for more details.\n\nArgs:\n    dataset (Dataset): dataset from which to load the data.\n    batch_size (int, optional): how many samples per batch to load\n        (default: ``1``).\n    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n        at every epoch (default: ``False``).\n    sampler (Sampler or Iterable, optional): defines the strategy to draw\n        samples from the dataset. Can be any ``Iterable`` with ``__len__``\n        implemented. If specified, :attr:`shuffle` must not be specified.\n    batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n        returns a batch of indices at a time. Mutually exclusive with\n        :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n        and :attr:`drop_last`.\n    num_workers (int, optional): how many subprocesses to use for data\n        loading. ``0`` means that the data will be loaded in the main process.\n        (default: ``0``)\n    collate_fn (Callable, optional): merges a list of samples to form a\n        mini-batch of Tensor(s).  Used when using batched loading from a\n        map-style dataset.\n    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n        into device/CUDA pinned memory before returning them.  If your data elements\n        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n        see the example below.\n    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n        if the dataset size is not divisible by the batch size. If ``False`` and\n        the size of dataset is not divisible by the batch size, then the last batch\n        will be smaller. (default: ``False``)\n    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n        from workers. Should always be non-negative. (default: ``0``)\n    worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n        input, after seeding and before data loading. (default: ``None``)\n    multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n        ``None``, the default `multiprocessing context`_ of your operating system will\n        be used. (default: ``None``)\n    generator (torch.Generator, optional): If not ``None``, this RNG will be used\n        by RandomSampler to generate random indexes and multiprocessing to generate\n        ``base_seed`` for workers. (default: ``None``)\n    prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n        in advance by each worker. ``2`` means there will be a total of\n        2 * num_workers batches prefetched across all workers. (default value depends\n        on the set value for num_workers. If value of num_workers=0 default is ``None``.\n        Otherwise, if value of ``num_workers > 0`` default is ``2``).\n    persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n        the worker processes after a dataset has been consumed once. This allows to\n        maintain the workers `Dataset` instances alive. (default: ``False``)\n    pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n        ``True``.\n\n\n.. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n             cannot be an unpicklable object, e.g., a lambda function. See\n             :ref:`multiprocessing-best-practices` on more details related\n             to multiprocessing in PyTorch.\n\n.. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n             When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n             it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n             rounding depending on :attr:`drop_last`, regardless of multi-process loading\n             configurations. This represents the best guess PyTorch can make because PyTorch\n             trusts user :attr:`dataset` code in correctly handling multi-process\n             loading to avoid duplicate data.\n\n             However, if sharding results in multiple workers having incomplete last batches,\n             this estimate can still be inaccurate, because (1) an otherwise complete batch can\n             be broken into multiple ones and (2) more than one batch worth of samples can be\n             dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n             cases in general.\n\n             See `Dataset Types`_ for more details on these two types of datasets and how\n             :class:`~torch.utils.data.IterableDataset` interacts with\n             `Multi-process data loading`_.\n\n.. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n             :ref:`data-loading-randomness` notes for random seed related questions.\n\n.. _multiprocessing context:\n    https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods", "Library": "PyTorch"}
{"API_Name": "torch.utils.data.Dataset", "Docstring": "An abstract class representing a :class:`Dataset`.\n\nAll datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n:meth:`__len__`, which is expected to return the size of the dataset by many\n:class:`~torch.utils.data.Sampler` implementations and the default options\nof :class:`~torch.utils.data.DataLoader`. Subclasses could also\noptionally implement :meth:`__getitems__`, for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.\n\n.. note::\n  :class:`~torch.utils.data.DataLoader` by default constructs an index\n  sampler that yields integral indices.  To make it work with a map-style\n  dataset with non-integral indices/keys, a custom sampler must be provided.", "Library": "PyTorch"}
{"API_Name": "torch.utils.data.RandomSampler", "Docstring": "Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n\nIf with replacement, then user can specify :attr:`num_samples` to draw.\n\nArgs:\n    data_source (Dataset): dataset to sample from\n    replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\n    num_samples (int): number of samples to draw, default=`len(dataset)`.\n    generator (Generator): Generator used in sampling.", "Library": "PyTorch"}
{"API_Name": "torch.where", "Docstring": "where(condition, input, other, *, out=None) -> Tensor\n\nReturn a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`.\n\nThe operation is defined as:\n\n.. math::\n    \\text{out}_i = \\begin{cases}\n        \\text{input}_i & \\text{if } \\text{condition}_i \\\\\n        \\text{other}_i & \\text{otherwise} \\\\\n    \\end{cases}\n\n.. note::\n    The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable <broadcasting-semantics>`.\n\nArguments:\n    condition (BoolTensor): When True (nonzero), yield input, otherwise yield other\n    input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices\n                          where :attr:`condition` is ``True``\n    other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices\n                          where :attr:`condition` is ``False``\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n\nReturns:\n    Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other`\n\nExample::\n\n    >>> x = torch.randn(3, 2)\n    >>> y = torch.ones(3, 2)\n    >>> x\n    tensor([[-0.4620,  0.3139],\n            [ 0.3898, -0.7197],\n            [ 0.0478, -0.1657]])\n    >>> torch.where(x > 0, 1.0, 0.0)\n    tensor([[0., 1.],\n            [1., 0.],\n            [1., 0.]])\n    >>> torch.where(x > 0, x, y)\n    tensor([[ 1.0000,  0.3139],\n            [ 0.3898,  1.0000],\n            [ 0.0478,  1.0000]])\n    >>> x = torch.randn(2, 2, dtype=torch.double)\n    >>> x\n    tensor([[ 1.0779,  0.0383],\n            [-0.8785, -1.1089]], dtype=torch.float64)\n    >>> torch.where(x > 0, x, 0.)\n    tensor([[1.0779, 0.0383],\n            [0.0000, 0.0000]], dtype=torch.float64)\n\n.. function:: where(condition) -> tuple of LongTensor\n   :noindex:\n\n``torch.where(condition)`` is identical to\n``torch.nonzero(condition, as_tuple=True)``.\n\n.. note::\n    See also :func:`torch.nonzero`.", "Library": "PyTorch"}
{"API_Name": "torch.zeros", "Docstring": "zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n\nReturns a tensor filled with the scalar value `0`, with the shape defined\nby the variable argument :attr:`size`.\n\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor.\n        Can be a variable number of arguments or a collection like a list or tuple.\n\nKeyword args:\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n\nExample::\n\n    >>> torch.zeros(2, 3)\n    tensor([[ 0.,  0.,  0.],\n            [ 0.,  0.,  0.]])\n\n    >>> torch.zeros(5)\n    tensor([ 0.,  0.,  0.,  0.,  0.])", "Library": "PyTorch"}
{"API_Name": "torch.zeros_like", "Docstring": "zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor\n\nReturns a tensor filled with the scalar value `0`, with the same size as\n:attr:`input`. ``torch.zeros_like(input)`` is equivalent to\n``torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.\n\n.. warning::\n    As of 0.4, this function does not support an :attr:`out` keyword. As an alternative,\n    the old ``torch.zeros_like(input, out=output)`` is equivalent to\n    ``torch.zeros(input.size(), out=output)``.\n\nArgs:\n    input (Tensor): the size of :attr:`input` will determine size of the output tensor.\n\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.\n        Default: if ``None``, defaults to the dtype of :attr:`input`.\n    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.\n        Default: if ``None``, defaults to the layout of :attr:`input`.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, defaults to the device of :attr:`input`.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n    memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n        returned Tensor. Default: ``torch.preserve_format``.\n\nExample::\n\n    >>> input = torch.empty(2, 3)\n    >>> torch.zeros_like(input)\n    tensor([[ 0.,  0.,  0.],\n            [ 0.,  0.,  0.]])", "Library": "PyTorch"}
{"API_Name": "torchvision.datasets.FashionMNIST", "Docstring": "`Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n\nArgs:\n    root (string): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n        and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n        otherwise from ``t10k-images-idx3-ubyte``.\n    download (bool, optional): If True, downloads the dataset from the internet and\n        puts it in root directory. If dataset is already downloaded, it is not\n        downloaded again.\n    transform (callable, optional): A function/transform that  takes in an PIL image\n        and returns a transformed version. E.g, ``transforms.RandomCrop``\n    target_transform (callable, optional): A function/transform that takes in the\n        target and transforms it.", "Library": "PyTorch"}
{"API_Name": "torchvision.datasets.MNIST", "Docstring": "`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n\nArgs:\n    root (string): Root directory of dataset where ``MNIST/raw/train-images-idx3-ubyte``\n        and  ``MNIST/raw/t10k-images-idx3-ubyte`` exist.\n    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n        otherwise from ``t10k-images-idx3-ubyte``.\n    download (bool, optional): If True, downloads the dataset from the internet and\n        puts it in root directory. If dataset is already downloaded, it is not\n        downloaded again.\n    transform (callable, optional): A function/transform that  takes in an PIL image\n        and returns a transformed version. E.g, ``transforms.RandomCrop``\n    target_transform (callable, optional): A function/transform that takes in the\n        target and transforms it.", "Library": "PyTorch"}
{"API_Name": "torchvision.transforms.Compose", "Docstring": "Composes several transforms together. This transform does not support torchscript.\nPlease, see the note below.\n\nArgs:\n    transforms (list of ``Transform`` objects): list of transforms to compose.\n\nExample:\n    >>> transforms.Compose([\n    >>>     transforms.CenterCrop(10),\n    >>>     transforms.PILToTensor(),\n    >>>     transforms.ConvertImageDtype(torch.float),\n    >>> ])\n\n.. note::\n    In order to script the transformations, please use ``torch.nn.Sequential`` as below.\n\n    >>> transforms = torch.nn.Sequential(\n    >>>     transforms.CenterCrop(10),\n    >>>     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    >>> )\n    >>> scripted_transforms = torch.jit.script(transforms)\n\n    Make sure to use only scriptable transformations, i.e. that work with ``torch.Tensor``, does not require\n    `lambda` functions or ``PIL.Image``.", "Library": "PyTorch"}
{"API_Name": "torchvision.transforms.Normalize", "Docstring": "Normalize a tensor image with mean and standard deviation.\nThis transform does not support PIL Image.\nGiven mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n``\nchannels, this transform will normalize each channel of the input\n``torch.*Tensor`` i.e.,\n``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n.. note::\n    This transform acts out of place, i.e., it does not mutate the input tensor.\n\nArgs:\n    mean (sequence): Sequence of means for each channel.\n    std (sequence): Sequence of standard deviations for each channel.\n    inplace(bool,optional): Bool to make this operation in-place.", "Library": "PyTorch"}
{"API_Name": "torchvision.transforms.ToTensor", "Docstring": "Convert a PIL Image or ndarray to tensor and scale the values accordingly.\n\nThis transform does not support torchscript.\n\nConverts a PIL Image or numpy.ndarray (H x W x C) in the range\n[0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\nif the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1)\nor if the numpy.ndarray has dtype = np.uint8\n\nIn the other cases, tensors are returned without scaling.\n\n.. note::\n    Because the input image is scaled to [0.0, 1.0], this transformation should not be used when\n    transforming target image masks. See the `references`_ for implementing the transforms for image masks.\n\n.. _references: https://github.com/pytorch/vision/tree/main/references/segmentation", "Library": "PyTorch"}
{"API_Name": "keras.Input", "Docstring": "Used to instantiate a Keras tensor.\n\nA Keras tensor is a symbolic tensor-like object, which we augment with\ncertain attributes that allow us to build a Keras model just by knowing the\ninputs and outputs of the model.\n\nFor instance, if `a`, `b` and `c` are Keras tensors,\nit becomes possible to do:\n`model = Model(input=[a, b], output=c)`\n\nArgs:\n    shape: A shape tuple (tuple of integers or `None` objects),\n        not including the batch size.\n        For instance, `shape=(32,)` indicates that the expected input\n        will be batches of 32-dimensional vectors. Elements of this tuple\n        can be `None`; `None` elements represent dimensions where the shape\n        is not known and may vary (e.g. sequence length).\n    batch_size: Optional static batch size (integer).\n    dtype: The data type expected by the input, as a string\n        (e.g. `\"float32\"`, `\"int32\"`...)\n    sparse: A boolean specifying whether the expected input will be sparse\n        tensors. Note that, if `sparse` is `False`, sparse tensors can still\n        be passed into the input - they will be densified with a default\n        value of 0. This feature is only supported with the TensorFlow\n        backend. Defaults to `False`.\n    name: Optional name string for the layer.\n        Should be unique in a model (do not reuse the same name twice).\n        It will be autogenerated if it isn't provided.\n    tensor: Optional existing tensor to wrap into the `Input` layer.\n        If set, the layer will use this tensor rather\n        than creating a new placeholder tensor.\n    optional: Boolean, whether the input is optional or not.\n        An optional input can accept `None` values.\n\nReturns:\n  A Keras tensor.\n\nExample:\n\n```python\n# This is a logistic regression in Keras\nx = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)\n```", "Library": "TensorFlow"}
{"API_Name": "keras.initializers.RandomNormal", "Docstring": "Random normal initializer.\n\nDraws samples from a normal distribution for given parameters.\n\nExamples:\n\n>>> # Standalone usage:\n>>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n>>> values = initializer(shape=(2, 2))\n\n>>> # Usage in a Keras layer:\n>>> initializer = RandomNormal(mean=0.0, stddev=1.0)\n>>> layer = Dense(3, kernel_initializer=initializer)\n\nArgs:\n    mean: A python scalar or a scalar keras tensor. Mean of the random\n        values to generate.\n    stddev: A python scalar or a scalar keras tensor. Standard deviation of\n       the random values to generate.\n    seed: A Python integer or instance of\n        `keras.backend.SeedGenerator`.\n        Used to make the behavior of the initializer\n        deterministic. Note that an initializer seeded with an integer\n        or `None` (unseeded) will produce the same random values\n        across multiple calls. To get different random values\n        across multiple calls, use as seed an instance\n        of `keras.backend.SeedGenerator`.", "Library": "TensorFlow"}
{"API_Name": "keras.Sequential", "Docstring": "`Sequential` groups a linear stack of layers into a `Model`.\n\nExamples:\n\n```python\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(16,)))\nmodel.add(keras.layers.Dense(8))\n\n# Note that you can also omit the initial `Input`.\n# In that case the model doesn't have any weights until the first call\n# to a training/evaluation method (since it isn't yet built):\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(8))\nmodel.add(keras.layers.Dense(4))\n# model.weights not created yet\n\n# Whereas if you specify an `Input`, the model gets built\n# continuously as you are adding layers:\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(16,)))\nmodel.add(keras.layers.Dense(8))\nlen(model.weights)  # Returns \"2\"\n\n# When using the delayed-build pattern (no input shape specified), you can\n# choose to manually build your model by calling\n# `build(batch_input_shape)`:\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(8))\nmodel.add(keras.layers.Dense(4))\nmodel.build((None, 16))\nlen(model.weights)  # Returns \"4\"\n\n# Note that when using the delayed-build pattern (no input shape specified),\n# the model gets built the first time you call `fit`, `eval`, or `predict`,\n# or the first time you call the model on some input data.\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(8))\nmodel.add(keras.layers.Dense(1))\nmodel.compile(optimizer='sgd', loss='mse')\n# This builds the model for the first time:\nmodel.fit(x, y, batch_size=32, epochs=10)\n```", "Library": "TensorFlow"}
{"API_Name": "layers.BatchNormalization", "Docstring": "Layer that normalizes its inputs.\n\nBatch normalization applies a transformation that maintains the mean output\nclose to 0 and the output standard deviation close to 1.\n\nImportantly, batch normalization works differently during training and\nduring inference.\n\n**During training** (i.e. when using `fit()` or when calling the layer/model\nwith the argument `training=True`), the layer normalizes its output using\nthe mean and standard deviation of the current batch of inputs. That is to\nsay, for each channel being normalized, the layer returns\n`gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta`, where:\n\n- `epsilon` is small constant (configurable as part of the constructor\narguments)\n- `gamma` is a learned scaling factor (initialized as 1), which\ncan be disabled by passing `scale=False` to the constructor.\n- `beta` is a learned offset factor (initialized as 0), which\ncan be disabled by passing `center=False` to the constructor.\n\n**During inference** (i.e. when using `evaluate()` or `predict()` or when\ncalling the layer/model with the argument `training=False` (which is the\ndefault), the layer normalizes its output using a moving average of the\nmean and standard deviation of the batches it has seen during training. That\nis to say, it returns\n`gamma * (batch - self.moving_mean) / sqrt(self.moving_var+epsilon) + beta`.\n\n`self.moving_mean` and `self.moving_var` are non-trainable variables that\nare updated each time the layer in called in training mode, as such:\n\n- `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`\n- `moving_var = moving_var * momentum + var(batch) * (1 - momentum)`\n\nAs such, the layer will only normalize its inputs during inference\n*after having been trained on data that has similar statistics as the\ninference data*.\n\nArgs:\n    axis: Integer, the axis that should be normalized\n        (typically the features axis). For instance, after a `Conv2D` layer\n        with `data_format=\"channels_first\"`, use `axis=1`.\n    momentum: Momentum for the moving average.\n    epsilon: Small float added to variance to avoid dividing by zero.\n    center: If `True`, add offset of `beta` to normalized tensor.\n        If `False`, `beta` is ignored.\n    scale: If `True`, multiply by `gamma`. If `False`, `gamma` is not used.\n        When the next layer is linear this can be disabled\n        since the scaling will be done by the next layer.\n    beta_initializer: Initializer for the beta weight.\n    gamma_initializer: Initializer for the gamma weight.\n    moving_mean_initializer: Initializer for the moving mean.\n    moving_variance_initializer: Initializer for the moving variance.\n    beta_regularizer: Optional regularizer for the beta weight.\n    gamma_regularizer: Optional regularizer for the gamma weight.\n    beta_constraint: Optional constraint for the beta weight.\n    gamma_constraint: Optional constraint for the gamma weight.\n    synchronized: Only applicable with the TensorFlow backend.\n        If `True`, synchronizes the global batch statistics (mean and\n        variance) for the layer across all devices at each training step\n        in a distributed training strategy.\n        If `False`, each replica uses its own local batch statistics.\n    **kwargs: Base layer keyword arguments (e.g. `name` and `dtype`).\n\nCall arguments:\n    inputs: Input tensor (of any rank).\n    training: Python boolean indicating whether the layer should behave in\n        training mode or in inference mode.\n        - `training=True`: The layer will normalize its inputs using\n        the mean and variance of the current batch of inputs.\n        - `training=False`: The layer will normalize its inputs using\n        the mean and variance of its moving statistics, learned during\n        training.\n    mask: Binary tensor of shape broadcastable to `inputs` tensor, with\n        `True` values indicating the positions for which mean and variance\n        should be computed. Masked elements of the current inputs are not\n        taken into account for mean and variance computation during\n        training. Any prior unmasked element values will be taken into\n        account until their momentum expires.\n\nReference:\n\n- [Ioffe and Szegedy, 2015](https://arxiv.org/abs/1502.03167).\n\n**About setting `layer.trainable = False` on a `BatchNormalization` layer:**\n\nThe meaning of setting `layer.trainable = False` is to freeze the layer,\ni.e. its internal state will not change during training:\nits trainable weights will not be updated\nduring `fit()` or `train_on_batch()`, and its state updates will not be run.\n\nUsually, this does not necessarily mean that the layer is run in inference\nmode (which is normally controlled by the `training` argument that can\nbe passed when calling a layer). \"Frozen state\" and \"inference mode\"\nare two separate concepts.\n\nHowever, in the case of the `BatchNormalization` layer, **setting\n`trainable = False` on the layer means that the layer will be\nsubsequently run in inference mode** (meaning that it will use\nthe moving mean and the moving variance to normalize the current batch,\nrather than using the mean and variance of the current batch).\n\nNote that:\n\n- Setting `trainable` on an model containing other layers will recursively\n    set the `trainable` value of all inner layers.\n- If the value of the `trainable` attribute is changed after calling\n    `compile()` on a model, the new value doesn't take effect for this model\n    until `compile()` is called again.", "Library": "TensorFlow"}
{"API_Name": "layers.Conv2D", "Docstring": "2D convolution layer.\n\nThis layer creates a convolution kernel that is convolved with the layer\ninput over a single spatial (or temporal) dimension to produce a tensor of\noutputs. If `use_bias` is True, a bias vector is created and added to the\noutputs. Finally, if `activation` is not `None`, it is applied to the\noutputs as well.\n\nArgs:\n    filters: int, the dimension of the output space (the number of filters\n        in the convolution).\n    kernel_size: int or tuple/list of 2 integer, specifying the size of the\n        convolution window.\n    strides: int or tuple/list of 2 integer, specifying the stride length\n        of the convolution. `strides > 1` is incompatible with\n        `dilation_rate > 1`.\n    padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n        the left/right or up/down of the input. When `padding=\"same\"` and\n        `strides=1`, the output has the same size as the input.\n    data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs. `\"channels_last\"`\n        corresponds to inputs with shape\n        `(batch_size, height, width, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch_size, channels, height, width)`. It defaults to the\n        `image_data_format` value found in your Keras config file at\n        `~/.keras/keras.json`. If you never set it, then it will be\n        `\"channels_last\"`.\n    dilation_rate: int or tuple/list of 2 integers, specifying the dilation\n        rate to use for dilated convolution.\n    groups: A positive int specifying the number of groups in which the\n        input is split along the channel axis. Each group is convolved\n        separately with `filters // groups` filters. The output is the\n        concatenation of all the `groups` results along the channel axis.\n        Input channels and `filters` must both be divisible by `groups`.\n    activation: Activation function. If `None`, no activation is applied.\n    use_bias: bool, if `True`, bias will be added to the output.\n    kernel_initializer: Initializer for the convolution kernel. If `None`,\n        the default initializer (`\"glorot_uniform\"`) will be used.\n    bias_initializer: Initializer for the bias vector. If `None`, the\n        default initializer (`\"zeros\"`) will be used.\n    kernel_regularizer: Optional regularizer for the convolution kernel.\n    bias_regularizer: Optional regularizer for the bias vector.\n    activity_regularizer: Optional regularizer function for the output.\n    kernel_constraint: Optional projection function to be applied to the\n        kernel after being updated by an `Optimizer` (e.g. used to implement\n        norm constraints or value constraints for layer weights). The\n        function must take as input the unprojected variable and must return\n        the projected variable (which must have the same shape). Constraints\n        are not safe to use when doing asynchronous distributed training.\n    bias_constraint: Optional projection function to be applied to the\n        bias after being updated by an `Optimizer`.\n\nInput shape:\n\n- If `data_format=\"channels_last\"`:\n    A 4D tensor with shape: `(batch_size, height, width, channels)`\n- If `data_format=\"channels_first\"`:\n    A 4D tensor with shape: `(batch_size, channels, height, width)`\n\nOutput shape:\n\n- If `data_format=\"channels_last\"`:\n    A 4D tensor with shape: `(batch_size, new_height, new_width, filters)`\n- If `data_format=\"channels_first\"`:\n    A 4D tensor with shape: `(batch_size, filters, new_height, new_width)`\n\nReturns:\n    A 4D tensor representing `activation(conv2d(inputs, kernel) + bias)`.\n\nRaises:\n    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n\nExample:\n\n>>> x = np.random.rand(4, 10, 10, 128)\n>>> y = keras.layers.Conv2D(32, 3, activation='relu')(x)\n>>> print(y.shape)\n(4, 8, 8, 32)", "Library": "TensorFlow"}
{"API_Name": "layers.Dense", "Docstring": "Just your regular densely-connected NN layer.\n\n`Dense` implements the operation:\n`output = activation(dot(input, kernel) + bias)`\nwhere `activation` is the element-wise activation function\npassed as the `activation` argument, `kernel` is a weights matrix\ncreated by the layer, and `bias` is a bias vector created by the layer\n(only applicable if `use_bias` is `True`).\n\nNote: If the input to the layer has a rank greater than 2, `Dense`\ncomputes the dot product between the `inputs` and the `kernel` along the\nlast axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\nFor example, if input has dimensions `(batch_size, d0, d1)`, then we create\na `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\nof the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n`batch_size * d0` such sub-tensors). The output in this case will have\nshape `(batch_size, d0, units)`.\n\nArgs:\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use.\n        If you don't specify anything, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix.\n    bias_initializer: Initializer for the bias vector.\n    kernel_regularizer: Regularizer function applied to\n        the `kernel` weights matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to\n        the output of the layer (its \"activation\").\n    kernel_constraint: Constraint function applied to\n        the `kernel` weights matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n    lora_rank: Optional integer. If set, the layer's forward pass\n        will implement LoRA (Low-Rank Adaptation)\n        with the provided rank. LoRA sets the layer's kernel\n        to non-trainable and replaces it with a delta over the\n        original kernel, obtained via multiplying two lower-rank\n        trainable matrices. This can be useful to reduce the\n        computation cost of fine-tuning large dense layers.\n        You can also enable LoRA on an existing\n        `Dense` layer by calling `layer.enable_lora(rank)`.\n\nInput shape:\n    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n    The most common situation would be\n    a 2D input with shape `(batch_size, input_dim)`.\n\nOutput shape:\n    N-D tensor with shape: `(batch_size, ..., units)`.\n    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n    the output would have shape `(batch_size, units)`.", "Library": "TensorFlow"}
{"API_Name": "layers.Dropout", "Docstring": "Applies dropout to the input.\n\nThe `Dropout` layer randomly sets input units to 0 with a frequency of\n`rate` at each step during training time, which helps prevent overfitting.\nInputs not set to 0 are scaled up by `1 / (1 - rate)` such that the sum over\nall inputs is unchanged.\n\nNote that the `Dropout` layer only applies when `training` is set to `True`\nin `call()`, such that no values are dropped during inference.\nWhen using `model.fit`, `training` will be appropriately set to `True`\nautomatically. In other contexts, you can set the argument explicitly\nto `True` when calling the layer.\n\n(This is in contrast to setting `trainable=False` for a `Dropout` layer.\n`trainable` does not affect the layer's behavior, as `Dropout` does\nnot have any variables/weights that can be frozen during training.)\n\nArgs:\n    rate: Float between 0 and 1. Fraction of the input units to drop.\n    noise_shape: 1D integer tensor representing the shape of the\n        binary dropout mask that will be multiplied with the input.\n        For instance, if your inputs have shape\n        `(batch_size, timesteps, features)` and\n        you want the dropout mask to be the same for all timesteps,\n        you can use `noise_shape=(batch_size, 1, features)`.\n    seed: A Python integer to use as random seed.\n\nCall arguments:\n    inputs: Input tensor (of any rank).\n    training: Python boolean indicating whether the layer should behave in\n        training mode (adding dropout) or in inference mode (doing nothing).", "Library": "TensorFlow"}
{"API_Name": "layers.Embedding", "Docstring": "Turns positive integers (indexes) into dense vectors of fixed size.\n\ne.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n\nThis layer can only be used on positive integer inputs of a fixed range.\n\nExample:\n\n>>> model = keras.Sequential()\n>>> model.add(keras.layers.Embedding(1000, 64))\n>>> # The model will take as input an integer matrix of size (batch,\n>>> # input_length), and the largest integer (i.e. word index) in the input\n>>> # should be no larger than 999 (vocabulary size).\n>>> # Now model.output_shape is (None, 10, 64), where `None` is the batch\n>>> # dimension.\n>>> input_array = np.random.randint(1000, size=(32, 10))\n>>> model.compile('rmsprop', 'mse')\n>>> output_array = model.predict(input_array)\n>>> print(output_array.shape)\n(32, 10, 64)\n\nArgs:\n    input_dim: Integer. Size of the vocabulary,\n        i.e. maximum integer index + 1.\n    output_dim: Integer. Dimension of the dense embedding.\n    embeddings_initializer: Initializer for the `embeddings`\n        matrix (see `keras.initializers`).\n    embeddings_regularizer: Regularizer function applied to\n        the `embeddings` matrix (see `keras.regularizers`).\n    embeddings_constraint: Constraint function applied to\n        the `embeddings` matrix (see `keras.constraints`).\n    mask_zero: Boolean, whether or not the input value 0 is a special\n        \"padding\" value that should be masked out.\n        This is useful when using recurrent layers which\n        may take variable length input. If this is `True`,\n        then all subsequent layers in the model need\n        to support masking or an exception will be raised.\n        If `mask_zero` is set to `True`, as a consequence,\n        index 0 cannot be used in the vocabulary (`input_dim` should\n        equal size of vocabulary + 1).\n    weights: Optional floating-point matrix of size\n        `(input_dim, output_dim)`. The initial embeddings values\n        to use.\n    lora_rank: Optional integer. If set, the layer's forward pass\n        will implement LoRA (Low-Rank Adaptation)\n        with the provided rank. LoRA sets the layer's embeddings\n        matrix to non-trainable and replaces it with a delta over the\n        original matrix, obtained via multiplying two lower-rank\n        trainable matrices. This can be useful to reduce the\n        computation cost of fine-tuning large embedding layers.\n        You can also enable LoRA on an existing\n        `Embedding` layer by calling `layer.enable_lora(rank)`.\n\nInput shape:\n    2D tensor with shape: `(batch_size, input_length)`.\n\nOutput shape:\n    3D tensor with shape: `(batch_size, input_length, output_dim)`.", "Library": "TensorFlow"}
{"API_Name": "layers.Flatten", "Docstring": "Flattens the input. Does not affect the batch size.\n\nNote: If inputs are shaped `(batch,)` without a feature axis, then\nflattening adds an extra channel dimension and output shape is `(batch, 1)`.\n\nArgs:\n    data_format: A string, one of `\"channels_last\"` (default) or\n        `\"channels_first\"`. The ordering of the dimensions in the inputs.\n        `\"channels_last\"` corresponds to inputs with shape\n        `(batch, ..., channels)` while `\"channels_first\"` corresponds to\n        inputs with shape `(batch, channels, ...)`.\n        When unspecified, uses `image_data_format` value found in your Keras\n        config file at `~/.keras/keras.json` (if exists). Defaults to\n        `\"channels_last\"`.\n\nExample:\n\n>>> x = keras.Input(shape=(10, 64))\n>>> y = keras.layers.Flatten()(x)\n>>> y.shape\n(None, 640)", "Library": "TensorFlow"}
{"API_Name": "layers.MaxPooling2D", "Docstring": "Max pooling operation for 2D spatial data.\n\nDownsamples the input along its spatial dimensions (height and width)\nby taking the maximum value over an input window\n(of size defined by `pool_size`) for each channel of the input.\nThe window is shifted by `strides` along each dimension.\n\nThe resulting output when using the `\"valid\"` padding option has a spatial\nshape (number of rows or columns) of:\n`output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n(when `input_shape >= pool_size`)\n\nThe resulting output shape when using the `\"same\"` padding option is:\n`output_shape = math.floor((input_shape - 1) / strides) + 1`\n\nArgs:\n    pool_size: int or tuple of 2 integers, factors by which to downscale\n        (dim1, dim2). If only one integer is specified, the same\n        window length will be used for all dimensions.\n    strides: int or tuple of 2 integers, or None. Strides values. If None,\n        it will default to `pool_size`. If only one int is specified, the\n        same stride size will be used for all dimensions.\n    padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n        the left/right or up/down of the input such that output has the same\n        height/width dimension as the input.\n    data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs. `\"channels_last\"`\n        corresponds to inputs with shape `(batch, height, width, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, channels, height, width)`. It defaults to the\n        `image_data_format` value found in your Keras config file at\n        `~/.keras/keras.json`. If you never set it, then it will be\n        `\"channels_last\"`.\n\nInput shape:\n\n- If `data_format=\"channels_last\"`:\n    4D tensor with shape `(batch_size, height, width, channels)`.\n- If `data_format=\"channels_first\"`:\n    4D tensor with shape `(batch_size, channels, height, width)`.\n\nOutput shape:\n\n- If `data_format=\"channels_last\"`:\n    4D tensor with shape\n    `(batch_size, pooled_height, pooled_width, channels)`.\n- If `data_format=\"channels_first\"`:\n    4D tensor with shape\n    `(batch_size, channels, pooled_height, pooled_width)`.\n\nExamples:\n\n`strides=(1, 1)` and `padding=\"valid\"`:\n\n>>> x = np.array([[1., 2., 3.],\n...               [4., 5., 6.],\n...               [7., 8., 9.]])\n>>> x = np.reshape(x, [1, 3, 3, 1])\n>>> max_pool_2d = keras.layers.MaxPooling2D(pool_size=(2, 2),\n...    strides=(1, 1), padding=\"valid\")\n>>> max_pool_2d(x)\n\n`strides=(2, 2)` and `padding=\"valid\"`:\n\n>>> x = np.array([[1., 2., 3., 4.],\n...               [5., 6., 7., 8.],\n...               [9., 10., 11., 12.]])\n>>> x = np.reshape(x, [1, 3, 4, 1])\n>>> max_pool_2d = keras.layers.MaxPooling2D(pool_size=(2, 2),\n...    strides=(2, 2), padding=\"valid\")\n>>> max_pool_2d(x)\n\n`stride=(1, 1)` and `padding=\"same\"`:\n\n>>> x = np.array([[1., 2., 3.],\n...               [4., 5., 6.],\n...               [7., 8., 9.]])\n>>> x = np.reshape(x, [1, 3, 3, 1])\n>>> max_pool_2d = keras.layers.MaxPooling2D(pool_size=(2, 2),\n...    strides=(1, 1), padding=\"same\")\n>>> max_pool_2d(x)", "Library": "TensorFlow"}
{"API_Name": "layers.ReLU", "Docstring": "Rectified Linear Unit activation function layer.\n\nFormula:\n``` python\nf(x) = max(x,0)\nf(x) = max_value if x >= max_value\nf(x) = x if threshold <= x < max_value\nf(x) = negative_slope * (x - threshold) otherwise\n```\n\nExample:\n``` python\nrelu_layer = keras.layers.activations.ReLU(\n    max_value=10,\n    negative_slope=0.5,\n    threshold=0,\n)\ninput = np.array([-10, -5, 0.0, 5, 10])\nresult = relu_layer(input)\n# result = [-5. , -2.5,  0. ,  5. , 10.]\n```\n\nArgs:\n    max_value: Float >= 0. Maximum activation value. None means unlimited.\n        Defaults to `None`.\n    negative_slope: Float >= 0. Negative slope coefficient.\n        Defaults to `0.0`.\n    threshold: Float >= 0. Threshold value for thresholded activation.\n        Defaults to `0.0`.\n    **kwargs: Base layer keyword arguments, such as `name` and `dtype`.", "Library": "TensorFlow"}
{"API_Name": "tape.gradient", "Docstring": "Computes the gradient using operations recorded in context of this tape.\n\nNote: Unless you set `persistent=True` a GradientTape can only be used to\ncompute one set of gradients (or jacobians).\n\nIn addition to Tensors, gradient also supports RaggedTensors. For example,\n\n>>> x = tf.ragged.constant([[1.0, 2.0], [3.0]])\n>>> with tf.GradientTape() as g:\n...   g.watch(x)\n...   y = x * x\n>>> g.gradient(y, x)\n<tf.RaggedTensor [[2.0, 4.0], [6.0]]>\n\nArgs:\n  target: a list or nested structure of Tensors or Variables or\n    CompositeTensors to be differentiated.\n  sources: a list or nested structure of Tensors or Variables or\n    CompositeTensors. `target` will be differentiated against elements in\n    `sources`.\n  output_gradients: a list of gradients, one for each differentiable\n    element of target. Defaults to None.\n  unconnected_gradients: a value which can either hold 'none' or 'zero' and\n    alters the value which will be returned if the target and sources are\n    unconnected. The possible values and effects are detailed in\n    'UnconnectedGradients' and it defaults to 'none'.\n\nReturns:\n  a list or nested structure of Tensors (or IndexedSlices, or None, or\n  CompositeTensor), one for each element in `sources`. Returned structure\n  is the same as the structure of `sources`.\n\nRaises:\n  RuntimeError: If called on a used, non-persistent tape.\n  RuntimeError: If called inside the context of the tape.\n  TypeError: If the target is a None object.\n  ValueError: If the target is a variable or if unconnected gradients is\n   called with an unknown value.", "Library": "TensorFlow"}
{"API_Name": "tape.watch", "Docstring": "Ensures that `tensor` is being traced by this tape.\n\nArgs:\n  tensor: a Tensor/Variable or list of Tensors/Variables.\n\nRaises:\n  ValueError: if it encounters something that is not a tensor.", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.initializers.HeUniform", "Docstring": "He uniform variance scaling initializer.\n\nDraws samples from a uniform distribution within `[-limit, limit]`, where\n`limit = sqrt(6 / fan_in)` (`fan_in` is the number of input units in the\nweight tensor).\n\nExamples:\n\n>>> # Standalone usage:\n>>> initializer = HeUniform()\n>>> values = initializer(shape=(2, 2))\n\n>>> # Usage in a Keras layer:\n>>> initializer = HeUniform()\n>>> layer = Dense(3, kernel_initializer=initializer)\n\nArgs:\n    seed: A Python integer or instance of\n        `keras.backend.SeedGenerator`.\n        Used to make the behavior of the initializer\n        deterministic. Note that an initializer seeded with an integer\n        or `None` (unseeded) will produce the same random values\n        across multiple calls. To get different random values\n        across multiple calls, use as seed an instance\n        of `keras.backend.SeedGenerator`.\n\nReference:\n\n- [He et al., 2015](https://arxiv.org/abs/1502.01852)", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.Activation", "Docstring": "Applies an activation function to an output.\n\nArgs:\n    activation: Activation function. It could be a callable, or the name of\n        an activation from the `keras.activations` namespace.\n    **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n\nExample:\n\n>>> layer = keras.layers.Activation('relu')\n>>> layer([-3.0, -1.0, 0.0, 2.0])\n[0.0, 0.0, 0.0, 2.0]\n>>> layer = keras.layers.Activation(keras.activations.relu)\n>>> layer([-3.0, -1.0, 0.0, 2.0])\n[0.0, 0.0, 0.0, 2.0]", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.AvgPool2D", "Docstring": "Average pooling operation for 2D spatial data.\n\nDownsamples the input along its spatial dimensions (height and width)\nby taking the average value over an input window\n(of size defined by `pool_size`) for each channel of the input.\nThe window is shifted by `strides` along each dimension.\n\nThe resulting output when using the `\"valid\"` padding option has a spatial\nshape (number of rows or columns) of:\n`output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n(when `input_shape >= pool_size`)\n\nThe resulting output shape when using the `\"same\"` padding option is:\n`output_shape = math.floor((input_shape - 1) / strides) + 1`\n\nArgs:\n    pool_size: int or tuple of 2 integers, factors by which to downscale\n        (dim1, dim2). If only one integer is specified, the same\n        window length will be used for all dimensions.\n    strides: int or tuple of 2 integers, or None. Strides values. If None,\n        it will default to `pool_size`. If only one int is specified, the\n        same stride size will be used for all dimensions.\n    padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n        the left/right or up/down of the input such that output has the same\n        height/width dimension as the input.\n    data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs. `\"channels_last\"`\n        corresponds to inputs with shape `(batch, height, width, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, channels, height, width)`. It defaults to the\n        `image_data_format` value found in your Keras config file at\n        `~/.keras/keras.json`. If you never set it, then it will be\n        `\"channels_last\"`.\n\nInput shape:\n\n- If `data_format=\"channels_last\"`:\n    4D tensor with shape `(batch_size, height, width, channels)`.\n- If `data_format=\"channels_first\"`:\n    4D tensor with shape `(batch_size, channels, height, width)`.\n\nOutput shape:\n\n- If `data_format=\"channels_last\"`:\n    4D tensor with shape\n    `(batch_size, pooled_height, pooled_width, channels)`.\n- If `data_format=\"channels_first\"`:\n    4D tensor with shape\n    `(batch_size, channels, pooled_height, pooled_width)`.\n\nExamples:\n\n`strides=(1, 1)` and `padding=\"valid\"`:\n\n>>> x = np.array([[1., 2., 3.],\n...               [4., 5., 6.],\n...               [7., 8., 9.]])\n>>> x = np.reshape(x, [1, 3, 3, 1])\n>>> avg_pool_2d = keras.layers.AveragePooling2D(pool_size=(2, 2),\n...    strides=(1, 1), padding=\"valid\")\n>>> avg_pool_2d(x)\n\n`strides=(2, 2)` and `padding=\"valid\"`:\n\n>>> x = np.array([[1., 2., 3., 4.],\n...              [5., 6., 7., 8.],\n...              [9., 10., 11., 12.]])\n>>> x = np.reshape(x, [1, 3, 4, 1])\n>>> avg_pool_2d = keras.layers.AveragePooling2D(pool_size=(2, 2),\n...    strides=(2, 2), padding=\"valid\")\n>>> avg_pool_2d(x)\n\n`stride=(1, 1)` and `padding=\"same\"`:\n\n>>> x = np.array([[1., 2., 3.],\n...                  [4., 5., 6.],\n...                  [7., 8., 9.]])\n>>> x = np.reshape(x, [1, 3, 3, 1])\n>>> avg_pool_2d = keras.layers.AveragePooling2D(pool_size=(2, 2),\n...    strides=(1, 1), padding=\"same\")\n>>> avg_pool_2d(x)", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.Bidirectional", "Docstring": "Bidirectional wrapper for RNNs.\n\nArgs:\n    layer: `keras.layers.RNN` instance, such as\n        `keras.layers.LSTM` or `keras.layers.GRU`.\n        It could also be a `keras.layers.Layer` instance\n        that meets the following criteria:\n        1. Be a sequence-processing layer (accepts 3D+ inputs).\n        2. Have a `go_backwards`, `return_sequences` and `return_state`\n        attribute (with the same semantics as for the `RNN` class).\n        3. Have an `input_spec` attribute.\n        4. Implement serialization via `get_config()` and `from_config()`.\n        Note that the recommended way to create new RNN layers is to write a\n        custom RNN cell and use it with `keras.layers.RNN`, instead of\n        subclassing `keras.layers.Layer` directly.\n        When `return_sequences` is `True`, the output of the masked\n        timestep will be zero regardless of the layer's original\n        `zero_output_for_mask` value.\n    merge_mode: Mode by which outputs of the forward and backward RNNs\n        will be combined. One of `{\"sum\", \"mul\", \"concat\", \"ave\", None}`.\n        If `None`, the outputs will not be combined,\n        they will be returned as a list. Defaults to `\"concat\"`.\n    backward_layer: Optional `keras.layers.RNN`,\n        or `keras.layers.Layer` instance to be used to handle\n        backwards input processing.\n        If `backward_layer` is not provided, the layer instance passed\n        as the `layer` argument will be used to generate the backward layer\n        automatically.\n        Note that the provided `backward_layer` layer should have properties\n        matching those of the `layer` argument, in particular\n        it should have the same values for `stateful`, `return_states`,\n        `return_sequences`, etc. In addition, `backward_layer`\n        and `layer` should have different `go_backwards` argument values.\n        A `ValueError` will be raised if these requirements are not met.\n\nCall arguments:\n    The call arguments for this layer are the same as those of the\n    wrapped RNN layer. Beware that when passing the `initial_state`\n    argument during the call of this layer, the first half in the\n    list of elements in the `initial_state` list will be passed to\n    the forward RNN call and the last half in the list of elements\n    will be passed to the backward RNN call.\n\nNote: instantiating a `Bidirectional` layer from an existing RNN layer\ninstance will not reuse the weights state of the RNN layer instance -- the\n`Bidirectional` layer will have freshly initialized weights.\n\nExamples:\n\n```python\nmodel = Sequential([\n    Input(shape=(5, 10)),\n    Bidirectional(LSTM(10, return_sequences=True),\n    Bidirectional(LSTM(10)),\n    Dense(5, activation=\"softmax\"),\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n# With custom backward layer\nforward_layer = LSTM(10, return_sequences=True)\nbackward_layer = LSTM(10, activation='relu', return_sequences=True,\n                      go_backwards=True)\nmodel = Sequential([\n    Input(shape=(5, 10)),\n    Bidirectional(forward_layer, backward_layer=backward_layer),\n    Dense(5, activation=\"softmax\"),\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n```", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.Conv2DTranspose", "Docstring": "2D transposed convolution layer.\n\nThe need for transposed convolutions generally arise from the desire to use\na transformation going in the opposite direction of a normal convolution,\ni.e., from something that has the shape of the output of some convolution\nto something that has the shape of its input while maintaining a\nconnectivity pattern that is compatible with said convolution.\n\nArgs:\n    filters: int, the dimension of the output space (the number of filters\n        in the transposed convolution).\n    kernel_size: int or tuple/list of 1 integer, specifying the size of the\n        transposed convolution window.\n    strides: int or tuple/list of 1 integer, specifying the stride length\n        of the transposed convolution. `strides > 1` is incompatible with\n        `dilation_rate > 1`.\n    padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n        the left/right or up/down of the input. When `padding=\"same\"` and\n        `strides=1`, the output has the same size as the input.\n    data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs. `\"channels_last\"`\n        corresponds to inputs with shape\n        `(batch_size, height, width, channels)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch_size, channels, height, width)`. It defaults to the\n        `image_data_format` value found in your Keras config file at\n        `~/.keras/keras.json`. If you never set it, then it will be\n        `\"channels_last\"`.\n    dilation_rate: int or tuple/list of 1 integers, specifying the dilation\n        rate to use for dilated transposed convolution.\n    activation: Activation function. If `None`, no activation is applied.\n    use_bias: bool, if `True`, bias will be added to the output.\n    kernel_initializer: Initializer for the convolution kernel. If `None`,\n        the default initializer (`\"glorot_uniform\"`) will be used.\n    bias_initializer: Initializer for the bias vector. If `None`, the\n        default initializer (`\"zeros\"`) will be used.\n    kernel_regularizer: Optional regularizer for the convolution kernel.\n    bias_regularizer: Optional regularizer for the bias vector.\n    activity_regularizer: Optional regularizer function for the output.\n    kernel_constraint: Optional projection function to be applied to the\n        kernel after being updated by an `Optimizer` (e.g. used to implement\n        norm constraints or value constraints for layer weights). The\n        function must take as input the unprojected variable and must return\n        the projected variable (which must have the same shape). Constraints\n        are not safe to use when doing asynchronous distributed training.\n    bias_constraint: Optional projection function to be applied to the\n        bias after being updated by an `Optimizer`.\n\nInput shape:\n\n- If `data_format=\"channels_last\"`:\n    A 4D tensor with shape: `(batch_size, height, width, channels)`\n- If `data_format=\"channels_first\"`:\n    A 4D tensor with shape: `(batch_size, channels, height, width)`\n\nOutput shape:\n\n- If `data_format=\"channels_last\"`:\n    A 4D tensor with shape: `(batch_size, new_height, new_width, filters)`\n- If `data_format=\"channels_first\"`:\n    A 4D tensor with shape: `(batch_size, filters, new_height, new_width)`\n\nReturns:\n    A 4D tensor representing\n    `activation(conv2d_transpose(inputs, kernel) + bias)`.\n\nRaises:\n    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n\nReferences:\n- [A guide to convolution arithmetic for deep learning](\n    https://arxiv.org/abs/1603.07285v1)\n- [Deconvolutional Networks](\n    https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n\nExample:\n\n>>> x = np.random.rand(4, 10, 8, 128)\n>>> y = keras.layers.Conv2DTranspose(32, 2, 2, activation='relu')(x)\n>>> print(y.shape)\n(4, 20, 16, 32)", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.GRU", "Docstring": "Gated Recurrent Unit - Cho et al. 2014.\n\nBased on available runtime hardware and constraints, this layer\nwill choose different implementations (cuDNN-based or backend-native)\nto maximize the performance. If a GPU is available and all\nthe arguments to the layer meet the requirement of the cuDNN kernel\n(see below for details), the layer will use a fast cuDNN implementation\nwhen using the TensorFlow backend.\n\nThe requirements to use the cuDNN implementation are:\n\n1. `activation` == `tanh`\n2. `recurrent_activation` == `sigmoid`\n3. `dropout` == 0 and `recurrent_dropout` == 0\n4. `unroll` is `False`\n5. `use_bias` is `True`\n6. `reset_after` is `True`\n7. Inputs, if use masking, are strictly right-padded.\n8. Eager execution is enabled in the outermost context.\n\nThere are two variants of the GRU implementation. The default one is based\non [v3](https://arxiv.org/abs/1406.1078v3) and has reset gate applied to\nhidden state before matrix multiplication. The other one is based on\n[original](https://arxiv.org/abs/1406.1078v1) and has the order reversed.\n\nThe second variant is compatible with CuDNNGRU (GPU-only) and allows\ninference on CPU. Thus it has separate biases for `kernel` and\n`recurrent_kernel`. To use this variant, set `reset_after=True` and\n`recurrent_activation='sigmoid'`.\n\nFor example:\n\n>>> inputs = np.random.random((32, 10, 8))\n>>> gru = keras.layers.GRU(4)\n>>> output = gru(inputs)\n>>> output.shape\n(32, 4)\n>>> gru = keras.layers.GRU(4, return_sequences=True, return_state=True)\n>>> whole_sequence_output, final_state = gru(inputs)\n>>> whole_sequence_output.shape\n(32, 10, 4)\n>>> final_state.shape\n(32, 4)\n\nArgs:\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use.\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step.\n        Default: sigmoid (`sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, (default `True`), whether the layer\n        should use a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs. Default:\n        `\"glorot_uniform\"`.\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix, used for the linear transformation of the recurrent\n        state. Default: `\"orthogonal\"`.\n    bias_initializer: Initializer for the bias vector. Default: `\"zeros\"`.\n    kernel_regularizer: Regularizer function applied to the `kernel` weights\n        matrix. Default: `None`.\n    recurrent_regularizer: Regularizer function applied to the\n        `recurrent_kernel` weights matrix. Default: `None`.\n    bias_regularizer: Regularizer function applied to the bias vector.\n        Default: `None`.\n    activity_regularizer: Regularizer function applied to the output of the\n        layer (its \"activation\"). Default: `None`.\n    kernel_constraint: Constraint function applied to the `kernel` weights\n        matrix. Default: `None`.\n    recurrent_constraint: Constraint function applied to the\n        `recurrent_kernel` weights matrix. Default: `None`.\n    bias_constraint: Constraint function applied to the bias vector.\n        Default: `None`.\n    dropout: Float between 0 and 1. Fraction of the units to drop for the\n        linear transformation of the inputs. Default: 0.\n    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n        for the linear transformation of the recurrent state. Default: 0.\n    seed: Random seed for dropout.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence. Default: `False`.\n    return_state: Boolean. Whether to return the last state in addition\n        to the output. Default: `False`.\n    go_backwards: Boolean (default `False`).\n        If `True`, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default: `False`). If `True`, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default: `False`).\n        If `True`, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    reset_after: GRU convention (whether to apply reset gate after or\n        before matrix multiplication). `False` is `\"before\"`,\n        `True` is `\"after\"` (default and cuDNN compatible).\n    use_cudnn: Whether to use a cuDNN-backed implementation. `\"auto\"` will\n        attempt to use cuDNN when feasible, and will fallback to the\n        default implementation if not.\n\nCall arguments:\n    inputs: A 3D tensor, with shape `(batch, timesteps, feature)`.\n    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n        a given timestep should be masked  (optional).\n        An individual `True` entry indicates that the corresponding timestep\n        should be utilized, while a `False` entry indicates that the\n        corresponding timestep should be ignored. Defaults to `None`.\n    training: Python boolean indicating whether the layer should behave in\n        training mode or in inference mode. This argument is passed to the\n        cell when calling it. This is only relevant if `dropout` or\n        `recurrent_dropout` is used  (optional). Defaults to `None`.\n    initial_state: List of initial state tensors to be passed to the first\n        call of the cell (optional, `None` causes creation\n        of zero-filled initial state tensors). Defaults to `None`.", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.Layer", "Docstring": "This is the class from which all layers inherit.\n\nA layer is a callable object that takes as input one or more tensors and\nthat outputs one or more tensors. It involves *computation*, defined\nin the `call()` method, and a *state* (weight variables). State can be\ncreated:\n\n* in `__init__()`, for instance via `self.add_weight()`;\n* in the optional `build()` method, which is invoked by the first\n  `__call__()` to the layer, and supplies the shape(s) of the input(s),\n  which may not have been known at initialization time.\n\nLayers are recursively composable: If you assign a Layer instance as an\nattribute of another Layer, the outer layer will start tracking the weights\ncreated by the inner layer. Nested layers should be instantiated in the\n`__init__()` method or `build()` method.\n\nUsers will just instantiate a layer and then treat it as a callable.\n\nArgs:\n    trainable: Boolean, whether the layer's variables should be trainable.\n    name: String name of the layer.\n    dtype: The dtype of the layer's computations and weights. Can also be a\n        `keras.DTypePolicy`,\n        which allows the computation and\n        weight dtype to differ. Defaults to `None`. `None` means to use\n        `keras.config.dtype_policy()`,\n        which is a `float32` policy unless set to different value\n        (via `keras.config.set_dtype_policy()`).\n\nAttributes:\n    name: The name of the layer (string).\n    dtype: Dtype of the layer's weights. Alias of `layer.variable_dtype`.\n    variable_dtype: Dtype of the layer's weights.\n    compute_dtype: The dtype of the layer's computations.\n        Layers automatically cast inputs to this dtype, which causes\n        the computations and output to also be in this dtype.\n        When mixed precision is used with a\n        `keras.DTypePolicy`, this will be different\n        than `variable_dtype`.\n    trainable_weights: List of variables to be included in backprop.\n    non_trainable_weights: List of variables that should not be\n        included in backprop.\n    weights: The concatenation of the lists trainable_weights and\n        non_trainable_weights (in this order).\n    trainable: Whether the layer should be trained (boolean), i.e.\n        whether its potentially-trainable weights should be returned\n        as part of `layer.trainable_weights`.\n    input_spec: Optional (list of) `InputSpec` object(s) specifying the\n        constraints on inputs that can be accepted by the layer.\n\nWe recommend that descendants of `Layer` implement the following methods:\n\n* `__init__()`: Defines custom layer attributes, and creates layer weights\n    that do not depend on input shapes, using `add_weight()`,\n    or other state.\n* `build(self, input_shape)`: This method can be used to create weights that\n    depend on the shape(s) of the input(s), using `add_weight()`, or other\n    state. `__call__()` will automatically build the layer\n    (if it has not been built yet) by calling `build()`.\n* `call(self, *args, **kwargs)`: Called in `__call__` after making\n    sure `build()` has been called. `call()` performs the logic of applying\n    the layer to the input arguments.\n    Two reserved keyword arguments you can optionally use in `call()` are:\n        1. `training` (boolean, whether the call is in inference mode or\n            training mode).\n        2. `mask` (boolean tensor encoding masked timesteps in the input,\n            used e.g. in RNN layers).\n    A typical signature for this method is `call(self, inputs)`, and user\n    could optionally add `training` and `mask` if the layer need them.\n* `get_config(self)`: Returns a dictionary containing the configuration\n    used to initialize this layer. If the keys differ from the arguments\n    in `__init__()`, then override `from_config(self)` as well.\n    This method is used when saving\n    the layer or a model that contains this layer.\n\nExamples:\n\nHere's a basic example: a layer with two variables, `w` and `b`,\nthat returns `y = w . x + b`.\nIt shows how to implement `build()` and `call()`.\nVariables set as attributes of a layer are tracked as weights\nof the layers (in `layer.weights`).\n\n```python\nclass SimpleDense(Layer):\n    def __init__(self, units=32):\n        super().__init__()\n        self.units = units\n\n    # Create the state of the layer (weights)\n    def build(self, input_shape):\n        self.kernel = self.add_weight(\n            shape=(input_shape[-1], self.units),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n            name=\"kernel\",\n        )\n        self.bias = self.add_weight(\n            shape=(self.units,),\n            initializer=\"zeros\",\n            trainable=True,\n            name=\"bias\",\n        )\n\n    # Defines the computation\n    def call(self, inputs):\n        return ops.matmul(inputs, self.kernel) + self.bias\n\n# Instantiates the layer.\nlinear_layer = SimpleDense(4)\n\n# This will also call `build(input_shape)` and create the weights.\ny = linear_layer(ops.ones((2, 2)))\nassert len(linear_layer.weights) == 2\n\n# These weights are trainable, so they're listed in `trainable_weights`:\nassert len(linear_layer.trainable_weights) == 2\n```\n\nBesides trainable weights, updated via backpropagation during training,\nlayers can also have non-trainable weights. These weights are meant to\nbe updated manually during `call()`. Here's a example layer that computes\nthe running sum of its inputs:\n\n```python\nclass ComputeSum(Layer):\n\n  def __init__(self, input_dim):\n      super(ComputeSum, self).__init__()\n      # Create a non-trainable weight.\n      self.total = self.add_weight(\n        shape=(),\n        initializer=\"zeros\",\n        trainable=False,\n        name=\"total\",\n      )\n\n  def call(self, inputs):\n      self.total.assign(self.total + ops.sum(inputs))\n      return self.total\n\nmy_sum = ComputeSum(2)\nx = ops.ones((2, 2))\ny = my_sum(x)\n\nassert my_sum.weights == [my_sum.total]\nassert my_sum.non_trainable_weights == [my_sum.total]\nassert my_sum.trainable_weights == []\n```", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.Layer.count_params", "Docstring": "Count the total number of scalars composing the weights.\n\nReturns:\n    An integer count.", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.LSTM", "Docstring": "Long Short-Term Memory layer - Hochreiter 1997.\n\nBased on available runtime hardware and constraints, this layer\nwill choose different implementations (cuDNN-based or backend-native)\nto maximize the performance. If a GPU is available and all\nthe arguments to the layer meet the requirement of the cuDNN kernel\n(see below for details), the layer will use a fast cuDNN implementation\nwhen using the TensorFlow backend.\nThe requirements to use the cuDNN implementation are:\n\n1. `activation` == `tanh`\n2. `recurrent_activation` == `sigmoid`\n3. `dropout` == 0 and `recurrent_dropout` == 0\n4. `unroll` is `False`\n5. `use_bias` is `True`\n6. Inputs, if use masking, are strictly right-padded.\n7. Eager execution is enabled in the outermost context.\n\nFor example:\n\n>>> inputs = np.random.random((32, 10, 8))\n>>> lstm = keras.layers.LSTM(4)\n>>> output = lstm(inputs)\n>>> output.shape\n(32, 4)\n>>> lstm = keras.layers.LSTM(\n...     4, return_sequences=True, return_state=True)\n>>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n>>> whole_seq_output.shape\n(32, 10, 4)\n>>> final_memory_state.shape\n(32, 4)\n>>> final_carry_state.shape\n(32, 4)\n\nArgs:\n    units: Positive integer, dimensionality of the output space.\n    activation: Activation function to use.\n        Default: hyperbolic tangent (`tanh`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    recurrent_activation: Activation function to use\n        for the recurrent step.\n        Default: sigmoid (`sigmoid`).\n        If you pass `None`, no activation is applied\n        (ie. \"linear\" activation: `a(x) = x`).\n    use_bias: Boolean, (default `True`), whether the layer\n        should use a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix,\n        used for the linear transformation of the inputs. Default:\n        `\"glorot_uniform\"`.\n    recurrent_initializer: Initializer for the `recurrent_kernel`\n        weights matrix, used for the linear transformation of the recurrent\n        state. Default: `\"orthogonal\"`.\n    bias_initializer: Initializer for the bias vector. Default: `\"zeros\"`.\n    unit_forget_bias: Boolean (default `True`). If `True`,\n        add 1 to the bias of the forget gate at initialization.\n        Setting it to `True` will also force `bias_initializer=\"zeros\"`.\n        This is recommended in [Jozefowicz et al.](\n        https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)\n    kernel_regularizer: Regularizer function applied to the `kernel` weights\n        matrix. Default: `None`.\n    recurrent_regularizer: Regularizer function applied to the\n        `recurrent_kernel` weights matrix. Default: `None`.\n    bias_regularizer: Regularizer function applied to the bias vector.\n        Default: `None`.\n    activity_regularizer: Regularizer function applied to the output of the\n        layer (its \"activation\"). Default: `None`.\n    kernel_constraint: Constraint function applied to the `kernel` weights\n        matrix. Default: `None`.\n    recurrent_constraint: Constraint function applied to the\n        `recurrent_kernel` weights matrix. Default: `None`.\n    bias_constraint: Constraint function applied to the bias vector.\n        Default: `None`.\n    dropout: Float between 0 and 1. Fraction of the units to drop for the\n        linear transformation of the inputs. Default: 0.\n    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n        for the linear transformation of the recurrent state. Default: 0.\n    seed: Random seed for dropout.\n    return_sequences: Boolean. Whether to return the last output\n        in the output sequence, or the full sequence. Default: `False`.\n    return_state: Boolean. Whether to return the last state in addition\n        to the output. Default: `False`.\n    go_backwards: Boolean (default: `False`).\n        If `True`, process the input sequence backwards and return the\n        reversed sequence.\n    stateful: Boolean (default: `False`). If `True`, the last state\n        for each sample at index i in a batch will be used as initial\n        state for the sample of index i in the following batch.\n    unroll: Boolean (default False).\n        If `True`, the network will be unrolled,\n        else a symbolic loop will be used.\n        Unrolling can speed-up a RNN,\n        although it tends to be more memory-intensive.\n        Unrolling is only suitable for short sequences.\n    use_cudnn: Whether to use a cuDNN-backed implementation. `\"auto\"` will\n        attempt to use cuDNN when feasible, and will fallback to the\n        default implementation if not.\n\nCall arguments:\n    inputs: A 3D tensor, with shape `(batch, timesteps, feature)`.\n    mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n        a given timestep should be masked  (optional).\n        An individual `True` entry indicates that the corresponding timestep\n        should be utilized, while a `False` entry indicates that the\n        corresponding timestep should be ignored. Defaults to `None`.\n    training: Python boolean indicating whether the layer should behave in\n        training mode or in inference mode. This argument is passed to the\n        cell when calling it. This is only relevant if `dropout` or\n        `recurrent_dropout` is used  (optional). Defaults to `None`.\n    initial_state: List of initial state tensors to be passed to the first\n        call of the cell (optional, `None` causes creation\n        of zero-filled initial state tensors). Defaults to `None`.", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.layers.StringLookup", "Docstring": "A preprocessing layer that maps strings to (possibly encoded) indices.\n\nThis layer translates a set of arbitrary strings into integer output via a\ntable-based vocabulary lookup. This layer will perform no splitting or\ntransformation of input strings. For a layer than can split and tokenize\nnatural language, see the `keras.layers.TextVectorization` layer.\n\nThe vocabulary for the layer must be either supplied on construction or\nlearned via `adapt()`. During `adapt()`, the layer will analyze a data set,\ndetermine the frequency of individual strings tokens, and create a\nvocabulary from them. If the vocabulary is capped in size, the most frequent\ntokens will be used to create the vocabulary and all others will be treated\nas out-of-vocabulary (OOV).\n\nThere are two possible output modes for the layer.\nWhen `output_mode` is `\"int\"`,\ninput strings are converted to their index in the vocabulary (an integer).\nWhen `output_mode` is `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`, input strings\nare encoded into an array where each dimension corresponds to an element in\nthe vocabulary.\n\nThe vocabulary can optionally contain a mask token as well as an OOV token\n(which can optionally occupy multiple indices in the vocabulary, as set\nby `num_oov_indices`).\nThe position of these tokens in the vocabulary is fixed. When `output_mode`\nis `\"int\"`, the vocabulary will begin with the mask token (if set), followed\nby OOV indices, followed by the rest of the vocabulary. When `output_mode`\nis `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will begin with\nOOV indices and instances of the mask token will be dropped.\n\n**Note:** This layer uses TensorFlow internally. It cannot\nbe used as part of the compiled computation graph of a model with\nany backend other than TensorFlow.\nIt can however be used with any backend when running eagerly.\nIt can also always be used as part of an input preprocessing pipeline\nwith any backend (outside the model itself), which is how we recommend\nto use this layer.\n\n**Note:** This layer is safe to use inside a `tf.data` pipeline\n(independently of which backend you're using).\n\nArgs:\n    max_tokens: Maximum size of the vocabulary for this layer. This should\n        only be specified when adapting the vocabulary or when setting\n        `pad_to_max_tokens=True`. If None, there is no cap on the size of\n        the vocabulary. Note that this size includes the OOV\n        and mask tokens. Defaults to `None`.\n    num_oov_indices: The number of out-of-vocabulary tokens to use.\n        If this value is more than 1, OOV inputs are modulated to\n        determine their OOV value.\n        If this value is 0, OOV inputs will cause an error when calling\n        the layer. Defaults to `1`.\n    mask_token: A token that represents masked inputs. When `output_mode` is\n        `\"int\"`, the token is included in vocabulary and mapped to index 0.\n        In other output modes, the token will not appear\n        in the vocabulary and instances of the mask token\n        in the input will be dropped. If set to `None`,\n        no mask term will be added. Defaults to `None`.\n    oov_token: Only used when `invert` is True. The token to return for OOV\n        indices. Defaults to `\"[UNK]\"`.\n    vocabulary: Optional. Either an array of integers or a string path to a\n        text file. If passing an array, can pass a tuple, list,\n        1D NumPy array, or 1D tensor containing the integer vocbulary terms.\n        If passing a file path, the file should contain one line per term\n        in the vocabulary. If this argument is set,\n        there is no need to `adapt()` the layer.\n    vocabulary_dtype: The dtype of the vocabulary terms, for example\n        `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n    idf_weights: Only valid when `output_mode` is `\"tf_idf\"`.\n        A tuple, list, 1D NumPy array, or 1D tensor or the same length\n        as the vocabulary, containing the floating point inverse document\n        frequency weights, which will be multiplied by per sample term\n        counts for the final TF-IDF weight.\n        If the `vocabulary` argument is set, and `output_mode` is\n        `\"tf_idf\"`, this argument must be supplied.\n    invert: Only valid when `output_mode` is `\"int\"`.\n        If `True`, this layer will map indices to vocabulary items\n        instead of mapping vocabulary items to indices.\n        Defaults to `False`.\n    output_mode: Specification for the output of the layer. Values can be\n        `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`\n        configuring the layer as follows:\n        - `\"int\"`: Return the vocabulary indices of the input tokens.\n        - `\"one_hot\"`: Encodes each individual element in the input into an\n            array the same size as the vocabulary,\n            containing a 1 at the element index. If the last dimension\n            is size 1, will encode on that dimension.\n            If the last dimension is not size 1, will append a new\n            dimension for the encoded output.\n        - `\"multi_hot\"`: Encodes each sample in the input into a single\n            array the same size as the vocabulary,\n            containing a 1 for each vocabulary term present in the sample.\n            Treats the last dimension as the sample dimension,\n            if input shape is `(..., sample_length)`,\n            output shape will be `(..., num_tokens)`.\n        - `\"count\"`: As `\"multi_hot\"`, but the int array contains\n            a count of the number of times the token at that index\n            appeared in the sample.\n        - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is\n            applied to find the value in each token slot.\n        For `\"int\"` output, any shape of input and output is supported.\n        For all other output modes, currently only output up to rank 2\n        is supported. Defaults to `\"int\"`.\n    pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n        `\"count\"`, or `\"tf_idf\"`. If `True`, the output will have\n        its feature axis padded to `max_tokens` even if the number\n        of unique tokens in the vocabulary is less than `max_tokens`,\n        resulting in a tensor of shape `(batch_size, max_tokens)`\n        regardless of vocabulary size. Defaults to `False`.\n    sparse: Boolean. Only applicable to `\"multi_hot\"`, `\"count\"`, and\n        `\"tf_idf\"` output modes. Only supported with TensorFlow\n        backend. If `True`, returns a `SparseTensor`\n        instead of a dense `Tensor`. Defaults to `False`.\n    encoding: Optional. The text encoding to use to interpret the input\n        strings. Defaults to `\"utf-8\"`.\n\nExamples:\n\n**Creating a lookup layer with a known vocabulary**\n\nThis example creates a lookup layer with a pre-existing vocabulary.\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> data = [[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]]\n>>> layer = StringLookup(vocabulary=vocab)\n>>> layer(data)\narray([[1, 3, 4],\n       [4, 0, 2]])\n\n**Creating a lookup layer with an adapted vocabulary**\n\nThis example creates a lookup layer and generates the vocabulary by\nanalyzing the dataset.\n\n>>> data = [[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]]\n>>> layer = StringLookup()\n>>> layer.adapt(data)\n>>> layer.get_vocabulary()\n['[UNK]', 'd', 'z', 'c', 'b', 'a']\n\nNote that the OOV token `\"[UNK]\"` has been added to the vocabulary.\nThe remaining tokens are sorted by frequency\n(`\"d\"`, which has 2 occurrences, is first) then by inverse sort order.\n\n>>> data = [[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]]\n>>> layer = StringLookup()\n>>> layer.adapt(data)\n>>> layer(data)\narray([[5, 3, 1],\n       [1, 2, 4]])\n\n**Lookups with multiple OOV indices**\n\nThis example demonstrates how to use a lookup layer with multiple OOV\nindices.  When a layer is created with more than one OOV index, any OOV\nvalues are hashed into the number of OOV buckets, distributing OOV values in\na deterministic fashion across the set.\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> data = [[\"a\", \"c\", \"d\"], [\"m\", \"z\", \"b\"]]\n>>> layer = StringLookup(vocabulary=vocab, num_oov_indices=2)\n>>> layer(data)\narray([[2, 4, 5],\n       [0, 1, 3]])\n\nNote that the output for OOV value 'm' is 0, while the output for OOV value\n`\"z\"` is 1. The in-vocab terms have their output index increased by 1 from\nearlier examples (a maps to 2, etc) in order to make space for the extra OOV\nvalue.\n\n**One-hot output**\n\nConfigure the layer with `output_mode='one_hot'`. Note that the first\n`num_oov_indices` dimensions in the ont_hot encoding represent OOV values.\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> data = [\"a\", \"b\", \"c\", \"d\", \"z\"]\n>>> layer = StringLookup(vocabulary=vocab, output_mode='one_hot')\n>>> layer(data)\narray([[0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0.]], dtype=int64)\n\n**Multi-hot output**\n\nConfigure the layer with `output_mode='multi_hot'`. Note that the first\n`num_oov_indices` dimensions in the multi_hot encoding represent OOV values.\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> data = [[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]]\n>>> layer = StringLookup(vocabulary=vocab, output_mode='multi_hot')\n>>> layer(data)\narray([[0., 1., 0., 1., 1.],\n       [1., 0., 1., 0., 1.]], dtype=int64)\n\n**Token count output**\n\nConfigure the layer with `output_mode='count'`. As with multi_hot output,\nthe first `num_oov_indices` dimensions in the output represent OOV values.\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> data = [[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]]\n>>> layer = StringLookup(vocabulary=vocab, output_mode='count')\n>>> layer(data)\narray([[0., 1., 0., 1., 2.],\n       [2., 0., 1., 0., 1.]], dtype=int64)\n\n**TF-IDF output**\n\nConfigure the layer with `output_mode=\"tf_idf\"`. As with multi_hot output,\nthe first `num_oov_indices` dimensions in the output represent OOV values.\n\nEach token bin will output `token_count * idf_weight`, where the idf weights\nare the inverse document frequency weights per token. These should be\nprovided along with the vocabulary. Note that the `idf_weight` for OOV\nvalues will default to the average of all idf weights passed in.\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n>>> data = [[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]]\n>>> layer = StringLookup(output_mode=\"tf_idf\")\n>>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n>>> layer(data)\narray([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n       [1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)\n\nTo specify the idf weights for oov values, you will need to pass the entire\nvocabulary including the leading oov token.\n\n>>> vocab = [\"[UNK]\", \"a\", \"b\", \"c\", \"d\"]\n>>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n>>> data = [[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]]\n>>> layer = StringLookup(output_mode=\"tf_idf\")\n>>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n>>> layer(data)\narray([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n       [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)\n\nWhen adapting the layer in `\"tf_idf\"` mode, each input sample will be\nconsidered a document, and IDF weight per token will be calculated as\n`log(1 + num_documents / (1 + token_document_count))`.\n\n**Inverse lookup**\n\nThis example demonstrates how to map indices to strings using this layer.\n(You can also use `adapt()` with `inverse=True`, but for simplicity we'll\npass the vocab in this example.)\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> data = [[1, 3, 4], [4, 0, 2]]\n>>> layer = StringLookup(vocabulary=vocab, invert=True)\n>>> layer(data)\narray([[b'a', b'c', b'd'],\n       [b'd', b'[UNK]', b'b']], dtype=object)\n\nNote that the first index correspond to the oov token by default.\n\n\n**Forward and inverse lookup pairs**\n\nThis example demonstrates how to use the vocabulary of a standard lookup\nlayer to create an inverse lookup layer.\n\n>>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n>>> data = [[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]]\n>>> layer = StringLookup(vocabulary=vocab)\n>>> i_layer = StringLookup(vocabulary=vocab, invert=True)\n>>> int_data = layer(data)\n>>> i_layer(int_data)\narray([[b'a', b'c', b'd'],\n       [b'd', b'[UNK]', b'b']], dtype=object)\n\nIn this example, the input value `\"z\"` resulted in an output of `\"[UNK]\"`,\nsince 1000 was not in the vocabulary - it got represented as an OOV, and all\nOOV values are returned as `\"[UNK]\"` in the inverse layer. Also, note that\nfor the inverse to work, you must have already set the forward layer\nvocabulary either directly or via `adapt()` before calling\n`get_vocabulary()`.", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.Model", "Docstring": "A model grouping layers into an object with training/inference features.\n\nThere are three ways to instantiate a `Model`:\n\n## With the \"Functional API\"\n\nYou start from `Input`,\nyou chain layer calls to specify the model's forward pass,\nand finally, you create your model from inputs and outputs:\n\n```python\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n```\n\nNote: Only dicts, lists, and tuples of input tensors are supported. Nested\ninputs are not supported (e.g. lists of list or dicts of dict).\n\nA new Functional API model can also be created by using the\nintermediate tensors. This enables you to quickly extract sub-components\nof the model.\n\nExample:\n\n```python\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\n```\n\nNote that the `backbone` and `activations` models are not\ncreated with `keras.Input` objects, but with the tensors that originate\nfrom `keras.Input` objects. Under the hood, the layers and weights will\nbe shared across these models, so that user can train the `full_model`, and\nuse `backbone` or `activations` to do feature extraction.\nThe inputs and outputs of the model can be nested structures of tensors as\nwell, and the created models are standard Functional API models that support\nall the existing APIs.\n\n## By subclassing the `Model` class\n\nIn that case, you should define your\nlayers in `__init__()` and you should implement the model's forward pass\nin `call()`.\n\n```python\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    def call(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\n```\n\nIf you subclass `Model`, you can optionally have\na `training` argument (boolean) in `call()`, which you can use to specify\na different behavior in training and inference:\n\n```python\nclass MyModel(keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    def call(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\n```\n\nOnce the model is created, you can config the model with losses and metrics\nwith `model.compile()`, train the model with `model.fit()`, or use the model\nto do prediction with `model.predict()`.\n\n## With the `Sequential` class\n\nIn addition, `keras.Sequential` is a special case of model where\nthe model is purely a stack of single-input, single-output layers.\n\n```python\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n```", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.models.Sequential", "Docstring": "`Sequential` groups a linear stack of layers into a `Model`.\n\nExamples:\n\n```python\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(16,)))\nmodel.add(keras.layers.Dense(8))\n\n# Note that you can also omit the initial `Input`.\n# In that case the model doesn't have any weights until the first call\n# to a training/evaluation method (since it isn't yet built):\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(8))\nmodel.add(keras.layers.Dense(4))\n# model.weights not created yet\n\n# Whereas if you specify an `Input`, the model gets built\n# continuously as you are adding layers:\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(16,)))\nmodel.add(keras.layers.Dense(8))\nlen(model.weights)  # Returns \"2\"\n\n# When using the delayed-build pattern (no input shape specified), you can\n# choose to manually build your model by calling\n# `build(batch_input_shape)`:\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(8))\nmodel.add(keras.layers.Dense(4))\nmodel.build((None, 16))\nlen(model.weights)  # Returns \"4\"\n\n# Note that when using the delayed-build pattern (no input shape specified),\n# the model gets built the first time you call `fit`, `eval`, or `predict`,\n# or the first time you call the model on some input data.\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(8))\nmodel.add(keras.layers.Dense(1))\nmodel.compile(optimizer='sgd', loss='mse')\n# This builds the model for the first time:\nmodel.fit(x, y, batch_size=32, epochs=10)\n```", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.optimizers.AdamW", "Docstring": "Optimizer that implements the AdamW algorithm.\n\nAdamW optimization is a stochastic gradient descent method that is based on\nadaptive estimation of first-order and second-order moments with an added\nmethod to decay weights per the techniques discussed in the paper,\n'Decoupled Weight Decay Regularization' by\n[Loshchilov, Hutter et al., 2019](https://arxiv.org/abs/1711.05101).\n\nAccording to\n[Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\nthe underying Adam method is \"*computationally\nefficient, has little memory requirement, invariant to diagonal rescaling of\ngradients, and is well suited for problems that are large in terms of\ndata/parameters*\".\n\nArgs:\n    learning_rate: A float, a\n        `keras.optimizers.schedules.LearningRateSchedule` instance, or\n        a callable that takes no arguments and returns the actual value to\n        use. The learning rate. Defaults to `0.001`.\n    beta_1: A float value or a constant float tensor, or a callable\n        that takes no arguments and returns the actual value to use. The\n        exponential decay rate for the 1st moment estimates.\n        Defaults to `0.9`.\n    beta_2: A float value or a constant float tensor, or a callable\n        that takes no arguments and returns the actual value to use. The\n        exponential decay rate for the 2nd moment estimates.\n        Defaults to `0.999`.\n    epsilon: A small constant for numerical stability. This epsilon is\n        \"epsilon hat\" in the Kingma and Ba paper (in the formula just\n        before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n        Defaults to 1e-7.\n    amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm\n        from the paper \"On the Convergence of Adam and beyond\".\n        Defaults to `False`.\n    name: String. The name to use\n        for momentum accumulator weights created by\n        the optimizer.\n    weight_decay: Float. If set, weight decay is applied.\n    clipnorm: Float. If set, the gradient of each weight is individually\n        clipped so that its norm is no higher than this value.\n    clipvalue: Float. If set, the gradient of each weight is clipped to be\n        no higher than this value.\n    global_clipnorm: Float. If set, the gradient of all weights is clipped\n        so that their global norm is no higher than this value.\n    use_ema: Boolean, defaults to `False`.\n        If `True`, exponential moving average\n        (EMA) is applied. EMA consists of computing an exponential moving\n        average of the weights of the model (as the weight values change\n        after each training batch), and periodically overwriting the\n        weights with their moving average.\n    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n        This is the momentum to use when computing\n        the EMA of the model's weights:\n        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n        current_variable_value`.\n    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n        we overwrite the model variable by its moving average.\n        If None, the optimizer\n        does not overwrite model variables in the middle of training,\n        and you need to explicitly overwrite the variables\n        at the end of training by calling\n        `optimizer.finalize_variable_values()` (which updates the model\n        variables in-place). When using the built-in `fit()` training loop,\n        this happens automatically after the last epoch,\n        and you don't need to do anything.\n    loss_scale_factor: Float or `None`. If a float, the scale factor will\n        be multiplied the loss before computing gradients, and the inverse\n        of the scale factor will be multiplied by the gradients before\n        updating variables. Useful for preventing underflow during\n        mixed precision training. Alternately,\n        `keras.optimizers.LossScaleOptimizer` will\n        automatically set a loss scale factor.\n    gradient_accumulation_steps: Int or `None`. If an int, model & optimizer\n        variables will not be updated at every step; instead they will be\n        updated every `gradient_accumulation_steps` steps, using the average\n        value of the gradients since the last update. This is known as\n        \"gradient accumulation\". This can be useful\n        when your batch size is very small, in order to reduce gradient\n        noise at each update step.\n\n\nReferences:\n\n- [Loshchilov et al., 2019](https://arxiv.org/abs/1711.05101)\n- [Kingma et al., 2014](http://arxiv.org/abs/1412.6980) for `adam`\n- [Reddi et al., 2018](\n    https://openreview.net/pdf?id=ryQu7f-RZ) for `amsgrad`.", "Library": "TensorFlow"}
{"API_Name": "tensorflow.keras.optimizers.RMSprop", "Docstring": "Optimizer that implements the RMSprop algorithm.\n\nThe gist of RMSprop is to:\n\n- Maintain a moving (discounted) average of the square of gradients\n- Divide the gradient by the root of this average\n\nThis implementation of RMSprop uses plain momentum, not Nesterov momentum.\n\nThe centered version additionally maintains a moving average of the\ngradients, and uses that average to estimate the variance.\n\nArgs:\n    learning_rate: A float, a\n        `keras.optimizers.schedules.LearningRateSchedule` instance, or\n        a callable that takes no arguments and returns the actual value to\n        use. The learning rate. Defaults to `0.001`.\n    rho: float, defaults to 0.9. Discounting factor for the old gradients.\n    momentum: float, defaults to 0.0. If not 0.0., the optimizer tracks the\n        momentum value, with a decay rate equals to `1 - momentum`.\n    epsilon: A small constant for numerical stability. This epsilon is\n        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults\n        to 1e-7.\n    centered: Boolean. If `True`, gradients are normalized by the estimated\n        variance of the gradient; if False, by the uncentered second moment.\n        Setting this to `True` may help with training, but is slightly more\n        expensive in terms of computation and memory. Defaults to `False`.\n    name: String. The name to use\n        for momentum accumulator weights created by\n        the optimizer.\n    weight_decay: Float. If set, weight decay is applied.\n    clipnorm: Float. If set, the gradient of each weight is individually\n        clipped so that its norm is no higher than this value.\n    clipvalue: Float. If set, the gradient of each weight is clipped to be\n        no higher than this value.\n    global_clipnorm: Float. If set, the gradient of all weights is clipped\n        so that their global norm is no higher than this value.\n    use_ema: Boolean, defaults to `False`.\n        If `True`, exponential moving average\n        (EMA) is applied. EMA consists of computing an exponential moving\n        average of the weights of the model (as the weight values change\n        after each training batch), and periodically overwriting the\n        weights with their moving average.\n    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n        This is the momentum to use when computing\n        the EMA of the model's weights:\n        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n        current_variable_value`.\n    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n        we overwrite the model variable by its moving average.\n        If None, the optimizer\n        does not overwrite model variables in the middle of training,\n        and you need to explicitly overwrite the variables\n        at the end of training by calling\n        `optimizer.finalize_variable_values()` (which updates the model\n        variables in-place). When using the built-in `fit()` training loop,\n        this happens automatically after the last epoch,\n        and you don't need to do anything.\n    loss_scale_factor: Float or `None`. If a float, the scale factor will\n        be multiplied the loss before computing gradients, and the inverse\n        of the scale factor will be multiplied by the gradients before\n        updating variables. Useful for preventing underflow during\n        mixed precision training. Alternately,\n        `keras.optimizers.LossScaleOptimizer` will\n        automatically set a loss scale factor.\n    gradient_accumulation_steps: Int or `None`. If an int, model & optimizer\n        variables will not be updated at every step; instead they will be\n        updated every `gradient_accumulation_steps` steps, using the average\n        value of the gradients since the last update. This is known as\n        \"gradient accumulation\". This can be useful\n        when your batch size is very small, in order to reduce gradient\n        noise at each update step.\n\n\nExample:\n\n>>> opt = keras.optimizers.RMSprop(learning_rate=0.1)\n>>> var1 = keras.backend.Variable(10.0)\n>>> loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1\n>>> opt.minimize(loss, [var1])\n>>> var1\n9.683772\n\nReference:\n\n- [Hinton, 2012](\n    http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)", "Library": "TensorFlow"}
{"API_Name": "tf.abs", "Docstring": "Computes the absolute value of a tensor.\n\nGiven a tensor of integer or floating-point values, this operation returns a\ntensor of the same type, where each element contains the absolute value of the\ncorresponding element in the input.\n\nGiven a tensor `x` of complex numbers, this operation returns a tensor of type\n`float32` or `float64` that is the absolute value of each element in `x`. For\na complex number \\\\(a + bj\\\\), its absolute value is computed as\n\\\\(\\sqrt{a^2 + b^2}\\\\).\n\nFor example:\n\n>>> # real number\n>>> x = tf.constant([-2.25, 3.25])\n>>> tf.abs(x)\n<tf.Tensor: shape=(2,), dtype=float32,\nnumpy=array([2.25, 3.25], dtype=float32)>\n\n>>> # complex number\n>>> x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n>>> tf.abs(x)\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[5.25594901],\n       [6.60492241]])>\n\nArgs:\n  x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,\n    `int32`, `int64`, `complex64` or `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` or `SparseTensor` of the same size, type and sparsity as `x`,\n    with absolute values. Note, for `complex64` or `complex128` input, the\n    returned `Tensor` will be of type `float32` or `float64`, respectively.\n\n  If `x` is a `SparseTensor`, returns\n  `SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)`", "Library": "TensorFlow"}
{"API_Name": "tf.argmax", "Docstring": "Returns the index with the largest value across axes of a tensor.\n\nIn case of identity returns the smallest index.\n\nFor example:\n\n>>> A = tf.constant([2, 20, 30, 3, 6])\n>>> tf.math.argmax(A)  # A[2] is maximum in tensor A\n<tf.Tensor: shape=(), dtype=int64, numpy=2>\n>>> B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],\n...                  [14, 45, 23, 5, 27]])\n>>> tf.math.argmax(B, 0)\n<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>\n>>> tf.math.argmax(B, 1)\n<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>\n>>> C = tf.constant([0, 0, 0, 0])\n>>> tf.math.argmax(C) # Returns smallest index in case of ties\n<tf.Tensor: shape=(), dtype=int64, numpy=0>\n\nArgs:\n  input: A `Tensor`.\n  axis: An integer, the axis to reduce across. Default to 0.\n  output_type: An optional output dtype (`tf.int32` or `tf.int64`). Defaults\n    to `tf.int64`.\n  name: An optional name for the operation.\n\nReturns:\n  A `Tensor` of type `output_type`.", "Library": "TensorFlow"}
{"API_Name": "tf.argsort", "Docstring": "Returns the indices of a tensor that give its sorted order along an axis.\n\n>>> values = [1, 10, 26.9, 2.8, 166.32, 62.3]\n>>> sort_order = tf.argsort(values)\n>>> sort_order.numpy()\narray([0, 3, 1, 2, 5, 4], dtype=int32)\n\nFor a 1D tensor:\n\n>>> sorted = tf.gather(values, sort_order)\n>>> assert tf.reduce_all(sorted == tf.sort(values))\n\nFor higher dimensions, the output has the same shape as\n`values`, but along the given axis, values represent the index of the sorted\nelement in that slice of the tensor at the given position.\n\n>>> mat = [[30,20,10],\n...        [20,10,30],\n...        [10,30,20]]\n>>> indices = tf.argsort(mat)\n>>> indices.numpy()\narray([[2, 1, 0],\n       [1, 0, 2],\n       [0, 2, 1]], dtype=int32)\n\nIf `axis=-1` these indices can be used to apply a sort using `tf.gather`:\n\n>>> tf.gather(mat, indices, batch_dims=-1).numpy()\narray([[10, 20, 30],\n       [10, 20, 30],\n       [10, 20, 30]], dtype=int32)\n\nSee also:\n\n  * `tf.sort`: Sort along an axis.\n  * `tf.math.top_k`: A partial sort that returns a fixed number of top values\n    and corresponding indices.\n\nArgs:\n  values: 1-D or higher **numeric** `Tensor`.\n  axis: The axis along which to sort. The default is -1, which sorts the last\n    axis.\n  direction: The direction in which to sort the values (`'ASCENDING'` or\n    `'DESCENDING'`).\n  stable: If True, equal elements in the original tensor will not be\n    re-ordered in the returned order. Unstable sort is not yet implemented,\n    but will eventually be the default for performance reasons. If you require\n    a stable order, pass `stable=True` for forwards compatibility.\n  name: Optional name for the operation.\n\nReturns:\n  An int32 `Tensor` with the same shape as `values`. The indices that would\n      sort each slice of the given `values` along the given `axis`.\n\nRaises:\n  ValueError: If axis is not a constant scalar, or the direction is invalid.\n  tf.errors.InvalidArgumentError: If the `values.dtype` is not a `float` or\n      `int` type.", "Library": "TensorFlow"}
{"API_Name": "tf.bitwise.bitwise_xor", "Docstring": "Elementwise computes the bitwise XOR of `x` and `y`.\n\nThe result will have those bits set, that are different in `x` and `y`. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\nFor example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.python.ops import bitwise_ops\ndtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,\n              tf.uint8, tf.uint16, tf.uint32, tf.uint64]\n\nfor dtype in dtype_list:\n  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)\n  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)\n  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)\n\n  res = bitwise_ops.bitwise_xor(lhs, rhs)\n  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE\n```\n\nArgs:\n  x: A `Tensor`. Must be one of the following types: `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`.\n  y: A `Tensor`. Must have the same type as `x`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `x`.", "Library": "TensorFlow"}
{"API_Name": "tf.boolean_mask", "Docstring": "Apply boolean mask to tensor.\n\nNumpy equivalent is `tensor[mask]`.\n\nIn general, `0 < dim(mask) = K <= dim(tensor)`, and `mask`'s shape must match\nthe first K dimensions of `tensor`'s shape.  We then have:\n  `boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]`\nwhere `(i1,...,iK)` is the ith `True` entry of `mask` (row-major order).\nThe `axis` could be used with `mask` to indicate the axis to mask from.\nIn that case, `axis + dim(mask) <= dim(tensor)` and `mask`'s shape must match\nthe first `axis + dim(mask)` dimensions of `tensor`'s shape.\n\nSee also: `tf.ragged.boolean_mask`, which can be applied to both dense and\nragged tensors, and can be used if you need to preserve the masked dimensions\nof `tensor` (rather than flattening them, as `tf.boolean_mask` does).\n\nExamples:\n\n>>> tensor = [0, 1, 2, 3]  # 1-D example\n>>> mask = np.array([True, False, True, False])\n>>> tf.boolean_mask(tensor, mask)\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>\n\n>>> tensor = [[1, 2], [3, 4], [5, 6]] # 2-D example\n>>> mask = np.array([True, False, True])\n>>> tf.boolean_mask(tensor, mask)\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [5, 6]], dtype=int32)>\n\nArgs:\n  tensor:  N-D Tensor.\n  mask:  K-D boolean Tensor, K <= N and K must be known statically.\n  axis:  A 0-D int Tensor representing the axis in `tensor` to mask from. By\n    default, axis is 0 which will mask from the first dimension. Otherwise K +\n    axis <= N.\n  name:  A name for this operation (optional).\n\nReturns:\n  (N-K+1)-dimensional tensor populated by entries in `tensor` corresponding\n  to `True` values in `mask`.\n\nRaises:\n  ValueError:  If shapes do not conform.\n\nExamples:\n\n```python\n# 2-D example\ntensor = [[1, 2], [3, 4], [5, 6]]\nmask = np.array([True, False, True])\nboolean_mask(tensor, mask)  # [[1, 2], [5, 6]]\n```", "Library": "TensorFlow"}
{"API_Name": "tf.cast", "Docstring": "Casts a tensor to a new type.\n\nThe operation casts `x` (in case of `Tensor`) or `x.values`\n(in case of `SparseTensor` or `IndexedSlices`) to `dtype`.\n\nFor example:\n\n>>> x = tf.constant([1.8, 2.2], dtype=tf.float32)\n>>> tf.cast(x, tf.int32)\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\nNotice `tf.cast` has an alias `tf.dtypes.cast`:\n\n>>> x = tf.constant([1.8, 2.2], dtype=tf.float32)\n>>> tf.dtypes.cast(x, tf.int32)\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\nThe operation supports data types (for `x` and `dtype`) of\n`uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`,\n`float16`, `float32`, `float64`, `complex64`, `complex128`, `bfloat16`.\nIn case of casting from complex types (`complex64`, `complex128`) to real\ntypes, only the real part of `x` is returned. In case of casting from real\ntypes to complex types (`complex64`, `complex128`), the imaginary part of the\nreturned value is set to `0`. The handling of complex types here matches the\nbehavior of numpy.\n\nNote casting nan and inf values to integral types has undefined behavior.\n\nNote this operation can lead to a loss of precision when converting native\nPython `float` and `complex` variables to `tf.float64` or `tf.complex128`\ntensors, since the input is first converted to the `float32` data type and\nthen widened. It is recommended to use `tf.convert_to_tensor` instead of\n`tf.cast` for any non-tensor inputs.\n\nArgs:\n  x: A `Tensor` or `SparseTensor` or `IndexedSlices` of numeric type. It could\n    be `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`,\n    `int64`, `float16`, `float32`, `float64`, `complex64`, `complex128`,\n    `bfloat16`.\n  dtype: The destination type. The list of supported dtypes is the same as\n    `x`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` and\n    same type as `dtype`.\n\nRaises:\n  TypeError: If `x` cannot be cast to the `dtype`.", "Library": "TensorFlow"}
{"API_Name": "tf.complex", "Docstring": "Converts two real numbers to a complex number.\n\nGiven a tensor `real` representing the real part of a complex number, and a\ntensor `imag` representing the imaginary part of a complex number, this\noperation returns complex numbers elementwise of the form \\\\(a + bj\\\\), where\n*a* represents the `real` part and *b* represents the `imag` part.\n\nThe input tensors `real` and `imag` must have the same shape.\n\nFor example:\n\n```python\nreal = tf.constant([2.25, 3.25])\nimag = tf.constant([4.75, 5.75])\ntf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]\n```\n\nArgs:\n  real: A `Tensor`. Must be one of the following types: `float32`, `float64`.\n  imag: A `Tensor`. Must have the same type as `real`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` of type `complex64` or `complex128`.\n\nRaises:\n  TypeError: Real and imag must be correct types", "Library": "TensorFlow"}
{"API_Name": "tf.concat", "Docstring": "Concatenates tensors along one dimension.\n\nSee also `tf.tile`, `tf.stack`, `tf.repeat`.\n\nConcatenates the list of tensors `values` along dimension `axis`.  If\n`values[i].shape = [D0, D1, ... Daxis(i), ...Dn]`, the concatenated\nresult has shape\n\n    [D0, D1, ... Raxis, ...Dn]\n\nwhere\n\n    Raxis = sum(Daxis(i))\n\nThat is, the data from the input tensors is joined along the `axis`\ndimension.\n\nThe number of dimensions of the input tensors must match, and all dimensions\nexcept `axis` must be equal.\n\nFor example:\n\n>>> t1 = [[1, 2, 3], [4, 5, 6]]\n>>> t2 = [[7, 8, 9], [10, 11, 12]]\n>>> tf.concat([t1, t2], 0)\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)>\n\n>>> tf.concat([t1, t2], 1)\n<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[ 1,  2,  3,  7,  8,  9],\n       [ 4,  5,  6, 10, 11, 12]], dtype=int32)>\n\nAs in Python, the `axis` could also be negative numbers. Negative `axis`\nare interpreted as counting from the end of the rank, i.e.,\n `axis + rank(values)`-th dimension.\n\nFor example:\n\n>>> t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]\n>>> t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]\n>>> tf.concat([t1, t2], -1)\n<tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy=\n  array([[[ 1,  2,  7,  4],\n          [ 2,  3,  8,  4]],\n         [[ 4,  4,  2, 10],\n          [ 5,  3, 15, 11]]], dtype=int32)>\n\nNote: If you are concatenating along a new axis consider using stack.\nE.g.\n\n```python\ntf.concat([tf.expand_dims(t, axis) for t in tensors], axis)\n```\n\ncan be rewritten as\n\n```python\ntf.stack(tensors, axis=axis)\n```\n\nArgs:\n  values: A list of `Tensor` objects or a single `Tensor`.\n  axis: 0-D `int32` `Tensor`.  Dimension along which to concatenate. Must be\n    in the range `[-rank(values), rank(values))`. As in Python, indexing for\n    axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers\n    to `axis`-th dimension. And negative axis refers to `axis +\n    rank(values)`-th dimension.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` resulting from concatenation of the input tensors.", "Library": "TensorFlow"}
{"API_Name": "tf.config.list_physical_devices", "Docstring": "Return a list of physical devices visible to the host runtime.\n\nPhysical devices are hardware devices present on the host machine. By default\nall discovered CPU and GPU devices are considered visible.\n\nThis API allows querying the physical hardware resources prior to runtime\ninitialization. Thus, giving an opportunity to call any additional\nconfiguration APIs. This is in contrast to `tf.config.list_logical_devices`,\nwhich triggers runtime initialization in order to list the configured devices.\n\nThe following example lists the number of visible GPUs on the host.\n\n>>> physical_devices = tf.config.list_physical_devices('GPU')\n>>> print(\"Num GPUs:\", len(physical_devices))\nNum GPUs: ...\n\nHowever, the number of GPUs available to the runtime may change during runtime\ninitialization due to marking certain devices as not visible or configuring\nmultiple logical devices.\n\nArgs:\n  device_type: (optional string) Only include devices matching this device\n    type. For example \"CPU\" or \"GPU\".\n  Notes: 1. If provided with any numerical values or any string other than\n    supported device type such as 'CPU' it returns an empty list instead of\n    raising error. 2. For default value it returns all physical devices\n\nReturns:\n  List of discovered `tf.config.PhysicalDevice` objects", "Library": "TensorFlow"}
{"API_Name": "tf.config.set_soft_device_placement", "Docstring": "Enable or disable soft device placement.\n\nIf enabled, ops can be placed on different devices than the device explicitly\nassigned by the user. This potentially has a large performance cost due to an\nincrease in data communication between devices.\n\nSome cases where soft_device_placement would modify device assignment are:\n  1. no GPU/TPU implementation for the OP\n  2. no GPU devices are known or registered\n  3. need to co-locate with reftype input(s) which are from CPU\n  4. an OP can not be compiled by XLA.  Common for TPU which always requires\n       the XLA compiler.\n\nFor TPUs, if this option is true, a feature called automatic outside\ncompilation is enabled. Automatic outside compilation will move uncompilable\nops within a TPU program to instead run on the host. This can be used when\nencountering compilation failures due to unsupported ops.\n\nNote: by default soft device placement is enabled when running in eager mode\n(for convenience) and disabled in graph mode (for performance).\n\nArgs:\n  enabled: A boolean indicating whether to enable soft placement.", "Library": "TensorFlow"}
{"API_Name": "tf.constant", "Docstring": "Creates a constant tensor from a tensor-like object.\n\nNote: All eager `tf.Tensor` values are immutable (in contrast to\n`tf.Variable`). There is nothing especially _constant_ about the value\nreturned from `tf.constant`. This function is not fundamentally different from\n`tf.convert_to_tensor`. The name `tf.constant` comes from the `value` being\nembedded in a `Const` node in the `tf.Graph`. `tf.constant` is useful\nfor asserting that the value can be embedded that way.\n\nIf the argument `dtype` is not specified, then the type is inferred from\nthe type of `value`.\n\n>>> # Constant 1-D Tensor from a python list.\n>>> tf.constant([1, 2, 3, 4, 5, 6])\n<tf.Tensor: shape=(6,), dtype=int32,\n    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n>>> # Or a numpy array\n>>> a = np.array([[1, 2, 3], [4, 5, 6]])\n>>> tf.constant(a)\n<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n  array([[1, 2, 3],\n         [4, 5, 6]])>\n\nIf `dtype` is specified, the resulting tensor values are cast to the requested\n`dtype`.\n\n>>> tf.constant([1, 2, 3, 4, 5, 6], dtype=tf.float64)\n<tf.Tensor: shape=(6,), dtype=float64,\n    numpy=array([1., 2., 3., 4., 5., 6.])>\n\nIf `shape` is set, the `value` is reshaped to match. Scalars are expanded to\nfill the `shape`:\n\n>>> tf.constant(0, shape=(2, 3))\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[0, 0, 0],\n         [0, 0, 0]], dtype=int32)>\n>>> tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[1, 2, 3],\n         [4, 5, 6]], dtype=int32)>\n\n`tf.constant` has no effect if an eager Tensor is passed as the `value`, it\neven transmits gradients:\n\n>>> v = tf.Variable([0.0])\n>>> with tf.GradientTape() as g:\n...     loss = tf.constant(v + v)\n>>> g.gradient(loss, v).numpy()\narray([2.], dtype=float32)\n\nBut, since `tf.constant` embeds the value in the `tf.Graph` this fails for\nsymbolic tensors:\n\n>>> with tf.compat.v1.Graph().as_default():\n...   i = tf.compat.v1.placeholder(shape=[None, None], dtype=tf.float32)\n...   t = tf.constant(i)\nTraceback (most recent call last):\n...\nTypeError: ...\n\n`tf.constant` will create tensors on the current device. Inputs which are\nalready tensors maintain their placements unchanged.\n\nRelated Ops:\n\n* `tf.convert_to_tensor` is similar but:\n  * It has no `shape` argument.\n  * Symbolic tensors are allowed to pass through.\n\n  >>> with tf.compat.v1.Graph().as_default():\n  ...   i = tf.compat.v1.placeholder(shape=[None, None], dtype=tf.float32)\n  ...   t = tf.convert_to_tensor(i)\n\n* `tf.fill`: differs in a few ways:\n  *   `tf.constant` supports arbitrary constants, not just uniform scalar\n      Tensors like `tf.fill`.\n  *   `tf.fill` creates an Op in the graph that is expanded at runtime, so it\n      can efficiently represent large tensors.\n  *   Since `tf.fill` does not embed the value, it can produce dynamically\n      sized outputs.\n\nArgs:\n  value: A constant value (or list) of output type `dtype`.\n  dtype: The type of the elements of the resulting tensor.\n  shape: Optional dimensions of resulting tensor.\n  name: Optional name for the tensor.\n\nReturns:\n  A Constant Tensor.\n\nRaises:\n  TypeError: if shape is incorrectly specified or unsupported.\n  ValueError: if called on a symbolic tensor.", "Library": "TensorFlow"}
{"API_Name": "tf.convert_to_tensor", "Docstring": "Converts the given `value` to a `Tensor`.\n\nThis function converts Python objects of various types to `Tensor`\nobjects. It accepts `Tensor` objects, numpy arrays, Python lists,\nand Python scalars.\n\nFor example:\n\n>>> import numpy as np\n>>> def my_func(arg):\n...   arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n...   return arg\n\n>>> # The following calls are equivalent.\n...\n>>> value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\n>>> print(value_1)\ntf.Tensor(\n  [[1. 2.]\n   [3. 4.]], shape=(2, 2), dtype=float32)\n>>> value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\n>>> print(value_2)\ntf.Tensor(\n  [[1. 2.]\n   [3. 4.]], shape=(2, 2), dtype=float32)\n>>> value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n>>> print(value_3)\ntf.Tensor(\n  [[1. 2.]\n   [3. 4.]], shape=(2, 2), dtype=float32)\n\nThis function can be useful when composing a new operation in Python\n(such as `my_func` in the example above). All standard Python op\nconstructors apply this function to each of their Tensor-valued\ninputs, which allows those ops to accept numpy arrays, Python lists,\nand scalars in addition to `Tensor` objects.\n\nNote: This function diverges from default Numpy behavior for `float` and\n  `string` types when `None` is present in a Python list or scalar. Rather\n  than silently converting `None` values, an error will be thrown.\n\nArgs:\n  value: An object whose type has a registered `Tensor` conversion function.\n  dtype: Optional element type for the returned tensor. If missing, the type\n    is inferred from the type of `value`.\n  dtype_hint: Optional element type for the returned tensor, used when dtype\n    is None. In some cases, a caller may not have a dtype in mind when\n    converting to a tensor, so dtype_hint can be used as a soft preference. If\n    the conversion to `dtype_hint` is not possible, this argument has no\n    effect.\n  name: Optional name to use if a new `Tensor` is created.\n\nReturns:\n  A `Tensor` based on `value`.\n\nRaises:\n  TypeError: If no conversion function is registered for `value` to `dtype`.\n  RuntimeError: If a registered conversion function returns an invalid value.\n  ValueError: If the `value` is a tensor not of given `dtype` in graph mode.", "Library": "TensorFlow"}
{"API_Name": "tf.cos", "Docstring": "Computes cos of x element-wise.\n\n  Given an input tensor, this function computes cosine of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `[-1,1]`. If input lies outside the boundary, `nan`\n  is returned.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10000, float(\"inf\")])\n  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]\n  ```\n\nArgs:\n  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `x`.", "Library": "TensorFlow"}
{"API_Name": "tf.data.Dataset.batch", "Docstring": "Combines consecutive elements of this dataset into batches.\n\n>>> dataset = tf.data.Dataset.range(8)\n>>> dataset = dataset.batch(3)\n>>> list(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n\n>>> dataset = tf.data.Dataset.range(8)\n>>> dataset = dataset.batch(3, drop_remainder=True)\n>>> list(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nNote: If your program requires data to have a statically known shape (e.g.,\nwhen using XLA), you should use `drop_remainder=True`. Without\n`drop_remainder=True` the shape of the output dataset will have an unknown\nleading dimension due to the possibility of a smaller final batch.\n\nArgs:\n  batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n    consecutive elements of this dataset to combine in a single batch.\n  drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n    whether the last batch should be dropped in the case it has fewer than\n    `batch_size` elements; the default behavior is not to drop the smaller\n    batch.\n  num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n    representing the number of batches to compute asynchronously in\n    parallel.\n    If not specified, batches will be computed sequentially. If the value\n    `tf.data.AUTOTUNE` is used, then the number of parallel\n    calls is set dynamically based on available resources.\n  deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n    boolean is specified (`True` or `False`), it controls the order in which\n    the transformation produces elements. If set to `False`, the\n    transformation is allowed to yield elements out of order to trade\n    determinism for performance. If not specified, the\n    `tf.data.Options.deterministic` option (`True` by default) controls the\n    behavior.\n  name: (Optional.) A name for the tf.data operation.\n\nReturns:\n  A new `Dataset` with the transformation applied as described above.", "Library": "TensorFlow"}
{"API_Name": "tf.data.Dataset.from_tensor_slices", "Docstring": "Creates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension\nof each tensor and using it as the dataset dimension. All input tensors\nmust have the same size in their first dimensions.\n\n>>> # Slicing a 1D tensor produces scalar tensor elements.\n>>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n>>> list(dataset.as_numpy_iterator())\n[1, 2, 3]\n\n>>> # Slicing a 2D tensor produces 1D tensor elements.\n>>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n>>> list(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n\n>>> # Slicing a tuple of 1D tensors produces tuple elements containing\n>>> # scalar tensors.\n>>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n>>> list(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n\n>>> # Dictionary structure is also preserved.\n>>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\n>>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n...                                       {'a': 2, 'b': 4}]\nTrue\n\n>>> # Two tensors can be combined into one Dataset object.\n>>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n>>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n>>> dataset = Dataset.from_tensor_slices((features, labels))\n>>> # Both the features and the labels tensors can be converted\n>>> # to a Dataset object separately and combined after.\n>>> features_dataset = Dataset.from_tensor_slices(features)\n>>> labels_dataset = Dataset.from_tensor_slices(labels)\n>>> dataset = Dataset.zip((features_dataset, labels_dataset))\n>>> # A batched feature and label set can be converted to a Dataset\n>>> # in similar fashion.\n>>> batched_features = tf.constant([[[1, 3], [2, 3]],\n...                                 [[2, 1], [1, 2]],\n...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\n>>> batched_labels = tf.constant([['A', 'A'],\n...                               ['B', 'B'],\n...                               ['A', 'B']], shape=(3, 2, 1))\n>>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n>>> for element in dataset.as_numpy_iterator():\n...   print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more\n`tf.constant` operations. For large datasets (> 1 GB), this can waste\nmemory and run into byte limits of graph serialization. If `tensors`\ncontains one or more large NumPy arrays, consider the alternative described\nin [this guide](\nhttps://tensorflow.org/guide/data#consuming_numpy_arrays).\n\nArgs:\n  tensors: A dataset element, whose components have the same first\n    dimension. Supported values are documented\n    [here](https://www.tensorflow.org/guide/data#dataset_structure).\n  name: (Optional.) A name for the tf.data operation.\n\nReturns:\n  Dataset: A `Dataset`.", "Library": "TensorFlow"}
{"API_Name": "tf.data.Dataset.repeat", "Docstring": "Repeats this dataset so each original value is seen `count` times.\n\n>>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n>>> dataset = dataset.repeat(3)\n>>> list(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\nNote: If the input dataset depends on global state (e.g. a random number\ngenerator) or its output is non-deterministic (e.g. because of upstream\n`shuffle`), then different repetitions may produce different elements.\n\nArgs:\n  count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n    number of times the dataset should be repeated. The default behavior (if\n    `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\n  name: (Optional.) A name for the tf.data operation.\n\nReturns:\n  A new `Dataset` with the transformation applied as described above.", "Library": "TensorFlow"}
{"API_Name": "tf.data.Dataset.take", "Docstring": "Creates a `Dataset` with at most `count` elements from this dataset.\n\n>>> dataset = tf.data.Dataset.range(10)\n>>> dataset = dataset.take(3)\n>>> list(dataset.as_numpy_iterator())\n[0, 1, 2]\n\nArgs:\n  count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n    elements of this dataset that should be taken to form the new dataset.\n    If `count` is -1, or if `count` is greater than the size of this\n    dataset, the new dataset will contain all elements of this dataset.\n  name: (Optional.) A name for the tf.data operation.\n\nReturns:\n  A new `Dataset` with the transformation applied as described above.", "Library": "TensorFlow"}
{"API_Name": "tf.debugging.set_log_device_placement", "Docstring": "Turns logging for device placement decisions on or off.\n\nOperations execute on a particular device, producing and consuming tensors on\nthat device. This may change the performance of the operation or require\nTensorFlow to copy data to or from an accelerator, so knowing where operations\nexecute is useful for debugging performance issues.\n\nFor more advanced profiling, use the [TensorFlow\nprofiler](https://www.tensorflow.org/guide/profiler).\n\nDevice placement for operations is typically controlled by a `tf.device`\nscope, but there are exceptions, for example operations on a `tf.Variable`\nwhich follow the initial placement of the variable. Turning off soft device\nplacement (with `tf.config.set_soft_device_placement`) provides more explicit\ncontrol.\n\n>>> tf.debugging.set_log_device_placement(True)\n>>> tf.ones([])\n>>> # [...] op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n>>> with tf.device(\"CPU\"):\n...  tf.ones([])\n>>> # [...] op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n>>> tf.debugging.set_log_device_placement(False)\n\nTurning on `tf.debugging.set_log_device_placement` also logs the placement of\nops inside `tf.function` when the function is called.\n\nArgs:\n  enabled: Whether to enabled device placement logging.", "Library": "TensorFlow"}
{"API_Name": "tf.device", "Docstring": "Specifies the device for ops created/executed in this context.\n\nThis function specifies the device to be used for ops created/executed in a\nparticular context. Nested contexts will inherit and also create/execute\ntheir ops on the specified device. If a specific device is not required,\nconsider not using this function so that a device can be automatically\nassigned.  In general the use of this function is optional. `device_name` can\nbe fully specified, as in \"/job:worker/task:1/device:cpu:0\", or partially\nspecified, containing only a subset of the \"/\"-separated fields. Any fields\nwhich are specified will override device annotations from outer scopes.\n\nFor example:\n\n```python\nwith tf.device('/job:foo'):\n  # ops created here have devices with /job:foo\n  with tf.device('/job:bar/task:0/device:gpu:2'):\n    # ops created here have the fully specified device above\n  with tf.device('/device:gpu:1'):\n    # ops created here have the device '/job:foo/device:gpu:1'\n```\n\nArgs:\n  device_name: The device name to use in the context.\n\nReturns:\n  A context manager that specifies the default device to use for newly\n  created ops.\n\nRaises:\n  RuntimeError: If a function is passed in.", "Library": "TensorFlow"}
{"API_Name": "tf.equal", "Docstring": "Returns the truth value of (x == y) element-wise.\n\nPerforms a [broadcast](\nhttps://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) with the\narguments and then an element-wise equality comparison, returning a Tensor of\nboolean values.\n\nFor example:\n\n>>> x = tf.constant([2, 4])\n>>> y = tf.constant(2)\n>>> tf.math.equal(x, y)\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>\n\n>>> x = tf.constant([2, 4])\n>>> y = tf.constant([2, 4])\n>>> tf.math.equal(x, y)\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>\n\nArgs:\n  x: A `tf.Tensor`.\n  y: A `tf.Tensor`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `tf.Tensor` of type bool with the same size as that of x or y.\n\nRaises:\n  `tf.errors.InvalidArgumentError`: If shapes of arguments are incompatible", "Library": "TensorFlow"}
{"API_Name": "tf.exp", "Docstring": "Computes exponential of x element-wise.  \\\\(y = e^x\\\\).\n\nThis function computes the exponential of the input tensor element-wise.\ni.e. `math.exp(x)` or \\\\(e^x\\\\), where `x` is the input tensor.\n\\\\(e\\\\) denotes Euler's number and is approximately equal to 2.718281.\nOutput is positive for any real input.\n\n>>> x = tf.constant(2.0)\n>>> tf.math.exp(x)\n<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>\n\n>>> x = tf.constant([2.0, 8.0])\n>>> tf.math.exp(x)\n<tf.Tensor: shape=(2,), dtype=float32,\nnumpy=array([   7.389056, 2980.958   ], dtype=float32)>\n\nFor complex numbers, the exponential value is calculated as\n$$\ne^{x+iy} = {e^x} {e^{iy}} = {e^x} ({\\cos (y) + i \\sin (y)})\n$$\n\nFor `1+1j` the value would be computed as:\n$$\ne^1 (\\cos (1) + i \\sin (1)) = 2.7182817 \\times (0.5403023+0.84147096j)\n$$\n\n>>> x = tf.constant(1 + 1j)\n>>> tf.math.exp(x)\n<tf.Tensor: shape=(), dtype=complex128,\nnumpy=(1.4686939399158851+2.2873552871788423j)>\n\nArgs:\n  x: A `tf.Tensor`. Must be one of the following types: `bfloat16`, `half`,\n    `float32`, `float64`, `complex64`, `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `tf.Tensor`. Has the same type as `x`.\n\n@compatibility(numpy)\nEquivalent to np.exp\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.expand_dims", "Docstring": "Returns a tensor with a length 1 axis inserted at index `axis`.\n\nGiven a tensor `input`, this operation inserts a dimension of length 1 at the\ndimension index `axis` of `input`'s shape. The dimension index follows Python\nindexing rules: It's zero-based, a negative index it is counted backward\nfrom the end.\n\nThis operation is useful to:\n\n* Add an outer \"batch\" dimension to a single element.\n* Align axes for broadcasting.\n* To add an inner vector length axis to a tensor of scalars.\n\nFor example:\n\nIf you have a single image of shape `[height, width, channels]`:\n\n>>> image = tf.zeros([10,10,3])\n\nYou can add an outer `batch` axis by passing `axis=0`:\n\n>>> tf.expand_dims(image, axis=0).shape.as_list()\n[1, 10, 10, 3]\n\nThe new axis location matches Python `list.insert(axis, 1)`:\n\n>>> tf.expand_dims(image, axis=1).shape.as_list()\n[10, 1, 10, 3]\n\nFollowing standard Python indexing rules, a negative `axis` counts from the\nend so `axis=-1` adds an inner most dimension:\n\n>>> tf.expand_dims(image, -1).shape.as_list()\n[10, 10, 3, 1]\n\nThis operation requires that `axis` is a valid index for `input.shape`,\nfollowing Python indexing rules:\n\n```\n-1-tf.rank(input) <= axis <= tf.rank(input)\n```\n\nThis operation is related to:\n\n* `tf.squeeze`, which removes dimensions of size 1.\n* `tf.reshape`, which provides more flexible reshaping capability.\n* `tf.sparse.expand_dims`, which provides this functionality for\n  `tf.SparseTensor`\n\nArgs:\n  input: A `Tensor`.\n  axis: Integer specifying the dimension index at which to expand the\n    shape of `input`. Given an input of D dimensions, `axis` must be in range\n    `[-(D+1), D]` (inclusive).\n  name: Optional string. The name of the output `Tensor`.\n\nReturns:\n  A tensor with the same data as `input`, with an additional dimension\n  inserted at the index specified by `axis`.\n\nRaises:\n  TypeError: If `axis` is not specified.\n  InvalidArgumentError: If `axis` is out of range `[-(D+1), D]`.", "Library": "TensorFlow"}
{"API_Name": "tf.eye", "Docstring": "Construct an identity matrix, or a batch of matrices.\n\nSee also `tf.ones`, `tf.zeros`, `tf.fill`, `tf.one_hot`.\n\n```python\n# Construct one identity matrix.\ntf.eye(2)\n==> [[1., 0.],\n     [0., 1.]]\n\n# Construct a batch of 3 identity matrices, each 2 x 2.\n# batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2.\nbatch_identity = tf.eye(2, batch_shape=[3])\n\n# Construct one 2 x 3 \"identity\" matrix\ntf.eye(2, num_columns=3)\n==> [[ 1.,  0.,  0.],\n     [ 0.,  1.,  0.]]\n```\n\nArgs:\n  num_rows: Non-negative `int32` scalar `Tensor` giving the number of rows\n    in each batch matrix.\n  num_columns: Optional non-negative `int32` scalar `Tensor` giving the number\n    of columns in each batch matrix.  Defaults to `num_rows`.\n  batch_shape:  A list or tuple of Python integers or a 1-D `int32` `Tensor`.\n    If provided, the returned `Tensor` will have leading batch dimensions of\n    this shape.\n  dtype:  The type of an element in the resulting `Tensor`\n  name:  A name for this `Op`.  Defaults to \"eye\".\n\nReturns:\n  A `Tensor` of shape `batch_shape + [num_rows, num_columns]`", "Library": "TensorFlow"}
{"API_Name": "tf.fill", "Docstring": "Creates a tensor filled with a scalar value.\n\nSee also `tf.ones`, `tf.zeros`, `tf.one_hot`, `tf.eye`.\n\nThis operation creates a tensor of shape `dims` and fills it with `value`.\n\nFor example:\n\n>>> tf.fill([2, 3], 9)\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[9, 9, 9],\n       [9, 9, 9]], dtype=int32)>\n\n`tf.fill` evaluates at graph runtime and supports dynamic shapes based on\nother runtime `tf.Tensors`, unlike `tf.constant(value, shape=dims)`, which\nembeds the value as a `Const` node.\n\nArgs:\n  dims: A 1-D sequence of non-negative numbers. Represents the shape of the\n    output `tf.Tensor`. Entries should be of type: `int32`, `int64`.\n  value: A value to fill the returned `tf.Tensor`.\n  name: Optional string. The name of the output `tf.Tensor`.\n  layout: Optional, `tf.experimental.dtensor.Layout`. If provided, the result\n    is a [DTensor](https://www.tensorflow.org/guide/dtensor_overview) with the\n    provided layout.\n\nReturns:\n  A `tf.Tensor` with shape `dims` and the same dtype as `value`.\n\nRaises:\n  InvalidArgumentError: `dims` contains negative entries.\n  NotFoundError: `dims` contains non-integer entries.\n\n@compatibility(numpy)\nSimilar to `np.full`. In `numpy`, more parameters are supported. Passing a\nnumber argument as the shape (`np.full(5, value)`) is valid in `numpy` for\nspecifying a 1-D shaped result, while TensorFlow does not support this syntax.\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.float32", "Docstring": "32-bit (single precision) floating-point.", "Library": "TensorFlow"}
{"API_Name": "tf.gather", "Docstring": "Gather slices from params axis `axis` according to indices. (deprecated arguments)\n\nDeprecated: SOME ARGUMENTS ARE DEPRECATED: `(validate_indices)`. They will be removed in a future version.\nInstructions for updating:\nThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n\nGather slices from `params` axis `axis` according to `indices`.  `indices`\nmust be an integer tensor of any dimension (often 1-D).\n\n`Tensor.__getitem__` works for scalars, `tf.newaxis`, and\n[python slices](https://numpy.org/doc/stable/reference/arrays.indexing.html#basic-slicing-and-indexing)\n\n`tf.gather` extends indexing to handle tensors of indices.\n\nIn the simplest case it's identical to scalar indexing:\n\n>>> params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])\n>>> params[3].numpy()\nb'p3'\n>>> tf.gather(params, 3).numpy()\nb'p3'\n\nThe most common case is to pass a single axis tensor of indices (this\ncan't be expressed as a python slice because the indices are not sequential):\n\n>>> indices = [2, 0, 2, 5]\n>>> tf.gather(params, indices).numpy()\narray([b'p2', b'p0', b'p2', b'p5'], dtype=object)\n\n<div style=\"width:70%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/Gather.png\"\nalt>\n</div>\n\nThe indices can have any shape. When the `params` has 1 axis, the\noutput shape is equal to the input shape:\n\n>>> tf.gather(params, [[2, 0], [2, 5]]).numpy()\narray([[b'p2', b'p0'],\n       [b'p2', b'p5']], dtype=object)\n\nThe `params` may also have any shape. `gather` can select slices\nacross any axis depending on the `axis` argument (which defaults to 0).\nBelow it is used to gather first rows, then columns from a matrix:\n\n>>> params = tf.constant([[0, 1.0, 2.0],\n...                       [10.0, 11.0, 12.0],\n...                       [20.0, 21.0, 22.0],\n...                       [30.0, 31.0, 32.0]])\n>>> tf.gather(params, indices=[3,1]).numpy()\narray([[30., 31., 32.],\n       [10., 11., 12.]], dtype=float32)\n>>> tf.gather(params, indices=[2,1], axis=1).numpy()\narray([[ 2.,  1.],\n       [12., 11.],\n       [22., 21.],\n       [32., 31.]], dtype=float32)\n\nMore generally: The output shape has the same shape as the input, with the\nindexed-axis replaced by the shape of the indices.\n\n>>> def result_shape(p_shape, i_shape, axis=0):\n...   return p_shape[:axis] + i_shape + p_shape[axis+1:]\n>>>\n>>> result_shape([1, 2, 3], [], axis=1)\n[1, 3]\n>>> result_shape([1, 2, 3], [7], axis=1)\n[1, 7, 3]\n>>> result_shape([1, 2, 3], [7, 5], axis=1)\n[1, 7, 5, 3]\n\nHere are some examples:\n\n>>> params.shape.as_list()\n[4, 3]\n>>> indices = tf.constant([[0, 2]])\n>>> tf.gather(params, indices=indices, axis=0).shape.as_list()\n[1, 2, 3]\n>>> tf.gather(params, indices=indices, axis=1).shape.as_list()\n[4, 1, 2]\n\n>>> params = tf.random.normal(shape=(5, 6, 7, 8))\n>>> indices = tf.random.uniform(shape=(10, 11), maxval=7, dtype=tf.int32)\n>>> result = tf.gather(params, indices, axis=2)\n>>> result.shape.as_list()\n[5, 6, 10, 11, 8]\n\nThis is because each index takes a slice from `params`, and\nplaces it at the corresponding location in the output. For the above example\n\n>>> # For any location in indices\n>>> a, b = 0, 1\n>>> tf.reduce_all(\n...     # the corresponding slice of the result\n...     result[:, :, a, b, :] ==\n...     # is equal to the slice of `params` along `axis` at the index.\n...     params[:, :, indices[a, b], :]\n... ).numpy()\nTrue\n\n### Batching:\n\nThe `batch_dims` argument lets you gather different items from each element\nof a batch.\n\nUsing `batch_dims=1` is equivalent to having an outer loop over the first\naxis of `params` and `indices`:\n\n>>> params = tf.constant([\n...     [0, 0, 1, 0, 2],\n...     [3, 0, 0, 0, 4],\n...     [0, 5, 0, 6, 0]])\n>>> indices = tf.constant([\n...     [2, 4],\n...     [0, 4],\n...     [1, 3]])\n\n>>> tf.gather(params, indices, axis=1, batch_dims=1).numpy()\narray([[1, 2],\n       [3, 4],\n       [5, 6]], dtype=int32)\n\nThis is equivalent to:\n\n>>> def manually_batched_gather(params, indices, axis):\n...   batch_dims=1\n...   result = []\n...   for p,i in zip(params, indices):\n...     r = tf.gather(p, i, axis=axis-batch_dims)\n...     result.append(r)\n...   return tf.stack(result)\n>>> manually_batched_gather(params, indices, axis=1).numpy()\narray([[1, 2],\n       [3, 4],\n       [5, 6]], dtype=int32)\n\nHigher values of `batch_dims` are equivalent to multiple nested loops over\nthe outer axes of `params` and `indices`. So the overall shape function is\n\n>>> def batched_result_shape(p_shape, i_shape, axis=0, batch_dims=0):\n...   return p_shape[:axis] + i_shape[batch_dims:] + p_shape[axis+1:]\n>>>\n>>> batched_result_shape(\n...     p_shape=params.shape.as_list(),\n...     i_shape=indices.shape.as_list(),\n...     axis=1,\n...     batch_dims=1)\n[3, 2]\n\n>>> tf.gather(params, indices, axis=1, batch_dims=1).shape.as_list()\n[3, 2]\n\nThis comes up naturally if you need to use the indices of an operation like\n`tf.argsort`, or `tf.math.top_k` where the last dimension of the indices\nindexes into the last dimension of input, at the corresponding location.\nIn this case you can use `tf.gather(values, indices, batch_dims=-1)`.\n\nSee also:\n\n* `tf.Tensor.__getitem__`: The direct tensor index operation (`t[]`), handles\n  scalars and python-slices `tensor[..., 7, 1:-1]`\n* `tf.scatter`: A collection of operations similar to `__setitem__`\n  (`t[i] = x`)\n* `tf.gather_nd`: An operation similar to `tf.gather` but gathers across\n  multiple axis at once (it can gather elements of a matrix instead of rows\n  or columns)\n* `tf.boolean_mask`, `tf.where`: Binary indexing.\n* `tf.slice` and `tf.strided_slice`: For lower level access to the\n  implementation of `__getitem__`'s python-slice handling (`t[1:-1:2]`)\n\nArgs:\n  params: The `Tensor` from which to gather values. Must be at least rank\n    `axis + 1`.\n  indices: The index `Tensor`.  Must be one of the following types: `int32`,\n    `int64`. The values must be in range `[0, params.shape[axis])`.\n  validate_indices: Deprecated, does nothing. Indices are always validated on\n    CPU, never validated on GPU.\n\n    Caution: On CPU, if an out of bound index is found, an error is raised.\n    On GPU, if an out of bound index is found, a 0 is stored in the\n    corresponding output value.\n  axis: A `Tensor`. Must be one of the following types: `int32`, `int64`. The\n    `axis` in `params` to gather `indices` from. Must be greater than or equal\n    to `batch_dims`.  Defaults to the first non-batch dimension. Supports\n    negative indexes.\n  batch_dims: An `integer`.  The number of batch dimensions.  Must be less\n    than or equal to `rank(indices)`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `params`.", "Library": "TensorFlow"}
{"API_Name": "tf.gather_nd", "Docstring": "Gather slices from `params` into a Tensor with shape specified by `indices`.\n\n`indices` is a `Tensor` of indices into `params`. The index vectors are\narranged along the last axis of `indices`.\n\nThis is similar to `tf.gather`, in which `indices` defines slices into the\nfirst dimension of `params`. In `tf.gather_nd`, `indices` defines slices into\nthe first `N` dimensions of `params`, where `N = indices.shape[-1]`.\n\nCaution: On CPU, if an out of bound index is found, an error is returned.\nOn GPU, if an out of bound index is found, a 0 is stored in the\ncorresponding output value.\n\n## Gathering scalars\n\nIn the simplest case the vectors in `indices` index the full rank of `params`:\n\n>>> tf.gather_nd(\n...     indices=[[0, 0],\n...              [1, 1]],\n...     params = [['a', 'b'],\n...               ['c', 'd']]).numpy()\narray([b'a', b'd'], dtype=object)\n\nIn this case the result has 1-axis fewer than `indices`, and each index vector\nis replaced by the scalar indexed from `params`.\n\nIn this case the shape relationship is:\n\n```\nindex_depth = indices.shape[-1]\nassert index_depth == params.shape.rank\nresult_shape = indices.shape[:-1]\n```\n\nIf `indices` has a rank of `K`, it is helpful to think `indices` as a\n(K-1)-dimensional tensor of indices into `params`.\n\n## Gathering slices\n\nIf the index vectors do not index the full rank of `params` then each location\nin the result contains a slice of params. This example collects rows from a\nmatrix:\n\n>>> tf.gather_nd(\n...     indices = [[1],\n...                [0]],\n...     params = [['a', 'b', 'c'],\n...               ['d', 'e', 'f']]).numpy()\narray([[b'd', b'e', b'f'],\n       [b'a', b'b', b'c']], dtype=object)\n\nHere `indices` contains `[2]` index vectors, each with a length of `1`.\nThe index vectors each refer to rows of the `params` matrix. Each\nrow has a shape of `[3]` so the output shape is `[2, 3]`.\n\nIn this case, the relationship between the shapes is:\n\n```\nindex_depth = indices.shape[-1]\nouter_shape = indices.shape[:-1]\nassert index_depth <= params.shape.rank\ninner_shape = params.shape[index_depth:]\noutput_shape = outer_shape + inner_shape\n```\n\nIt is helpful to think of the results in this case as tensors-of-tensors.\nThe shape of the outer tensor is set by the leading dimensions of `indices`.\nWhile the shape of the inner tensors is the shape of a single slice.\n\n## Batches\n\nAdditionally, both `params` and `indices` can have `M` leading batch\ndimensions that exactly match. In this case `batch_dims` must be set to `M`.\n\nFor example, to collect one row from each of a batch of matrices you could\nset the leading elements of the index vectors to be their location in the\nbatch:\n\n>>> tf.gather_nd(\n...     indices = [[0, 1],\n...                [1, 0],\n...                [2, 4],\n...                [3, 2],\n...                [4, 1]],\n...     params=tf.zeros([5, 7, 3])).shape.as_list()\n[5, 3]\n\nThe `batch_dims` argument lets you omit those leading location dimensions\nfrom the index:\n\n>>> tf.gather_nd(\n...     batch_dims=1,\n...     indices = [[1],\n...                [0],\n...                [4],\n...                [2],\n...                [1]],\n...     params=tf.zeros([5, 7, 3])).shape.as_list()\n[5, 3]\n\nThis is equivalent to caling a separate `gather_nd` for each location in the\nbatch dimensions.\n\n\n>>> params=tf.zeros([5, 7, 3])\n>>> indices=tf.zeros([5, 1])\n>>> batch_dims = 1\n>>>\n>>> index_depth = indices.shape[-1]\n>>> batch_shape = indices.shape[:batch_dims]\n>>> assert params.shape[:batch_dims] == batch_shape\n>>> outer_shape = indices.shape[batch_dims:-1]\n>>> assert index_depth <= params.shape.rank\n>>> inner_shape = params.shape[batch_dims + index_depth:]\n>>> output_shape = batch_shape + outer_shape + inner_shape\n>>> output_shape.as_list()\n[5, 3]\n\n### More examples\n\nIndexing into a 3-tensor:\n\n>>> tf.gather_nd(\n...     indices = [[1]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([[[b'a1', b'b1'],\n        [b'c1', b'd1']]], dtype=object)\n\n\n\n>>> tf.gather_nd(\n...     indices = [[0, 1], [1, 0]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([[b'c0', b'd0'],\n       [b'a1', b'b1']], dtype=object)\n\n\n>>> tf.gather_nd(\n...     indices = [[0, 0, 1], [1, 0, 1]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([b'b0', b'b1'], dtype=object)\n\nThe examples below are for the case when only indices have leading extra\ndimensions. If both 'params' and 'indices' have leading batch dimensions, use\nthe 'batch_dims' parameter to run gather_nd in batch mode.\n\nBatched indexing into a matrix:\n\n>>> tf.gather_nd(\n...     indices = [[[0, 0]], [[0, 1]]],\n...     params = [['a', 'b'], ['c', 'd']]).numpy()\narray([[b'a'],\n       [b'b']], dtype=object)\n\n\n\nBatched slice indexing into a matrix:\n\n>>> tf.gather_nd(\n...     indices = [[[1]], [[0]]],\n...     params = [['a', 'b'], ['c', 'd']]).numpy()\narray([[[b'c', b'd']],\n       [[b'a', b'b']]], dtype=object)\n\n\nBatched indexing into a 3-tensor:\n\n>>> tf.gather_nd(\n...     indices = [[[1]], [[0]]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([[[[b'a1', b'b1'],\n         [b'c1', b'd1']]],\n       [[[b'a0', b'b0'],\n         [b'c0', b'd0']]]], dtype=object)\n\n\n>>> tf.gather_nd(\n...     indices = [[[0, 1], [1, 0]], [[0, 0], [1, 1]]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([[[b'c0', b'd0'],\n        [b'a1', b'b1']],\n       [[b'a0', b'b0'],\n        [b'c1', b'd1']]], dtype=object)\n\n>>> tf.gather_nd(\n...     indices = [[[0, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 1, 0]]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([[b'b0', b'b1'],\n       [b'd0', b'c1']], dtype=object)\n\n\nExamples with batched 'params' and 'indices':\n\n>>> tf.gather_nd(\n...     batch_dims = 1,\n...     indices = [[1],\n...                [0]],\n...     params = [[['a0', 'b0'],\n...                ['c0', 'd0']],\n...               [['a1', 'b1'],\n...                ['c1', 'd1']]]).numpy()\narray([[b'c0', b'd0'],\n       [b'a1', b'b1']], dtype=object)\n\n\n>>> tf.gather_nd(\n...     batch_dims = 1,\n...     indices = [[[1]], [[0]]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([[[b'c0', b'd0']],\n       [[b'a1', b'b1']]], dtype=object)\n\n>>> tf.gather_nd(\n...     batch_dims = 1,\n...     indices = [[[1, 0]], [[0, 1]]],\n...     params = [[['a0', 'b0'], ['c0', 'd0']],\n...               [['a1', 'b1'], ['c1', 'd1']]]).numpy()\narray([[b'c0'],\n       [b'b1']], dtype=object)\n\n\nSee also `tf.gather`.\n\nArgs:\n  params: A `Tensor`. The tensor from which to gather values.\n  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    Index tensor.\n  name: A name for the operation (optional).\n  batch_dims: An integer or a scalar 'Tensor'. The number of batch dimensions.\n\nReturns:\n  A `Tensor`. Has the same type as `params`.", "Library": "TensorFlow"}
{"API_Name": "tf.GradientTape", "Docstring": "Record operations for automatic differentiation.\n\nOperations are recorded if they are executed within this context manager and\nat least one of their inputs is being \"watched\".\n\nTrainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\nwhere `trainable=True` is default in both cases) are automatically watched.\nTensors can be manually watched by invoking the `watch` method on this context\nmanager.\n\nFor example, consider the function `y = x * x`. The gradient at `x = 3.0` can\nbe computed as:\n\n>>> x = tf.constant(3.0)\n>>> with tf.GradientTape() as g:\n...   g.watch(x)\n...   y = x * x\n>>> dy_dx = g.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n\nGradientTapes can be nested to compute higher-order derivatives. For example,\n\n>>> x = tf.constant(5.0)\n>>> with tf.GradientTape() as g:\n...   g.watch(x)\n...   with tf.GradientTape() as gg:\n...     gg.watch(x)\n...     y = x * x\n...   dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n>>> d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n>>> print(dy_dx)\ntf.Tensor(10.0, shape=(), dtype=float32)\n>>> print(d2y_dx2)\ntf.Tensor(2.0, shape=(), dtype=float32)\n\nBy default, the resources held by a GradientTape are released as soon as\nGradientTape.gradient() method is called. To compute multiple gradients over\nthe same computation, create a persistent gradient tape. This allows multiple\ncalls to the gradient() method as resources are released when the tape object\nis garbage collected. For example:\n\n>>> x = tf.constant(3.0)\n>>> with tf.GradientTape(persistent=True) as g:\n...   g.watch(x)\n...   y = x * x\n...   z = y * y\n>>> dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\n>>> print(dz_dx)\ntf.Tensor(108.0, shape=(), dtype=float32)\n>>> dy_dx = g.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n\nBy default GradientTape will automatically watch any trainable variables that\nare accessed inside the context. If you want fine grained control over which\nvariables are watched you can disable automatic tracking by passing\n`watch_accessed_variables=False` to the tape constructor:\n\n>>> x = tf.Variable(2.0)\n>>> w = tf.Variable(5.0)\n>>> with tf.GradientTape(\n...     watch_accessed_variables=False, persistent=True) as tape:\n...   tape.watch(x)\n...   y = x ** 2  # Gradients will be available for `x`.\n...   z = w ** 3  # No gradients will be available as `w` isn't being watched.\n>>> dy_dx = tape.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(4.0, shape=(), dtype=float32)\n>>> # No gradients will be available as `w` isn't being watched.\n>>> dz_dw = tape.gradient(z, w)\n>>> print(dz_dw)\nNone\n\nNote that when using models you should ensure that your variables exist when\nusing `watch_accessed_variables=False`. Otherwise it's quite easy to make your\nfirst iteration not have any gradients:\n\n```python\na = tf.keras.layers.Dense(32)\nb = tf.keras.layers.Dense(32)\n\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                           # `a.variables` will return an empty list and the\n                           # tape will not be watching anything.\n  result = b(a(inputs))\n  tape.gradient(result, a.variables)  # The result of this computation will be\n                                      # a list of `None`s since a's variables\n                                      # are not being watched.\n```\n\nNote that only tensors with real or complex dtypes are differentiable.", "Library": "TensorFlow"}
{"API_Name": "tf.image.resize", "Docstring": "Resize `images` to `size` using the specified `method`.\n\nResized images will be distorted if their original aspect ratio is not\nthe same as `size`.  To avoid distortions see\n`tf.image.resize_with_pad`.\n\n>>> image = tf.constant([\n...  [1,0,0,0,0],\n...  [0,1,0,0,0],\n...  [0,0,1,0,0],\n...  [0,0,0,1,0],\n...  [0,0,0,0,1],\n... ])\n>>> # Add \"batch\" and \"channels\" dimensions\n>>> image = image[tf.newaxis, ..., tf.newaxis]\n>>> image.shape.as_list()  # [batch, height, width, channels]\n[1, 5, 5, 1]\n>>> tf.image.resize(image, [3,5])[0,...,0].numpy()\narray([[0.6666667, 0.3333333, 0.       , 0.       , 0.       ],\n       [0.       , 0.       , 1.       , 0.       , 0.       ],\n       [0.       , 0.       , 0.       , 0.3333335, 0.6666665]],\n      dtype=float32)\n\nIt works equally well with a single image instead of a batch of images:\n\n>>> tf.image.resize(image[0], [3,5]).shape.as_list()\n[3, 5, 1]\n\nWhen `antialias` is true, the sampling filter will anti-alias the input image\nas well as interpolate.  When downsampling an image with [anti-aliasing](\nhttps://en.wikipedia.org/wiki/Spatial_anti-aliasing) the sampling filter\nkernel is scaled in order to properly anti-alias the input image signal.\n`antialias` has no effect when upsampling an image:\n\n>>> a = tf.image.resize(image, [5,10])\n>>> b = tf.image.resize(image, [5,10], antialias=True)\n>>> tf.reduce_max(abs(a - b)).numpy()\n0.0\n\nThe `method` argument expects an item from the `image.ResizeMethod` enum, or\nthe string equivalent. The options are:\n\n*   <b>`bilinear`</b>: [Bilinear interpolation.](\n  https://en.wikipedia.org/wiki/Bilinear_interpolation) If `antialias` is\n  true, becomes a hat/tent filter function with radius 1 when downsampling.\n*   <b>`lanczos3`</b>:  [Lanczos kernel](\n  https://en.wikipedia.org/wiki/Lanczos_resampling) with radius 3.\n  High-quality practical filter but may have some ringing, especially on\n  synthetic images.\n*   <b>`lanczos5`</b>: [Lanczos kernel] (\n  https://en.wikipedia.org/wiki/Lanczos_resampling) with radius 5.\n  Very-high-quality filter but may have stronger ringing.\n*   <b>`bicubic`</b>: [Cubic interpolant](\n  https://en.wikipedia.org/wiki/Bicubic_interpolation) of Keys. Equivalent to\n  Catmull-Rom kernel. Reasonably good quality and faster than Lanczos3Kernel,\n  particularly when upsampling.\n*   <b>`gaussian`</b>: [Gaussian kernel](\n  https://en.wikipedia.org/wiki/Gaussian_filter) with radius 3,\n  sigma = 1.5 / 3.0.\n*   <b>`nearest`</b>: [Nearest neighbor interpolation.](\n  https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation)\n  `antialias` has no effect when used with nearest neighbor interpolation.\n*   <b>`area`</b>: Anti-aliased resampling with area interpolation.\n  `antialias` has no effect when used with area interpolation; it\n  always anti-aliases.\n*   <b>`mitchellcubic`</b>: Mitchell-Netravali Cubic non-interpolating filter.\n  For synthetic images (especially those lacking proper prefiltering), less\n  ringing than Keys cubic kernel but less sharp.\n\nNote: Near image edges the filtering kernel may be partially outside the\nimage boundaries. For these pixels, only input pixels inside the image will be\nincluded in the filter sum, and the output value will be appropriately\nnormalized.\n\nThe return value has type `float32`, unless the `method` is\n`ResizeMethod.NEAREST_NEIGHBOR`, then the return dtype is the dtype\nof `images`:\n\n>>> nn = tf.image.resize(image, [5,7], method='nearest')\n>>> nn[0,...,0].numpy()\narray([[1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 1, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1]], dtype=int32)\n\nWith `preserve_aspect_ratio=True`, the aspect ratio is preserved, so `size`\nis the maximum for each dimension:\n\n>>> max_10_20 = tf.image.resize(image, [10,20], preserve_aspect_ratio=True)\n>>> max_10_20.shape.as_list()\n[1, 10, 10, 1]\n\nArgs:\n  images: 4-D Tensor of shape `[batch, height, width, channels]` or 3-D Tensor\n    of shape `[height, width, channels]`.\n  size: A 1-D int32 Tensor of 2 elements: `new_height, new_width`.  The new\n    size for the images.\n  method: An `image.ResizeMethod`, or string equivalent.  Defaults to\n    `bilinear`.\n  preserve_aspect_ratio: Whether to preserve the aspect ratio. If this is set,\n    then `images` will be resized to a size that fits in `size` while\n    preserving the aspect ratio of the original image. Scales up the image if\n    `size` is bigger than the current size of the `image`. Defaults to False.\n  antialias: Whether to use an anti-aliasing filter when downsampling an\n    image.\n  name: A name for this operation (optional).\n\nRaises:\n  ValueError: if the shape of `images` is incompatible with the\n    shape arguments to this function\n  ValueError: if `size` has an invalid shape or type.\n  ValueError: if an unsupported resize method is specified.\n\nReturns:\n  If `images` was 4-D, a 4-D float Tensor of shape\n  `[batch, new_height, new_width, channels]`.\n  If `images` was 3-D, a 3-D float Tensor of shape\n  `[new_height, new_width, channels]`.", "Library": "TensorFlow"}
{"API_Name": "tf.int32", "Docstring": "Signed 32-bit integer.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.datasets.fashion_mnist.load_data", "Docstring": "Loads the Fashion-MNIST dataset.\n\nThis is a dataset of 60,000 28x28 grayscale images of 10 fashion categories,\nalong with a test set of 10,000 images. This dataset can be used as\na drop-in replacement for MNIST.\n\nThe classes are:\n\n| Label | Description |\n|:-----:|-------------|\n|   0   | T-shirt/top |\n|   1   | Trouser     |\n|   2   | Pullover    |\n|   3   | Dress       |\n|   4   | Coat        |\n|   5   | Sandal      |\n|   6   | Shirt       |\n|   7   | Sneaker     |\n|   8   | Bag         |\n|   9   | Ankle boot  |\n\nReturns:\n\nTuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`.\n\n**`x_train`**: `uint8` NumPy array of grayscale image data with shapes\n  `(60000, 28, 28)`, containing the training data.\n\n**`y_train`**: `uint8` NumPy array of labels (integers in range 0-9)\n  with shape `(60000,)` for the training data.\n\n**`x_test`**: `uint8` NumPy array of grayscale image data with shapes\n  (10000, 28, 28), containing the test data.\n\n**`y_test`**: `uint8` NumPy array of labels (integers in range 0-9)\n  with shape `(10000,)` for the test data.\n\nExample:\n\n```python\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\nassert x_train.shape == (60000, 28, 28)\nassert x_test.shape == (10000, 28, 28)\nassert y_train.shape == (60000,)\nassert y_test.shape == (10000,)\n```\n\nLicense:\n\nThe copyright for Fashion-MNIST is held by Zalando SE.\nFashion-MNIST is licensed under the [MIT license](\n    https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.initializers.GlorotUniform", "Docstring": "The Glorot uniform initializer, also called Xavier uniform initializer.\n\nDraws samples from a uniform distribution within `[-limit, limit]`, where\n`limit = sqrt(6 / (fan_in + fan_out))` (`fan_in` is the number of input\nunits in the weight tensor and `fan_out` is the number of output units).\n\nExamples:\n\n>>> # Standalone usage:\n>>> initializer = GlorotUniform()\n>>> values = initializer(shape=(2, 2))\n\n>>> # Usage in a Keras layer:\n>>> initializer = GlorotUniform()\n>>> layer = Dense(3, kernel_initializer=initializer)\n\nArgs:\n    seed: A Python integer or instance of\n        `keras.backend.SeedGenerator`.\n        Used to make the behavior of the initializer\n        deterministic. Note that an initializer seeded with an integer\n        or `None` (unseeded) will produce the same random values\n        across multiple calls. To get different random values\n        across multiple calls, use as seed an instance\n        of `keras.backend.SeedGenerator`.\n\nReference:\n\n- [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)", "Library": "TensorFlow"}
{"API_Name": "tf.keras.layers.AveragePooling1D", "Docstring": "Average pooling for temporal data.\n\nDownsamples the input representation by taking the average value over the\nwindow defined by `pool_size`. The window is shifted by `strides`.  The\nresulting output when using \"valid\" padding option has a shape of:\n`output_shape = (input_shape - pool_size + 1) / strides)`\n\nThe resulting output shape when using the \"same\" padding option is:\n`output_shape = input_shape / strides`\n\nArgs:\n    pool_size: int, size of the max pooling window.\n    strides: int or None. Specifies how much the pooling window moves\n        for each pooling step. If None, it will default to `pool_size`.\n    padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n        `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n        the left/right or up/down of the input such that output has the same\n        height/width dimension as the input.\n    data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n        The ordering of the dimensions in the inputs. `\"channels_last\"`\n        corresponds to inputs with shape `(batch, steps, features)`\n        while `\"channels_first\"` corresponds to inputs with shape\n        `(batch, features, steps)`. It defaults to the `image_data_format`\n        value found in your Keras config file at `~/.keras/keras.json`.\n        If you never set it, then it will be `\"channels_last\"`.\n\nInput shape:\n\n- If `data_format=\"channels_last\"`:\n    3D tensor with shape `(batch_size, steps, features)`.\n- If `data_format=\"channels_first\"`:\n    3D tensor with shape `(batch_size, features, steps)`.\n\nOutput shape:\n\n- If `data_format=\"channels_last\"`:\n    3D tensor with shape `(batch_size, downsampled_steps, features)`.\n- If `data_format=\"channels_first\"`:\n    3D tensor with shape `(batch_size, features, downsampled_steps)`.\n\nExamples:\n\n`strides=1` and `padding=\"valid\"`:\n\n>>> x = np.array([1., 2., 3., 4., 5.])\n>>> x = np.reshape(x, [1, 5, 1])\n>>> avg_pool_1d = keras.layers.AveragePooling1D(pool_size=2,\n...    strides=1, padding=\"valid\")\n>>> avg_pool_1d(x)\n\n`strides=2` and `padding=\"valid\"`:\n\n>>> x = np.array([1., 2., 3., 4., 5.])\n>>> x = np.reshape(x, [1, 5, 1])\n>>> avg_pool_1d = keras.layers.AveragePooling1D(pool_size=2,\n...    strides=2, padding=\"valid\")\n>>> avg_pool_1d(x)\n\n`strides=1` and `padding=\"same\"`:\n\n>>> x = np.array([1., 2., 3., 4., 5.])\n>>> x = np.reshape(x, [1, 5, 1])\n>>> avg_pool_1d = keras.layers.AveragePooling1D(pool_size=2,\n...    strides=1, padding=\"same\")\n>>> avg_pool_1d(x)", "Library": "TensorFlow"}
{"API_Name": "tf.keras.layers.Layer.add_weight", "Docstring": "Add a weight variable to the layer.\n\nArgs:\n    shape: Shape tuple for the variable. Must be fully-defined\n        (no `None` entries). Defaults to `()` (scalar) if unspecified.\n    initializer: Initializer object to use to populate the initial\n        variable value, or string name of a built-in initializer\n        (e.g. `\"random_normal\"`). If unspecified, defaults to\n        `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n        for all other types (e.g. int, bool).\n    dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n        unspecified, defaults to the layer's variable dtype\n        (which itself defaults to `\"float32\"` if unspecified).\n    trainable: Boolean, whether the variable should be trainable via\n        backprop or whether its updates are managed manually. Defaults\n        to `True`.\n    autocast: Boolean, whether to autocast layers variables when\n        accessing them. Defaults to `True`.\n    regularizer: Regularizer object to call to apply penalty on the\n        weight. These penalties are summed into the loss function\n        during optimization. Defaults to `None`.\n    constraint: Contrainst object to call on the variable after any\n        optimizer update, or string name of a built-in constraint.\n        Defaults to `None`.\n    aggregation: String, one of `'mean'`, `'sum'`,\n        `'only_first_replica'`. Annotates the variable with the type\n        of multi-replica aggregation to be used for this variable\n        when writing custom data parallel training loops.\n    name: String name of the variable. Useful for debugging purposes.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.binary_crossentropy", "Docstring": "Computes the binary crossentropy loss.\n\nArgs:\n    y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n    y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n    from_logits: Whether `y_pred` is expected to be a logits tensor. By\n        default, we assume that `y_pred` encodes a probability distribution.\n    label_smoothing: Float in `[0, 1]`. If > `0` then smooth the labels by\n        squeezing them towards 0.5, that is,\n        using `1. - 0.5 * label_smoothing` for the target class\n        and `0.5 * label_smoothing` for the non-target class.\n    axis: The axis along which the mean is computed. Defaults to `-1`.\n\nReturns:\n    Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n\nExample:\n\n>>> y_true = [[0, 1], [0, 0]]\n>>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n>>> loss = keras.losses.binary_crossentropy(y_true, y_pred)\n>>> assert loss.shape == (2,)\n>>> loss\narray([0.916 , 0.714], dtype=float32)", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.BinaryCrossentropy", "Docstring": "Computes the cross-entropy loss between true labels and predicted labels.\n\nUse this cross-entropy loss for binary (0 or 1) classification applications.\nThe loss function requires the following inputs:\n\n- `y_true` (true label): This is either 0 or 1.\n- `y_pred` (predicted value): This is the model's prediction, i.e, a single\n    floating-point value which either represents a\n    [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n    when `from_logits=True`) or a probability (i.e, value in [0., 1.] when\n    `from_logits=False`).\n\nArgs:\n    from_logits: Whether to interpret `y_pred` as a tensor of\n        [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n        assume that `y_pred` is probabilities (i.e., values in [0, 1]).\n    label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.\n        When > 0, we compute the loss between the predicted labels\n        and a smoothed version of the true labels, where the smoothing\n        squeezes the labels towards 0.5. Larger values of\n        `label_smoothing` correspond to heavier smoothing.\n    axis: The axis along which to compute crossentropy (the features axis).\n        Defaults to `-1`.\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`.\n        Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).\n\nExamples:\n\n**Recommended Usage:** (set `from_logits=True`)\n\nWith `compile()` API:\n\n```python\nmodel.compile(\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    ...\n)\n```\n\nAs a standalone function:\n\n>>> # Example 1: (batch_size = 1, number of samples = 4)\n>>> y_true = [0, 1, 0, 0]\n>>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n>>> bce = keras.losses.BinaryCrossentropy(from_logits=True)\n>>> bce(y_true, y_pred)\n0.865\n\n>>> # Example 2: (batch_size = 2, number of samples = 4)\n>>> y_true = [[0, 1], [0, 0]]\n>>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n>>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n>>> bce = keras.losses.BinaryCrossentropy(from_logits=True)\n>>> bce(y_true, y_pred)\n0.865\n>>> # Using 'sample_weight' attribute\n>>> bce(y_true, y_pred, sample_weight=[0.8, 0.2])\n0.243\n>>> # Using 'sum' reduction` type.\n>>> bce = keras.losses.BinaryCrossentropy(from_logits=True,\n...     reduction=\"sum\")\n>>> bce(y_true, y_pred)\n1.730\n>>> # Using 'none' reduction type.\n>>> bce = keras.losses.BinaryCrossentropy(from_logits=True,\n...     reduction=None)\n>>> bce(y_true, y_pred)\narray([0.235, 1.496], dtype=float32)\n\n**Default Usage:** (set `from_logits=False`)\n\n>>> # Make the following updates to the above \"Recommended Usage\" section\n>>> # 1. Set `from_logits=False`\n>>> keras.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n>>> # 2. Update `y_pred` to use probabilities instead of logits\n>>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.cosine_similarity", "Docstring": "Computes the cosine similarity between labels and predictions.\n\nFormula:\n```python\nloss = -sum(l2_norm(y_true) * l2_norm(y_pred))\n```\n\nNote that it is a number between -1 and 1. When it is a negative number\nbetween -1 and 0, 0 indicates orthogonality and values closer to -1\nindicate greater similarity. This makes it usable as a loss function in a\nsetting where you try to maximize the proximity between predictions and\ntargets. If either `y_true` or `y_pred` is a zero vector, cosine\nsimilarity will be 0 regardless of the proximity between predictions\nand targets.\n\nArgs:\n    y_true: Tensor of true targets.\n    y_pred: Tensor of predicted targets.\n    axis: Axis along which to determine similarity. Defaults to `-1`.\n\nReturns:\n    Cosine similarity tensor.\n\nExample:\n\n>>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n>>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n>>> loss = keras.losses.cosine_similarity(y_true, y_pred, axis=-1)\n[-0., -0.99999994, 0.99999994]", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.Hinge", "Docstring": "Computes the hinge loss between `y_true` & `y_pred`.\n\nFormula:\n\n```python\nloss = maximum(1 - y_true * y_pred, 0)\n```\n\n`y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\nprovided we will convert them to -1 or 1.\n\nArgs:\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`.\n        Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.Huber", "Docstring": "Computes the Huber loss between `y_true` & `y_pred`.\n\nFormula:\n\n```python\nfor x in error:\n    if abs(x) <= delta:\n        loss.append(0.5 * x^2)\n    elif abs(x) > delta:\n        loss.append(delta * abs(x) - 0.5 * delta^2)\n\nloss = mean(loss, axis=-1)\n```\nSee: [Huber loss](https://en.wikipedia.org/wiki/Huber_loss).\n\nArgs:\n    delta: A float, the point where the Huber loss function changes from a\n        quadratic to linear.\n    reduction: Type of reduction to apply to loss. Options are `\"sum\"`,\n        `\"sum_over_batch_size\"` or `None`. Defaults to\n        `\"sum_over_batch_size\"`.\n    name: Optional name for the instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.KLDivergence", "Docstring": "Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n\nFormula:\n\n```python\nloss = y_true * log(y_true / y_pred)\n```\n\n`y_true` and `y_pred` are expected to be probability\ndistributions, with values between 0 and 1. They will get\nclipped to the `[0, 1]` range.\n\nArgs:\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`.\n        Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.Loss", "Docstring": "Loss base class.\n\nTo be implemented by subclasses:\n\n* `call()`: Contains the logic for loss calculation using `y_true`,\n    `y_pred`.\n\nExample subclass implementation:\n\n```python\nclass MeanSquaredError(Loss):\n    def call(self, y_true, y_pred):\n        return ops.mean(ops.square(y_pred - y_true), axis=-1)\n```", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.MeanAbsoluteError", "Docstring": "Computes the mean of absolute difference between labels and predictions.\n\nFormula:\n\n```python\nloss = mean(abs(y_true - y_pred))\n```\n\nArgs:\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`.\n        Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.MeanSquaredError", "Docstring": "Computes the mean of squares of errors between labels and predictions.\n\nFormula:\n\n```python\nloss = mean(square(y_true - y_pred))\n```\n\nArgs:\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`.\n        Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.Poisson", "Docstring": "Computes the Poisson loss between `y_true` & `y_pred`.\n\nFormula:\n\n```python\nloss = y_pred - y_true * log(y_pred)\n```\n\nArgs:\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`.\n        Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE", "Docstring": "str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object from the given object. If encoding or\nerrors is specified, then the object must expose a data buffer\nthat will be decoded using the given encoding and error handler.\nOtherwise, returns the result of object.__str__() (if defined)\nor repr(object).\nencoding defaults to sys.getdefaultencoding().\nerrors defaults to 'strict'.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.losses.SparseCategoricalCrossentropy", "Docstring": "Computes the crossentropy loss between the labels and predictions.\n\nUse this crossentropy loss function when there are two or more label\nclasses.  We expect labels to be provided as integers. If you want to\nprovide labels using `one-hot` representation, please use\n`CategoricalCrossentropy` loss.  There should be `# classes` floating point\nvalues per feature for `y_pred` and a single floating point value per\nfeature for `y_true`.\n\nIn the snippet below, there is a single floating point value per example for\n`y_true` and `num_classes` floating pointing values per example for\n`y_pred`. The shape of `y_true` is `[batch_size]` and the shape of `y_pred`\nis `[batch_size, num_classes]`.\n\nArgs:\n    from_logits: Whether `y_pred` is expected to be a logits tensor. By\n        default, we assume that `y_pred` encodes a probability distribution.\n    reduction: Type of reduction to apply to the loss. In almost all cases\n        this should be `\"sum_over_batch_size\"`.\n        Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n    name: Optional name for the loss instance.\n    dtype: The dtype of the loss's computations. Defaults to `None`, which\n        means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n        `\"float32\"` unless set to different value\n        (via `keras.backend.set_floatx()`).\n\nExamples:\n\n>>> y_true = [1, 2]\n>>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n>>> # Using 'auto'/'sum_over_batch_size' reduction type.\n>>> scce = keras.losses.SparseCategoricalCrossentropy()\n>>> scce(y_true, y_pred)\n1.177\n\n>>> # Calling with 'sample_weight'.\n>>> scce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))\n0.814\n\n>>> # Using 'sum' reduction type.\n>>> scce = keras.losses.SparseCategoricalCrossentropy(\n...     reduction=\"sum\")\n>>> scce(y_true, y_pred)\n2.354\n\n>>> # Using 'none' reduction type.\n>>> scce = keras.losses.SparseCategoricalCrossentropy(\n...     reduction=None)\n>>> scce(y_true, y_pred)\narray([0.0513, 2.303], dtype=float32)\n\nUsage with the `compile()` API:\n\n```python\nmodel.compile(optimizer='sgd',\n              loss=keras.losses.SparseCategoricalCrossentropy())\n```", "Library": "TensorFlow"}
{"API_Name": "tf.keras.Model.compile", "Docstring": "Configures the model for training.\n\nExample:\n\n```python\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss=keras.losses.BinaryCrossentropy(),\n    metrics=[\n        keras.metrics.BinaryAccuracy(),\n        keras.metrics.FalseNegatives(),\n    ],\n)\n```\n\nArgs:\n    optimizer: String (name of optimizer) or optimizer instance. See\n        `keras.optimizers`.\n    loss: Loss function. May be a string (name of loss function), or\n        a `keras.losses.Loss` instance. See `keras.losses`. A\n        loss function is any callable with the signature\n        `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth\n        values, and `y_pred` are the model's predictions.\n        `y_true` should have shape `(batch_size, d0, .. dN)`\n        (except in the case of sparse loss functions such as\n        sparse categorical crossentropy which expects integer arrays of\n        shape `(batch_size, d0, .. dN-1)`).\n        `y_pred` should have shape `(batch_size, d0, .. dN)`.\n        The loss function should return a float tensor.\n    loss_weights: Optional list or dictionary specifying scalar\n        coefficients (Python floats) to weight the loss contributions of\n        different model outputs. The loss value that will be minimized\n        by the model will then be the *weighted sum* of all individual\n        losses, weighted by the `loss_weights` coefficients.  If a list,\n        it is expected to have a 1:1 mapping to the model's outputs. If\n        a dict, it is expected to map output names (strings) to scalar\n        coefficients.\n    metrics: List of metrics to be evaluated by the model during\n        training and testing. Each of this can be a string (name of a\n        built-in function), function or a `keras.metrics.Metric`\n        instance. See `keras.metrics`. Typically you will use\n        `metrics=['accuracy']`. A function is any callable with the\n        signature `result = fn(y_true, _pred)`. To specify different\n        metrics for different outputs of a multi-output model, you could\n        also pass a dictionary, such as\n        `metrics={'a':'accuracy', 'b':['accuracy', 'mse']}`.\n        You can also pass a list to specify a metric or a list of\n        metrics for each output, such as\n        `metrics=[['accuracy'], ['accuracy', 'mse']]`\n        or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass\n        the strings 'accuracy' or 'acc', we convert this to one of\n        `keras.metrics.BinaryAccuracy`,\n        `keras.metrics.CategoricalAccuracy`,\n        `keras.metrics.SparseCategoricalAccuracy` based on the\n        shapes of the targets and of the model output. A similar\n        conversion is done for the strings `\"crossentropy\"`\n        and `\"ce\"` as well.\n        The metrics passed here are evaluated without sample weighting;\n        if you would like sample weighting to apply, you can specify\n        your metrics via the `weighted_metrics` argument instead.\n    weighted_metrics: List of metrics to be evaluated and weighted by\n        `sample_weight` or `class_weight` during training and testing.\n    run_eagerly: Bool. If `True`, this model's forward pass\n         will never be compiled. It is recommended to leave this\n         as `False` when training (for best performance),\n         and to set it to `True` when debugging.\n    steps_per_execution: Int. The number of batches to run\n        during each a single compiled function call. Running multiple\n        batches inside a single compiled function call can\n        greatly improve performance on TPUs or small models with a large\n        Python overhead. At most, one full epoch will be run each\n        execution. If a number larger than the size of the epoch is\n        passed, the execution will be truncated to the size of the\n        epoch. Note that if `steps_per_execution` is set to `N`,\n        `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n        will only be called every `N` batches (i.e. before/after\n        each compiled function execution).\n        Not supported with the PyTorch backend.\n    jit_compile: Bool or `\"auto\"`. Whether to use XLA compilation when\n        compiling a model. For `jax` and `tensorflow` backends,\n        `jit_compile=\"auto\"` enables XLA compilation if the model\n        supports it, and disabled otherwise.\n        For `torch` backend, `\"auto\"` will default to eager\n        execution and `jit_compile=True` will run with `torch.compile`\n        with the `\"inductor\"` backend.\n    auto_scale_loss: Bool. If `True` and the model dtype policy is\n        `\"mixed_float16\"`, the passed optimizer will be automatically\n        wrapped in a `LossScaleOptimizer`, which will dynamically\n        scale the loss to prevent underflow.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.Model.count_params", "Docstring": "Count the total number of scalars composing the weights.\n\nReturns:\n    An integer count.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.Model.fit", "Docstring": "Trains the model for a fixed number of epochs (dataset iterations).\n\nArgs:\n    x: Input data. It could be:\n        - A NumPy array (or array-like), or a list of arrays\n        (in case the model has multiple inputs).\n        - A tensor, or a list of tensors\n        (in case the model has multiple inputs).\n        - A dict mapping input names to the corresponding array/tensors,\n        if the model has named inputs.\n        - A `tf.data.Dataset`. Should return a tuple\n        of either `(inputs, targets)` or\n        `(inputs, targets, sample_weights)`.\n        - A `keras.utils.PyDataset` returning `(inputs,\n        targets)` or `(inputs, targets, sample_weights)`.\n    y: Target data. Like the input data `x`,\n        it could be either NumPy array(s) or backend-native tensor(s).\n        If `x` is a dataset, generator,\n        or `keras.utils.PyDataset` instance, `y` should\n        not be specified (since targets will be obtained from `x`).\n    batch_size: Integer or `None`.\n        Number of samples per gradient update.\n        If unspecified, `batch_size` will default to 32.\n        Do not specify the `batch_size` if your data is in the\n        form of datasets, generators, or `keras.utils.PyDataset`\n        instances (since they generate batches).\n    epochs: Integer. Number of epochs to train the model.\n        An epoch is an iteration over the entire `x` and `y`\n        data provided\n        (unless the `steps_per_epoch` flag is set to\n        something other than None).\n        Note that in conjunction with `initial_epoch`,\n        `epochs` is to be understood as \"final epoch\".\n        The model is not trained for a number of iterations\n        given by `epochs`, but merely until the epoch\n        of index `epochs` is reached.\n    verbose: `\"auto\"`, 0, 1, or 2. Verbosity mode.\n        0 = silent, 1 = progress bar, 2 = one line per epoch.\n        \"auto\" becomes 1 for most cases.\n        Note that the progress bar is not\n        particularly useful when logged to a file,\n        so `verbose=2` is recommended when not running interactively\n        (e.g., in a production environment). Defaults to `\"auto\"`.\n    callbacks: List of `keras.callbacks.Callback` instances.\n        List of callbacks to apply during training.\n        See `keras.callbacks`. Note\n        `keras.callbacks.ProgbarLogger` and\n        `keras.callbacks.History` callbacks are created\n        automatically and need not be passed to `model.fit()`.\n        `keras.callbacks.ProgbarLogger` is created\n        or not based on the `verbose` argument in `model.fit()`.\n    validation_split: Float between 0 and 1.\n        Fraction of the training data to be used as validation data.\n        The model will set apart this fraction of the training data,\n        will not train on it, and will evaluate\n        the loss and any model metrics\n        on this data at the end of each epoch.\n        The validation data is selected from the last samples\n        in the `x` and `y` data provided, before shuffling. This\n        argument is not supported when `x` is a dataset, generator or\n        `keras.utils.PyDataset` instance.\n        If both `validation_data` and `validation_split` are provided,\n        `validation_data` will override `validation_split`.\n    validation_data: Data on which to evaluate\n        the loss and any model metrics at the end of each epoch.\n        The model will not be trained on this data. Thus, note the fact\n        that the validation loss of data provided using\n        `validation_split` or `validation_data` is not affected by\n        regularization layers like noise and dropout.\n        `validation_data` will override `validation_split`.\n        It could be:\n        - A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n        - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n        arrays.\n        - A `tf.data.Dataset`.\n        - A Python generator or `keras.utils.PyDataset` returning\n        `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n    shuffle: Boolean, whether to shuffle the training data\n        before each epoch. This argument is\n        ignored when `x` is a generator or a `tf.data.Dataset`.\n    class_weight: Optional dictionary mapping class indices (integers)\n        to a weight (float) value, used for weighting the loss function\n        (during training only).\n        This can be useful to tell the model to\n        \"pay more attention\" to samples from\n        an under-represented class. When `class_weight` is specified\n        and targets have a rank of 2 or greater, either `y` must be\n        one-hot encoded, or an explicit final dimension of `1` must\n        be included for sparse class labels.\n    sample_weight: Optional NumPy array of weights for\n        the training samples, used for weighting the loss function\n        (during training only). You can either pass a flat (1D)\n        NumPy array with the same length as the input samples\n        (1:1 mapping between weights and samples),\n        or in the case of temporal data,\n        you can pass a 2D array with shape\n        `(samples, sequence_length)`,\n        to apply a different weight to every timestep of every sample.\n        This argument is not supported when `x` is a dataset, generator,\n        or `keras.utils.PyDataset` instance, instead provide the\n        sample_weights as the third element of `x`.\n        Note that sample weighting does not apply to metrics specified\n        via the `metrics` argument in `compile()`. To apply sample\n        weighting to your metrics, you can specify them via the\n        `weighted_metrics` in `compile()` instead.\n    initial_epoch: Integer.\n        Epoch at which to start training\n        (useful for resuming a previous training run).\n    steps_per_epoch: Integer or `None`.\n        Total number of steps (batches of samples)\n        before declaring one epoch finished and starting the\n        next epoch. When training with input tensors such as\n        backend-native tensors, the default `None` is equal to\n        the number of samples in your dataset divided by\n        the batch size, or 1 if that cannot be determined. If `x` is a\n        `tf.data.Dataset`, and `steps_per_epoch`\n        is `None`, the epoch will run until the input dataset is\n        exhausted.  When passing an infinitely repeating dataset, you\n        must specify the `steps_per_epoch` argument. If\n        `steps_per_epoch=-1` the training will run indefinitely with an\n        infinitely repeating dataset.\n    validation_steps: Only relevant if `validation_data` is provided.\n        Total number of steps (batches of\n        samples) to draw before stopping when performing validation\n        at the end of every epoch. If `validation_steps` is `None`,\n        validation will run until the `validation_data` dataset is\n        exhausted. In the case of an infinitely repeated dataset, it\n        will run into an infinite loop. If `validation_steps` is\n        specified and only part of the dataset will be consumed, the\n        evaluation will start from the beginning of the dataset at each\n        epoch. This ensures that the same validation samples are used\n        every time.\n    validation_batch_size: Integer or `None`.\n        Number of samples per validation batch.\n        If unspecified, will default to `batch_size`.\n        Do not specify the `validation_batch_size` if your data is in\n        the form of datasets or `keras.utils.PyDataset`\n        instances (since they generate batches).\n    validation_freq: Only relevant if validation data is provided.\n        Specifies how many training epochs to run\n        before a new validation run is performed,\n        e.g. `validation_freq=2` runs validation every 2 epochs.\n\nUnpacking behavior for iterator-like inputs:\n    A common pattern is to pass an iterator like object such as a\n    `tf.data.Dataset` or a `keras.utils.PyDataset` to `fit()`,\n    which will in fact yield not only features (`x`)\n    but optionally targets (`y`) and sample weights (`sample_weight`).\n    Keras requires that the output of such iterator-likes be\n    unambiguous. The iterator should return a tuple\n    of length 1, 2, or 3, where the optional second and third elements\n    will be used for `y` and `sample_weight` respectively.\n    Any other type provided will be wrapped in\n    a length-one tuple, effectively treating everything as `x`. When\n    yielding dicts, they should still adhere to the top-level tuple\n    structure,\n    e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n    features, targets, and weights from the keys of a single dict.\n    A notable unsupported data type is the `namedtuple`. The reason is\n    that it behaves like both an ordered datatype (tuple) and a mapping\n    datatype (dict). So given a namedtuple of the form:\n    `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n    it is ambiguous whether to reverse the order of the elements when\n    interpreting the value. Even worse is a tuple of the form:\n    `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n    where it is unclear if the tuple was intended to be unpacked\n    into `x`, `y`, and `sample_weight` or passed through\n    as a single element to `x`.\n\nReturns:\n    A `History` object. Its `History.history` attribute is\n    a record of training loss values and metrics values\n    at successive epochs, as well as validation loss values\n    and validation metrics values (if applicable).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.Model.save", "Docstring": "Saves a model as a `.keras` file.\n\nArgs:\n    filepath: `str` or `pathlib.Path` object.\n        The path where to save the model. Must end in `.keras`\n        (unless saving the model as an unzipped directory\n        via `zipped=False`).\n    overwrite: Whether we should overwrite any existing model at\n        the target location, or instead ask the user via\n        an interactive prompt.\n    zipped: Whether to save the model as a zipped `.keras`\n        archive (default), or as an unzipped directory.\n\nExample:\n\n```python\nmodel = keras.Sequential(\n    [\n        keras.layers.Dense(5, input_shape=(3,)),\n        keras.layers.Softmax(),\n    ],\n)\nmodel.save(\"model.keras\")\nloaded_model = keras.saving.load_model(\"model.keras\")\nx = keras.random.uniform((10, 3))\nassert np.allclose(model.predict(x), loaded_model.predict(x))\n```\n\nNote that `model.save()` is an alias for `keras.saving.save_model()`.\n\nThe saved `.keras` file contains:\n\n- The model's configuration (architecture)\n- The model's weights\n- The model's optimizer's state (if any)\n\nThus models can be reinstantiated in the exact same state.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.Model.train_on_batch", "Docstring": "Runs a single gradient update on a single batch of data.\n\nArgs:\n    x: Input data. Must be array-like.\n    y: Target data. Must be array-like.\n    sample_weight: Optional array of the same length as x, containing\n        weights to apply to the model's loss for each sample.\n        In the case of temporal data, you can pass a 2D array\n        with shape `(samples, sequence_length)`, to apply a different\n        weight to every timestep of every sample.\n    class_weight: Optional dictionary mapping class indices (integers)\n        to a weight (float) to apply to the model's loss for the samples\n        from this class during training. This can be useful to tell the\n        model to \"pay more attention\" to samples from an\n        under-represented class. When `class_weight` is specified\n        and targets have a rank of 2 or greater, either `y` must\n        be one-hot encoded, or an explicit final dimension of 1\n        must be included for sparse class labels.\n    return_dict: If `True`, loss and metric results are returned as a\n        dict, with each key being the name of the metric. If `False`,\n        they are returned as a list.\n\nReturns:\n    A scalar loss value (when no metrics and `return_dict=False`),\n    a list of loss and metric values\n    (if there are metrics and `return_dict=False`), or a dict of\n    metric and loss values (if `return_dict=True`).", "Library": "TensorFlow"}
{"API_Name": "tf.keras.models.load_model", "Docstring": "Loads a model saved via `model.save()`.\n\nArgs:\n    filepath: `str` or `pathlib.Path` object, path to the saved model file.\n    custom_objects: Optional dictionary mapping names\n        (strings) to custom classes or functions to be\n        considered during deserialization.\n    compile: Boolean, whether to compile the model after loading.\n    safe_mode: Boolean, whether to disallow unsafe `lambda` deserialization.\n        When `safe_mode=False`, loading an object has the potential to\n        trigger arbitrary code execution. This argument is only\n        applicable to the Keras v3 model format. Defaults to `True`.\n\nReturns:\n    A Keras model instance. If the original model was compiled,\n    and the argument `compile=True` is set, then the returned model\n    will be compiled. Otherwise, the model will be left uncompiled.\n\nExample:\n\n```python\nmodel = keras.Sequential([\n    keras.layers.Dense(5, input_shape=(3,)),\n    keras.layers.Softmax()])\nmodel.save(\"model.keras\")\nloaded_model = keras.saving.load_model(\"model.keras\")\nx = np.random.random((10, 3))\nassert np.allclose(model.predict(x), loaded_model.predict(x))\n```\n\nNote that the model variables may have different name values\n(`var.name` property, e.g. `\"dense_1/kernel:0\"`) after being reloaded.\nIt is recommended that you use layer attributes to\naccess specific variables, e.g. `model.get_layer(\"dense_1\").kernel`.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.optimizers.Adam", "Docstring": "Optimizer that implements the Adam algorithm.\n\nAdam optimization is a stochastic gradient descent method that is based on\nadaptive estimation of first-order and second-order moments.\n\nAccording to\n[Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\nthe method is \"*computationally\nefficient, has little memory requirement, invariant to diagonal rescaling of\ngradients, and is well suited for problems that are large in terms of\ndata/parameters*\".\n\nArgs:\n    learning_rate: A float, a\n        `keras.optimizers.schedules.LearningRateSchedule` instance, or\n        a callable that takes no arguments and returns the actual value to\n        use. The learning rate. Defaults to `0.001`.\n    beta_1: A float value or a constant float tensor, or a callable\n        that takes no arguments and returns the actual value to use. The\n        exponential decay rate for the 1st moment estimates. Defaults to\n        `0.9`.\n    beta_2: A float value or a constant float tensor, or a callable\n        that takes no arguments and returns the actual value to use. The\n        exponential decay rate for the 2nd moment estimates. Defaults to\n        `0.999`.\n    epsilon: A small constant for numerical stability. This epsilon is\n        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults\n        to `1e-7`.\n    amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm\n        from the paper \"On the Convergence of Adam and beyond\". Defaults\n        to `False`.\n    name: String. The name to use\n        for momentum accumulator weights created by\n        the optimizer.\n    weight_decay: Float. If set, weight decay is applied.\n    clipnorm: Float. If set, the gradient of each weight is individually\n        clipped so that its norm is no higher than this value.\n    clipvalue: Float. If set, the gradient of each weight is clipped to be\n        no higher than this value.\n    global_clipnorm: Float. If set, the gradient of all weights is clipped\n        so that their global norm is no higher than this value.\n    use_ema: Boolean, defaults to `False`.\n        If `True`, exponential moving average\n        (EMA) is applied. EMA consists of computing an exponential moving\n        average of the weights of the model (as the weight values change\n        after each training batch), and periodically overwriting the\n        weights with their moving average.\n    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n        This is the momentum to use when computing\n        the EMA of the model's weights:\n        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n        current_variable_value`.\n    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n        we overwrite the model variable by its moving average.\n        If None, the optimizer\n        does not overwrite model variables in the middle of training,\n        and you need to explicitly overwrite the variables\n        at the end of training by calling\n        `optimizer.finalize_variable_values()` (which updates the model\n        variables in-place). When using the built-in `fit()` training loop,\n        this happens automatically after the last epoch,\n        and you don't need to do anything.\n    loss_scale_factor: Float or `None`. If a float, the scale factor will\n        be multiplied the loss before computing gradients, and the inverse\n        of the scale factor will be multiplied by the gradients before\n        updating variables. Useful for preventing underflow during\n        mixed precision training. Alternately,\n        `keras.optimizers.LossScaleOptimizer` will\n        automatically set a loss scale factor.\n    gradient_accumulation_steps: Int or `None`. If an int, model & optimizer\n        variables will not be updated at every step; instead they will be\n        updated every `gradient_accumulation_steps` steps, using the average\n        value of the gradients since the last update. This is known as\n        \"gradient accumulation\". This can be useful\n        when your batch size is very small, in order to reduce gradient\n        noise at each update step.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.optimizers.SGD", "Docstring": "Gradient descent (with momentum) optimizer.\n\nUpdate rule for parameter `w` with gradient `g` when `momentum` is 0:\n\n```python\nw = w - learning_rate * g\n```\n\nUpdate rule when `momentum` is larger than 0:\n\n```python\nvelocity = momentum * velocity - learning_rate * g\nw = w + velocity\n```\n\nWhen `nesterov=True`, this rule becomes:\n\n```python\nvelocity = momentum * velocity - learning_rate * g\nw = w + momentum * velocity - learning_rate * g\n```\n\nArgs:\n    learning_rate: A float, a\n        `keras.optimizers.schedules.LearningRateSchedule` instance, or\n        a callable that takes no arguments and returns the actual value to\n        use. The learning rate. Defaults to `0.01`.\n    momentum: float hyperparameter >= 0 that accelerates gradient descent in\n        the relevant direction and dampens oscillations. 0 is vanilla\n        gradient descent. Defaults to `0.0`.\n    nesterov: boolean. Whether to apply Nesterov momentum.\n        Defaults to `False`.\n    name: String. The name to use\n        for momentum accumulator weights created by\n        the optimizer.\n    weight_decay: Float. If set, weight decay is applied.\n    clipnorm: Float. If set, the gradient of each weight is individually\n        clipped so that its norm is no higher than this value.\n    clipvalue: Float. If set, the gradient of each weight is clipped to be\n        no higher than this value.\n    global_clipnorm: Float. If set, the gradient of all weights is clipped\n        so that their global norm is no higher than this value.\n    use_ema: Boolean, defaults to `False`.\n        If `True`, exponential moving average\n        (EMA) is applied. EMA consists of computing an exponential moving\n        average of the weights of the model (as the weight values change\n        after each training batch), and periodically overwriting the\n        weights with their moving average.\n    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n        This is the momentum to use when computing\n        the EMA of the model's weights:\n        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n        current_variable_value`.\n    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n        we overwrite the model variable by its moving average.\n        If None, the optimizer\n        does not overwrite model variables in the middle of training,\n        and you need to explicitly overwrite the variables\n        at the end of training by calling\n        `optimizer.finalize_variable_values()` (which updates the model\n        variables in-place). When using the built-in `fit()` training loop,\n        this happens automatically after the last epoch,\n        and you don't need to do anything.\n    loss_scale_factor: Float or `None`. If a float, the scale factor will\n        be multiplied the loss before computing gradients, and the inverse\n        of the scale factor will be multiplied by the gradients before\n        updating variables. Useful for preventing underflow during\n        mixed precision training. Alternately,\n        `keras.optimizers.LossScaleOptimizer` will\n        automatically set a loss scale factor.\n    gradient_accumulation_steps: Int or `None`. If an int, model & optimizer\n        variables will not be updated at every step; instead they will be\n        updated every `gradient_accumulation_steps` steps, using the average\n        value of the gradients since the last update. This is known as\n        \"gradient accumulation\". This can be useful\n        when your batch size is very small, in order to reduce gradient\n        noise at each update step.", "Library": "TensorFlow"}
{"API_Name": "tf.keras.preprocessing.sequence.pad_sequences", "Docstring": "Pads sequences to the same length.\n\nThis function transforms a list (of length `num_samples`)\nof sequences (lists of integers)\ninto a 2D NumPy array of shape `(num_samples, num_timesteps)`.\n`num_timesteps` is either the `maxlen` argument if provided,\nor the length of the longest sequence in the list.\n\nSequences that are shorter than `num_timesteps`\nare padded with `value` until they are `num_timesteps` long.\n\nSequences longer than `num_timesteps` are truncated\nso that they fit the desired length.\n\nThe position where padding or truncation happens is determined by\nthe arguments `padding` and `truncating`, respectively.\nPre-padding or removing values from the beginning of the sequence is the\ndefault.\n\n>>> sequence = [[1], [2, 3], [4, 5, 6]]\n>>> keras.utils.pad_sequences(sequence)\narray([[0, 0, 1],\n       [0, 2, 3],\n       [4, 5, 6]], dtype=int32)\n\n>>> keras.utils.pad_sequences(sequence, value=-1)\narray([[-1, -1,  1],\n       [-1,  2,  3],\n       [ 4,  5,  6]], dtype=int32)\n\n>>> keras.utils.pad_sequences(sequence, padding='post')\narray([[1, 0, 0],\n       [2, 3, 0],\n       [4, 5, 6]], dtype=int32)\n\n>>> keras.utils.pad_sequences(sequence, maxlen=2)\narray([[0, 1],\n       [2, 3],\n       [5, 6]], dtype=int32)\n\nArgs:\n    sequences: List of sequences (each sequence is a list of integers).\n    maxlen: Optional Int, maximum length of all sequences. If not provided,\n        sequences will be padded to the length of the longest individual\n        sequence.\n    dtype: (Optional, defaults to `\"int32\"`). Type of the output sequences.\n        To pad sequences with variable length strings, you can use `object`.\n    padding: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n        pad either before or after each sequence.\n    truncating: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n        remove values from sequences larger than\n        `maxlen`, either at the beginning or at the end of the sequences.\n    value: Float or String, padding value. (Optional, defaults to 0.)\n\nReturns:\n    NumPy array with shape `(len(sequences), maxlen)`", "Library": "TensorFlow"}
{"API_Name": "tf.keras.utils.to_categorical", "Docstring": "Converts a class vector (integers) to binary class matrix.\n\nE.g. for use with `categorical_crossentropy`.\n\nArgs:\n    x: Array-like with class values to be converted into a matrix\n        (integers from 0 to `num_classes - 1`).\n    num_classes: Total number of classes. If `None`, this would be inferred\n        as `max(x) + 1`. Defaults to `None`.\n\nReturns:\n    A binary matrix representation of the input as a NumPy array. The class\n    axis is placed last.\n\nExample:\n\n>>> a = keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n>>> print(a)\n[[1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\n\n>>> b = np.array([.9, .04, .03, .03,\n...               .3, .45, .15, .13,\n...               .04, .01, .94, .05,\n...               .12, .21, .5, .17],\n...               shape=[4, 4])\n>>> loss = keras.ops.categorical_crossentropy(a, b)\n>>> print(np.around(loss, 5))\n[0.10536 0.82807 0.1011  1.77196]\n\n>>> loss = keras.ops.categorical_crossentropy(a, a)\n>>> print(np.around(loss, 5))\n[0. 0. 0. 0.]", "Library": "TensorFlow"}
{"API_Name": "tf.linalg.band_part", "Docstring": "Copy a tensor setting everything outside a central band in each innermost matrix to zero.\n\nThe `band` part is computed as follows:\nAssume `input` has `k` dimensions `[I, J, K, ..., M, N]`, then the output is a\ntensor with the same shape where\n\n`band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.\n\nThe indicator function\n\n`in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower)) &&\n                 (num_upper < 0 || (n-m) <= num_upper)`.\n\nFor example:\n\n```\n# if 'input' is [[ 0,  1,  2, 3]\n#                [-1,  0,  1, 2]\n#                [-2, -1,  0, 1]\n#                [-3, -2, -1, 0]],\n\ntf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]\n                                       [-1,  0,  1, 2]\n                                       [ 0, -1,  0, 1]\n                                       [ 0,  0, -1, 0]],\n\ntf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]\n                                      [-1,  0,  1, 0]\n                                      [-2, -1,  0, 1]\n                                      [ 0, -2, -1, 0]]\n```\n\nUseful special cases:\n\n```\n tf.linalg.band_part(input, 0, -1) ==> Upper triangular part.\n tf.linalg.band_part(input, -1, 0) ==> Lower triangular part.\n tf.linalg.band_part(input, 0, 0) ==> Diagonal.\n```\n\nArgs:\n  input: A `Tensor`. Rank `k` tensor.\n  num_lower: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    0-D tensor. Number of subdiagonals to keep. If negative, keep entire\n    lower triangle.\n  num_upper: A `Tensor`. Must have the same type as `num_lower`.\n    0-D tensor. Number of superdiagonals to keep. If negative, keep\n    entire upper triangle.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `input`.", "Library": "TensorFlow"}
{"API_Name": "tf.linalg.diag", "Docstring": "Returns a batched diagonal tensor with given batched diagonal values.\n\nReturns a tensor with the contents in `diagonal` as `k[0]`-th to `k[1]`-th\ndiagonals of a matrix, with everything else padded with `padding`. `num_rows`\nand `num_cols` specify the dimension of the innermost matrix of the output. If\nboth are not specified, the op assumes the innermost matrix is square and\ninfers its size from `k` and the innermost dimension of `diagonal`. If only\none of them is specified, the op assumes the unspecified value is the smallest\npossible based on other criteria.\n\nLet `diagonal` have `r` dimensions `[I, J, ..., L, M, N]`. The output tensor\nhas rank `r+1` with shape `[I, J, ..., L, M, num_rows, num_cols]` when only\none diagonal is given (`k` is an integer or `k[0] == k[1]`). Otherwise, it has\nrank `r` with shape `[I, J, ..., L, num_rows, num_cols]`.\n\nThe second innermost dimension of `diagonal` has double meaning. When `k` is\nscalar or `k[0] == k[1]`, `M` is part of the batch size [I, J, ..., M], and\nthe output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper\n    padding_value                             ; otherwise\n```\n\nOtherwise, `M` is treated as the number of diagonals for the matrix in the\nsame batch (`M = k[1]-k[0]+1`), and the output tensor is:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    padding_value                                     ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and\n`index_in_diag = n - max(d, 0) + offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nFor example:\n\n```\n# The main diagonal.\ndiagonal = np.array([[1, 2, 3, 4],            # Input shape: (2, 4)\n                     [5, 6, 7, 8]])\ntf.linalg.diag(diagonal) ==> [[[1, 0, 0, 0],  # Output shape: (2, 4, 4)\n                               [0, 2, 0, 0],\n                               [0, 0, 3, 0],\n                               [0, 0, 0, 4]],\n                              [[5, 0, 0, 0],\n                               [0, 6, 0, 0],\n                               [0, 0, 7, 0],\n                               [0, 0, 0, 8]]]\n\n# A superdiagonal (per batch).\ndiagonal = np.array([[1, 2, 3],  # Input shape: (2, 3)\n                     [4, 5, 6]])\ntf.linalg.diag(diagonal, k = 1)\n  ==> [[[0, 1, 0, 0],  # Output shape: (2, 4, 4)\n        [0, 0, 2, 0],\n        [0, 0, 0, 3],\n        [0, 0, 0, 0]],\n       [[0, 4, 0, 0],\n        [0, 0, 5, 0],\n        [0, 0, 0, 6],\n        [0, 0, 0, 0]]]\n\n# A tridiagonal band (per batch).\ndiagonals = np.array([[[8, 9, 0],  # Input shape: (2, 2, 3)\n                       [1, 2, 3],\n                       [0, 4, 5]],\n                      [[2, 3, 0],\n                       [6, 7, 9],\n                       [0, 9, 1]]])\ntf.linalg.diag(diagonals, k = (-1, 1))\n  ==> [[[1, 8, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 9],\n        [0, 5, 3]],\n       [[6, 2, 0],\n        [9, 7, 3],\n        [0, 1, 9]]]\n\n# RIGHT_LEFT alignment.\ndiagonals = np.array([[[0, 8, 9],  # Input shape: (2, 2, 3)\n                       [1, 2, 3],\n                       [4, 5, 0]],\n                      [[0, 2, 3],\n                       [6, 7, 9],\n                       [9, 1, 0]]])\ntf.linalg.diag(diagonals, k = (-1, 1), align=\"RIGHT_LEFT\")\n  ==> [[[1, 8, 0],  # Output shape: (2, 3, 3)\n        [4, 2, 9],\n        [0, 5, 3]],\n       [[6, 2, 0],\n        [9, 7, 3],\n        [0, 1, 9]]]\n\n# Rectangular matrix.\ndiagonal = np.array([1, 2])  # Input shape: (2)\ntf.linalg.diag(diagonal, k = -1, num_rows = 3, num_cols = 4)\n  ==> [[0, 0, 0, 0],  # Output shape: (3, 4)\n       [1, 0, 0, 0],\n       [0, 2, 0, 0]]\n\n# Rectangular matrix with inferred num_cols and padding_value = 9.\ntf.linalg.diag(diagonal, k = -1, num_rows = 3, padding_value = 9)\n  ==> [[9, 9],  # Output shape: (3, 2)\n       [1, 9],\n       [9, 2]]\n```\n\nArgs:\n  diagonal: A `Tensor` with `rank k >= 1`.\n  name: A name for the operation (optional).\n  k: Diagonal offset(s). Positive value means superdiagonal, 0 refers to the\n    main diagonal, and negative value means subdiagonals. `k` can be a single\n    integer (for a single diagonal) or a pair of integers specifying the low\n    and high ends of a matrix band. `k[0]` must not be larger than `k[1]`.\n  num_rows: The number of rows of the output matrix. If it is not provided,\n    the op assumes the output matrix is a square matrix and infers the matrix\n    size from `d_lower`, `d_upper`, and the innermost dimension of `diagonal`.\n  num_cols: The number of columns of the output matrix. If it is not provided,\n    the op assumes the output matrix is a square matrix and infers the matrix\n    size from `d_lower`, `d_upper`, and the innermost dimension of `diagonal`.\n  padding_value: The value to fill the area outside the specified diagonal\n    band with. Default is 0.\n  align: Some diagonals are shorter than `max_diag_len` and need to be padded.\n    `align` is a string specifying how superdiagonals and subdiagonals should\n    be aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\"\n    (default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\"\n    aligns superdiagonals to the right (left-pads the row) and subdiagonals to\n    the left (right-pads the row). It is the packing format LAPACK uses.\n    cuSPARSE uses \"LEFT_RIGHT\", which is the opposite alignment.\n\nReturns:\n  A Tensor. Has the same type as `diagonal`.", "Library": "TensorFlow"}
{"API_Name": "tf.linalg.eigvals", "Docstring": "Computes the eigenvalues of one or more matrices.\n\nNote: If your program backpropagates through this function, you should replace\nit with a call to tf.linalg.eig (possibly ignoring the second output) to\navoid computing the eigen decomposition twice. This is because the\neigenvectors are used to compute the gradient w.r.t. the eigenvalues. See\n_SelfAdjointEigV2Grad in linalg_grad.py.\n\nArgs:\n  tensor: `Tensor` of shape `[..., N, N]`.\n  name: string, optional name of the operation.\n\nReturns:\n  e: Eigenvalues. Shape is `[..., N]`. The vector `e[..., :]` contains the `N`\n    eigenvalues of `tensor[..., :, :]`.", "Library": "TensorFlow"}
{"API_Name": "tf.linalg.set_diag", "Docstring": "Returns a batched matrix tensor with new batched diagonal values.\n\nGiven `input` and `diagonal`, this operation returns a tensor with the\nsame shape and values as `input`, except for the specified diagonals of the\ninnermost matrices. These will be overwritten by the values in `diagonal`.\n\n`input` has `r+1` dimensions `[I, J, ..., L, M, N]`. When `k` is scalar or\n`k[0] == k[1]`, `diagonal` has `r` dimensions `[I, J, ..., L, max_diag_len]`.\nOtherwise, it has `r+1` dimensions `[I, J, ..., L, num_diags, max_diag_len]`.\n`num_diags` is the number of diagonals, `num_diags = k[1] - k[0] + 1`.\n`max_diag_len` is the longest diagonal in the range `[k[0], k[1]]`,\n`max_diag_len = min(M + min(k[1], 0), N + min(-k[0], 0))`\n\nThe output is a tensor of rank `k+1` with dimensions `[I, J, ..., L, M, N]`.\nIf `k` is scalar or `k[0] == k[1]`:\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]\n    input[i, j, ..., l, m, n]              ; otherwise\n```\n\nOtherwise,\n\n```\noutput[i, j, ..., l, m, n]\n  = diagonal[i, j, ..., l, diag_index, index_in_diag] ; if k[0] <= d <= k[1]\n    input[i, j, ..., l, m, n]                         ; otherwise\n```\nwhere `d = n - m`, `diag_index = k[1] - d`, and\n`index_in_diag = n - max(d, 0) + offset`.\n\n`offset` is zero except when the alignment of the diagonal is to the right.\n```\noffset = max_diag_len - diag_len(d) ; if (`align` in {RIGHT_LEFT, RIGHT_RIGHT}\n                                           and `d >= 0`) or\n                                         (`align` in {LEFT_RIGHT, RIGHT_RIGHT}\n                                           and `d <= 0`)\n         0                          ; otherwise\n```\nwhere `diag_len(d) = min(cols - max(d, 0), rows + min(d, 0))`.\n\nFor example:\n\n```\n# The main diagonal.\ninput = np.array([[[7, 7, 7, 7],              # Input shape: (2, 3, 4)\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]],\n                  [[7, 7, 7, 7],\n                   [7, 7, 7, 7],\n                   [7, 7, 7, 7]]])\ndiagonal = np.array([[1, 2, 3],               # Diagonal shape: (2, 3)\n                     [4, 5, 6]])\ntf.matrix_set_diag(input, diagonal)\n  ==> [[[1, 7, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 2, 7, 7],\n        [7, 7, 3, 7]],\n       [[4, 7, 7, 7],\n        [7, 5, 7, 7],\n        [7, 7, 6, 7]]]\n\n# A superdiagonal (per batch).\ntf.matrix_set_diag(input, diagonal, k = 1)\n  ==> [[[7, 1, 7, 7],  # Output shape: (2, 3, 4)\n        [7, 7, 2, 7],\n        [7, 7, 7, 3]],\n       [[7, 4, 7, 7],\n        [7, 7, 5, 7],\n        [7, 7, 7, 6]]]\n\n# A band of diagonals.\ndiagonals = np.array([[[9, 1, 0],  # Diagonal shape: (2, 4, 3)\n                       [6, 5, 8],\n                       [1, 2, 3],\n                       [0, 4, 5]],\n                      [[1, 2, 0],\n                       [5, 6, 4],\n                       [6, 1, 2],\n                       [0, 3, 4]]])\ntf.matrix_set_diag(input, diagonals, k = (-1, 2))\n  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 5, 1],\n        [7, 5, 3, 8]],\n       [[6, 5, 1, 7],\n        [3, 1, 6, 2],\n        [7, 4, 2, 4]]]\n\n# RIGHT_LEFT alignment.\ndiagonals = np.array([[[0, 9, 1],  # Diagonal shape: (2, 4, 3)\n                       [6, 5, 8],\n                       [1, 2, 3],\n                       [4, 5, 0]],\n                      [[0, 1, 2],\n                       [5, 6, 4],\n                       [6, 1, 2],\n                       [3, 4, 0]]])\ntf.matrix_set_diag(input, diagonals, k = (-1, 2), align=\"RIGHT_LEFT\")\n  ==> [[[1, 6, 9, 7],  # Output shape: (2, 3, 4)\n        [4, 2, 5, 1],\n        [7, 5, 3, 8]],\n       [[6, 5, 1, 7],\n        [3, 1, 6, 2],\n        [7, 4, 2, 4]]]\n\n```\n\nArgs:\n  input: A `Tensor` with rank `k + 1`, where `k >= 1`.\n  diagonal:  A `Tensor` with rank `k`, when `d_lower == d_upper`, or `k + 1`,\n    otherwise. `k >= 1`.\n  name: A name for the operation (optional).\n  k: Diagonal offset(s). Positive value means superdiagonal, 0 refers to the\n    main diagonal, and negative value means subdiagonals. `k` can be a single\n    integer (for a single diagonal) or a pair of integers specifying the low\n    and high ends of a matrix band. `k[0]` must not be larger than `k[1]`.\n  align: Some diagonals are shorter than `max_diag_len` and need to be padded.\n    `align` is a string specifying how superdiagonals and subdiagonals should\n    be aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\"\n    (default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". \"RIGHT_LEFT\"\n    aligns superdiagonals to the right (left-pads the row) and subdiagonals to\n    the left (right-pads the row). It is the packing format LAPACK uses.\n    cuSPARSE uses \"LEFT_RIGHT\", which is the opposite alignment.", "Library": "TensorFlow"}
{"API_Name": "tf.linalg.trace", "Docstring": "Compute the trace of a tensor `x`.\n\n`trace(x)` returns the sum along the main diagonal of each inner-most matrix\nin x. If x is of rank `k` with shape `[I, J, K, ..., L, M, N]`, then output\nis a tensor of rank `k-2` with dimensions `[I, J, K, ..., L]` where\n\n`output[i, j, k, ..., l] = trace(x[i, j, k, ..., l, :, :])`\n\nFor example:\n\n```python\nx = tf.constant([[1, 2], [3, 4]])\ntf.linalg.trace(x)  # 5\n\nx = tf.constant([[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]])\ntf.linalg.trace(x)  # 15\n\nx = tf.constant([[[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]],\n                 [[-1, -2, -3],\n                  [-4, -5, -6],\n                  [-7, -8, -9]]])\ntf.linalg.trace(x)  # [15, -15]\n```\n\nArgs:\n  x: tensor.\n  name: A name for the operation (optional).\n\nReturns:\n  The trace of input tensor.", "Library": "TensorFlow"}
{"API_Name": "tf.logical_and", "Docstring": "Returns the truth value of x AND y element-wise.\n\nLogical AND function.\n\nRequires that `x` and `y` have the same shape or have\n[broadcast-compatible](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\nshapes. For example, `x` and `y` can be:\n\n  - Two single elements of type `bool`.\n  - One `tf.Tensor` of type `bool` and one single `bool`, where the result will\n    be calculated by applying logical AND with the single element to each\n    element in the larger Tensor.\n  - Two `tf.Tensor` objects of type `bool` of the same shape. In this case,\n    the result will be the element-wise logical AND of the two input tensors.\n\nYou can also use the `&` operator instead.\n\nUsage:\n\n  >>> a = tf.constant([True])\n  >>> b = tf.constant([False])\n  >>> tf.math.logical_and(a, b)\n  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n  >>> a & b\n  <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>\n\n  >>> c = tf.constant([True])\n  >>> x = tf.constant([False, True, True, False])\n  >>> tf.math.logical_and(c, x)\n  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n  >>> c & x\n  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])>\n\n  >>> y = tf.constant([False, False, True, True])\n  >>> z = tf.constant([False, True, False, True])\n  >>> tf.math.logical_and(y, z)\n  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False, True])>\n  >>> y & z\n  <tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False, True])>\n\n  This op also supports broadcasting\n\n  >>> tf.logical_and([[True, False]], [[True], [False]])\n  <tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n    array([[ True, False],\n           [False, False]])>\n\nThe reduction version of this elementwise operation is `tf.math.reduce_all`.\n\nArgs:\n    x: A `tf.Tensor` of type bool.\n    y: A `tf.Tensor` of type bool.\n    name: A name for the operation (optional).\n\nReturns:\n  A `tf.Tensor` of type bool with the shape that `x` and `y` broadcast to.\n\nArgs:\n  x: A `Tensor` of type `bool`.\n  y: A `Tensor` of type `bool`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` of type `bool`.", "Library": "TensorFlow"}
{"API_Name": "tf.math.atanh", "Docstring": "Computes inverse hyperbolic tangent of x element-wise.\n\n  Given an input tensor, this function computes inverse hyperbolic tangent\n  for every element in the tensor. Input range is `[-1,1]` and output range is\n  `[-inf, inf]`. If input is `-1`, output will be `-inf` and if the\n  input is `1`, output will be `inf`. Values outside the range will have\n  `nan` as output.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -1, -0.5, 1, 0, 0.5, 10, float(\"inf\")])\n  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]\n  ```\n\nArgs:\n  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `x`.", "Library": "TensorFlow"}
{"API_Name": "tf.math.exp", "Docstring": "Computes exponential of x element-wise.  \\\\(y = e^x\\\\).\n\nThis function computes the exponential of the input tensor element-wise.\ni.e. `math.exp(x)` or \\\\(e^x\\\\), where `x` is the input tensor.\n\\\\(e\\\\) denotes Euler's number and is approximately equal to 2.718281.\nOutput is positive for any real input.\n\n>>> x = tf.constant(2.0)\n>>> tf.math.exp(x)\n<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>\n\n>>> x = tf.constant([2.0, 8.0])\n>>> tf.math.exp(x)\n<tf.Tensor: shape=(2,), dtype=float32,\nnumpy=array([   7.389056, 2980.958   ], dtype=float32)>\n\nFor complex numbers, the exponential value is calculated as\n$$\ne^{x+iy} = {e^x} {e^{iy}} = {e^x} ({\\cos (y) + i \\sin (y)})\n$$\n\nFor `1+1j` the value would be computed as:\n$$\ne^1 (\\cos (1) + i \\sin (1)) = 2.7182817 \\times (0.5403023+0.84147096j)\n$$\n\n>>> x = tf.constant(1 + 1j)\n>>> tf.math.exp(x)\n<tf.Tensor: shape=(), dtype=complex128,\nnumpy=(1.4686939399158851+2.2873552871788423j)>\n\nArgs:\n  x: A `tf.Tensor`. Must be one of the following types: `bfloat16`, `half`,\n    `float32`, `float64`, `complex64`, `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `tf.Tensor`. Has the same type as `x`.\n\n@compatibility(numpy)\nEquivalent to np.exp\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.math.sqrt", "Docstring": "Computes element-wise square root of the input tensor.\n\nNote: This operation does not support integer types.\n\n>>> x = tf.constant([[4.0], [16.0]])\n>>> tf.sqrt(x)\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n  array([[2.],\n         [4.]], dtype=float32)>\n>>> y = tf.constant([[-4.0], [16.0]])\n>>> tf.sqrt(y)\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n  array([[nan],\n         [ 4.]], dtype=float32)>\n>>> z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)\n>>> tf.sqrt(z)\n<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=\n  array([[0.0+1.j],\n         [4.0+0.j]])>\n\nNote: In order to support complex type, please provide an input tensor\nof `complex64` or `complex128`.\n\nArgs:\n  x: A `tf.Tensor` of type `bfloat16`, `half`, `float32`, `float64`,\n    `complex64`, `complex128`\n  name: A name for the operation (optional).\n\nReturns:\n  A `tf.Tensor` of same size, type and sparsity as `x`.\n\n  If `x` is a `SparseTensor`, returns\n  `SparseTensor(x.indices, tf.math.sqrt(x.values, ...), x.dense_shape)`", "Library": "TensorFlow"}
{"API_Name": "tf.math.top_k", "Docstring": "Finds values and indices of the `k` largest entries for the last dimension.\n\nIf the input is a vector (rank=1), finds the `k` largest entries in the vector\nand outputs their values and indices as vectors.  Thus `values[j]` is the\n`j`-th largest entry in `input`, and its index is `indices[j]`.\n\n>>> result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],\n...                         k=3)\n>>> result.values.numpy()\narray([99, 98, 96], dtype=int32)\n>>> result.indices.numpy()\narray([5, 2, 9], dtype=int32)\n\nFor matrices (resp. higher rank input), computes the top `k` entries in each\nrow (resp. vector along the last dimension).  Thus,\n\n>>> input = tf.random.normal(shape=(3,4,5,6))\n>>> k = 2\n>>> values, indices  = tf.math.top_k(input, k=k)\n>>> values.shape.as_list()\n[3, 4, 5, 2]\n>>>\n>>> values.shape == indices.shape == input.shape[:-1] + [k]\nTrue\n\nThe indices can be used to `gather` from a tensor who's shape matches `input`.\n\n>>> gathered_values = tf.gather(input, indices, batch_dims=-1)\n>>> assert tf.reduce_all(gathered_values == values)\n\nIf two elements are equal, the lower-index element appears first.\n\n>>> result = tf.math.top_k([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n...                        k=3)\n>>> result.indices.numpy()\narray([0, 1, 3], dtype=int32)\n\nBy default, indices are returned as type `int32`, however, this can be changed\nby specifying the `index_type`.\n\n>>> result = tf.math.top_k([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n...                        k=3, index_type=tf.int16)\n>>> result.indices.numpy()\narray([0, 1, 3], dtype=int16)\n\nArgs:\n  input: 1-D or higher `Tensor` with last dimension at least `k`.\n  k: 0-D `Tensor` of type `int16`, `int32` or `int64`.  Number of top element\n    to look for along the last dimension (along each row for matrices).\n  sorted: If true the resulting `k` elements will be sorted by the values in\n    descending order.\n  index_type: Optional dtype for output indices.\n  name: Optional name for the operation.\n\nReturns:\n  A tuple with two named fields:\n  values: The `k` largest elements along each last dimensional slice.\n  indices: The indices of `values` within the last dimension of `input`.", "Library": "TensorFlow"}
{"API_Name": "tf.matmul", "Docstring": "Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n\nThe inputs must, following any transpositions, be tensors of rank >= 2\nwhere the inner 2 dimensions specify valid matrix multiplication dimensions,\nand any further outer dimensions specify matching batch size.\n\nBoth matrices must be of the same type. The supported types are:\n`bfloat16`, `float16`, `float32`, `float64`, `int32`, `int64`,\n`complex64`, `complex128`.\n\nEither matrix can be transposed or adjointed (conjugated and transposed) on\nthe fly by setting one of the corresponding flag to `True`. These are `False`\nby default.\n\nIf one or both of the matrices contain a lot of zeros, a more efficient\nmultiplication algorithm can be used by setting the corresponding\n`a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\nThis optimization is only available for plain matrices (rank-2 tensors) with\ndatatypes `bfloat16` or `float32`.\n\nA simple 2-D tensor matrix multiplication:\n\n>>> a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n>>> a  # 2-D tensor\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)>\n>>> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n>>> b  # 2-D tensor\n<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[ 7,  8],\n       [ 9, 10],\n       [11, 12]], dtype=int32)>\n>>> c = tf.matmul(a, b)\n>>> c  # `a` * `b`\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 58,  64],\n       [139, 154]], dtype=int32)>\n\nA batch matrix multiplication with batch shape [2]:\n\n>>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n>>> a  # 3-D tensor\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n>>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n>>> b  # 3-D tensor\n<tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\narray([[[13, 14],\n        [15, 16],\n        [17, 18]],\n       [[19, 20],\n        [21, 22],\n        [23, 24]]], dtype=int32)>\n>>> c = tf.matmul(a, b)\n>>> c  # `a` * `b`\n<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[ 94, 100],\n        [229, 244]],\n       [[508, 532],\n        [697, 730]]], dtype=int32)>\n\nSince python >= 3.5 the @ operator is supported\n(see [PEP 465](https://www.python.org/dev/peps/pep-0465/)). In TensorFlow,\nit simply calls the `tf.matmul()` function, so the following lines are\nequivalent:\n\n>>> d = a @ b @ [[10], [11]]\n>>> d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n\nArgs:\n  a: `tf.Tensor` of type `float16`, `float32`, `float64`, `int32`,\n    `complex64`, `complex128` and rank > 1.\n  b: `tf.Tensor` with same type and rank as `a`.\n  transpose_a: If `True`, `a` is transposed before multiplication.\n  transpose_b: If `True`, `b` is transposed before multiplication.\n  adjoint_a: If `True`, `a` is conjugated and transposed before\n    multiplication.\n  adjoint_b: If `True`, `b` is conjugated and transposed before\n    multiplication.\n  a_is_sparse: If `True`, `a` is treated as a sparse matrix. Notice, this\n    **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n    that assume most values in `a` are zero. See\n    `tf.sparse.sparse_dense_matmul` for some support for\n    `tf.sparse.SparseTensor` multiplication.\n  b_is_sparse: If `True`, `b` is treated as a sparse matrix. Notice, this\n    **does not support `tf.sparse.SparseTensor`**, it just makes optimizations\n    that assume most values in `b` are zero. See\n    `tf.sparse.sparse_dense_matmul` for some support for\n    `tf.sparse.SparseTensor` multiplication.\n  output_type: The output datatype if needed. Defaults to None in which case\n    the output_type is the same as input type. Currently only works when input\n    tensors are type (u)int8 and output_type can be int32.\n  grad_a: Set it to `True` to hint that Tensor `a` is for the backward pass.\n  grad_b: Set it to `True` to hint that Tensor `b` is for the backward pass.\n  name: Name for the operation (optional).\n\nReturns:\n  A `tf.Tensor` of the same type as `a` and `b` where each inner-most matrix\n  is the product of the corresponding matrices in `a` and `b`, e.g. if all\n  transpose or adjoint attributes are `False`:\n\n  `output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])`,\n  for all indices `i`, `j`.\n\n  Note: This is matrix product, not element-wise product.\n\n\nRaises:\n  ValueError: If `transpose_a` and `adjoint_a`, or `transpose_b` and\n    `adjoint_b` are both set to `True`.\n  TypeError: If output_type is specified but the types of `a`, `b` and\n    `output_type` is not (u)int8, (u)int8 and int32.", "Library": "TensorFlow"}
{"API_Name": "tf.maximum", "Docstring": "Returns the max of x and y (i.e. x > y ? x : y) element-wise.\n\nExample:\n\n>>> x = tf.constant([0., 0., 0., 0.])\n>>> y = tf.constant([-2., 0., 2., 5.])\n>>> tf.math.maximum(x, y)\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>\n\nNote that `maximum` supports [broadcast semantics](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for `x` and `y`.\n\n>>> x = tf.constant([-5., 0., 0., 0.])\n>>> y = tf.constant([-3.])\n>>> tf.math.maximum(x, y)\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-3., 0., 0., 0.], dtype=float32)>\n\nThe reduction version of this elementwise operation is `tf.math.reduce_max`\n\nArgs:\n  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `uint32`, `int64`, `uint64`.\n  y: A `Tensor`. Must have the same type as `x`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `x`.", "Library": "TensorFlow"}
{"API_Name": "tf.nn.depth_to_space", "Docstring": "DepthToSpace for tensors of type T.\n\nRearranges data from depth into blocks of spatial data.\nThis is the reverse transformation of SpaceToDepth. More specifically,\nthis op outputs a copy of the input tensor where values from the `depth`\ndimension are moved in spatial blocks to the `height` and `width` dimensions.\nThe attr `block_size` indicates the input block size and how the data is moved.\n\n  * Chunks of data of size `block_size * block_size` from depth are rearranged\n    into non-overlapping blocks of size `block_size x block_size`\n  * The width of the output tensor is `input_depth * block_size`, whereas the\n    height is `input_height * block_size`.\n  * The Y, X coordinates within each block of the output image are determined\n    by the high order component of the input channel index.\n  * The depth of the input tensor must be divisible by\n    `block_size * block_size`.\n\nThe `data_format` attr specifies the layout of the input and output tensors\nwith the following options:\n  \"NHWC\": `[ batch, height, width, channels ]`\n  \"NCHW\": `[ batch, channels, height, width ]`\n  \"NCHW_VECT_C\":\n      `qint8 [ batch, channels / 4, height, width, 4 ]`\n\nIt is useful to consider the operation as transforming a 6-D Tensor.\ne.g. for data_format = NHWC,\n     Each element in the input tensor can be specified via 6 coordinates,\n     ordered by decreasing memory layout significance as:\n     n,iY,iX,bY,bX,oC  (where n=batch index, iX, iY means X or Y coordinates\n                        within the input image, bX, bY means coordinates\n                        within the output block, oC means output channels).\n     The output would be the input transposed to the following layout:\n     n,iY,bY,iX,bX,oC\n\nThis operation is useful for resizing the activations between convolutions\n(but keeping all data), e.g. instead of pooling. It is also useful for training\npurely convolutional models.\n\nFor example, given an input of shape `[1, 1, 1, 4]`, data_format = \"NHWC\" and\nblock_size = 2:\n\n```\nx = [[[[1, 2, 3, 4]]]]\n\n```\n\nThis operation will output a tensor of shape `[1, 2, 2, 1]`:\n\n```\n   [[[[1], [2]],\n     [[3], [4]]]]\n```\n\nHere, the input has a batch of 1 and each batch element has shape `[1, 1, 4]`,\nthe corresponding output will have 2x2 elements and will have a depth of\n1 channel (1 = `4 / (block_size * block_size)`).\nThe output element shape is `[2, 2, 1]`.\n\nFor an input tensor with larger depth, here of shape `[1, 1, 1, 12]`, e.g.\n\n```\nx = [[[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]]]\n```\n\nThis operation, for block size of 2, will return the following tensor of shape\n`[1, 2, 2, 3]`\n\n```\n   [[[[1, 2, 3], [4, 5, 6]],\n     [[7, 8, 9], [10, 11, 12]]]]\n\n```\n\nSimilarly, for the following input of shape `[1 2 2 4]`, and a block size of 2:\n\n```\nx =  [[[[1, 2, 3, 4],\n       [5, 6, 7, 8]],\n      [[9, 10, 11, 12],\n       [13, 14, 15, 16]]]]\n```\n\nthe operator will return the following tensor of shape `[1 4 4 1]`:\n\n```\nx = [[[ [1],   [2],  [5],  [6]],\n      [ [3],   [4],  [7],  [8]],\n      [ [9],  [10], [13],  [14]],\n      [ [11], [12], [15],  [16]]]]\n\n```\n\nArgs:\n  input: A `Tensor`.\n  block_size: An `int` that is `>= 2`.\n    The size of the spatial block, same as in Space2Depth.\n  data_format: An optional `string` from: `\"NHWC\", \"NCHW\", \"NCHW_VECT_C\"`. Defaults to `\"NHWC\"`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `input`.", "Library": "TensorFlow"}
{"API_Name": "tf.nn.dropout", "Docstring": "Computes dropout: randomly sets elements to zero to prevent overfitting.\n\nWarning: You should consider using\n`tf.nn.experimental.stateless_dropout` instead of this function. The\ndifference between `tf.nn.experimental.stateless_dropout` and this\nfunction is analogous to the difference between\n`tf.random.stateless_uniform` and `tf.random.uniform`. Please see\n[Random number\ngeneration](https://www.tensorflow.org/guide/random_numbers) guide\nfor a detailed description of the various RNG systems in TF. As the\nguide states, legacy stateful RNG ops like `tf.random.uniform` and\n`tf.nn.dropout` are not deprecated yet but highly discouraged,\nbecause their states are hard to control.\n\nNote: The behavior of dropout has changed between TensorFlow 1.x and 2.x.\nWhen converting 1.x code, please use named arguments to ensure behavior stays\nconsistent.\n\nSee also: `tf.keras.layers.Dropout` for a dropout layer.\n\n[Dropout](https://arxiv.org/abs/1207.0580) is useful for regularizing DNN\nmodels. Inputs elements are randomly set to zero (and the other elements are\nrescaled). This encourages each node to be independently useful, as it cannot\nrely on the output of other nodes.\n\nMore precisely: With probability `rate` elements of `x` are set to `0`.\nThe remaining elements are scaled up by `1.0 / (1 - rate)`, so that the\nexpected value is preserved.\n\n>>> tf.random.set_seed(0)\n>>> x = tf.ones([3,5])\n>>> tf.nn.dropout(x, rate = 0.5, seed = 1).numpy()\narray([[2., 0., 0., 2., 2.],\n     [2., 2., 2., 2., 2.],\n     [2., 0., 2., 0., 2.]], dtype=float32)\n\n>>> tf.random.set_seed(0)\n>>> x = tf.ones([3,5])\n>>> tf.nn.dropout(x, rate = 0.8, seed = 1).numpy()\narray([[0., 0., 0., 5., 5.],\n     [0., 5., 0., 5., 0.],\n     [5., 0., 5., 0., 5.]], dtype=float32)\n\n>>> tf.nn.dropout(x, rate = 0.0) == x\n<tf.Tensor: shape=(3, 5), dtype=bool, numpy=\n  array([[ True,  True,  True,  True,  True],\n         [ True,  True,  True,  True,  True],\n         [ True,  True,  True,  True,  True]])>\n\n\nBy default, each element is kept or dropped independently.  If `noise_shape`\nis specified, it must be\n[broadcastable](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\nto the shape of `x`, and only dimensions with `noise_shape[i] == shape(x)[i]`\nwill make independent decisions. This is useful for dropping whole\nchannels from an image or sequence. For example:\n\n>>> tf.random.set_seed(0)\n>>> x = tf.ones([3,10])\n>>> tf.nn.dropout(x, rate = 2/3, noise_shape=[1,10], seed=1).numpy()\narray([[0., 0., 0., 3., 3., 0., 3., 3., 3., 0.],\n     [0., 0., 0., 3., 3., 0., 3., 3., 3., 0.],\n     [0., 0., 0., 3., 3., 0., 3., 3., 3., 0.]], dtype=float32)\n\nArgs:\n  x: A floating point tensor.\n  rate: A scalar `Tensor` with the same type as x. The probability\n    that each element is dropped. For example, setting rate=0.1 would drop\n    10% of input elements.\n  noise_shape: A 1-D integer `Tensor`, representing the\n    shape for randomly generated keep/drop flags.\n  seed: A Python integer. Used to create random seeds. See\n    `tf.random.set_seed` for behavior.\n  name: A name for this operation (optional).\n\nReturns:\n  A Tensor of the same shape of `x`.\n\nRaises:\n  ValueError: If `rate` is not in `[0, 1)` or if `x` is not a floating point\n    tensor. `rate=1` is disallowed, because the output would be all zeros,\n    which is likely not what was intended.", "Library": "TensorFlow"}
{"API_Name": "tf.nn.log_softmax", "Docstring": "Computes log softmax activations.\n\nFor each batch `i` and class `j` we have\n\n    logsoftmax = logits - log(reduce_sum(exp(logits), axis))\n\nArgs:\n  logits: A non-empty `Tensor`. Must be one of the following types: `half`,\n    `float32`, `float64`.\n  axis: The dimension softmax would be performed on. The default is -1 which\n    indicates the last dimension.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `logits`. Same shape as `logits`.\n\nRaises:\n  InvalidArgumentError: if `logits` is empty or `axis` is beyond the last\n    dimension of `logits`.", "Library": "TensorFlow"}
{"API_Name": "tf.nn.max_pool2d", "Docstring": "Performs max pooling on 2D spatial data such as images.\n\nThis is a more specific version of `tf.nn.max_pool` where the input tensor\nis 4D, representing 2D spatial data such as images. Using these APIs are\nequivalent\n\nDownsamples the input images along theirs spatial dimensions (height and\nwidth) by taking its maximum over an input window defined by `ksize`.\nThe window is shifted by `strides` along each dimension.\n\nFor example, for `strides=(2, 2)` and `padding=VALID` windows that extend\noutside of the input are not included in the output:\n\n>>> x = tf.constant([[1., 2., 3., 4.],\n...                  [5., 6., 7., 8.],\n...                  [9., 10., 11., 12.]])\n>>> # Add the `batch` and `channels` dimensions.\n>>> x = x[tf.newaxis, :, :, tf.newaxis]\n>>> result = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2),\n...                           padding=\"VALID\")\n>>> result[0, :, :, 0]\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=\narray([[6., 8.]], dtype=float32)>\n\nWith `padding=SAME`, we get:\n\n>>> x = tf.constant([[1., 2., 3., 4.],\n...                  [5., 6., 7., 8.],\n...                  [9., 10., 11., 12.]])\n>>> x = x[tf.newaxis, :, :, tf.newaxis]\n>>> result = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2),\n...                           padding='SAME')\n>>> result[0, :, :, 0]\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[ 6., 8.],\n       [10.,12.]], dtype=float32)>\n\nWe can also specify padding explicitly. The following example adds width-1\npadding on all sides (top, bottom, left, right):\n\n>>> x = tf.constant([[1., 2., 3., 4.],\n...                  [5., 6., 7., 8.],\n...                  [9., 10., 11., 12.]])\n>>> x = x[tf.newaxis, :, :, tf.newaxis]\n>>> result = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2),\n...                           padding=[[0, 0], [1, 1], [1, 1], [0, 0]])\n>>> result[0, :, :, 0]\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 1., 3., 4.],\n       [ 9., 11., 12.]], dtype=float32)>\n\nFor more examples and detail, see `tf.nn.max_pool`.\n\nArgs:\n  input: A 4-D `Tensor` of the format specified by `data_format`.\n  ksize: An int or list of `ints` that has length `1`, `2` or `4`. The size of\n    the window for each dimension of the input tensor. If only one integer is\n    specified, then we apply the same window for all 4 dims. If two are\n    provided then we use those for H, W dimensions and keep N, C dimension\n    window size = 1.\n  strides: An int or list of `ints` that has length `1`, `2` or `4`. The\n    stride of the sliding window for each dimension of the input tensor. If\n    only one integer is specified, we apply the same stride to all 4 dims. If\n    two are provided we use those for the H, W dimensions and keep N, C of\n    stride = 1.\n  padding: Either the `string` `\"SAME\"` or `\"VALID\"` indicating the type of\n    padding algorithm to use, or a list indicating the explicit paddings at\n    the start and end of each dimension. See\n    [here](https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2)\n      for more information. When explicit padding is used and data_format is\n      `\"NHWC\"`, this should be in the form `[[0, 0], [pad_top, pad_bottom],\n      [pad_left, pad_right], [0, 0]]`. When explicit padding used and\n      data_format is `\"NCHW\"`, this should be in the form `[[0, 0], [0, 0],\n      [pad_top, pad_bottom], [pad_left, pad_right]]`. When using explicit\n      padding, the size of the paddings cannot be greater than the sliding\n      window size.\n  data_format: A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported.\n  name: Optional name for the operation.\n\nReturns:\n  A `Tensor` of format specified by `data_format`.\n  The max pooled output tensor.\n\nRaises:\n  ValueError: If explicit padding is used with data_format='NCHW_VECT_C'.", "Library": "TensorFlow"}
{"API_Name": "tf.nn.relu", "Docstring": "Computes rectified linear: `max(features, 0)`.\n\nSee: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\nExample usage:\n>>> tf.nn.relu([-2., 0., 3.]).numpy()\narray([0., 0., 3.], dtype=float32)\n\nArgs:\n  features: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`, `qint8`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `features`.", "Library": "TensorFlow"}
{"API_Name": "tf.nn.relu6", "Docstring": "Computes Rectified Linear 6: `min(max(features, 0), 6)`.\n\nIn comparison with `tf.nn.relu`, relu6 activation functions have shown to\nempirically perform better under low-precision conditions (e.g. fixed point\ninference) by encouraging the model to learn sparse features earlier.\nSource: [Convolutional Deep Belief Networks on CIFAR-10: Krizhevsky et al.,\n2010](http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf).\n\nFor example:\n\n>>> x = tf.constant([-3.0, -1.0, 0.0, 6.0, 10.0], dtype=tf.float32)\n>>> y = tf.nn.relu6(x)\n>>> y.numpy()\narray([0., 0., 0., 6., 6.], dtype=float32)\n\nArgs:\n  features: A `Tensor` with type `float`, `double`, `int32`, `int64`, `uint8`,\n    `int16`, or `int8`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` with the same type as `features`.\n\nReferences:\n  Convolutional Deep Belief Networks on CIFAR-10:\n    Krizhevsky et al., 2010\n    ([pdf](http://www.cs.utoronto.ca/~kriz/conv-cifar10-aug2010.pdf))", "Library": "TensorFlow"}
{"API_Name": "tf.nn.sigmoid", "Docstring": "Computes sigmoid of `x` element-wise.\n\nFormula for calculating $\\mathrm{sigmoid}(x) = y = 1 / (1 + \\exp(-x))$.\n\nFor $x \\in (-\\infty, \\infty)$, $\\mathrm{sigmoid}(x) \\in (0, 1)$.\n\nExample Usage:\n\nIf a positive number is large, then its sigmoid will approach to 1 since the\nformula will be `y = <large_num> / (1 + <large_num>)`\n\n>>> x = tf.constant([0.0, 1.0, 50.0, 100.0])\n>>> tf.math.sigmoid(x)\n<tf.Tensor: shape=(4,), dtype=float32,\nnumpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>\n\nIf a negative number is large, its sigmoid will approach to 0 since the\nformula will be `y = 1 / (1 + <large_num>)`\n\n>>> x = tf.constant([-100.0, -50.0, -1.0, 0.0])\n>>> tf.math.sigmoid(x)\n<tf.Tensor: shape=(4,), dtype=float32, numpy=\narray([0.0000000e+00, 1.9287499e-22, 2.6894143e-01, 0.5],\n      dtype=float32)>\n\nArgs:\n  x: A Tensor with type `float16`, `float32`, `float64`, `complex64`, or\n    `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A Tensor with the same type as `x`.\n\nUsage Example:\n\n>>> x = tf.constant([-128.0, 0.0, 128.0], dtype=tf.float32)\n>>> tf.sigmoid(x)\n<tf.Tensor: shape=(3,), dtype=float32,\nnumpy=array([0. , 0.5, 1. ], dtype=float32)>\n\n@compatibility(scipy)\nEquivalent to scipy.special.expit\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.nn.softmax", "Docstring": "Computes softmax activations.\n\nUsed for multi-class predictions. The sum of all outputs generated by softmax\nis 1.\n\nThis function performs the equivalent of\n\n```python\nsoftmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)\n```\nExample usage:\n\n>>> softmax = tf.nn.softmax([-1, 0., 1.])\n>>> softmax\n<tf.Tensor: shape=(3,), dtype=float32,\nnumpy=array([0.09003057, 0.24472848, 0.66524094], dtype=float32)>\n>>> sum(softmax)\n<tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n\nArgs:\n  logits: A non-empty `Tensor`. Must be one of the following types: `half`,\n    `float32`, `float64`.\n  axis: The dimension softmax would be performed on. The default is -1 which\n    indicates the last dimension.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type and shape as `logits`.\n\nRaises:\n  InvalidArgumentError: if `logits` is empty or `axis` is beyond the last\n    dimension of `logits`.", "Library": "TensorFlow"}
{"API_Name": "tf.nn.tanh", "Docstring": "Computes hyperbolic tangent of `x` element-wise.\n\n  Given an input tensor, this function computes hyperbolic tangent of every\n  element in the tensor. Input range is `[-inf, inf]` and\n  output range is `[-1,1]`.\n\n  >>> x = tf.constant([-float(\"inf\"), -5, -0.5, 1, 1.2, 2, 3, float(\"inf\")])\n  >>> tf.math.tanh(x)\n  <tf.Tensor: shape=(8,), dtype=float32, numpy=\n  array([-1.0, -0.99990916, -0.46211717,  0.7615942 ,  0.8336547 ,\n          0.9640276 ,  0.9950547 ,  1.0], dtype=float32)>\n\nArgs:\n  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `x`.\n\n  If `x` is a `SparseTensor`, returns\n  `SparseTensor(x.indices, tf.math.tanh(x.values, ...), x.dense_shape)`", "Library": "TensorFlow"}
{"API_Name": "tf.norm", "Docstring": "Computes the norm of vectors, matrices, and tensors.\n\nThis function can compute several different vector norms (the 1-norm, the\nEuclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and\nmatrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n\nArgs:\n  tensor: `Tensor` of types `float32`, `float64`, `complex64`, `complex128`\n  ord: Order of the norm. Supported values are `'fro'`, `'euclidean'`,\n    `1`, `2`, `np.inf` and any positive real number yielding the corresponding\n    p-norm. Default is `'euclidean'` which is equivalent to Frobenius norm if\n    `tensor` is a matrix and equivalent to 2-norm for vectors.\n    Some restrictions apply:\n      a) The Frobenius norm `'fro'` is not defined for vectors,\n      b) If axis is a 2-tuple (matrix norm), only `'euclidean'`, '`fro'`, `1`,\n         `2`, `np.inf` are supported.\n    See the description of `axis` on how to compute norms for a batch of\n    vectors or matrices stored in a tensor.\n  axis: If `axis` is `None` (the default), the input is considered a vector\n    and a single vector norm is computed over the entire set of values in the\n    tensor, i.e. `norm(tensor, ord=ord)` is equivalent to\n    `norm(reshape(tensor, [-1]), ord=ord)`.\n    If `axis` is a Python integer, the input is considered a batch of vectors,\n    and `axis` determines the axis in `tensor` over which to compute vector\n    norms.\n    If `axis` is a 2-tuple of Python integers it is considered a batch of\n    matrices and `axis` determines the axes in `tensor` over which to compute\n    a matrix norm.\n    Negative indices are supported. Example: If you are passing a tensor that\n    can be either a matrix or a batch of matrices at runtime, pass\n    `axis=[-2,-1]` instead of `axis=None` to make sure that matrix norms are\n    computed.\n  keepdims: If True, the axis indicated in `axis` are kept with size 1.\n    Otherwise, the dimensions in `axis` are removed from the output shape.\n  name: The name of the op.\n\nReturns:\n  output: A `Tensor` of the same type as tensor, containing the vector or\n    matrix norms. If `keepdims` is True then the rank of output is equal to\n    the rank of `tensor`. Otherwise, if `axis` is none the output is a scalar,\n    if `axis` is an integer, the rank of `output` is one less than the rank\n    of `tensor`, if `axis` is a 2-tuple the rank of `output` is two less\n    than the rank of `tensor`.\n\nRaises:\n  ValueError: If `ord` or `axis` is invalid.\n\n@compatibility(numpy)\nMostly equivalent to numpy.linalg.norm.\nNot supported: ord <= 0, 2-norm for matrices, nuclear norm.\nOther differences:\n  a) If axis is `None`, treats the flattened `tensor` as a vector\n   regardless of rank.\n  b) Explicitly supports 'euclidean' norm as the default, including for\n   higher order tensors.\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.not_equal", "Docstring": "Returns the truth value of (x != y) element-wise.\n\nPerforms a [broadcast](\nhttps://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) with the\narguments and then an element-wise inequality comparison, returning a Tensor\nof boolean values.\n\nFor example:\n\n>>> x = tf.constant([2, 4])\n>>> y = tf.constant(2)\n>>> tf.math.not_equal(x, y)\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>\n\n>>> x = tf.constant([2, 4])\n>>> y = tf.constant([2, 4])\n>>> tf.math.not_equal(x, y)\n<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  False])>\n\nArgs:\n  x: A `tf.Tensor`.\n  y: A `tf.Tensor`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `tf.Tensor` of type bool with the same size as that of x or y.\n\nRaises:\n  `tf.errors.InvalidArgumentError`: If shapes of arguments are incompatible", "Library": "TensorFlow"}
{"API_Name": "tf.one_hot", "Docstring": "Returns a one-hot tensor.\n\nSee also `tf.fill`, `tf.eye`.\n\nThe locations represented by indices in `indices` take value `on_value`,\nwhile all other locations take value `off_value`.\n\n`on_value` and `off_value` must have matching data types. If `dtype` is also\nprovided, they must be the same data type as specified by `dtype`.\n\nIf `on_value` is not provided, it will default to the value `1` with type\n`dtype`\n\nIf `off_value` is not provided, it will default to the value `0` with type\n`dtype`\n\nIf the input `indices` is rank `N`, the output will have rank `N+1`. The\nnew axis is created at dimension `axis` (default: the new axis is appended\nat the end).\n\nIf `indices` is a scalar the output shape will be a vector of length `depth`\n\nIf `indices` is a vector of length `features`, the output shape will be:\n\n```\n  features x depth if axis == -1\n  depth x features if axis == 0\n```\n\nIf `indices` is a matrix (batch) with shape `[batch, features]`, the output\nshape will be:\n\n```\n  batch x features x depth if axis == -1\n  batch x depth x features if axis == 1\n  depth x batch x features if axis == 0\n```\n\nIf `indices` is a RaggedTensor, the 'axis' argument must be positive and refer\nto a non-ragged axis. The output will be equivalent to applying 'one_hot' on\nthe values of the RaggedTensor, and creating a new RaggedTensor from the\nresult.\n\nIf `dtype` is not provided, it will attempt to assume the data type of\n`on_value` or `off_value`, if one or both are passed in. If none of\n`on_value`, `off_value`, or `dtype` are provided, `dtype` will default to the\nvalue `tf.float32`.\n\nNote: If a non-numeric data type output is desired (`tf.string`, `tf.bool`,\netc.), both `on_value` and `off_value` _must_ be provided to `one_hot`.\n\nFor example:\n\n```python\nindices = [0, 1, 2]\ndepth = 3\ntf.one_hot(indices, depth)  # output: [3 x 3]\n# [[1., 0., 0.],\n#  [0., 1., 0.],\n#  [0., 0., 1.]]\n\nindices = [0, 2, -1, 1]\ndepth = 3\ntf.one_hot(indices, depth,\n           on_value=5.0, off_value=0.0,\n           axis=-1)  # output: [4 x 3]\n# [[5.0, 0.0, 0.0],  # one_hot(0)\n#  [0.0, 0.0, 5.0],  # one_hot(2)\n#  [0.0, 0.0, 0.0],  # one_hot(-1)\n#  [0.0, 5.0, 0.0]]  # one_hot(1)\n\nindices = [[0, 2], [1, -1]]\ndepth = 3\ntf.one_hot(indices, depth,\n           on_value=1.0, off_value=0.0,\n           axis=-1)  # output: [2 x 2 x 3]\n# [[[1.0, 0.0, 0.0],   # one_hot(0)\n#   [0.0, 0.0, 1.0]],  # one_hot(2)\n#  [[0.0, 1.0, 0.0],   # one_hot(1)\n#   [0.0, 0.0, 0.0]]]  # one_hot(-1)\n\nindices = tf.ragged.constant([[0, 1], [2]])\ndepth = 3\ntf.one_hot(indices, depth)  # output: [2 x None x 3]\n# [[[1., 0., 0.],\n#   [0., 1., 0.]],\n#  [[0., 0., 1.]]]\n```\n\nArgs:\n  indices: A `Tensor` of indices.\n  depth: A scalar defining the depth of the one hot dimension.\n  on_value: A scalar defining the value to fill in output when `indices[j]\n    = i`. (default: 1)\n  off_value: A scalar defining the value to fill in output when `indices[j]\n    != i`. (default: 0)\n  axis: The axis to fill (default: -1, a new inner-most axis).\n  dtype: The data type of the output tensor.\n  name: A name for the operation (optional).\n\nReturns:\n  output: The one-hot tensor.\n\nRaises:\n  TypeError: If dtype of either `on_value` or `off_value` don't match `dtype`\n  TypeError: If dtype of `on_value` and `off_value` don't match one another", "Library": "TensorFlow"}
{"API_Name": "tf.ones", "Docstring": "Creates a tensor with all elements set to one (1).\n\nSee also `tf.ones_like`, `tf.zeros`, `tf.fill`, `tf.eye`.\n\nThis operation returns a tensor of type `dtype` with shape `shape` and\nall elements set to one.\n\n>>> tf.ones([3, 4], tf.int32)\n<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[1, 1, 1, 1],\n       [1, 1, 1, 1],\n       [1, 1, 1, 1]], dtype=int32)>\n\nArgs:\n  shape: A `list` of integers, a `tuple` of integers, or a 1-D `Tensor` of\n    type `int32`.\n  dtype: Optional DType of an element in the resulting `Tensor`. Default is\n    `tf.float32`.\n  name: Optional string. A name for the operation.\n  layout: Optional, `tf.experimental.dtensor.Layout`. If provided, the result\n    is a [DTensor](https://www.tensorflow.org/guide/dtensor_overview) with the\n    provided layout.\n\nReturns:\n  A `Tensor` with all elements set to one (1).", "Library": "TensorFlow"}
{"API_Name": "tf.pad", "Docstring": "Pads a tensor.\n\nThis operation pads a `tensor` according to the `paddings` you specify.\n`paddings` is an integer tensor with shape `[n, 2]`, where n is the rank of\n`tensor`. For each dimension D of `input`, `paddings[D, 0]` indicates how\nmany values to add before the contents of `tensor` in that dimension, and\n`paddings[D, 1]` indicates how many values to add after the contents of\n`tensor` in that dimension. If `mode` is \"REFLECT\" then both `paddings[D, 0]`\nand `paddings[D, 1]` must be no greater than `tensor.dim_size(D) - 1`. If\n`mode` is \"SYMMETRIC\" then both `paddings[D, 0]` and `paddings[D, 1]` must be\nno greater than `tensor.dim_size(D)`.\n\nThe padded size of each dimension D of the output is:\n\n`paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]`\n\nFor example:\n\n```python\nt = tf.constant([[1, 2, 3], [4, 5, 6]])\npaddings = tf.constant([[1, 1,], [2, 2]])\n# 'constant_values' is 0.\n# rank of 't' is 2.\ntf.pad(t, paddings, \"CONSTANT\")  # [[0, 0, 0, 0, 0, 0, 0],\n                                 #  [0, 0, 1, 2, 3, 0, 0],\n                                 #  [0, 0, 4, 5, 6, 0, 0],\n                                 #  [0, 0, 0, 0, 0, 0, 0]]\n\ntf.pad(t, paddings, \"REFLECT\")  # [[6, 5, 4, 5, 6, 5, 4],\n                                #  [3, 2, 1, 2, 3, 2, 1],\n                                #  [6, 5, 4, 5, 6, 5, 4],\n                                #  [3, 2, 1, 2, 3, 2, 1]]\n\ntf.pad(t, paddings, \"SYMMETRIC\")  # [[2, 1, 1, 2, 3, 3, 2],\n                                  #  [2, 1, 1, 2, 3, 3, 2],\n                                  #  [5, 4, 4, 5, 6, 6, 5],\n                                  #  [5, 4, 4, 5, 6, 6, 5]]\n```\n\nArgs:\n  tensor: A `Tensor`.\n  paddings: A `Tensor` of type `int32`.\n  mode: One of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\" (case-insensitive)\n  constant_values: In \"CONSTANT\" mode, the scalar pad value to use. Must be\n    same type as `tensor`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `tensor`.\n\nRaises:\n  ValueError: When mode is not one of \"CONSTANT\", \"REFLECT\", or \"SYMMETRIC\".", "Library": "TensorFlow"}
{"API_Name": "tf.random.normal", "Docstring": "Outputs random values from a normal distribution.\n\nExample that generates a new set of random values every time:\n\n>>> tf.random.set_seed(5);\n>>> tf.random.normal([4], 0, 1, tf.float32)\n<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>\n\nExample that outputs a reproducible result:\n\n>>> tf.random.set_seed(5);\n>>> tf.random.normal([2,2], 0, 1, tf.float32, seed=1)\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-1.3768897 , -0.01258316],\n      [-0.169515   ,  1.0824056 ]], dtype=float32)>\n\nIn this case, we are setting both the global and operation-level seed to\nensure this result is reproducible.  See `tf.random.set_seed` for more\ninformation.\n\nArgs:\n  shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n  mean: A Tensor or Python value of type `dtype`, broadcastable with `stddev`.\n    The mean of the normal distribution.\n  stddev: A Tensor or Python value of type `dtype`, broadcastable with `mean`.\n    The standard deviation of the normal distribution.\n  dtype: The float type of the output: `float16`, `bfloat16`, `float32`,\n    `float64`. Defaults to `float32`.\n  seed: A Python integer. Used to create a random seed for the distribution.\n    See\n    `tf.random.set_seed`\n    for behavior.\n  name: A name for the operation (optional).\n\nReturns:\n  A tensor of the specified shape filled with random normal values.", "Library": "TensorFlow"}
{"API_Name": "tf.random.set_seed", "Docstring": "Sets the global random seed.\n\nOperations that rely on a random seed actually derive it from two seeds:\nthe global and operation-level seeds. This sets the global seed.\n\nIts interactions with operation-level seeds is as follows:\n\n  1. If neither the global seed nor the operation seed is set: A randomly\n    picked seed is used for this op.\n  2. If the global seed is set, but the operation seed is not:\n    The system deterministically picks an operation seed in conjunction with\n    the global seed so that it gets a unique random sequence. Within the\n    same version of tensorflow and user code, this sequence is deterministic.\n    However across different versions, this sequence might change. If the\n    code depends on particular seeds to work, specify both global\n    and operation-level seeds explicitly.\n  3. If the operation seed is set, but the global seed is not set:\n    A default global seed and the specified operation seed are used to\n    determine the random sequence.\n  4. If both the global and the operation seed are set:\n    Both seeds are used in conjunction to determine the random sequence.\n\nTo illustrate the user-visible effects, consider these examples:\n\nIf neither the global seed nor the operation seed is set, we get different\nresults for every call to the random op and every re-run of the program:\n\n```python\nprint(tf.random.uniform([1]))  # generates 'A1'\nprint(tf.random.uniform([1]))  # generates 'A2'\n```\n\n(now close the program and run it again)\n\n```python\nprint(tf.random.uniform([1]))  # generates 'A3'\nprint(tf.random.uniform([1]))  # generates 'A4'\n```\n\nIf the global seed is set but the operation seed is not set, we get different\nresults for every call to the random op, but the same sequence for every\nre-run of the program:\n\n```python\ntf.random.set_seed(1234)\nprint(tf.random.uniform([1]))  # generates 'A1'\nprint(tf.random.uniform([1]))  # generates 'A2'\n```\n\n(now close the program and run it again)\n\n```python\ntf.random.set_seed(1234)\nprint(tf.random.uniform([1]))  # generates 'A1'\nprint(tf.random.uniform([1]))  # generates 'A2'\n```\n\nThe reason we get 'A2' instead 'A1' on the second call of `tf.random.uniform`\nabove is because the second call uses a different operation seed.\n\nNote that `tf.function` acts like a re-run of a program in this case. When\nthe global seed is set but operation seeds are not set, the sequence of random\nnumbers are the same for each `tf.function`. For example:\n\n```python\ntf.random.set_seed(1234)\n\n@tf.function\ndef f():\n  a = tf.random.uniform([1])\n  b = tf.random.uniform([1])\n  return a, b\n\n@tf.function\ndef g():\n  a = tf.random.uniform([1])\n  b = tf.random.uniform([1])\n  return a, b\n\nprint(f())  # prints '(A1, A2)'\nprint(g())  # prints '(A1, A2)'\n```\n\nIf the operation seed is set, we get different results for every call to the\nrandom op, but the same sequence for every re-run of the program:\n\n```python\nprint(tf.random.uniform([1], seed=1))  # generates 'A1'\nprint(tf.random.uniform([1], seed=1))  # generates 'A2'\n```\n\n(now close the program and run it again)\n\n```python\nprint(tf.random.uniform([1], seed=1))  # generates 'A1'\nprint(tf.random.uniform([1], seed=1))  # generates 'A2'\n```\n\nThe reason we get 'A2' instead 'A1' on the second call of `tf.random.uniform`\nabove is because the same `tf.random.uniform` kernel (i.e. internal\nrepresentation) is used by TensorFlow for all calls of it with the same\narguments, and the kernel maintains an internal counter which is incremented\nevery time it is executed, generating different results.\n\nCalling `tf.random.set_seed` will reset any such counters:\n\n```python\ntf.random.set_seed(1234)\nprint(tf.random.uniform([1], seed=1))  # generates 'A1'\nprint(tf.random.uniform([1], seed=1))  # generates 'A2'\ntf.random.set_seed(1234)\nprint(tf.random.uniform([1], seed=1))  # generates 'A1'\nprint(tf.random.uniform([1], seed=1))  # generates 'A2'\n```\n\nWhen multiple identical random ops are wrapped in a `tf.function`, their\nbehaviors change because the ops no long share the same counter. For example:\n\n```python\n@tf.function\ndef foo():\n  a = tf.random.uniform([1], seed=1)\n  b = tf.random.uniform([1], seed=1)\n  return a, b\nprint(foo())  # prints '(A1, A1)'\nprint(foo())  # prints '(A2, A2)'\n\n@tf.function\ndef bar():\n  a = tf.random.uniform([1])\n  b = tf.random.uniform([1])\n  return a, b\nprint(bar())  # prints '(A1, A2)'\nprint(bar())  # prints '(A3, A4)'\n```\n\nThe second call of `foo` returns '(A2, A2)' instead of '(A1, A1)' because\n`tf.random.uniform` maintains an internal counter. If you want `foo` to return\n'(A1, A1)' every time, use the stateless random ops such as\n`tf.random.stateless_uniform`. Also see `tf.random.experimental.Generator` for\na new set of stateful random ops that use external variables to manage their\nstates.\n\nArgs:\n  seed: integer.", "Library": "TensorFlow"}
{"API_Name": "tf.random.uniform", "Docstring": "Outputs random values from a uniform distribution.\n\nThe generated values follow a uniform distribution in the range\n`[minval, maxval)`. The lower bound `minval` is included in the range, while\nthe upper bound `maxval` is excluded.\n\nFor floats, the default range is `[0, 1)`.  For ints, at least `maxval` must\nbe specified explicitly.\n\nIn the integer case, the random integers are slightly biased unless\n`maxval - minval` is an exact power of two.  The bias is small for values of\n`maxval - minval` significantly smaller than the range of the output (either\n`2**32` or `2**64`).\n\nExamples:\n\n>>> tf.random.uniform(shape=[2])\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>\n>>> tf.random.uniform(shape=[], minval=-1., maxval=0.)\n<tf.Tensor: shape=(), dtype=float32, numpy=-...>\n>>> tf.random.uniform(shape=[], minval=5, maxval=10, dtype=tf.int64)\n<tf.Tensor: shape=(), dtype=int64, numpy=...>\n\nThe `seed` argument produces a deterministic sequence of tensors across\nmultiple calls. To repeat that sequence, use `tf.random.set_seed`:\n\n>>> tf.random.set_seed(5)\n>>> tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\n<tf.Tensor: shape=(), dtype=int32, numpy=2>\n>>> tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\n<tf.Tensor: shape=(), dtype=int32, numpy=0>\n>>> tf.random.set_seed(5)\n>>> tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\n<tf.Tensor: shape=(), dtype=int32, numpy=2>\n>>> tf.random.uniform(shape=[], maxval=3, dtype=tf.int32, seed=10)\n<tf.Tensor: shape=(), dtype=int32, numpy=0>\n\nWithout `tf.random.set_seed` but with a `seed` argument is specified, small\nchanges to function graphs or previously executed operations will change the\nreturned value. See `tf.random.set_seed` for details.\n\nArgs:\n  shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n  minval: A Tensor or Python value of type `dtype`, broadcastable with\n    `shape` (for integer types, broadcasting is not supported, so it needs to\n    be a scalar). The lower bound on the range of random values to generate\n    (inclusive).  Defaults to 0.\n  maxval: A Tensor or Python value of type `dtype`, broadcastable with\n    `shape` (for integer types, broadcasting is not supported, so it needs to\n    be a scalar). The upper bound on the range of random values to generate\n    (exclusive). Defaults to 1 if `dtype` is floating point.\n  dtype: The type of the output: `float16`, `bfloat16`, `float32`, `float64`,\n    `int32`, or `int64`. Defaults to `float32`.\n  seed: A Python integer. Used in combination with `tf.random.set_seed` to\n    create a reproducible sequence of tensors across multiple calls.\n  name: A name for the operation (optional).\n\nReturns:\n  A tensor of the specified shape filled with random uniform values.\n\nRaises:\n  ValueError: If `dtype` is integral and `maxval` is not specified.", "Library": "TensorFlow"}
{"API_Name": "tf.range", "Docstring": "Creates a sequence of numbers.\n\nCreates a sequence of numbers that begins at `start` and extends by\nincrements of `delta` up to but not including `limit`.\n\nThe dtype of the resulting tensor is inferred from the inputs unless\nit is provided explicitly.\n\nLike the Python builtin `range`, `start` defaults to 0, so that\n`range(n) = range(0, n)`.\n\nFor example:\n\n>>> start = 3\n>>> limit = 18\n>>> delta = 3\n>>> tf.range(start, limit, delta)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([ 3,  6,  9, 12, 15], dtype=int32)>\n\n>>> start = 3\n>>> limit = 1\n>>> delta = -0.5\n>>> tf.range(start, limit, delta)\n<tf.Tensor: shape=(4,), dtype=float32,\nnumpy=array([3. , 2.5, 2. , 1.5], dtype=float32)>\n\n>>> limit = 5\n>>> tf.range(limit)\n<tf.Tensor: shape=(5,), dtype=int32,\nnumpy=array([0, 1, 2, 3, 4], dtype=int32)>\n\nArgs:\n  start: A 0-D `Tensor` (scalar). Acts as first entry in the range if `limit`\n    is not None; otherwise, acts as range limit and first entry defaults to 0.\n  limit: A 0-D `Tensor` (scalar). Upper limit of sequence, exclusive. If None,\n    defaults to the value of `start` while the first entry of the range\n    defaults to 0.\n  delta: A 0-D `Tensor` (scalar). Number that increments `start`. Defaults to\n    1.\n  dtype: The type of the elements of the resulting tensor.\n  name: A name for the operation. Defaults to \"range\".\n\nReturns:\n  An 1-D `Tensor` of type `dtype`.\n\n@compatibility(numpy)\nEquivalent to np.arange\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.rank", "Docstring": "Returns the rank of a tensor.\n\nSee also `tf.shape`.\n\nReturns a 0-D `int32` `Tensor` representing the rank of `input`.\n\nFor example:\n\n```python\n# shape of tensor 't' is [2, 2, 3]\nt = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\ntf.rank(t)  # 3\n```\n\n**Note**: The rank of a tensor is not the same as the rank of a matrix. The\nrank of a tensor is the number of indices required to uniquely select each\nelement of the tensor. Rank is also known as \"order\", \"degree\", or \"ndims.\"\n\nArgs:\n  input: A `Tensor` or `SparseTensor`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` of type `int32`.\n\n@compatibility(numpy)\nEquivalent to np.ndim\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.reduce_all", "Docstring": "Computes `tf.math.logical_and` of elements across dimensions of a tensor.\n\nThis is the reduction operation for the elementwise `tf.math.logical_and` op.\n\nReduces `input_tensor` along the dimensions given in `axis`.\nUnless `keepdims` is true, the rank of the tensor is reduced by 1 for each\nof the entries in `axis`, which must be unique. If `keepdims` is true, the\nreduced dimensions are retained with length 1.\n\nIf `axis` is None, all dimensions are reduced, and a\ntensor with a single element is returned.\n\nFor example:\n\n  >>> x = tf.constant([[True,  True], [False, False]])\n  >>> tf.math.reduce_all(x)\n  <tf.Tensor: shape=(), dtype=bool, numpy=False>\n  >>> tf.math.reduce_all(x, 0)\n  <tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>\n  >>> tf.math.reduce_all(x, 1)\n  <tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>\n\nArgs:\n  input_tensor: The boolean tensor to reduce.\n  axis: The dimensions to reduce. If `None` (the default), reduces all\n    dimensions. Must be in the range `[-rank(input_tensor),\n    rank(input_tensor))`.\n  keepdims: If true, retains reduced dimensions with length 1.\n  name: A name for the operation (optional).\n\nReturns:\n  The reduced tensor.\n\n@compatibility(numpy)\nEquivalent to np.all\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.reduce_any", "Docstring": "Computes `tf.math.logical_or` of elements across dimensions of a tensor.\n\nThis is the reduction operation for the elementwise `tf.math.logical_or` op.\n\nReduces `input_tensor` along the dimensions given in `axis`.\nUnless `keepdims` is true, the rank of the tensor is reduced by 1 for each\nof the entries in `axis`, which must be unique. If `keepdims` is true, the\nreduced dimensions are retained with length 1.\n\nIf `axis` is None, all dimensions are reduced, and a\ntensor with a single element is returned.\n\nFor example:\n\n  >>> x = tf.constant([[True,  True], [False, False]])\n  >>> tf.reduce_any(x)\n  <tf.Tensor: shape=(), dtype=bool, numpy=True>\n  >>> tf.reduce_any(x, 0)\n  <tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>\n  >>> tf.reduce_any(x, 1)\n  <tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>\n\nArgs:\n  input_tensor: The boolean tensor to reduce.\n  axis: The dimensions to reduce. If `None` (the default), reduces all\n    dimensions. Must be in the range `[-rank(input_tensor),\n    rank(input_tensor))`.\n  keepdims: If true, retains reduced dimensions with length 1.\n  name: A name for the operation (optional).\n\nReturns:\n  The reduced tensor.\n\n@compatibility(numpy)\nEquivalent to np.any\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.reduce_max", "Docstring": "Computes `tf.math.maximum` of elements across dimensions of a tensor.\n\nThis is the reduction operation for the elementwise `tf.math.maximum` op.\n\nReduces `input_tensor` along the dimensions given in `axis`.\nUnless `keepdims` is true, the rank of the tensor is reduced by 1 for each\nof the entries in `axis`, which must be unique. If `keepdims` is true, the\nreduced dimensions are retained with length 1.\n\nIf `axis` is None, all dimensions are reduced, and a\ntensor with a single element is returned.\n\nUsage example:\n\n  >>> x = tf.constant([5, 1, 2, 4])\n  >>> tf.reduce_max(x)\n  <tf.Tensor: shape=(), dtype=int32, numpy=5>\n  >>> x = tf.constant([-5, -1, -2, -4])\n  >>> tf.reduce_max(x)\n  <tf.Tensor: shape=(), dtype=int32, numpy=-1>\n  >>> x = tf.constant([4, float('nan')])\n  >>> tf.reduce_max(x)\n  <tf.Tensor: shape=(), dtype=float32, numpy=nan>\n  >>> x = tf.constant([float('nan'), float('nan')])\n  >>> tf.reduce_max(x)\n  <tf.Tensor: shape=(), dtype=float32, numpy=nan>\n  >>> x = tf.constant([float('-inf'), float('inf')])\n  >>> tf.reduce_max(x)\n  <tf.Tensor: shape=(), dtype=float32, numpy=inf>\n\nSee the numpy docs for `np.amax` and `np.nanmax` behavior.\n\nArgs:\n  input_tensor: The tensor to reduce. Should have real numeric type.\n  axis: The dimensions to reduce. If `None` (the default), reduces all\n    dimensions. Must be in the range `[-rank(input_tensor),\n    rank(input_tensor))`.\n  keepdims: If true, retains reduced dimensions with length 1.\n  name: A name for the operation (optional).\n\nReturns:\n  The reduced tensor.", "Library": "TensorFlow"}
{"API_Name": "tf.reduce_mean", "Docstring": "Computes the mean of elements across dimensions of a tensor.\n\nReduces `input_tensor` along the dimensions given in `axis` by computing the\nmean of elements across the dimensions in `axis`.\nUnless `keepdims` is true, the rank of the tensor is reduced by 1 for each\nof the entries in `axis`, which must be unique. If `keepdims` is true, the\nreduced dimensions are retained with length 1.\n\nIf `axis` is None, all dimensions are reduced, and a tensor with a single\nelement is returned.\n\nFor example:\n\n>>> x = tf.constant([[1., 1.], [2., 2.]])\n>>> tf.reduce_mean(x)\n<tf.Tensor: shape=(), dtype=float32, numpy=1.5>\n>>> tf.reduce_mean(x, 0)\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>\n>>> tf.reduce_mean(x, 1)\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>\n\nArgs:\n  input_tensor: The tensor to reduce. Should have numeric type.\n  axis: The dimensions to reduce. If `None` (the default), reduces all\n    dimensions. Must be in the range `[-rank(input_tensor),\n    rank(input_tensor))`.\n  keepdims: If true, retains reduced dimensions with length 1.\n  name: A name for the operation (optional).\n\nReturns:\n  The reduced tensor.\n\n@compatibility(numpy)\nEquivalent to np.mean\n\nPlease note that `np.mean` has a `dtype` parameter that could be used to\nspecify the output type. By default this is `dtype=float64`. On the other\nhand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,\nfor example:\n\n>>> x = tf.constant([1, 0, 1, 0])\n>>> tf.reduce_mean(x)\n<tf.Tensor: shape=(), dtype=int32, numpy=0>\n>>> y = tf.constant([1., 0., 1., 0.])\n>>> tf.reduce_mean(y)\n<tf.Tensor: shape=(), dtype=float32, numpy=0.5>\n\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.reduce_prod", "Docstring": "Computes `tf.math.multiply` of elements across dimensions of a tensor.\n\nThis is the reduction operation for the elementwise `tf.math.multiply` op.\n\nReduces `input_tensor` along the dimensions given in `axis`.\nUnless `keepdims` is true, the rank of the tensor is reduced by 1 for each\nentry in `axis`. If `keepdims` is true, the reduced dimensions\nare retained with length 1.\n\nIf `axis` is None, all dimensions are reduced, and a\ntensor with a single element is returned.\n\nFor example:\n\n  >>> x = tf.constant([[1., 2.], [3., 4.]])\n  >>> tf.math.reduce_prod(x)\n  <tf.Tensor: shape=(), dtype=float32, numpy=24.>\n  >>> tf.math.reduce_prod(x, 0)\n  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>\n  >>> tf.math.reduce_prod(x, 1)\n  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],\n  dtype=float32)>\n\nArgs:\n  input_tensor: The tensor to reduce. Should have numeric type.\n  axis: The dimensions to reduce. If `None` (the default), reduces all\n    dimensions. Must be in the range `[-rank(input_tensor),\n    rank(input_tensor))`.\n  keepdims: If true, retains reduced dimensions with length 1.\n  name: A name for the operation (optional).\n\nReturns:\n  The reduced tensor.\n\n@compatibility(numpy)\nEquivalent to np.prod\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.reduce_sum", "Docstring": "Computes the sum of elements across dimensions of a tensor.\n\nThis is the reduction operation for the elementwise `tf.math.add` op.\n\nReduces `input_tensor` along the dimensions given in `axis`.\nUnless `keepdims` is true, the rank of the tensor is reduced by 1 for each\nof the entries in `axis`, which must be unique. If `keepdims` is true, the\nreduced dimensions are retained with length 1.\n\nIf `axis` is None, all dimensions are reduced, and a\ntensor with a single element is returned.\n\nFor example:\n\n  >>> # x has a shape of (2, 3) (two rows and three columns):\n  >>> x = tf.constant([[1, 1, 1], [1, 1, 1]])\n  >>> x.numpy()\n  array([[1, 1, 1],\n         [1, 1, 1]], dtype=int32)\n  >>> # sum all the elements\n  >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6\n  >>> tf.reduce_sum(x).numpy()\n  6\n  >>> # reduce along the first dimension\n  >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n  >>> tf.reduce_sum(x, 0).numpy()\n  array([2, 2, 2], dtype=int32)\n  >>> # reduce along the second dimension\n  >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]\n  >>> tf.reduce_sum(x, 1).numpy()\n  array([3, 3], dtype=int32)\n  >>> # keep the original dimensions\n  >>> tf.reduce_sum(x, 1, keepdims=True).numpy()\n  array([[3],\n         [3]], dtype=int32)\n  >>> # reduce along both dimensions\n  >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6\n  >>> # or, equivalently, reduce along rows, then reduce the resultant array\n  >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]\n  >>> # 2 + 2 + 2 = 6\n  >>> tf.reduce_sum(x, [0, 1]).numpy()\n  6\n\nArgs:\n  input_tensor: The tensor to reduce. Should have numeric type.\n  axis: The dimensions to reduce. If `None` (the default), reduces all\n    dimensions. Must be in the range `[-rank(input_tensor),\n    rank(input_tensor)]`.\n  keepdims: If true, retains reduced dimensions with length 1.\n  name: A name for the operation (optional).\n\nReturns:\n  The reduced tensor, of the same dtype as the input_tensor.\n\n@compatibility(numpy)\nEquivalent to np.sum apart the fact that numpy upcast uint8 and int32 to\nint64 while tensorflow returns the same dtype as the input.\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.repeat", "Docstring": "Repeat elements of `input`.\n\nSee also `tf.concat`, `tf.stack`, `tf.tile`.\n\nArgs:\n  input: An `N`-dimensional Tensor.\n  repeats: An 1-D `int` Tensor. The number of repetitions for each element.\n    repeats is broadcasted to fit the shape of the given axis. `len(repeats)`\n    must equal `input.shape[axis]` if axis is not None.\n  axis: An int. The axis along which to repeat values. By default, (axis=None),\n    use the flattened input array, and return a flat output array.\n  name: A name for the operation.\n\nReturns:\n  A Tensor which has the same shape as `input`, except along the given axis.\n    If axis is None then the output array is flattened to match the flattened\n    input array.\n\nExample usage:\n\n>>> repeat(['a', 'b', 'c'], repeats=[3, 0, 2], axis=0)\n<tf.Tensor: shape=(5,), dtype=string,\nnumpy=array([b'a', b'a', b'a', b'c', b'c'], dtype=object)>\n\n>>> repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=0)\n<tf.Tensor: shape=(5, 2), dtype=int32, numpy=\narray([[1, 2],\n       [1, 2],\n       [3, 4],\n       [3, 4],\n       [3, 4]], dtype=int32)>\n\n>>> repeat([[1, 2], [3, 4]], repeats=[2, 3], axis=1)\n<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\narray([[1, 1, 2, 2, 2],\n       [3, 3, 4, 4, 4]], dtype=int32)>\n\n>>> repeat(3, repeats=4)\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 3, 3, 3], dtype=int32)>\n\n>>> repeat([[1,2], [3,4]], repeats=2)\n<tf.Tensor: shape=(8,), dtype=int32,\nnumpy=array([1, 1, 2, 2, 3, 3, 4, 4], dtype=int32)>", "Library": "TensorFlow"}
{"API_Name": "tf.reshape", "Docstring": "Reshapes a tensor.\n\nGiven `tensor`, this operation returns a new `tf.Tensor` that has the same\nvalues as `tensor` in the same order, except with a new shape given by\n`shape`.\n\n>>> t1 = [[1, 2, 3],\n...       [4, 5, 6]]\n>>> print(tf.shape(t1).numpy())\n[2 3]\n>>> t2 = tf.reshape(t1, [6])\n>>> t2\n<tf.Tensor: shape=(6,), dtype=int32,\n  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n>>> tf.reshape(t2, [3, 2])\n<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n  array([[1, 2],\n         [3, 4],\n         [5, 6]], dtype=int32)>\n\nThe `tf.reshape` does not change the order of or the total number of elements\nin the tensor, and so it can reuse the underlying data buffer. This makes it\na fast operation independent of how big of a tensor it is operating on.\n\n>>> tf.reshape([1, 2, 3], [2, 2])\nTraceback (most recent call last):\n...\nInvalidArgumentError: Input to reshape is a tensor with 3 values, but the\nrequested shape has 4\n\nTo instead reorder the data to rearrange the dimensions of a tensor, see\n`tf.transpose`.\n\n>>> t = [[1, 2, 3],\n...      [4, 5, 6]]\n>>> tf.reshape(t, [3, 2]).numpy()\narray([[1, 2],\n       [3, 4],\n       [5, 6]], dtype=int32)\n>>> tf.transpose(t, perm=[1, 0]).numpy()\narray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)\n\nIf one component of `shape` is the special value -1, the size of that\ndimension is computed so that the total size remains constant.  In particular,\na `shape` of `[-1]` flattens into 1-D.  At most one component of `shape` can\nbe -1.\n\n>>> t = [[1, 2, 3],\n...      [4, 5, 6]]\n>>> tf.reshape(t, [-1])\n<tf.Tensor: shape=(6,), dtype=int32,\n  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n>>> tf.reshape(t, [3, -1])\n<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n  array([[1, 2],\n         [3, 4],\n         [5, 6]], dtype=int32)>\n>>> tf.reshape(t, [-1, 2])\n<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n  array([[1, 2],\n         [3, 4],\n         [5, 6]], dtype=int32)>\n\n`tf.reshape(t, [])` reshapes a tensor `t` with one element to a scalar.\n\n>>> tf.reshape([7], []).numpy()\n7\n\nMore examples:\n\n>>> t = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> print(tf.shape(t).numpy())\n[9]\n>>> tf.reshape(t, [3, 3])\n<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n  array([[1, 2, 3],\n         [4, 5, 6],\n         [7, 8, 9]], dtype=int32)>\n\n>>> t = [[[1, 1], [2, 2]],\n...      [[3, 3], [4, 4]]]\n>>> print(tf.shape(t).numpy())\n[2 2 2]\n>>> tf.reshape(t, [2, 4])\n<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n  array([[1, 1, 2, 2],\n         [3, 3, 4, 4]], dtype=int32)>\n\n>>> t = [[[1, 1, 1],\n...       [2, 2, 2]],\n...      [[3, 3, 3],\n...       [4, 4, 4]],\n...      [[5, 5, 5],\n...       [6, 6, 6]]]\n>>> print(tf.shape(t).numpy())\n[3 2 3]\n>>> # Pass '[-1]' to flatten 't'.\n>>> tf.reshape(t, [-1])\n<tf.Tensor: shape=(18,), dtype=int32,\n  numpy=array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],\n  dtype=int32)>\n>>> # -- Using -1 to infer the shape --\n>>> # Here -1 is inferred to be 9:\n>>> tf.reshape(t, [2, -1])\n<tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n  array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n         [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n>>> # -1 is inferred to be 2:\n>>> tf.reshape(t, [-1, 9])\n<tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n  array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n         [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n>>> # -1 is inferred to be 3:\n>>> tf.reshape(t, [ 2, -1, 3])\n<tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=\n  array([[[1, 1, 1],\n          [2, 2, 2],\n          [3, 3, 3]],\n         [[4, 4, 4],\n          [5, 5, 5],\n          [6, 6, 6]]], dtype=int32)>\n\nArgs:\n  tensor: A `Tensor`.\n  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    Defines the shape of the output tensor.\n  name: Optional string. A name for the operation.\n\nReturns:\n  A `Tensor`. Has the same type as `tensor`.", "Library": "TensorFlow"}
{"API_Name": "tf.sequence_mask", "Docstring": "Returns a mask tensor representing the first N positions of each cell.\n\nIf `lengths` has shape `[d_1, d_2, ..., d_n]` the resulting tensor `mask` has\ndtype `dtype` and shape `[d_1, d_2, ..., d_n, maxlen]`, with\n\n```\nmask[i_1, i_2, ..., i_n, j] = (j < lengths[i_1, i_2, ..., i_n])\n```\n\nExamples:\n\n```python\ntf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n                                #  [True, True, True, False, False],\n                                #  [True, True, False, False, False]]\n\ntf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],\n                                  #   [True, True, True]],\n                                  #  [[True, True, False],\n                                  #   [False, False, False]]]\n```\n\nArgs:\n  lengths: integer tensor, all its values <= maxlen.\n  maxlen: scalar integer tensor, size of last dimension of returned tensor.\n    Default is the maximum value in `lengths`.\n  dtype: output type of the resulting tensor.\n  name: name of the op.\n\nReturns:\n  A mask tensor of shape `lengths.shape + (maxlen,)`, cast to specified dtype.\nRaises:\n  ValueError: if `maxlen` is not a scalar.", "Library": "TensorFlow"}
{"API_Name": "tf.shape", "Docstring": "Returns a tensor containing the shape of the input tensor.\n\nSee also `tf.size`, `tf.rank`.\n\n`tf.shape` returns a 1-D integer tensor representing the shape of `input`.\nFor a scalar input, the tensor returned has a shape of (0,) and its value is\nthe empty vector (i.e. []).\n\nFor example:\n\n>>> tf.shape(1.)\n<tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>\n\n>>> t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n>>> tf.shape(t)\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 2, 3], dtype=int32)>\n\nNote: When using symbolic tensors, such as when using the Keras API,\ntf.shape() will return the shape of the symbolic tensor.\n\n>>> a = tf.keras.layers.Input((None, 10))\n>>> tf.shape(a)\n<... shape=(3,) dtype=int32...>\n\nIn these cases, using `tf.Tensor.shape` will return more informative results.\n\n>>> a.shape\nTensorShape([None, None, 10])\n\n(The first `None` represents the as yet unknown batch size.)\n\n`tf.shape` and `Tensor.shape` should be identical in eager mode.  Within\n`tf.function` or within a `compat.v1` context, not all dimensions may be\nknown until execution time. Hence, when defining custom layers and models\nfor graph mode, prefer the dynamic `tf.shape(x)` over the static `x.shape`.\n\nArgs:\n  input: A `Tensor` or `SparseTensor`.\n  out_type: (Optional) The specified output type of the operation (`int32` or\n    `int64`). Defaults to `tf.int32`. (Note: there is an experimental\n    flag, `tf_shape_default_int64` that changes the default to `tf.int64`.\n    This is an unsupported, experimental setting that causes known breakages.)\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` of type `out_type`.", "Library": "TensorFlow"}
{"API_Name": "tf.sigmoid", "Docstring": "Computes sigmoid of `x` element-wise.\n\nFormula for calculating $\\mathrm{sigmoid}(x) = y = 1 / (1 + \\exp(-x))$.\n\nFor $x \\in (-\\infty, \\infty)$, $\\mathrm{sigmoid}(x) \\in (0, 1)$.\n\nExample Usage:\n\nIf a positive number is large, then its sigmoid will approach to 1 since the\nformula will be `y = <large_num> / (1 + <large_num>)`\n\n>>> x = tf.constant([0.0, 1.0, 50.0, 100.0])\n>>> tf.math.sigmoid(x)\n<tf.Tensor: shape=(4,), dtype=float32,\nnumpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>\n\nIf a negative number is large, its sigmoid will approach to 0 since the\nformula will be `y = 1 / (1 + <large_num>)`\n\n>>> x = tf.constant([-100.0, -50.0, -1.0, 0.0])\n>>> tf.math.sigmoid(x)\n<tf.Tensor: shape=(4,), dtype=float32, numpy=\narray([0.0000000e+00, 1.9287499e-22, 2.6894143e-01, 0.5],\n      dtype=float32)>\n\nArgs:\n  x: A Tensor with type `float16`, `float32`, `float64`, `complex64`, or\n    `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A Tensor with the same type as `x`.\n\nUsage Example:\n\n>>> x = tf.constant([-128.0, 0.0, 128.0], dtype=tf.float32)\n>>> tf.sigmoid(x)\n<tf.Tensor: shape=(3,), dtype=float32,\nnumpy=array([0. , 0.5, 1. ], dtype=float32)>\n\n@compatibility(scipy)\nEquivalent to scipy.special.expit\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.sin", "Docstring": "Computes sine of x element-wise.\n\n  Given an input tensor, this function computes sine of every\n  element in the tensor. Input range is `(-inf, inf)` and\n  output range is `[-1,1]`.\n\n  ```python\n  x = tf.constant([-float(\"inf\"), -9, -0.5, 1, 1.2, 200, 10, float(\"inf\")])\n  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]\n  ```\n\nArgs:\n  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `x`.", "Library": "TensorFlow"}
{"API_Name": "tf.size", "Docstring": "Returns the size of a tensor.\n\nSee also `tf.shape`.\n\nReturns a 0-D `Tensor` representing the number of elements in `input`\nof type `out_type`. Defaults to tf.int32.\n\nFor example:\n\n>>> t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\n>>> tf.size(t)\n<tf.Tensor: shape=(), dtype=int32, numpy=12>\n\nArgs:\n  input: A `Tensor` or `SparseTensor`.\n  out_type: (Optional) The specified non-quantized numeric output type of the\n    operation. Defaults to `tf.int32`. (Note: there is an experimental\n    flag, `tf_shape_default_int64` that changes the default to `tf.int64`.\n    This is an unsupported, experimental setting that causes known breakages.)\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` of type `out_type`. Defaults to `tf.int32`.\n\n@compatibility(numpy)\nEquivalent to np.size()\n@end_compatibility", "Library": "TensorFlow"}
{"API_Name": "tf.slice", "Docstring": "Extracts a slice from a tensor.\n\nSee also `tf.strided_slice`.\n\nThis operation extracts a slice of size `size` from a tensor `input_` starting\nat the location specified by `begin`. The slice `size` is represented as a\ntensor shape, where `size[i]` is the number of elements of the 'i'th dimension\nof `input_` that you want to slice. The starting location (`begin`) for the\nslice is represented as an offset in each dimension of `input_`. In other\nwords, `begin[i]` is the offset into the i'th dimension of `input_` that you\nwant to slice from.\n\nNote that `tf.Tensor.__getitem__` is typically a more pythonic way to\nperform slices, as it allows you to write `foo[3:7, :-2]` instead of\n`tf.slice(foo, [3, 0], [4, foo.get_shape()[1]-2])`.\n\n`begin` is zero-based; `size` is one-based. If `size[i]` is -1,\nall remaining elements in dimension i are included in the\nslice. In other words, this is equivalent to setting:\n\n`size[i] = input_.dim_size(i) - begin[i]`\n\nThis operation requires that:\n\n`0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n]`\n\nFor example:\n\n```python\nt = tf.constant([[[1, 1, 1], [2, 2, 2]],\n                 [[3, 3, 3], [4, 4, 4]],\n                 [[5, 5, 5], [6, 6, 6]]])\ntf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]\ntf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],\n                                   #   [4, 4, 4]]]\ntf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],\n                                   #  [[5, 5, 5]]]\n```\n\nArgs:\n  input_: A `Tensor`.\n  begin: An `int32` or `int64` `Tensor`.\n  size: An `int32` or `int64` `Tensor`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor` the same type as `input_`.", "Library": "TensorFlow"}
{"API_Name": "tf.sort", "Docstring": "Sorts a tensor.\n\nUsage:\n\n>>> a = [1, 10, 26.9, 2.8, 166.32, 62.3]\n>>> tf.sort(a).numpy()\narray([  1.  ,   2.8 ,  10.  ,  26.9 ,  62.3 , 166.32], dtype=float32)\n\n>>> tf.sort(a, direction='DESCENDING').numpy()\narray([166.32,  62.3 ,  26.9 ,  10.  ,   2.8 ,   1.  ], dtype=float32)\n\nFor multidimensional inputs you can control which axis the sort is applied\nalong. The default `axis=-1` sorts the innermost axis.\n\n>>> mat = [[3,2,1],\n...        [2,1,3],\n...        [1,3,2]]\n>>> tf.sort(mat, axis=-1).numpy()\narray([[1, 2, 3],\n       [1, 2, 3],\n       [1, 2, 3]], dtype=int32)\n>>> tf.sort(mat, axis=0).numpy()\narray([[1, 1, 1],\n       [2, 2, 2],\n       [3, 3, 3]], dtype=int32)\n\nSee also:\n\n  * `tf.argsort`: Like sort, but it returns the sort indices.\n  * `tf.math.top_k`: A partial sort that returns a fixed number of top values\n    and corresponding indices.\n\n\nArgs:\n  values: 1-D or higher **numeric** `Tensor`.\n  axis: The axis along which to sort. The default is -1, which sorts the last\n    axis.\n  direction: The direction in which to sort the values (`'ASCENDING'` or\n    `'DESCENDING'`).\n  name: Optional name for the operation.\n\nReturns:\n  A `Tensor` with the same dtype and shape as `values`, with the elements\n      sorted along the given `axis`.\n\nRaises:\n  tf.errors.InvalidArgumentError: If the `values.dtype` is not a `float` or\n      `int` type.\n  ValueError: If axis is not a constant scalar, or the direction is invalid.", "Library": "TensorFlow"}
{"API_Name": "tf.split", "Docstring": "Splits a tensor `value` into a list of sub tensors.\n\nSee also `tf.unstack`.\n\nIf `num_or_size_splits` is an `int`,  then it splits `value` along the\ndimension `axis` into `num_or_size_splits` smaller tensors. This requires that\n`value.shape[axis]` is divisible by `num_or_size_splits`.\n\nIf `num_or_size_splits` is a 1-D Tensor (or list), then `value` is split into\n`len(num_or_size_splits)` elements. The shape of the `i`-th\nelement has the same size as the `value` except along dimension `axis` where\nthe size is `num_or_size_splits[i]`.\n\nFor example:\n\n>>> x = tf.Variable(tf.random.uniform([5, 30], -1, 1))\n>>>\n>>> # Split `x` into 3 tensors along dimension 1\n>>> s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)\n>>> tf.shape(s0).numpy()\narray([ 5, 10], dtype=int32)\n>>>\n>>> # Split `x` into 3 tensors with sizes [4, 15, 11] along dimension 1\n>>> split0, split1, split2 = tf.split(x, [4, 15, 11], 1)\n>>> tf.shape(split0).numpy()\narray([5, 4], dtype=int32)\n>>> tf.shape(split1).numpy()\narray([ 5, 15], dtype=int32)\n>>> tf.shape(split2).numpy()\narray([ 5, 11], dtype=int32)\n\nArgs:\n  value: The `Tensor` to split.\n  num_or_size_splits: Either an `int` indicating the number of splits\n    along `axis` or a 1-D integer `Tensor` or Python list containing the sizes\n    of each output tensor along `axis`. If an `int`, then it must evenly\n    divide `value.shape[axis]`; otherwise the sum of sizes along the split\n    axis must match that of the `value`.\n  axis: An `int` or scalar `int32` `Tensor`. The dimension along which\n    to split. Must be in the range `[-rank(value), rank(value))`. Defaults to\n    0.\n  num: Optional, an `int`, used to specify the number of outputs when it\n    cannot be inferred from the shape of `size_splits`.\n  name: A name for the operation (optional).\n\nReturns:\n  if `num_or_size_splits` is an `int` returns a list of\n  `num_or_size_splits` `Tensor` objects; if `num_or_size_splits` is a 1-D\n  list or 1-D `Tensor` returns `num_or_size_splits.get_shape[0]`\n  `Tensor` objects resulting from splitting `value`.\n\nRaises:\n  ValueError: If `num` is unspecified and cannot be inferred.\n  ValueError: If `num_or_size_splits` is a scalar `Tensor`.", "Library": "TensorFlow"}
{"API_Name": "tf.sqrt", "Docstring": "Computes element-wise square root of the input tensor.\n\nNote: This operation does not support integer types.\n\n>>> x = tf.constant([[4.0], [16.0]])\n>>> tf.sqrt(x)\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n  array([[2.],\n         [4.]], dtype=float32)>\n>>> y = tf.constant([[-4.0], [16.0]])\n>>> tf.sqrt(y)\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n  array([[nan],\n         [ 4.]], dtype=float32)>\n>>> z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)\n>>> tf.sqrt(z)\n<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=\n  array([[0.0+1.j],\n         [4.0+0.j]])>\n\nNote: In order to support complex type, please provide an input tensor\nof `complex64` or `complex128`.\n\nArgs:\n  x: A `tf.Tensor` of type `bfloat16`, `half`, `float32`, `float64`,\n    `complex64`, `complex128`\n  name: A name for the operation (optional).\n\nReturns:\n  A `tf.Tensor` of same size, type and sparsity as `x`.\n\n  If `x` is a `SparseTensor`, returns\n  `SparseTensor(x.indices, tf.math.sqrt(x.values, ...), x.dense_shape)`", "Library": "TensorFlow"}
{"API_Name": "tf.square", "Docstring": "Computes square of x element-wise.\n\nI.e., \\\\(y = x * x = x^2\\\\).\n\n>>> tf.math.square([-2., 0., 3.])\n<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>\n\nArgs:\n  x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `uint8`, `uint16`, `uint32`, `uint64`, `complex64`, `complex128`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `x`.\n\n  If `x` is a `SparseTensor`, returns\n  `SparseTensor(x.indices, tf.math.square(x.values, ...), x.dense_shape)`", "Library": "TensorFlow"}
{"API_Name": "tf.squeeze", "Docstring": "Removes dimensions of size 1 from the shape of a tensor.\n\nGiven a tensor `input`, this operation returns a tensor of the same type with\nall dimensions of size 1 removed. If you don't want to remove all size 1\ndimensions, you can remove specific size 1 dimensions by specifying\n`axis`.\n\nFor example:\n\n```python\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\ntf.shape(tf.squeeze(t))  # [2, 3]\n```\n\nOr, to remove specific size 1 dimensions:\n\n```python\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\ntf.shape(tf.squeeze(t, [2, 4]))  # [1, 2, 3, 1]\n```\n\nUnlike the older op `tf.compat.v1.squeeze`, this op does not accept a\ndeprecated `squeeze_dims` argument.\n\nNote: if `input` is a `tf.RaggedTensor`, then this operation takes `O(N)`\ntime, where `N` is the number of elements in the squeezed dimensions.\n\nNote: If squeeze is performed on dimensions of unknown sizes, then the\nreturned Tensor will be of unknown shape. A common situation is when the\nfirst (batch) dimension is of size `None`, `tf.squeeze` returns\n`<unknown>` shape which may be a surprise. Specify the `axis=` argument\nto get the expected result, as illustrated in the following example:\n\n```python\n@tf.function\ndef func(x):\n  print('x.shape:', x.shape)\n  known_axes = [i for i, size in enumerate(x.shape) if size == 1]\n  y = tf.squeeze(x, axis=known_axes)\n  print('shape of tf.squeeze(x, axis=known_axes):', y.shape)\n  y = tf.squeeze(x)\n  print('shape of tf.squeeze(x):', y.shape)\n  return 0\n\n_ = func.get_concrete_function(tf.TensorSpec([None, 1, 2], dtype=tf.int32))\n# Output is.\n# x.shape: (None, 1, 2)\n# shape of tf.squeeze(x, axis=known_axes): (None, 2)\n# shape of tf.squeeze(x): <unknown>\n```\n\nArgs:\n  input: A `Tensor`. The `input` to squeeze.\n  axis: An optional list of `ints`. Defaults to `[]`. If specified, only\n    squeezes the dimensions listed. The dimension index starts at 0. It is an\n    error to squeeze a dimension that is not 1. Must be in the range\n    `[-rank(input), rank(input))`. Must be specified if `input` is a\n    `RaggedTensor`.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `input`.\n  Contains the same data as `input`, but has one or more dimensions of\n  size 1 removed.\n\nRaises:\n  ValueError: The input cannot be converted to a tensor, or the specified\n    axis cannot be squeezed.", "Library": "TensorFlow"}
{"API_Name": "tf.stack", "Docstring": "Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n\nSee also `tf.concat`, `tf.tile`, `tf.repeat`.\n\nPacks the list of tensors in `values` into a tensor with rank one higher than\neach tensor in `values`, by packing them along the `axis` dimension.\nGiven a list of length `N` of tensors of shape `(A, B, C)`;\n\nif `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\nif `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\nEtc.\n\nFor example:\n\n>>> x = tf.constant([1, 4])\n>>> y = tf.constant([2, 5])\n>>> z = tf.constant([3, 6])\n>>> tf.stack([x, y, z])\n<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)>\n>>> tf.stack([x, y, z], axis=1)\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)>\n\nThis is the opposite of unstack.  The numpy equivalent is `np.stack`\n\n>>> np.array_equal(np.stack([x, y, z]), tf.stack([x, y, z]))\nTrue\n\nArgs:\n  values: A list of `Tensor` objects with the same shape and type.\n  axis: An `int`. The axis to stack along. Defaults to the first dimension.\n    Negative values wrap around, so the valid range is `[-(R+1), R+1)`.\n  name: A name for this operation (optional).\n\nReturns:\n  output: A stacked `Tensor` with the same type as `values`.\n\nRaises:\n  ValueError: If `axis` is out of the range [-(R+1), R+1).", "Library": "TensorFlow"}
{"API_Name": "tf.Tensor", "Docstring": "A `tf.Tensor` represents a multidimensional array of elements.\n\nAll elements are of a single known data type.\n\nWhen writing a TensorFlow program, the main object that is\nmanipulated and passed around is the `tf.Tensor`.\n\nA `tf.Tensor` has the following properties:\n\n* a single data type (float32, int32, or string, for example)\n* a shape\n\nTensorFlow supports eager execution and graph execution.  In eager\nexecution, operations are evaluated immediately.  In graph\nexecution, a computational graph is constructed for later\nevaluation.\n\nTensorFlow defaults to eager execution.  In the example below, the\nmatrix multiplication results are calculated immediately.\n\n>>> # Compute some values using a Tensor\n>>> c = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n>>> d = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n>>> e = tf.matmul(c, d)\n>>> print(e)\ntf.Tensor(\n[[1. 3.]\n [3. 7.]], shape=(2, 2), dtype=float32)\n\nNote that during eager execution, you may discover your `Tensors` are actually\nof type `EagerTensor`.  This is an internal detail, but it does give you\naccess to a useful function, `numpy`:\n\n>>> type(e)\n<class '...ops.EagerTensor'>\n>>> print(e.numpy())\n  [[1. 3.]\n   [3. 7.]]\n\nIn TensorFlow, `tf.function`s are a common way to define graph execution.\n\nA Tensor's shape (that is, the rank of the Tensor and the size of\neach dimension) may not always be fully known.  In `tf.function`\ndefinitions, the shape may only be partially known.\n\nMost operations produce tensors of fully-known shapes if the shapes of their\ninputs are also fully known, but in some cases it's only possible to find the\nshape of a tensor at execution time.\n\nA number of specialized tensors are available: see `tf.Variable`,\n`tf.constant`, `tf.placeholder`, `tf.sparse.SparseTensor`, and\n`tf.RaggedTensor`.\n\nCaution: when constructing a tensor from a numpy array or pandas dataframe\nthe underlying buffer may be re-used:\n\n```python\na = np.array([1, 2, 3])\nb = tf.constant(a)\na[0] = 4\nprint(b)  # tf.Tensor([4 2 3], shape=(3,), dtype=int64)\n```\n\nNote: this is an implementation detail that is subject to change and users\nshould not rely on this behaviour.\n\nFor more on Tensors, see the [guide](https://tensorflow.org/guide/tensor).", "Library": "TensorFlow"}
{"API_Name": "tf.tensor_scatter_nd_add", "Docstring": "Adds sparse `updates` to an existing tensor according to `indices`.\n\nThis operation creates a new tensor by adding sparse `updates` to the passed\nin `tensor`.\nThis operation is very similar to `tf.compat.v1.scatter_nd_add`, except that the\nupdates are added onto an existing tensor (as opposed to a variable). If the\nmemory for the existing tensor cannot be re-used, a copy is made and updated.\n\n`indices` is an integer tensor containing indices into a new tensor of shape\n`tensor.shape`.  The last dimension of `indices` can be at most the rank of\n`tensor.shape`:\n\n```\nindices.shape[-1] <= tensor.shape.rank\n```\n\nThe last dimension of `indices` corresponds to indices into elements\n(if `indices.shape[-1] = tensor.shape.rank`) or slices\n(if `indices.shape[-1] < tensor.shape.rank`) along dimension\n`indices.shape[-1]` of `tensor.shape`.  `updates` is a tensor with shape\n\n```\nindices.shape[:-1] + tensor.shape[indices.shape[-1]:]\n```\n\nThe simplest form of `tensor_scatter_nd_add` is to add individual elements to a\ntensor by index. For example, say we want to add 4 elements in a rank-1\ntensor with 8 elements.\n\nIn Python, this scatter add operation would look like this:\n\n>>> indices = tf.constant([[4], [3], [1], [7]])\n>>> updates = tf.constant([9, 10, 11, 12])\n>>> tensor = tf.ones([8], dtype=tf.int32)\n>>> updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n>>> updated\n<tf.Tensor: shape=(8,), dtype=int32,\nnumpy=array([ 1, 12,  1, 11, 10,  1,  1, 13], dtype=int32)>\n\nWe can also, insert entire slices of a higher rank tensor all at once. For\nexample, if we wanted to insert two slices in the first dimension of a\nrank-3 tensor with two matrices of new values.\n\nIn Python, this scatter add operation would look like this:\n\n>>> indices = tf.constant([[0], [2]])\n>>> updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n...                         [7, 7, 7, 7], [8, 8, 8, 8]],\n...                        [[5, 5, 5, 5], [6, 6, 6, 6],\n...                         [7, 7, 7, 7], [8, 8, 8, 8]]])\n>>> tensor = tf.ones([4, 4, 4],dtype=tf.int32)\n>>> updated = tf.tensor_scatter_nd_add(tensor, indices, updates)\n>>> updated\n<tf.Tensor: shape=(4, 4, 4), dtype=int32,\nnumpy=array([[[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],\n             [[6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8], [9, 9, 9, 9]],\n             [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]], dtype=int32)>\n\nNote: on CPU, if an out of bound index is found, an error is returned.\nOn GPU, if an out of bound index is found, the index is ignored.\n\nArgs:\n  tensor: A `Tensor`. Tensor to copy/update.\n  indices: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    Index tensor.\n  updates: A `Tensor`. Must have the same type as `tensor`.\n    Updates to scatter into output.\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `tensor`.", "Library": "TensorFlow"}
{"API_Name": "tf.tile", "Docstring": "Constructs a tensor by tiling a given tensor.\n\nThis operation creates a new tensor by replicating `input` `multiples` times.\nThe output tensor's i'th dimension has `input.dims(i) * multiples[i]` elements,\nand the values of `input` are replicated `multiples[i]` times along the 'i'th\ndimension. For example, tiling `[a b c d]` by `[2]` produces\n`[a b c d a b c d]`.\n\n>>> a = tf.constant([[1,2,3],[4,5,6]], tf.int32)\n>>> b = tf.constant([1,2], tf.int32)\n>>> tf.tile(a, b)\n<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6]], dtype=int32)>\n>>> c = tf.constant([2,1], tf.int32)\n>>> tf.tile(a, c)\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6],\n       [1, 2, 3],\n       [4, 5, 6]], dtype=int32)>\n>>> d = tf.constant([2,2], tf.int32)\n>>> tf.tile(a, d)\n<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\narray([[1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6],\n       [1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6]], dtype=int32)>\n\nArgs:\n  input: A `Tensor`. Can be of any rank.\n  multiples: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n    1-D. Length must be the same as the number of dimensions in `input`\n  name: A name for the operation (optional).\n\nReturns:\n  A `Tensor`. Has the same type as `input`.", "Library": "TensorFlow"}
{"API_Name": "tf.transpose", "Docstring": "Transposes `a`, where `a` is a Tensor.\n\nPermutes the dimensions according to the value of `perm`.\n\nThe returned tensor's dimension `i` will correspond to the input dimension\n`perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank\nof the input tensor. Hence, by default, this operation performs a regular\nmatrix transpose on 2-D input Tensors.\n\nIf conjugate is `True` and `a.dtype` is either `complex64` or `complex128`\nthen the values of `a` are conjugated and transposed.\n\n@compatibility(numpy)\nIn `numpy` transposes are memory-efficient constant time operations as they\nsimply return a new view of the same data with adjusted `strides`.\n\nTensorFlow does not support strides, so `transpose` returns a new tensor with\nthe items permuted.\n@end_compatibility\n\nFor example:\n\n>>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n>>> tf.transpose(x)\n<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[1, 4],\n       [2, 5],\n       [3, 6]], dtype=int32)>\n\nEquivalently, you could call `tf.transpose(x, perm=[1, 0])`.\n\nIf `x` is complex, setting conjugate=True gives the conjugate transpose:\n\n>>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n...                  [4 + 4j, 5 + 5j, 6 + 6j]])\n>>> tf.transpose(x, conjugate=True)\n<tf.Tensor: shape=(3, 2), dtype=complex128, numpy=\narray([[1.-1.j, 4.-4.j],\n       [2.-2.j, 5.-5.j],\n       [3.-3.j, 6.-6.j]])>\n\n'perm' is more useful for n-dimensional tensors where n > 2:\n\n>>> x = tf.constant([[[ 1,  2,  3],\n...                   [ 4,  5,  6]],\n...                  [[ 7,  8,  9],\n...                   [10, 11, 12]]])\n\nAs above, simply calling `tf.transpose` will default to `perm=[2,1,0]`.\n\nTo take the transpose of the matrices in dimension-0 (such as when you are\ntransposing matrices where 0 is the batch dimension), you would set\n`perm=[0,2,1]`.\n\n>>> tf.transpose(x, perm=[0, 2, 1])\n<tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\narray([[[ 1,  4],\n        [ 2,  5],\n        [ 3,  6]],\n        [[ 7, 10],\n        [ 8, 11],\n        [ 9, 12]]], dtype=int32)>\n\nNote: This has a shorthand `linalg.matrix_transpose`):\n\nArgs:\n  a: A `Tensor`.\n  perm: A permutation of the dimensions of `a`.  This should be a vector.\n  conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n    to tf.math.conj(tf.transpose(input)).\n  name: A name for the operation (optional).\n\nReturns:\n  A transposed `Tensor`.", "Library": "TensorFlow"}
{"API_Name": "tf.unique", "Docstring": "Finds unique elements in a 1-D tensor.\n\nThis operation returns a tensor `y` containing all of the unique elements of `x`\nsorted in the same order that they occur in `x`; `x` does not need to be sorted.\nThis operation also returns a tensor `idx` the same size as `x` that contains\nthe index of each value of `x` in the unique output `y`. In other words:\n\n`y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n\nExamples:\n\n```\n# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx = unique(x)\ny ==> [1, 2, 4, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\n```\n\n```\n# tensor 'x' is [4, 5, 1, 2, 3, 3, 4, 5]\ny, idx = unique(x)\ny ==> [4, 5, 1, 2, 3]\nidx ==> [0, 1, 2, 3, 4, 4, 0, 1]\n```\n\nArgs:\n  x: A `Tensor`. 1-D.\n  out_idx: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.\n  name: A name for the operation (optional).\n\nReturns:\n  A tuple of `Tensor` objects (y, idx).\n\n  y: A `Tensor`. Has the same type as `x`.\n  idx: A `Tensor` of type `out_idx`.", "Library": "TensorFlow"}
{"API_Name": "tf.unique_with_counts", "Docstring": "Finds unique elements in a 1-D tensor.\n\nThis operation returns a tensor `y` containing all of the unique elements of `x`\nsorted in the same order that they occur in `x`. This operation also returns a\ntensor `idx` the same size as `x` that contains the index of each value of `x`\nin the unique output `y`. Finally, it returns a third tensor `count` that\ncontains the count of each element of `y` in `x`. In other words:\n\n`y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1]`\n\nFor example:\n\n```\n# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]\ny, idx, count = unique_with_counts(x)\ny ==> [1, 2, 4, 7, 8]\nidx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]\ncount ==> [2, 1, 3, 1, 2]\n```\n\nArgs:\n  x: A `Tensor`. 1-D.\n  out_idx: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.\n  name: A name for the operation (optional).\n\nReturns:\n  A tuple of `Tensor` objects (y, idx, count).\n\n  y: A `Tensor`. Has the same type as `x`.\n  idx: A `Tensor` of type `out_idx`.\n  count: A `Tensor` of type `out_idx`.", "Library": "TensorFlow"}
{"API_Name": "tf.Variable", "Docstring": "See the [variable guide](https://tensorflow.org/guide/variable).\n\nA variable maintains shared, persistent state manipulated by a program.\n\nThe `Variable()` constructor requires an initial value for the variable, which\ncan be a `Tensor` of any type and shape. This initial value defines the type\nand shape of the variable. After construction, the type and shape of the\nvariable are fixed. The value can be changed using one of the assign methods.\n\n>>> v = tf.Variable(1.)\n>>> v.assign(2.)\n<tf.Variable ... shape=() dtype=float32, numpy=2.0>\n>>> v.assign_add(0.5)\n<tf.Variable ... shape=() dtype=float32, numpy=2.5>\n\nThe `shape` argument to `Variable`'s constructor allows you to construct a\nvariable with a less defined shape than its `initial_value`:\n\n>>> v = tf.Variable(1., shape=tf.TensorShape(None))\n>>> v.assign([[1.]])\n<tf.Variable ... shape=<unknown> dtype=float32, numpy=array([[1.]], ...)>\n\nJust like any `Tensor`, variables created with `Variable()` can be used as\ninputs to operations. Additionally, all the operators overloaded for the\n`Tensor` class are carried over to variables.\n\n>>> w = tf.Variable([[1.], [2.]])\n>>> x = tf.constant([[3., 4.]])\n>>> tf.matmul(w, x)\n<tf.Tensor:... shape=(2, 2), ... numpy=\n  array([[3., 4.],\n         [6., 8.]], dtype=float32)>\n>>> tf.sigmoid(w + x)\n<tf.Tensor:... shape=(2, 2), ...>\n\nWhen building a machine learning model it is often convenient to distinguish\nbetween variables holding trainable model parameters and other variables such\nas a `step` variable used to count training steps. To make this easier, the\nvariable constructor supports a `trainable=<bool>`\nparameter. `tf.GradientTape` watches trainable variables by default:\n\n>>> with tf.GradientTape(persistent=True) as tape:\n...   trainable = tf.Variable(1.)\n...   non_trainable = tf.Variable(2., trainable=False)\n...   x1 = trainable * 2.\n...   x2 = non_trainable * 3.\n>>> tape.gradient(x1, trainable)\n<tf.Tensor:... shape=(), dtype=float32, numpy=2.0>\n>>> assert tape.gradient(x2, non_trainable) is None  # Unwatched\n\nVariables are automatically tracked when assigned to attributes of types\ninheriting from `tf.Module`.\n\n>>> m = tf.Module()\n>>> m.v = tf.Variable([1.])\n>>> m.trainable_variables\n(<tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)>,)\n\nThis tracking then allows saving variable values to\n[training checkpoints](https://www.tensorflow.org/guide/checkpoint), or to\n[SavedModels](https://www.tensorflow.org/guide/saved_model) which include\nserialized TensorFlow graphs.\n\nVariables are often captured and manipulated by `tf.function`s. This works the\nsame way the un-decorated function would have:\n\n>>> v = tf.Variable(0.)\n>>> read_and_decrement = tf.function(lambda: v.assign_sub(0.1))\n>>> read_and_decrement()\n<tf.Tensor: shape=(), dtype=float32, numpy=-0.1>\n>>> read_and_decrement()\n<tf.Tensor: shape=(), dtype=float32, numpy=-0.2>\n\nVariables created inside a `tf.function` must be owned outside the function\nand be created only once:\n\n>>> class M(tf.Module):\n...   @tf.function\n...   def __call__(self, x):\n...     if not hasattr(self, \"v\"):  # Or set self.v to None in __init__\n...       self.v = tf.Variable(x)\n...     return self.v * x\n>>> m = M()\n>>> m(2.)\n<tf.Tensor: shape=(), dtype=float32, numpy=4.0>\n>>> m(3.)\n<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n>>> m.v\n<tf.Variable ... shape=() dtype=float32, numpy=2.0>\n\nSee the `tf.function` documentation for details.", "Library": "TensorFlow"}
{"API_Name": "tf.where", "Docstring": "Returns the indices of non-zero elements, or multiplexes `x` and `y`.\n\nThis operation has two modes:\n\n1. **Return the indices of non-zero elements** - When only\n   `condition` is provided the result is an `int64` tensor where each row is\n   the index of a non-zero element of `condition`. The result's shape\n   is `[tf.math.count_nonzero(condition), tf.rank(condition)]`.\n2. **Multiplex `x` and `y`** - When both `x` and `y` are provided the\n   result has the shape of `x`, `y`, and `condition` broadcast together. The\n   result is taken from `x` where `condition` is non-zero\n   or `y` where `condition` is zero.\n\n#### 1. Return the indices of non-zero elements\n\nNote: In this mode `condition` can have a dtype of `bool` or any numeric\ndtype.\n\nIf `x` and `y` are not provided (both are None):\n\n`tf.where` will return the indices of `condition` that are non-zero,\nin the form of a 2-D tensor with shape `[n, d]`, where `n` is the number of\nnon-zero elements in `condition` (`tf.count_nonzero(condition)`), and `d` is\nthe number of axes of `condition` (`tf.rank(condition)`).\n\nIndices are output in row-major order. The `condition` can have a `dtype` of\n`tf.bool`, or any numeric `dtype`.\n\nHere `condition` is a 1-axis `bool` tensor with 2 `True` values. The result\nhas a shape of `[2,1]`\n\n>>> tf.where([True, False, False, True]).numpy()\narray([[0],\n       [3]])\n\nHere `condition` is a 2-axis integer tensor, with 3 non-zero values. The\nresult has a shape of `[3, 2]`.\n\n>>> tf.where([[1, 0, 0], [1, 0, 1]]).numpy()\narray([[0, 0],\n       [1, 0],\n       [1, 2]])\n\nHere `condition` is a 3-axis float tensor, with 5 non-zero values. The output\nshape is `[5, 3]`.\n\n>>> float_tensor = [[[0.1, 0], [0, 2.2], [3.5, 1e6]],\n...                 [[0,   0], [0,   0], [99,    0]]]\n>>> tf.where(float_tensor).numpy()\narray([[0, 0, 0],\n       [0, 1, 1],\n       [0, 2, 0],\n       [0, 2, 1],\n       [1, 2, 0]])\n\nThese indices are the same that `tf.sparse.SparseTensor` would use to\nrepresent the condition tensor:\n\n>>> sparse = tf.sparse.from_dense(float_tensor)\n>>> sparse.indices.numpy()\narray([[0, 0, 0],\n       [0, 1, 1],\n       [0, 2, 0],\n       [0, 2, 1],\n       [1, 2, 0]])\n\nA complex number is considered non-zero if either the real or imaginary\ncomponent is non-zero:\n\n>>> tf.where([complex(0.), complex(1.), 0+1j, 1+1j]).numpy()\narray([[1],\n       [2],\n       [3]])\n\n#### 2. Multiplex `x` and `y`\n\nNote: In this mode `condition` must have a dtype of `bool`.\n\nIf `x` and `y` are also provided (both have non-None values) the `condition`\ntensor acts as a mask that chooses whether the corresponding\nelement / row in the output should be taken from `x` (if the element in\n`condition` is `True`) or `y` (if it is `False`).\n\nThe shape of the result is formed by\n[broadcasting](https://docs.scipy.org/doc/numpy/reference/ufuncs.html)\ntogether the shapes of `condition`, `x`, and `y`.\n\nWhen all three inputs have the same size, each is handled element-wise.\n\n>>> tf.where([True, False, False, True],\n...          [1, 2, 3, 4],\n...          [100, 200, 300, 400]).numpy()\narray([  1, 200, 300,   4], dtype=int32)\n\nThere are two main rules for broadcasting:\n\n1. If a tensor has fewer axes than the others, length-1 axes are added to the\n   left of the shape.\n2. Axes with length-1 are streched to match the coresponding axes of the other\n   tensors.\n\nA length-1 vector is streched to match the other vectors:\n\n>>> tf.where([True, False, False, True], [1, 2, 3, 4], [100]).numpy()\narray([  1, 100, 100,   4], dtype=int32)\n\nA scalar is expanded to match the other arguments:\n\n>>> tf.where([[True, False], [False, True]], [[1, 2], [3, 4]], 100).numpy()\narray([[  1, 100], [100,   4]], dtype=int32)\n>>> tf.where([[True, False], [False, True]], 1, 100).numpy()\narray([[  1, 100], [100,   1]], dtype=int32)\n\nA scalar `condition` returns the complete `x` or `y` tensor, with\nbroadcasting applied.\n\n>>> tf.where(True, [1, 2, 3, 4], 100).numpy()\narray([1, 2, 3, 4], dtype=int32)\n>>> tf.where(False, [1, 2, 3, 4], 100).numpy()\narray([100, 100, 100, 100], dtype=int32)\n\nFor a non-trivial example of broadcasting, here `condition` has a shape of\n`[3]`, `x` has a shape of `[3,3]`, and `y` has a shape of `[3,1]`.\nBroadcasting first expands the shape of `condition` to `[1,3]`. The final\nbroadcast shape is `[3,3]`. `condition` will select columns from `x` and `y`.\nSince `y` only has one column, all columns from `y` will be identical.\n\n>>> tf.where([True, False, True],\n...          x=[[1, 2, 3],\n...             [4, 5, 6],\n...             [7, 8, 9]],\n...          y=[[100],\n...             [200],\n...             [300]]\n... ).numpy()\narray([[ 1, 100, 3],\n       [ 4, 200, 6],\n       [ 7, 300, 9]], dtype=int32)\n\nNote that if the gradient of either branch of the `tf.where` generates\na `NaN`, then the gradient of the entire `tf.where` will be `NaN`. This is\nbecause the gradient calculation for `tf.where` combines the two branches, for\nperformance reasons.\n\nA workaround is to use an inner `tf.where` to ensure the function has\nno asymptote, and to avoid computing a value whose gradient is `NaN` by\nreplacing dangerous inputs with safe inputs.\n\nInstead of this,\n\n>>> x = tf.constant(0., dtype=tf.float32)\n>>> with tf.GradientTape() as tape:\n...   tape.watch(x)\n...   y = tf.where(x < 1., 0., 1. / x)\n>>> print(tape.gradient(y, x))\ntf.Tensor(nan, shape=(), dtype=float32)\n\nAlthough, the `1. / x` values are never used, its gradient is a `NaN` when\n`x = 0`. Instead, we should guard that with another `tf.where`\n\n>>> x = tf.constant(0., dtype=tf.float32)\n>>> with tf.GradientTape() as tape:\n...   tape.watch(x)\n...   safe_x = tf.where(tf.equal(x, 0.), 1., x)\n...   y = tf.where(x < 1., 0., 1. / safe_x)\n>>> print(tape.gradient(y, x))\ntf.Tensor(0.0, shape=(), dtype=float32)\n\nSee also:\n\n* `tf.sparse` - The indices returned by the first form of `tf.where` can be\n   useful in `tf.sparse.SparseTensor` objects.\n* `tf.gather_nd`, `tf.scatter_nd`, and related ops - Given the\n  list of indices returned from `tf.where` the `scatter` and `gather` family\n  of ops can be used fetch values or insert values at those indices.\n* `tf.strings.length` - `tf.string` is not an allowed dtype for the\n  `condition`. Use the string length instead.\n\nArgs:\n  condition: A `tf.Tensor` of dtype bool, or any numeric dtype. `condition`\n    must have dtype `bool` when `x` and `y` are provided.\n  x: If provided, a Tensor which is of the same type as `y`, and has a shape\n    broadcastable with `condition` and `y`.\n  y: If provided, a Tensor which is of the same type as `x`, and has a shape\n    broadcastable with `condition` and `x`.\n  name: A name of the operation (optional).\n\nReturns:\n  If `x` and `y` are provided:\n    A `Tensor` with the same type as `x` and `y`, and shape that\n    is broadcast from `condition`, `x`, and `y`.\n  Otherwise, a `Tensor` with shape `[tf.math.count_nonzero(condition),\n  tf.rank(condition)]`.\n\nRaises:\n  ValueError: When exactly one of `x` or `y` is non-None, or the shapes\n    are not all broadcastable.", "Library": "TensorFlow"}
{"API_Name": "tf.zeros", "Docstring": "Creates a tensor with all elements set to zero.\n\nSee also `tf.zeros_like`, `tf.ones`, `tf.fill`, `tf.eye`.\n\nThis operation returns a tensor of type `dtype` with shape `shape` and\nall elements set to zero.\n\n>>> tf.zeros([3, 4], tf.int32)\n<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]], dtype=int32)>\n\nArgs:\n  shape: A `list` of integers, a `tuple` of integers, or a 1-D `Tensor` of\n    type `int32`.\n  dtype: The DType of an element in the resulting `Tensor`.\n  name: Optional string. A name for the operation.\n  layout: Optional, `tf.experimental.dtensor.Layout`. If provided, the result\n    is a [DTensor](https://www.tensorflow.org/guide/dtensor_overview) with the\n    provided layout.\n\nReturns:\n  A `Tensor` with all elements set to zero.", "Library": "TensorFlow"}
{"API_Name": "tf.zeros_like", "Docstring": "Creates a tensor with all elements set to zero.\n\nSee also `tf.zeros`.\n\nGiven a single tensor or array-like object (`input`), this operation returns\na tensor of the same type and shape as `input` with all elements set to zero.\nOptionally, you can use `dtype` to specify a new type for the returned tensor.\n\nNote that the layout of the input tensor is not preserved if the op\nis used inside tf.function. To obtain a tensor with the same layout as the\ninput, chain the returned value to a `dtensor.relayout_like`.\n\nExamples:\n\n  >>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n  >>> tf.zeros_like(tensor)\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[0, 0, 0],\n         [0, 0, 0]], dtype=int32)>\n\n  >>> tf.zeros_like(tensor, dtype=tf.float32)\n  <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n  array([[0., 0., 0.],\n         [0., 0., 0.]], dtype=float32)>\n\n  >>> tf.zeros_like([[1, 2, 3], [4, 5, 6]])\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[0, 0, 0],\n         [0, 0, 0]], dtype=int32)>\n\nArgs:\n  input: A `Tensor` or array-like object.\n  dtype: A type for the returned `Tensor`. Must be `float16`, `float32`,\n    `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`,\n    `complex64`, `complex128`, `bool` or `string` (optional).\n  name: A name for the operation (optional).\n  layout: Optional, `tf.experimental.dtensor.Layout`. If provided, the result\n    is a [DTensor](https://www.tensorflow.org/guide/dtensor_overview) with the\n    provided layout.\n\nReturns:\n  A `Tensor` with all elements set to zero.", "Library": "TensorFlow"}
{"API_Name": "tf.Variable.assign", "Docstring": "Assigns a new value to the variable.\n\nThis is essentially a shortcut for `assign(self, value)`.\n\nArgs:\n  value: A `Tensor`. The new value for this variable.\n  use_locking: If `True`, use locking during the assignment.\n  name: The name of the operation to be created\n  read_value: if True, will return something which evaluates to the new\n    value of the variable; if False will return the assign op.\n\nReturns:\n  The updated variable. If `read_value` is false, instead returns None in\n  Eager mode and the assign op in graph mode.", "Library": "TensorFlow"}
