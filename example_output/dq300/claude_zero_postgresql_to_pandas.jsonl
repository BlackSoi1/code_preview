{"question_id": 0, "response": "result = (customers['Currency'].value_counts()['EUR'] / customers['Currency'].value_counts()['CZK']).astype(float)"}
{"question_id": 1, "response": "result = customers[customers['Segment'] == 'LAM'].merge(yearmonth, on='CustomerID').loc[yearmonth['Date'].str[:4] == '2012'].groupby('CustomerID')['Consumption'].sum().sort_values(na_position='first').index[0]"}
{"question_id": 2, "response": "result = (yearmonth.merge(customers, on='CustomerID')\n         .query(\"Date.str[:4] == '2013' and Segment == 'SME'\")\n         .groupby('CustomerID')['Consumption']\n         .sum()\n         .mean() / 12)"}
{"question_id": 4, "response": "result = (customers.merge(yearmonth, on='CustomerID')\n         .loc[lambda x: x['Currency'] == 'CZK']\n         .assign(Year=lambda x: x['Date'].str[:4])\n         .groupby('Year')\n         .agg({'Consumption': 'sum'})\n         .sort_values('Consumption', ascending=False, na_position='last')\n         .head(1)\n         .index\n         .to_list()[0])"}
{"question_id": 5, "response": "result = yearmonth.merge(customers, on='CustomerID', how='inner')\\\n    .loc[lambda x: (x['Date'].str[:4] == '2013') & (x['Segment'] == 'SME')]\\\n    .assign(month=lambda x: x['Date'].str[4:6])\\\n    .groupby('month')['Consumption']\\\n    .sum()\\\n    .sort_values(ascending=False, na_position='last')\\\n    .index[0]"}
{"question_id": 6, "response": "min_consumption = yearmonth['Consumption'].min()\n\nfiltered_data = customers.merge(yearmonth, on='CustomerID')\nfiltered_data = filtered_data[\n    (filtered_data['Currency'] == 'CZK') & \n    (filtered_data['Consumption'] == min_consumption) &\n    (filtered_data['Date'].between('201301', '201312'))\n]\n\nsme_avg = filtered_data[filtered_data['Segment'] == 'SME']['Consumption'].sum() / len(filtered_data)\nlam_avg = filtered_data[filtered_data['Segment'] == 'LAM']['Consumption'].sum() / len(filtered_data) \nkam_avg = filtered_data[filtered_data['Segment'] == 'KAM']['Consumption'].sum() / len(filtered_data)\n\nresult = pd.DataFrame([[sme_avg - lam_avg, lam_avg - kam_avg, kam_avg - sme_avg]])"}
{"question_id": 7, "response": "result = pd.merge(customers, yearmonth, on='CustomerID').query(\"Currency == 'EUR'\").assign(\n    sme_2013=lambda x: x.apply(lambda r: r['Consumption'] if r['Segment']=='SME' and r['Date'].startswith('2013') else 0, axis=1),\n    sme_2012=lambda x: x.apply(lambda r: r['Consumption'] if r['Segment']=='SME' and r['Date'].startswith('2012') else 0, axis=1),\n    lam_2013=lambda x: x.apply(lambda r: r['Consumption'] if r['Segment']=='LAM' and r['Date'].startswith('2013') else 0, axis=1),\n    lam_2012=lambda x: x.apply(lambda r: r['Consumption'] if r['Segment']=='LAM' and r['Date'].startswith('2012') else 0, axis=1),\n    kam_2013=lambda x: x.apply(lambda r: r['Consumption'] if r['Segment']=='KAM' and r['Date'].startswith('2013') else 0, axis=1),\n    kam_2012=lambda x: x.apply(lambda r: r['Consumption'] if r['Segment']=='KAM' and r['Date'].startswith('2012') else 0, axis=1)\n).agg({\n    'sme_2013': 'sum',\n    'sme_2012': 'sum',\n    'lam_2013': 'sum',\n    'lam_2012': 'sum',\n    'kam_2013': 'sum',\n    'kam_2012': 'sum'\n}).pipe(lambda x: pd.Series([\n    (x['sme_2013'] - x['sme_2012']) * 100 / x['sme_2012'] if x['sme_2012'] != 0 else None,\n    (x['lam_2013'] - x['lam_2012']) * 100 / x['lam_2"}
{"question_id": 10, "response": "result = len(customers[(customers['Segment'] == 'SME') & (customers['Currency'] == 'CZK')]) - len(customers[(customers['Segment'] == 'SME') & (customers['Currency'] == 'EUR')])"}
{"question_id": 11, "response": "result = (customers[customers['Segment'] == 'LAM']\n         .merge(yearmonth, on='CustomerID', how='inner')\n         .groupby('CustomerID')['Consumption']\n         .max()\n         .gt(46.73)\n         .mean() * 100)"}
{"question_id": 12, "response": "result = (yearmonth[yearmonth['Date'] == '201202']['Consumption'].gt(528.3).sum() * 100 / yearmonth[yearmonth['Date'] == '201202']['CustomerID'].count())"}
{"question_id": 13, "response": "result = yearmonth.groupby(yearmonth['Date'].str[4:6])[['Consumption']].sum().loc[yearmonth['Date'].str[0:4] == '2012'].sort_values('Consumption', ascending=False, na_position='last').head(1)"}
{"question_id": 14, "response": "result = pd.merge(transactions_1k, yearmonth, on='CustomerID').merge(products, on='ProductID').query(\"Date_y == '201309'\")[['Description']]"}
{"question_id": 15, "response": "result = transactions_1k.merge(gasstations, on='GasStationID', how='inner')[transactions_1k['Date'].str.startswith('2012-08')]['Country'].unique()"}
{"question_id": 17, "response": "result = transactions_1k.merge(gasstations, on='GasStationID').merge(products, on='ProductID').query(\"Country == 'CZE'\")['Description'].drop_duplicates()"}
{"question_id": 18, "response": "result = pd.merge(transactions_1k, gasstations, on='GasStationID', how='inner')[['Time']].loc[lambda x: x['GasStationID'].isin(gasstations[gasstations['ChainID'] == 11]['GasStationID'])].drop_duplicates()"}
{"question_id": 19, "response": "result = len(transactions_1k.merge(gasstations, on='GasStationID', how='inner').query(\"Country == 'CZE' and Date.str[:4] >= '2012'\"))"}
{"question_id": 22, "response": "result = len(transactions_1k.merge(gasstations, on='GasStationID', how='inner')[(transactions_1k['Date'] == '2012-08-26') & (transactions_1k['Time'].between('08:00:00', '09:00:00')) & (gasstations['Country'] == 'CZE')])"}
{"question_id": 23, "response": "result = transactions_1k.merge(gasstations, on='GasStationID', how='inner').query(\"Date == '2012-08-24' and Price == 548.4\")[['Country']]"}
{"question_id": 24, "response": "result = len(transactions_1k[transactions_1k['Date'] == '2012-08-25'].merge(customers, on='CustomerID')[customers['Currency'] == 'EUR']['CustomerID'].unique()) * 100 / len(transactions_1k[transactions_1k['Date'] == '2012-08-25']['CustomerID'].unique())"}
{"question_id": 26, "response": "result = (gasstations[gasstations['Country'] == 'SVK'].groupby('Country').agg(\n    premium_count=('Segment', lambda x: (x == 'Premium').sum()),\n    total_count=('Segment', 'count')\n)).assign(percentage = lambda x: x['premium_count'] * 100 / x['total_count']).iloc[0]['percentage']"}
{"question_id": 27, "response": "result = pd.merge(transactions_1k, gasstations, on='GasStationID', how='inner').query('CustomerID == 38508').agg({\n    'Price': ['sum', lambda x: x[transactions_1k['Date'].str.startswith('2012-08')].sum()]\n}).iloc[0]"}
{"question_id": 29, "response": "result = pd.merge(transactions_1k, yearmonth, on='CustomerID', suffixes=('', '_y'))[\n    (transactions_1k['Price'] / transactions_1k['Amount'].replace(0, np.nan) > 29.00) & \n    (transactions_1k['ProductID'] == 5) & \n    (yearmonth['Date'] == '201208')\n]['Consumption']"}
{"question_id": 30, "response": "result = pd.merge(member, major, left_on='link_to_major', right_on='major_id').query(\"first_name == 'Angela' and last_name == 'Sanders'\")[['major_name']]"}
{"question_id": 33, "response": "result = pd.merge(event, attendance, left_on='event_id', right_on='link_to_event', how='inner')\\\n    .groupby(['event_id', 'event_name'])\\\n    .size()\\\n    .reset_index(name='count')\\\n    .query('count > 20')\\\n    .merge(event[event['type'] != 'Fundraiser'][['event_name']], on='event_name', how='left', indicator=True)\\\n    .query('_merge == \"left_only\"')[['event_name']]"}
{"question_id": 34, "response": "result = pd.merge(member, income, left_on='member_id', right_on='link_to_member')[['amount']].loc[member['position'] == 'Vice President']"}
{"question_id": 35, "response": "result = member.merge(zip_code, left_on='zip', right_on='zip_code')[['first_name', 'last_name']].loc[lambda x: zip_code['state'] == 'Illinois']"}
{"question_id": 36, "response": "result = pd.merge(event, budget, left_on='event_id', right_on='link_to_event').merge(expense, left_on='budget_id', right_on='link_to_budget').loc[(event['event_name'] == 'October Meeting') & (event['event_date'].str.startswith('2019-10-08')), ['approved']]"}
{"question_id": 37, "response": "result = expense.merge(member, left_on='link_to_member', right_on='member_id').query(\"last_name == 'Allen' and first_name == 'Elijah' and (expense_date.str[5:7] == '09' or expense_date.str[5:7] == '10')\")['cost'].mean()"}
{"question_id": 38, "response": "result = pd.merge(event, budget, left_on='event_id', right_on='link_to_event')[['event_date', 'spent']].assign(year_2019=lambda x: x['spent'].where(x['event_date'].str[:4] == '2019', 0), year_2020=lambda x: x['spent'].where(x['event_date'].str[:4] == '2020', 0)).agg({'year_2019': 'sum', 'year_2020': 'sum'}).pipe(lambda x: x['year_2019'] - x['year_2020']).to_frame('num')"}
{"question_id": 39, "response": "result = income[(income['source'] == 'Fundraising') & (income['date_received'] == '2019-09-14')][['notes']]"}
{"question_id": 44, "response": "result = pd.merge(member, major, left_on='link_to_major', right_on='major_id').loc[lambda x: x['position'] == 'President', ['department']]"}
{"question_id": 45, "response": "result = pd.merge(member, income, left_on='member_id', right_on='link_to_member').query(\"first_name == 'Connor' and last_name == 'Hilton' and source == 'Dues'\")[['date_received']]"}
{"question_id": 46, "response": "result = (budget.merge(event, left_on='link_to_event', right_on='event_id')\n         .query(\"category == 'Advertisement' and type == 'Meeting'\")\n         .assign(yearly_kickoff=lambda x: x.apply(lambda row: row['amount'] if row['event_name'] == 'Yearly Kickoff' else 0, axis=1),\n                october_meeting=lambda x: x.apply(lambda row: row['amount'] if row['event_name'] == 'October Meeting' else 0, axis=1))\n         .agg({'yearly_kickoff': 'sum', 'october_meeting': 'sum'})\n         .pipe(lambda x: x['yearly_kickoff'].astype(float) / x['october_meeting'].where(x['october_meeting'] != 0))\n         .iloc[0])"}
{"question_id": 47, "response": "result = expense[expense['expense_description'] == 'Pizza']['cost'].sum()"}
{"question_id": 49, "response": "result = pd.merge(member, major, left_on='link_to_major', right_on='major_id').query(\"phone == '809-555-3360'\")[['major_name']]"}
{"question_id": 50, "response": "result = len(event.merge(attendance, left_on='event_id', right_on='link_to_event').query(\"event_name == 'Women''s Soccer'\")['link_to_member'])"}
{"question_id": 52, "response": "result = pd.merge(budget, event, left_on='link_to_event', right_on='event_id', how='inner')\\\n    .query(\"status == 'Closed'\")\\\n    .assign(ratio=lambda x: x['spent'] / x['amount'].replace(0, np.nan))\\\n    .nlargest(1, 'ratio')[['event_name']]"}
{"question_id": 55, "response": "result = member.merge(attendance, left_on='member_id', right_on='link_to_member').groupby(['member_id', 'first_name', 'last_name']).agg({'link_to_event': 'count'}).reset_index().query('link_to_event > 7')[['first_name', 'last_name']]"}
{"question_id": 56, "response": "result = (\n    event.merge(budget, left_on='event_id', right_on='link_to_event')\n    .merge(expense, left_on='budget_id', right_on='link_to_budget')\n    .merge(member, left_on='link_to_member', right_on='member_id')\n    .loc[lambda x: x['event_name'] == 'Yearly Kickoff', ['first_name', 'last_name']]\n)"}
{"question_id": 59, "response": "result = income[(income['date_received'] >= '2019-09-01') & (income['date_received'] <= '2019-09-30')].sort_values('source', ascending=False, na_position='last').head(1)[['source']]"}
{"question_id": 60, "response": "result = len(member.merge(major, left_on='link_to_major', right_on='major_id').query(\"major_name == 'Physics Teaching'\"))"}
{"question_id": 61, "response": "result = pd.merge(budget, event, left_on='link_to_event', right_on='event_id').query('category == \"Advertisement\"').sort_values('spent', ascending=False, na_position='last')[['event_name']].head(1)"}
{"question_id": 63, "response": "result = pd.merge(event, budget, left_on='event_id', right_on='link_to_event').merge(expense, left_on='budget_id', right_on='link_to_budget').loc[(event['event_name'] == 'September Speaker') & (expense['expense_description'] == 'Posters'), ['cost']]"}
{"question_id": 64, "response": "result = pd.merge(budget, event, left_on='link_to_event', right_on='event_id').query('event_status == \"Closed\" and remaining < 0').sort_values('remaining', na_position='first')[['event_name']].head(1)"}
{"question_id": 65, "response": "result = pd.merge(event, budget, left_on='event_id', right_on='link_to_event').merge(expense, left_on='budget_id', right_on='link_to_budget').query('event_name == \"October Meeting\"').groupby('type')['cost'].sum().reset_index()"}
{"question_id": 66, "response": "result = pd.merge(event, budget, left_on='event_id', right_on='link_to_event').query('event_name == \"April Speaker\"').groupby('category')['amount'].sum().sort_values().reset_index()"}
{"question_id": 69, "response": "result = expense.merge(member, left_on='link_to_member', right_on='member_id').query(\"first_name == 'Sacha' and last_name == 'Harrison'\")[['expense_description']]"}
{"question_id": 70, "response": "result = pd.merge(event, budget, left_on='event_id', right_on='link_to_event').loc[lambda x: x['location'] == 'MU 215']['category'].drop_duplicates()"}
{"question_id": 73, "response": "result = (member.merge(income, left_on='member_id', right_on='link_to_member')\n         .query(\"position == 'Member' and t_shirt_size == 'Medium'\")\n         .assign(is_fifty=lambda x: (x['amount'] == 50).astype(float))\n         ['is_fifty'].mean() * 100)"}
{"question_id": 74, "response": "result = event[\n    (event['type'] == 'Game') & \n    (pd.to_datetime(event['event_date'].str[:10]).between('2019-03-15', '2020-03-20')) & \n    (event['status'] == 'Closed')\n]['event_name'].unique()"}
{"question_id": 75, "response": "result = member.merge(expense, left_on='member_id', right_on='link_to_member').merge(budget, left_on='link_to_budget', right_on='budget_id')[['first_name', 'last_name', 'phone', 'cost']].loc[lambda x: x['cost'] > member.merge(expense, left_on='member_id', right_on='link_to_member').merge(budget, left_on='link_to_budget', right_on='budget_id')['cost'].mean()][['first_name', 'last_name', 'phone']].drop_duplicates()"}
{"question_id": 77, "response": "result = (event.merge(attendance, left_on='event_id', right_on='link_to_event')\n         .merge(member, left_on='link_to_member', right_on='member_id')\n         .merge(income, left_on='member_id', right_on='link_to_member')\n         .loc[lambda x: x['date_received'] == '2019-09-09']\n         [['first_name', 'last_name', 'amount']]\n         .drop_duplicates())"}
{"question_id": 78, "response": "result = (Patient[Patient['SEX'] == 'M']\n         .assign(positive=lambda x: (x['Admission'] == '+').astype(int),\n                negative=lambda x: (x['Admission'] == '-').astype(int))\n         .agg({'positive': 'sum', 'negative': 'sum'})\n         .pipe(lambda x: x['positive'] * 100 / x['negative'] if x['negative'] != 0 else float('nan')))"}
{"question_id": 79, "response": "result = (len(Patient[Patient['Birthday'].astype('datetime64').dt.strftime('%Y').astype(float) > 1930][Patient['SEX'] == 'F']) * 100.0 / len(Patient[Patient['SEX'] == 'F']))"}
{"question_id": 80, "response": "result = len(Patient[(Patient['Diagnosis'] == 'SLE') & (Patient['Admission'] == '+')]) / len(Patient[(Patient['Diagnosis'] == 'SLE') & (Patient['Admission'] == '-')]) if len(Patient[(Patient['Diagnosis'] == 'SLE') & (Patient['Admission'] == '-')]) != 0 else None"}
{"question_id": 81, "response": "result = pd.merge(Patient[['ID', 'Diagnosis']], Laboratory[['ID', 'Date']], on='ID', how='inner').query('ID == 30609')[['Diagnosis', 'Date']]"}
{"question_id": 82, "response": "result = pd.merge(Patient[['ID', 'SEX', 'Birthday']], Laboratory[['ID', 'LDH']], on='ID', how='inner').query('LDH > 500')[['ID', 'SEX', 'Birthday']].drop_duplicates()"}
{"question_id": 83, "response": "result = pd.merge(Patient, Examination, on='ID', how='inner')[['ID', 'Birthday']].drop_duplicates()\nresult['Age'] = pd.Timestamp.now().year - pd.to_datetime(result['Birthday']).dt.year\nresult = result[result.index.isin(Examination[Examination['RVVT'] == '+'].index)][['ID', 'Age']]"}
{"question_id": 84, "response": "result = pd.merge(Patient, Examination, on='ID', how='inner')[['ID', 'SEX_x', 'Diagnosis_x']].loc[lambda x: x['Thrombosis'] == 2].drop_duplicates()"}
{"question_id": 85, "response": "result = len(Patient[(pd.to_datetime(Patient['Description']).dt.strftime('%Y') == '1997') & (Patient['SEX'] == 'F') & (Patient['Admission'] == '-')])"}
{"question_id": 86, "response": "result = len(Patient.merge(Examination, on='ID', how='inner')[\n    (Patient['SEX'] == 'F') & \n    (pd.to_datetime(Examination['Examination Date']).dt.strftime('%Y') == '1997') & \n    (Examination['Thrombosis'] == 1)\n])"}
{"question_id": 87, "response": "result = pd.merge(Patient, Examination, on='ID', suffixes=('', '_y'))\\\n    .loc[lambda x: x['Symptoms'].notna()]\\\n    .sort_values('Birthday', ascending=False)\\\n    .head(1)[['Symptoms', 'Diagnosis']]"}
{"question_id": 89, "response": "result = len(pd.merge(Patient, Laboratory, on='ID').query(\"UA <= 8.0 and SEX == 'M'\")['ID'].unique()) / len(pd.merge(Patient, Laboratory, on='ID').query(\"UA <= 6.5 and SEX == 'F'\")['ID'].unique())"}
{"question_id": 90, "response": "result = len(Patient.merge(Examination, on='ID').loc[\n    (pd.to_datetime(Examination['Examination Date']).dt.year.between(1990, 1993)) &\n    (pd.to_datetime(Examination['Examination Date']).dt.year - pd.to_datetime(Patient['Birthday']).dt.year < 18)\n])"}
{"question_id": 91, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner')[['ID', 'Birthday', 'Date', 'Diagnosis', 'HGB']].dropna(subset=['HGB']).assign(age_difference=lambda x: pd.to_datetime(x['Date']).dt.year - pd.to_datetime(x['Birthday']).dt.year)[['age_difference', 'Diagnosis']].sort_values('HGB', ascending=False).head(1)"}
{"question_id": 92, "response": "result = examination_df[examination_df['ID'].isin(patient_df[(patient_df['Diagnosis'] == 'SLE') & (patient_df['Description'] == '1994-02-19')]['ID']) & (examination_df['Examination Date'] == '1993-11-12')]['aCL IgM']"}
{"question_id": 93, "response": "result = float((Laboratory.merge(Patient, on='ID')\n         .query('Birthday == \"1959-02-18\"')\n         .assign(month=lambda x: x['Date'].str[:7])\n         .groupby('month')['T-CHO']\n         .sum()\n         .loc[['1981-11', '1981-12']]\n         .pipe(lambda x: (x['1981-11'] - x['1981-12']) / x['1981-12'] if x['1981-12'] != 0 else None))"}
{"question_id": 94, "response": "result = Laboratory[\n    (Laboratory['Date'].between('1987-07-06', '1996-01-31')) & \n    (Laboratory['GPT'] > 30) & \n    (Laboratory['ALB'] < 4)\n]['ID'].drop_duplicates()"}
{"question_id": 96, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner')[\n    (Patient['Admission'] == '-') & \n    (Laboratory['T-BIL'] < 2.0) & \n    (Laboratory['Date'].str.startswith('1991-10-'))\n]['ID'].drop_duplicates()"}
{"question_id": 97, "response": "result = pd.merge(Patient, Laboratory, on='ID').query(\"PLT > 400 and Diagnosis == 'SLE' and SEX == 'F'\")['ALB'].mean()"}
{"question_id": 99, "response": "result = (len(Patient[(Patient['SEX'] == 'F') & (Patient['Diagnosis'] == 'RA') & (pd.to_datetime(Patient['Birthday']).dt.strftime('%Y') == '1980')]) * 100.0 / len(Patient[(Patient['Diagnosis'] == 'RA') & (pd.to_datetime(Patient['Birthday']).dt.strftime('%Y') == '1980')]))"}
{"question_id": 101, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner').query('SEX == \"M\" and GPT >= 60')[['ID']].drop_duplicates()"}
{"question_id": 102, "response": "result = (\n    pd.merge(Patient, Laboratory, on='ID')\n    .loc[lambda x: x['GPT'] > 60]\n    .sort_values('Birthday')\n    .groupby('Birthday', as_index=False)\n    .agg({'Diagnosis': 'first'})\n    [['Birthday', 'Diagnosis']]\n    .drop_duplicates()\n)"}
{"question_id": 103, "response": "result = Patient.merge(Laboratory, on='ID', how='inner')[['ID', 'SEX', 'Birthday']].loc[Laboratory['UN'] == 29].drop_duplicates()"}
{"question_id": 105, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner')[['ID', 'Birthday', 'SEX', 'T-CHO']].drop_duplicates().query('`T-CHO` >= 250 and SEX == \"M\"').assign(age=lambda x: pd.to_datetime('today').year - pd.to_datetime(x['Birthday']).dt.year)['age'].mean()"}
{"question_id": 106, "response": "result = len(pd.merge(Patient, Laboratory, on='ID').query('TG >= \"200\" and (pd.Timestamp.now().year - pd.to_datetime(Birthday).dt.year) > 50')['ID'].unique())"}
{"question_id": 107, "response": "result = len(pd.merge(Patient, Laboratory, on='ID').query(\"Birthday.str[:4].between('1936', '1956') and SEX == 'M' and CPK >= 250\")['ID'].unique())"}
{"question_id": 108, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner')[['ID', 'SEX', 'Birthday']][\n    (Laboratory['GLU'] >= 180) & (Laboratory['T-CHO'] < 250)\n].drop_duplicates().assign(\n    Age=lambda x: pd.Timestamp.now().year - pd.to_datetime(x['Birthday']).dt.year\n)[['ID', 'SEX', 'Age']]"}
{"question_id": 109, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner')[['Diagnosis_x', 'ID', 'Birthday']][Laboratory['RBC'] < 3.5].drop_duplicates().assign(Age=lambda x: pd.Timestamp.now().year - pd.to_datetime(x['Birthday']).dt.year)[['Diagnosis_x', 'ID', 'Age']]"}
{"question_id": 111, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner')[['ID', 'Birthday']].drop_duplicates()\nresult = result[result['ID'].isin(Laboratory[Laboratory['HCT'] >= 52].groupby('ID').filter(lambda x: len(x) >= 2)['ID'])]\nresult['Age'] = pd.Timestamp.now().year - pd.to_datetime(result['Birthday']).dt.year\nresult = result[['ID', 'Age']]"}
{"question_id": 113, "response": "result = pd.merge(Patient, Laboratory, on='ID', how='inner')[\n    (Laboratory['PLT'].between(100, 400)) & \n    (pd.to_datetime('now').year - pd.to_datetime(Patient['Birthday']).dt.year < 50) & \n    (pd.to_datetime(Laboratory['Date']).dt.year == 1984)\n]['ID'].drop_duplicates()"}
{"question_id": 115, "response": "result = len(pd.merge(Patient, Laboratory, on='ID').query('(FG <= 150 | FG >= 450) & WBC > 3.5 & WBC < 9.0 & SEX == \"M\"')['ID'].unique())"}
{"question_id": 116, "response": "result = len(pd.merge(Patient, Laboratory, on='ID').query('IGG >= 2000')['ID'].unique())"}
{"question_id": 117, "response": "result = len(Patient.merge(Laboratory, on='ID').merge(Examination, on='ID').loc[(Laboratory['IGG'].between(900, 2000)) & (Examination['Symptoms'].notna()), 'ID'].unique())"}
{"question_id": 118, "response": "result = len(pd.merge(Patient, Laboratory, on='ID').query('IGA >= 80 and IGA <= 500 and pd.to_datetime(`First Date`).dt.strftime(\"%Y\") >= \"1990\"')['ID'].unique())"}
{"question_id": 119, "response": "result = Patient.merge(Laboratory, on='ID', how='inner')[['ID', 'Diagnosis_x', 'IGM']].query('IGM <= 40 or IGM >= 400').drop_duplicates()[['ID', 'Diagnosis_x']].groupby('Diagnosis_x').size().reset_index(name='count').sort_values('count', ascending=False, na_position='last').iloc[0:1]['Diagnosis_x']"}
{"question_id": 121, "response": "result = len(pd.merge(Patient, Laboratory, on='ID').query('CRE >= 1.5 and (pd.to_datetime(\"today\") - pd.to_datetime(Birthday)).dt.days / 365.25 < 70')['ID'].unique())"}
{"question_id": 123, "response": "result = len(Examination.merge(Laboratory, on='ID').loc[(Laboratory['SM'].isin(['negative', '0'])) & (Examination['Thrombosis'] == 0), 'ID'].unique())"}
{"question_id": 124, "response": "result = len(Patient.merge(Laboratory, on='ID').merge(Examination, on='ID').query(\"(SC170 == 'negative' or SC170 == '0') and SEX == 'F' and Symptoms.isnull()\").ID.unique())"}
{"question_id": 126, "response": "result = Patient.merge(Laboratory, on='ID', how='inner').query('GOT >= 60').nlargest(1, 'Birthday')[['Birthday']]"}
{"question_id": 127, "response": "result = len(pd.merge(pd.merge(Patient, Laboratory, on='ID'), Examination, on='ID').query('CPK < 250 and (KCT == \"+\" or RVVT == \"+\" or LAC == \"+\")').ID.unique())"}
{"question_id": 128, "response": "result = Match.merge(League, left_on='league_id', right_on='id').query(\"season == '2015/2016'\").groupby('name').agg({'home_team_goal': 'sum', 'away_team_goal': 'sum'}).assign(total_goals=lambda x: x['home_team_goal'] + x['away_team_goal']).nlargest(1, 'total_goals')[['name']].reset_index(drop=True)"}
{"question_id": 130, "response": "result = pd.merge(Team_Attributes, Team, on='team_api_id')[['buildUpPlaySpeed']].sort_values('buildUpPlaySpeed', ascending=False, na_position='last').head(4)"}
{"question_id": 131, "response": "result = Match.merge(League, left_on='league_id', right_on='id').query(\"season == '2015/2016' and home_team_goal == away_team_goal\").groupby('name').size().sort_values(ascending=False).head(1).index[0]"}
{"question_id": 132, "response": "merged_df = pd.merge(Player_Attributes, Player, on='player_api_id')\nfiltered_df = merged_df[\n    (pd.to_datetime(merged_df['date']).dt.year.between(2013, 2015)) & \n    (merged_df['sprint_speed'] >= 97)\n]\nfiltered_df['age'] = (pd.Timestamp.now() - pd.to_datetime(filtered_df['birthday'])).dt.total_years.apply(np.ceil)\nresult = filtered_df['age'].drop_duplicates()"}
{"question_id": 133, "response": "subquery = Match.groupby('league_id')['id'].count().reset_index(name='cnt')\nt1 = subquery.groupby('league_id')['cnt'].max().reset_index(name='max_count').nlargest(1, 'max_count')\nresult = pd.merge(League, t1, left_on='id', right_on='league_id')[['name', 'max_count']]"}
{"question_id": 134, "response": "result = Team_Attributes[\n    (Team_Attributes['buildUpPlaySpeed'] > 50) & \n    (Team_Attributes['buildUpPlaySpeed'] < 60)\n]['team_fifa_api_id'].unique()"}
{"question_id": 136, "response": "merged_df = pd.merge(Player, Player_Attributes, on='player_api_id')\nfiltered_df = merged_df[merged_df['birthday'].str[:4].between('1987', '1992')]\nleft_count = filtered_df[filtered_df['preferred_foot'] == 'left']['id_x'].nunique()\ntotal_count = filtered_df['id_x'].nunique()\nresult = (left_count * 100.0 / total_count) if total_count != 0 else 0"}
{"question_id": 138, "response": "result = (\n    Player.merge(Player_Attributes, on='player_api_id')\n    .query('height > 180')\n    .groupby('id')\n    .agg({\n        'player_name': 'first',\n        'heading_accuracy': 'sum',\n        'player_fifa_api_id': 'count'\n    })\n    .assign(avg_heading=lambda x: x['heading_accuracy'] / x['player_fifa_api_id'])\n    .sort_values('avg_heading', ascending=False, na_position='last')\n    ['player_name']\n    .head(10)\n)"}
{"question_id": 139, "response": "result = pd.merge(League, Match, left_on='id', right_on='league_id', how='inner')\\\n    .query(\"season == '2009/2010'\")\\\n    .groupby('name')\\\n    .agg({'home_team_goal': 'sum', 'away_team_goal': 'sum', 'id_y': 'nunique'})\\\n    .assign(avg_diff=lambda x: (x['home_team_goal']/x['id_y']) - (x['away_team_goal']/x['id_y']))\\\n    .query('avg_diff > 0')\\\n    .index.to_frame()['name']"}
{"question_id": 140, "response": "result = Player[Player['birthday'].str[:7] == '1970-10'][['player_name']]"}
{"question_id": 141, "response": "result = pd.merge(Player, Player_Attributes, on='player_api_id').query(\"player_name == 'Gabriel Tamas' and date.str[:4] == '2011'\")[['overall_rating']]"}
{"question_id": 143, "response": "max_height = Player['height'].max()\nmin_height = Player['height'].min()\n\nhighest_avg = pd.merge(Player[Player['height'] == max_height], Player_Attributes, on='player_api_id')['finishing'].mean()\nmin_avg = pd.merge(Player[Player['height'] == min_height], Player_Attributes, on='player_api_id')['finishing'].mean()\n\ndf = pd.DataFrame({'result': [highest_avg, min_avg], 'A': ['highest', 'Min']})\nresult = df.nlargest(1, 'result')['A']"}
{"question_id": 144, "response": "result = pd.merge(Player, Player_Attributes, on='player_api_id')\nresult = result[result['height'] > 170]\nresult = result[(pd.to_datetime(result['date']).dt.strftime('%Y') >= '2010') & (pd.to_datetime(result['date']).dt.strftime('%Y') <= '2015')]\nresult = result['overall_rating'].sum() / result['id_y'].count() if result['id_y'].count() != 0 else None"}
{"question_id": 152, "response": "match_counts = Match[Match['season'] == '2008/2009'].groupby('league_id').size()\nmax_matches = match_counts.max()\nresult = League.merge(Match[Match['season'] == '2008/2009'], left_on='id', right_on='league_id').groupby('name').size().reset_index()\nresult = result[result[0] == max_matches]['name']"}
{"question_id": 153, "response": "merged_df = pd.merge(Player, Player_Attributes, on='player_api_id')\nariel_sum = merged_df[merged_df['player_name'] == 'Ariel Borysiuk']['overall_rating'].sum()\npaulin_sum = merged_df[merged_df['player_name'] == 'Paulin Puel']['overall_rating'].sum()\nresult = ((ariel_sum - paulin_sum) * 100 / paulin_sum) if paulin_sum != 0 else None"}
{"question_id": 155, "response": "result = pd.merge(Team, Team_Attributes, on='team_api_id').loc[lambda x: x['team_long_name'] == 'Ajax'].nlargest(1, 'chanceCreationPassing')[['chanceCreationPassing', 'chanceCreationPassingClass']]"}
{"question_id": 156, "response": "result = Player.merge(Player_Attributes, on='player_api_id', how='inner').query('date.str[:10] == \"2016-06-23\" and overall_rating == 77')[['player_name']].sort_values('birthday', ascending=True, na_position='first').head(1)"}
{"question_id": 157, "response": "result = pd.merge(Player, Player_Attributes, on='player_api_id').query(\"player_name == 'Aaron Mooy' and date.str[:10] == '2016-02-04'\")[['overall_rating']]"}
{"question_id": 160, "response": "result = pd.merge(Team, Team_Attributes, on='team_api_id').query('team_long_name == \"FC Lorient\" and date.str.startswith(\"2010-02-22\")')['buildUpPlayPassingClass']"}
{"question_id": 161, "response": "result = pd.merge(Team, Team_Attributes, on='team_api_id').query(\"team_long_name == 'Hannover 96' and date.str.startswith('2015-09-10')\")['defenceAggressionClass']"}
{"question_id": 162, "response": "result = pd.merge(Player, Player_Attributes, on='player_fifa_api_id').query(\"player_name == 'Marko Arnautovic' and date.str[:10].between('2007-02-22', '2016-04-21')\")['overall_rating'].mean()"}
{"question_id": 163, "response": "merged_df = pd.merge(Player, Player_Attributes, on='player_fifa_api_id')\nfiltered_df = merged_df[merged_df['date'].str[:10] == '2013-07-12']\n\nlandon_sum = filtered_df[filtered_df['player_name'] == 'Landon Donovan']['overall_rating'].sum()\njordan_sum = filtered_df[filtered_df['player_name'] == 'Jordan Bowery']['overall_rating'].sum()\n\nresult = ((landon_sum - jordan_sum) * 100 / landon_sum) if landon_sum != 0 else None"}
{"question_id": 164, "response": "result = Player.assign(rank=Player.groupby('height')['height'].transform('rank', method='dense', ascending=False)).query('rank == 1')[['player_name']]"}
{"question_id": 166, "response": "result = pd.merge(Player, Player_Attributes, on='player_api_id', how='inner')[['player_name']].loc[lambda x: x['player_name'].isin(Player.merge(Player_Attributes.loc[Player_Attributes['attacking_work_rate'] == 'high'], on='player_api_id')['player_name'])].drop_duplicates()"}
{"question_id": 167, "response": "result = pd.merge(Team, Team_Attributes, on='team_api_id', how='inner')[['team_short_name']].loc[lambda x: x['team_short_name'].isin(Team.merge(Team_Attributes.loc[Team_Attributes['chanceCreationPassingClass'] == 'Safe'], on='team_api_id')['team_short_name'])].drop_duplicates()"}
{"question_id": 169, "response": "result = Player_Attributes[Player_Attributes['id'].isin([6, 23])].groupby('id')['jumping'].sum().reset_index()\nresult = result.set_index('id').loc[6, 'jumping'] - result.set_index('id').loc[23, 'jumping']"}
{"question_id": 170, "response": "result = Player_Attributes[Player_Attributes['preferred_foot'] == 'right'][['id']].sort_values('potential', ascending=True, na_position='last').head(4)"}
{"question_id": 171, "response": "result = len(Player_Attributes[\n    (Player_Attributes['preferred_foot'] == 'left') & \n    (Player_Attributes['crossing'] == Player_Attributes['crossing'].max())\n]['player_api_id'].unique())"}
{"question_id": 173, "response": "result = pd.merge(Team_Attributes, Team, on='team_api_id').loc[lambda x: x['team_long_name'] == 'KSV Cercle Brugge', 'buildUpPlaySpeedClass'].drop_duplicates()"}
{"question_id": 174, "response": "heaviest_player = Player.sort_values('weight', ascending=False, na_position='last').iloc[0]['player_api_id']\nresult = Player_Attributes[Player_Attributes['player_api_id'] == heaviest_player][['id', 'finishing', 'curve']].iloc[0]"}
{"question_id": 175, "response": "result = (League.merge(Match, left_on='id', right_on='league_id')\n         .query(\"season == '2015/2016'\")\n         .groupby('name')\n         .size()\n         .sort_values(ascending=False)\n         .head(4)\n         .index\n         .to_frame())"}
{"question_id": 176, "response": "result = pd.merge(Match, Team, left_on='away_team_api_id', right_on='team_api_id')[['team_long_name', 'away_team_goal']].sort_values('away_team_goal', ascending=False, na_position='last').head(1)[['team_long_name']]"}
{"question_id": 179, "response": "result = qualifying.merge(drivers, on='driverId', how='inner').query('raceId == 20').sort_values('q1', ascending=False, na_position='last')[['driverRef']].head(5)"}
{"question_id": 180, "response": "result = drivers.merge(qualifying, left_on='driverId', right_on='driverId')[['surname', 'raceId', 'q2']].query('raceId == 19').sort_values('q2').head(1)[['surname']]"}
{"question_id": 181, "response": "result = pd.merge(circuits, races, left_on='circuitId', right_on='circuitId')[['name_y']].loc[lambda x: x['name_y'].isin(pd.merge(circuits, races, left_on='circuitId', right_on='circuitId').loc[circuits['country'] == 'Germany', 'name_y'])].drop_duplicates()"}
{"question_id": 182, "response": "result = circuits.merge(races, left_on='circuitId', right_on='circuitId')[['lat', 'lng']].loc[lambda x: x.index.isin(races[races['name'] == 'Australian Grand Prix']['circuitId'])].drop_duplicates()"}
{"question_id": 187, "response": "result = drivers.merge(results, on='driverId', how='inner').query('raceId == 592 and time.notna() and dob.notna()').sort_values('dob', ascending=True)[['forename', 'surname']].head(1)"}
{"question_id": 188, "response": "result = (\n    lapTimes.merge(drivers, on='driverId')\n    .query('raceId == 161 & time.str.startswith(\"1:27\")')\n    [['forename', 'surname', 'url']]\n    .drop_duplicates()\n)"}
{"question_id": 190, "response": "result = pd.merge(constructorResults, constructors, on='constructorId', how='inner').query('raceId == 9').sort_values('points', ascending=False, na_position='last')[['url']].head(1)"}
{"question_id": 191, "response": "result = qualifying.merge(drivers, on='driverId', how='inner').query('raceId == 45 and q3.str.startswith(\"1:33\")')['code']"}
{"question_id": 194, "response": "result = drivers.merge(results, on='driverId', how='inner')[['nationality', 'fastestLapSpeed']].sort_values('fastestLapSpeed', ascending=False, na_position='last').head(1)[['nationality']]"}
{"question_id": 195, "response": "result = (\n    drivers.merge(results, on='driverId')\n    .query(\"forename == 'Paul' and surname == 'di Resta'\")\n    .assign(\n        speed_853=lambda x: x.apply(lambda row: float(row['fastestLapSpeed']) if row['raceId'] == 853 else 0, axis=1),\n        speed_854=lambda x: x.apply(lambda row: float(row['fastestLapSpeed']) if row['raceId'] == 854 else 0, axis=1)\n    )\n    .agg(lambda x: (x['speed_853'].sum() - x['speed_854'].sum()) * 100 / x['speed_853'].sum() if x['speed_853'].sum() != 0 else None)\n    .iloc[0]\n)"}
{"question_id": 196, "response": "result = (races.merge(results, on='raceId', how='inner')\n         .query(\"date == '1983-07-16'\")\n         .assign(has_time=lambda x: x['time'].notna())\n         .agg(percentage=('driverId', lambda x: x[results['time'].notna()].count() * 100 / x.count()))\n         .iloc[0]['percentage'])"}
{"question_id": 199, "response": "result = pd.merge(drivers, lapTimes, on='driverId').merge(races, on='raceId')[['milliseconds', 'forename', 'surname', 'name']].sort_values('milliseconds').head(1)"}
{"question_id": 202, "response": "result = (drivers.merge(driverStandings, on='driverId', how='inner')\n         .query('wins >= 1')\n         .groupby(['forename', 'surname', 'nationality'])\n         .agg({'points': 'max', 'wins': 'count'})\n         .sort_values('wins', ascending=False)\n         .head(1)\n         .reset_index()[['forename', 'surname', 'nationality', 'points']])"}
{"question_id": 204, "response": "result = pd.merge(circuits, races, left_on='circuitId', right_on='circuitId')[['name_y', 'name_x', 'location']].drop_duplicates().loc[(races['year'] == 2005) & (pd.to_datetime(races['date']).dt.strftime('%m') == '09')]"}
{"question_id": 208, "response": "result = (circuits.merge(races, on='circuitId')\n         .query('name == \"European Grand Prix\"')\n         .assign(germany_count=lambda x: x['country'] == 'Germany')\n         .agg({'germany_count': 'sum', 'circuitId': 'count'})\n         .pipe(lambda x: x['germany_count'] * 100 / x['circuitId'])\n         .iloc[0])"}
{"question_id": 221, "response": "result = pd.merge(constructorStandings, constructors, on='constructorId')[['name']].loc[(constructorStandings['points'] == 0) & (constructorStandings['raceId'] == 291)]"}
{"question_id": 222, "response": "result = len(constructorStandings.merge(constructors, on='constructorId')\n    .query('points == 0 and nationality == \"Japanese\"')\n    .groupby('constructorId')\n    .filter(lambda x: len(x) == 2)['constructorId'].unique())"}
{"question_id": 226, "response": "result = pd.merge(results, races, on='raceId').query(\"year == 2009 and name == 'Spanish Grand Prix'\")['fastestLapSpeed'].astype(float).mean()"}
{"question_id": 230, "response": "result = len(drivers.sort_values('dob', ascending=False).head(3)[drivers.sort_values('dob', ascending=False).head(3)['nationality'] == 'Dutch'])"}
{"question_id": 236, "response": "result = (\n    pd.merge(pitStops, drivers, on='driverId')\n    .query('nationality == \"German\" and dob.dt.year.between(1980, 1985)')\n    .groupby(['forename', 'surname'])\n    .agg({'duration': 'mean'})\n    .sort_values('duration')\n    .reset_index()[['forename', 'surname']]\n    .head(3)\n)"}
{"question_id": 240, "response": "result = (pd.merge(constructorResults, constructors, on='constructorId')\n         .merge(races, left_on='raceId', right_on='raceId')\n         .query('name == \"Monaco Grand Prix\" and year >= 1980 and year <= 2010')\n         .groupby(['name_y', 'nationality'])['points']\n         .sum()\n         .reset_index()\n         .sort_values('points', ascending=False)\n         .head(1))"}
{"question_id": 241, "response": "result = qualifying.merge(drivers, on='driverId').merge(races, on='raceId')\\\n    .loc[lambda x: (x['q3'].notna()) & \n                  (x['year'] == 2008) & \n                  (x['circuitId'].isin(circuits.loc[circuits['name'] == 'Marina Bay Street Circuit', 'circuitId']))]\\\n    .assign(time_in_seconds=lambda x: x['q3'].str.split(':').str[0].astype(float) * 60 + \n            x['q3'].str.split(':').str[1].str.split('.').str[0].astype(float) + \n            x['q3'].str.split('.').str[1].astype(float) / 1000)\\\n    .sort_values('time_in_seconds')\\\n    [['forename', 'surname']]\\\n    .head(1)"}
{"question_id": 243, "response": "result = pd.merge(results, races, on='raceId').merge(status, on='statusId').query('statusId == 3 and name == \"Canadian Grand Prix\"').groupby('driverId')['driverId'].count().sort_values(ascending=False, na_position='last').head(1)"}
{"question_id": 245, "response": "def convert_to_seconds(time_str):\n    if pd.isna(time_str):\n        return None\n    minutes = float(time_str.split(':')[0])\n    seconds = float(time_str.split(':')[1].split('.')[0])\n    milliseconds = float(time_str.split('.')[1]) / 1000\n    return minutes * 60 + seconds + milliseconds\n\nfastest_lap_times = results[results['fastestLapTime'].notna()].copy()\nfastest_lap_times['time_in_seconds'] = fastest_lap_times['fastestLapTime'].apply(convert_to_seconds)\n\nmin_time = fastest_lap_times.merge(races, on='raceId') \\\n                           .merge(circuits, on='circuitId') \\\n                           .query(\"country == 'Italy'\") \\\n                           .time_in_seconds.min()\n\nresult = results.merge(races, on='raceId') \\\n               .merge(circuits, on='circuitId') \\\n               .assign(time_in_seconds=lambda x: x['fastestLapTime'].apply(convert_to_seconds)) \\\n               .query(\"time_in_seconds == @min_time\") \\\n               [['fastestLapTime']] \\\n               .rename(columns={'fastestLapTime': 'lap_record'}) \\\n               .head(1)"}
{"question_id": 247, "response": "result = len(superhero.merge(hero_power, left_on='id', right_on='hero_id').merge(superpower, left_on='power_id', right_on='id').merge(colour, left_on='eye_colour_id', right_on='id').query(\"power_name == 'Agility' and colour == 'Blue'\"))"}
{"question_id": 248, "response": "result = superhero.merge(colour, left_on='eye_colour_id', right_on='id').merge(colour, left_on='hair_colour_id', right_on='id', suffixes=('_eye', '_hair')).query(\"colour_eye == 'Blue' and colour_hair == 'Blond'\")[['superhero_name']]"}
{"question_id": 251, "response": "result = superhero[\n    superhero['id'].isin(\n        hero_power.merge(\n            superpower[superpower['power_name'] == 'Super Strength'],\n            left_on='power_id',\n            right_on='id'\n        )['hero_id']\n    ) &\n    superhero['publisher_id'].isin(\n        publisher[publisher['publisher_name'] == 'Marvel Comics']['id']\n    )\n]['superhero_name']"}
{"question_id": 252, "response": "result = (\n    superhero.merge(publisher, left_on='publisher_id', right_on='id')\n    .merge(hero_attribute, left_on='id', right_on='hero_id')\n    .merge(attribute, left_on='attribute_id', right_on='id')\n    .query('attribute_name == \"Speed\"')\n    .sort_values('attribute_value', na_position='first')\n    .head(1)\n    [['publisher_name']]\n)"}
{"question_id": 254, "response": "result = superhero.merge(hero_attribute, left_on='id', right_on='hero_id').merge(attribute, left_on='attribute_id', right_on='id').query('attribute_name == \"Intelligence\"').sort_values('attribute_value', na_position='first')[['superhero_name']].head(1)"}
{"question_id": 255, "response": "result = superhero.merge(race, left_on='race_id', right_on='id')[['race']].loc[superhero['superhero_name'] == 'Copycat']"}
{"question_id": 257, "response": "result = superhero.merge(hero_power, left_on='id', right_on='hero_id').merge(superpower, left_on='power_id', right_on='id').loc[lambda x: x['power_name'] == 'Death Touch', ['superhero_name']]"}
{"question_id": 259, "response": "result = pd.DataFrame({\n    'percentage': [(superhero.merge(publisher, left_on='publisher_id', right_on='id')\n                   .merge(alignment, left_on='alignment_id', right_on='id')\n                   .query('alignment == \"Bad\"').shape[0] * 100.0 / superhero.shape[0])],\n    'marvel_count': [(superhero.merge(publisher, left_on='publisher_id', right_on='id')\n                     .merge(alignment, left_on='alignment_id', right_on='id')\n                     .query('alignment == \"Bad\" and publisher_name == \"Marvel Comics\"').shape[0] * 1.0)]\n})"}
{"question_id": 261, "response": "result = publisher[publisher['publisher_name'] == 'Star Trek']['id']"}
{"question_id": 265, "response": "result = superhero.merge(colour, left_on='eye_colour_id', right_on='id')[['superhero_name', 'height_cm', 'colour']].query('height_cm >= 170 and height_cm <= 190 and colour == \"No Colour\"')[['superhero_name']].drop_duplicates()"}
{"question_id": 266, "response": "result = superhero.merge(race, left_on='race_id', right_on='id').merge(colour, left_on='hair_colour_id', right_on='id').query('height_cm == 185 and race == \"Human\"')[['colour']].drop_duplicates()"}
{"question_id": 269, "response": "result = hero_power.merge(superpower, left_on='power_id', right_on='id')[['power_name']].loc[hero_power['hero_id'] == 1].drop_duplicates()"}
{"question_id": 270, "response": "result = len(hero_power.merge(superpower, left_on='power_id', right_on='id').query('power_name == \"Stealth\"'))"}
{"question_id": 272, "response": "result = (superhero.merge(hero_attribute, left_on='id', right_on='hero_id')\n         .merge(attribute, left_on='attribute_id', right_on='id')\n         .merge(publisher, left_on='publisher_id', right_on='id')\n         .query(\"publisher_name == 'Dark Horse Comics' and attribute_name == 'Durability'\")\n         .nlargest(1, 'attribute_value')[['superhero_name']])"}
{"question_id": 274, "response": "result = superhero.merge(publisher, left_on='publisher_id', right_on='id')[['superhero_name', 'publisher_name']].query('eye_colour_id == hair_colour_id and eye_colour_id == skin_colour_id')"}
{"question_id": 275, "response": "result = (superhero.merge(gender, left_on='gender_id', right_on='id')\n         .merge(colour, left_on='skin_colour_id', right_on='id')\n         .query('gender == \"Female\"')\n         .assign(blue_count=lambda x: (x['colour'] == 'Blue').astype(float))\n         .agg(percentage=('blue_count', lambda x: x.sum() * 100 / len(x)))\n         .iloc[0])"}
{"question_id": 276, "response": "result = len(hero_power.merge(superhero, left_on='hero_id', right_on='id').loc[lambda x: x['superhero_name'] == 'Amazo', 'power_id'])"}
{"question_id": 278, "response": "result = superhero.merge(colour, left_on=['eye_colour_id', 'hair_colour_id'], right_on=['id', 'id']).query(\"colour == 'Black'\")[['superhero_name']]"}
{"question_id": 279, "response": "result = superhero.merge(alignment, left_on='alignment_id', right_on='id')[['superhero_name']].loc[alignment['alignment'] == 'Neutral']"}
{"question_id": 280, "response": "result = len(hero_attribute.merge(attribute, left_on='attribute_id', right_on='id').query(\"attribute_name == 'Strength' and attribute_value == @hero_attribute['attribute_value'].max()\"))"}
{"question_id": 281, "response": "result = (superhero.merge(publisher, left_on='publisher_id', right_on='id', how='left')\n         .merge(gender, left_on='gender_id', right_on='id', how='inner')\n         .assign(female_marvel=lambda x: ((x['gender'] == 'Female') & (x['publisher_name'] == 'Marvel Comics')).astype(int),\n                female_total=lambda x: (x['gender'] == 'Female').astype(int))\n         .agg({'female_marvel': 'sum', 'female_total': 'sum'})\n         .pipe(lambda x: x['female_marvel'] / x['female_total'] * 100)\n         .iloc[0])"}
{"question_id": 283, "response": "result = superhero['height_cm'].sum().astype(float) / superhero['id'].count()"}
{"question_id": 284, "response": "result = pd.merge(superhero, hero_power, left_on='id', right_on='hero_id').merge(superpower, left_on='power_id', right_on='id').query(\"superhero_name == 'Abomination'\")[['power_name']]"}
{"question_id": 285, "response": "result = superhero.merge(hero_attribute, left_on='id', right_on='hero_id').merge(attribute, left_on='attribute_id', right_on='id').query('attribute_name == \"Speed\"').nlargest(1, 'attribute_value')[['superhero_name']]"}
{"question_id": 286, "response": "result = pd.merge(superhero, hero_attribute, left_on='id', right_on='hero_id').merge(attribute, left_on='attribute_id', right_on='id').query(\"superhero_name == '3-D Man'\")[['attribute_name', 'attribute_value']]"}
{"question_id": 289, "response": "result = (superhero.merge(colour, left_on='eye_colour_id', right_on='id', how='inner')\n         .assign(blue_eyes=lambda x: x['colour'] == 'Blue')\n         .agg(percentage=('blue_eyes', lambda x: x.sum() * 100 / len(x)))\n         .iloc[0]['percentage'])"}
{"question_id": 290, "response": "result = len(superhero.merge(gender, left_on='gender_id', right_on='id')[gender['gender'] == 'Male']) / len(superhero.merge(gender, left_on='gender_id', right_on='id')[gender['gender'] == 'Female'])"}
{"question_id": 291, "response": "result = superhero.merge(colour, left_on='eye_colour_id', right_on='id').query(\"full_name == 'Karen Beecher-Duncan'\")[['colour']]"}
{"question_id": 292, "response": "result = (superhero.merge(colour, left_on='eye_colour_id', right_on='id')\n         .query('weight_kg == 0 | weight_kg.isnull()')\n         .assign(diff=lambda x: (x['id_y'] == 7).astype(int) - (x['id_y'] == 1).astype(int))\n         .agg({'diff': 'sum'})\n         .iloc[0])"}
{"question_id": 293, "response": "result = len(superhero.merge(alignment, left_on='alignment_id', right_on='id').merge(colour, left_on='skin_colour_id', right_on='id').query(\"alignment == 'Bad' and colour == 'Green'\"))"}
{"question_id": 294, "response": "result = superhero.merge(hero_power, left_on='id', right_on='hero_id').merge(superpower, left_on='power_id', right_on='id').query(\"power_name == 'Wind Control'\")[['superhero_name']].sort_values('superhero_name', na_position='first')"}
{"question_id": 295, "response": "result = pd.merge(superhero, hero_power, left_on='id', right_on='hero_id')\\\n    .merge(superpower, left_on='power_id', right_on='id')\\\n    .merge(gender, left_on='gender_id', right_on='id')\\\n    .loc[lambda x: x['power_name'] == 'Phoenix Force', ['gender']]"}
{"question_id": 296, "response": "result = (superhero.merge(publisher, left_on='publisher_id', right_on='id')\n         .assign(dc=lambda x: (x['publisher_name'] == 'DC Comics').astype(int),\n                marvel=lambda x: (x['publisher_name'] == 'Marvel Comics').astype(int))\n         .agg({'dc': 'sum', 'marvel': 'sum'})\n         .pipe(lambda x: x['dc'] - x['marvel']))"}
{"question_id": 298, "response": "result = users[users['CreationDate'].str[:4] == '2011'][['DisplayName']]"}
{"question_id": 301, "response": "result = len(posts.merge(users, left_on='OwnerUserId', right_on='Id').query('DisplayName == \"csgillespie\"'))"}
{"question_id": 306, "response": "result = (pd.merge(posts, users, left_on='OwnerUserId', right_on='Id', suffixes=('_post', '_user'))\n         .query('Score > 5')\n         .assign(elderly=lambda x: (x['Age'] > 65).astype(int))\n         ['elderly'].mean() * 100)"}
{"question_id": 307, "response": "result = pd.merge(posts, comments, left_on='Id', right_on='PostId')[['FavoriteCount']].loc[(comments['CreationDate'] == '2014-04-23 20:29:39.0') & (comments['UserId'] == 3025)]"}
{"question_id": 308, "response": "result = pd.merge(comments, posts, left_on='PostId', right_on='Id', how='inner').query('UserId_x == 23853 and CreationDate_x == \"2013-07-12 09:08:18.0\"').assign(resylt=lambda x: np.where(x['ClosedDate'].isnull(), 'NOT well-finished', 'well-finished'))[['resylt']]"}
{"question_id": 309, "response": "result = len(users[users['DisplayName'] == 'Tiago Pasqualini'].merge(posts, left_on='Id', right_on='OwnerUserId'))"}
{"question_id": 310, "response": "result = users.merge(votes, left_on='Id', right_on='UserId')[['DisplayName']].loc[votes['Id'] == 6347]"}
{"question_id": 311, "response": "result = len(votes[votes['UserId'] == '24'].merge(posts, left_on='UserId', right_on='OwnerUserId')['Id_y'].unique()) / len(votes[votes['UserId'] == '24']['Id'].unique())"}
{"question_id": 312, "response": "result = posts[posts['Title'] == 'Integration of Weka and/or RapidMiner into Informatica PowerCenter/Developer'][['ViewCount']]"}
{"question_id": 313, "response": "result = comments[comments['Score'] == 17][['Text']]"}
{"question_id": 314, "response": "result = users.merge(comments, left_on='Id', right_on='UserId')[['DisplayName']].loc[comments['Text'] == 'thank you user93!']"}
{"question_id": 315, "response": "result = users.merge(posts, left_on='Id', right_on='OwnerUserId')[['DisplayName', 'Reputation']].loc[lambda x: x.Title == 'Understanding what Dassault iSight is doing?']"}
{"question_id": 316, "response": "result = pd.merge(posts, users, left_on='OwnerUserId', right_on='Id')[['DisplayName']].loc[posts['Title'] == 'Open source tools for visualizing multi-dimensional data?']"}
{"question_id": 317, "response": "result = pd.merge(posts, postHistory, left_on='Id', right_on='PostId').loc[posts['Title'] == 'Why square the difference instead of taking the absolute value in standard deviation?', ['Comment']]"}
{"question_id": 318, "response": "result = pd.merge(posts, votes, left_on='Id', right_on='PostId')\\\n    .merge(users, left_on='UserId', right_on='Id')\\\n    .loc[(votes['BountyAmount'] == 50) & (posts['Title'].str.contains('variance', case=False, na=False))]\\\n    [['DisplayName']]"}
{"question_id": 321, "response": "result = postHistory.merge(users, left_on='UserId', right_on='Id').merge(posts, left_on='PostId', right_on='Id')[['UserId_x']].where(users['Views'] >= 100).groupby('UserId_x').agg({'PostHistoryTypeId': lambda x: len(set(x))}).where(lambda x: x['PostHistoryTypeId'] == 1).reset_index()[['UserId_x']]"}
{"question_id": 322, "response": "result = (\n    badges[badges['Name'] == 'Student'].assign(\n        year=pd.to_datetime(badges['Date']).dt.strftime('%Y')\n    ).agg(\n        lambda x: (\n            (x['year'].eq('2010').sum() * 100.0 / len(x)) -\n            (x['year'].eq('2011').sum() * 100.0 / len(x))\n        )\n    ).iloc[0]\n)"}
{"question_id": 323, "response": "posts_grouped = posts.groupby('OwnerUserId').size().reset_index(name='post_count')\nfiltered_posts = posts_grouped[posts_grouped['post_count'] > 10]\nmerged_df = users.merge(filtered_posts, left_on='Id', right_on='OwnerUserId')\nresult = pd.DataFrame({'avg_upvotes': [merged_df['UpVotes'].mean()], 'avg_age': [merged_df['Age'].mean()]})"}
{"question_id": 324, "response": "result = votes['CreationDate'].astype('datetime64').dt.year.map(lambda x: 1 if x == 2010 else 0).sum() / votes['CreationDate'].astype('datetime64').dt.year.map(lambda x: 1 if x == 2011 else 0).sum()"}
{"question_id": 326, "response": "result = (users.merge(postHistory, left_on='Id', right_on='UserId')\n         .merge(posts, left_on='PostId', right_on='Id')\n         .query(\"DisplayName == 'Harvey Motulsky' or DisplayName == 'Noah Snyder'\")\n         .groupby('DisplayName')\n         .agg({'ViewCount': 'sum'})\n         .sort_values('ViewCount', ascending=False, na_position='last')\n         .head(1)\n         .reset_index()[['DisplayName']])"}
{"question_id": 327, "response": "result = pd.merge(users, postHistory, left_on='Id', right_on='UserId')\\\n    .merge(posts, left_on='PostId', right_on='Id')\\\n    .query(\"DisplayName == 'Mark Meckes' and CommentCount == 0\")\\\n    [['Tags']]"}
{"question_id": 328, "response": "result = (users.merge(postHistory, left_on='Id', right_on='UserId')\n         .merge(tags, left_on='PostId', right_on='ExcerptPostId')\n         .query('DisplayName == \"Community\"')\n         .assign(is_r=lambda x: x['TagName'] == 'r')\n         .agg(percentage=('is_r', lambda x: x.sum() * 100 / len(x) if len(x) > 0 else 0))\n         .iloc[0]['percentage'])"}
{"question_id": 329, "response": "result = pd.merge(users, postHistory, left_on='Id', right_on='UserId').merge(posts, left_on='PostId', right_on='Id')[['DisplayName', 'ViewCount']].groupby('DisplayName').agg({'ViewCount': 'sum'}).reset_index().assign(diff=lambda x: x.loc[x['DisplayName'] == 'Mornington', 'ViewCount'].sum() - x.loc[x['DisplayName'] == 'Amos', 'ViewCount'].sum())['diff'].iloc[0]"}
{"question_id": 333, "response": "result = len(users.merge(posts, left_on='Id', right_on='OwnerUserId').query('Location == \"United Kingdom\" and FavoriteCount >= 4')['Id_x'].unique())"}
{"question_id": 334, "response": "result = pd.merge(users, posts, left_on='Id', right_on='OwnerUserId').query('DisplayName == \"Harvey Motulsky\"')[['Id_y', 'Title']].sort_values('ViewCount', ascending=False, na_position='last').head(1)"}
{"question_id": 336, "response": "result = (pd.merge(users, posts, left_on='Id', right_on='OwnerUserId')\n         .assign(year=lambda x: pd.to_datetime(x['CreaionDate']).dt.strftime('%Y'))\n         .assign(condition=lambda x: ((x['year'] == '2011') & (x['Reputation'] > 1000)).astype(int))\n         ['condition'].mean() * 100)"}
{"question_id": 337, "response": "result = pd.merge(pd.merge(postHistory[postHistory['Text'] == 'Computer Game Datasets'], posts, left_on='PostId', right_on='Id'), users, left_on='LastEditorUserId', right_on='Id')[['ViewCount', 'DisplayName']]"}
{"question_id": 338, "response": "result = posts.merge(comments, left_on='Id', right_on='PostId').groupby('Id').size().sort_values(by=posts['Score'].iloc[0], ascending=False).head(1)"}
{"question_id": 340, "response": "result = (users.merge(posts, left_on='Id', right_on='OwnerUserId')\n         .merge(users.groupby('Reputation')['Reputation'].max().reset_index(), on='Reputation')\n         .assign(score_over_50=lambda x: (x['Score'] > 50).astype(int))\n         .agg(percentage=('score_over_50', lambda x: x.sum() * 100 / len(x) if len(x) > 0 else None))\n         .iloc[0]['percentage'])"}
{"question_id": 341, "response": "result = tags[tags['TagName'] == 'sample'][['ExcerptPostId', 'WikiPostId']]"}
{"question_id": 344, "response": "result = len(comments.merge(posts, left_on='PostId', right_on='Id').query('CommentCount == 1 and Score_x == 0'))"}
{"question_id": 346, "response": "result = cards[cards['cardKingdomFoilId'].notna() & cards['cardKingdomId'].notna()][['id']]"}
{"question_id": 347, "response": "result = cards[cards['borderColor'] == 'borderless' & ((cards['cardKingdomFoilId'].isnull()) | (cards['cardKingdomId'].isnull()))][['id']]"}
{"question_id": 348, "response": "result = cards.merge(legalities, on='uuid', how='inner')[['id']].loc[(legalities['format'] == 'gladiator') & (legalities['status'] == 'Banned') & (cards['rarity'] == 'mythic')].drop_duplicates()"}
{"question_id": 349, "response": "result = pd.merge(cards, legalities, on='uuid', how='inner')[['status']].loc[(cards['type'] == 'Artifact') & (legalities['format'] == 'vintage') & (cards['side'].isnull())].drop_duplicates()"}
{"question_id": 355, "response": "result = cards[cards['name'] == \"Ancestor's Chosen\"]['borderColor'].unique()"}
{"question_id": 357, "response": "result = (df_cards[df_cards['borderColor'] == 'borderless'].shape[0] * 100.0) / df_cards.shape[0]"}
{"question_id": 359, "response": "result = len(cards[(cards['originalType'] == 'Summon - Angel') & (cards['subtypes'] != 'Angel')])"}
{"question_id": 360, "response": "result = cards[cards['duelDeck'] == 'a'][['id']]"}
{"question_id": 361, "response": "result = len(cards.merge(legalities, on='uuid', how='inner')[(legalities['status'] == 'Banned') & (cards['borderColor'] == 'white')]['id'].unique())"}
{"question_id": 362, "response": "result = cards.merge(foreign_data, on='uuid', how='inner')[['name']].loc[(cards['originalType'] == 'Artifact') & (cards['colors'] == 'B')].drop_duplicates()"}
{"question_id": 363, "response": "result = cards[\n    (cards['availability'] == 'mtgo,paper') & \n    (cards['borderColor'] == 'black') & \n    (cards['frameVersion'] == '2003') & \n    (cards['layout'] == 'normal')\n]['manaCost']"}
{"question_id": 364, "response": "result = (df_cards[(df_cards['isTextless'] == 0) & (df_cards['isStorySpotlight'] == 1)].shape[0] * 100.0) / df_cards['id'].count()"}
{"question_id": 365, "response": "result = len(sets.merge(set_translations, left_on='code', right_on='setCode').query(\"language == 'Portuguese (Brazil)' and block == 'Commander'\"))"}
{"question_id": 366, "response": "result = pd.merge(cards, foreign_data, on='uuid').loc[(foreign_data['language'] == 'German') & (cards['subtypes'].notna()) & (cards['supertypes'].notna()), ['subtypes', 'supertypes']]"}
{"question_id": 368, "response": "result = len(cards.merge(legalities, on='uuid').merge(rulings, on='uuid').query(\"format == 'premodern' and text == 'This is a triggered mana ability.' and side.isna()\"))"}
{"question_id": 369, "response": "result = foreign_data[\n    (foreign_data['uuid'].isin(\n        cards[\n            (cards['types'] == 'Creature') & \n            (cards['layout'] == 'normal') & \n            (cards['borderColor'] == 'black') & \n            (cards['artist'] == 'Matthew D. Wilson')\n        ]['uuid']\n    )) &\n    (foreign_data['language'] == 'French')\n]['name']"}
{"question_id": 372, "response": "result = (cards.merge(foreign_data, on='uuid', how='inner')\n         .assign(french_count=lambda x: (x['language'] == 'French').astype(int))\n         .query('power.isna() | (power == \"*\")')\n         .agg({'french_count': 'sum', 'id_x': 'count'})\n         .pipe(lambda x: x['french_count'] * 100 / x['id_x'])\n         .iloc[0])"}
{"question_id": 376, "response": "result = foreign_data[foreign_data['name'] == 'A Pedra Fellwar']['language'].unique()"}
{"question_id": 377, "response": "result = cards[cards['name'].isin(['Serra Angel', 'Shrine Keeper'])].sort_values('convertedManaCost', ascending=False, na_position='last').head(1)[['name']]"}
{"question_id": 378, "response": "result = set_translations[\n    (set_translations['setCode'].isin(\n        cards[cards['name'] == \"Ancestor's Chosen\"]['setCode']\n    )) &\n    (set_translations['language'] == 'Italian')\n]['translation']"}
{"question_id": 381, "response": "result = pd.merge(sets, set_translations, left_on='code', right_on='setCode').query(\"name == 'Eighth Edition' and language == 'Chinese Simplified'\")[['translation']]"}
{"question_id": 385, "response": "result = len(sets.merge(set_translations, left_on='code', right_on='setCode').query('translation.notna() and baseSetSize < 100 and language == \"Italian\"'))"}
{"question_id": 387, "response": "result = cards.merge(sets, left_on='setCode', right_on='code').query('name_y == \"Coldsnap\" and convertedManaCost > 5')[(cards['power'] == '*') | (cards['power'].isna())].shape[0]"}
{"question_id": 389, "response": "result = pd.merge(foreign_data, cards, on='uuid').merge(sets, left_on='setCode', right_on='code').loc[lambda x: (x['name_y'] == 'Coldsnap') & (x['language'] == 'Italian'), 'text_x'].drop_duplicates()"}
{"question_id": 391, "response": "result = (cards.merge(sets, left_on='setCode', right_on='code')\n         .loc[lambda x: x['name_y'] == 'Coldsnap']\n         .assign(count_7_cmc = lambda x: (x['convertedManaCost'] == 7).astype(float))\n         .agg(percentage = ('count_7_cmc', lambda x: x.sum() * 100 / len(x)))\n         .iloc[0])"}
{"question_id": 392, "response": "result = (cards.merge(sets, left_on='setCode', right_on='code')\n          .loc[lambda x: x['name_y'] == 'Coldsnap']\n          .assign(has_both=lambda x: (~x['cardKingdomFoilId'].isna()) & (~x['cardKingdomId'].isna()))\n          .agg(pct=('has_both', lambda x: x.sum() * 100 / len(x)))['pct']\n)"}
{"question_id": 394, "response": "result = cards.merge(legalities, on='uuid', how='inner')[['name', 'format']].loc[(cards['edhrecRank'] == 1) & (legalities['status'] == 'Banned')].groupby(['name', 'format']).first().reset_index()"}
{"question_id": 395, "response": "hour_set = sets[sets['name'] == 'Hour of Devastation']['code'].iloc[0]\nresult = pd.merge(legalities, cards, on='uuid', how='inner')[['name', 'status', 'format']]\nresult = result[result['setCode'] == hour_set]\nresult['format'] = result.apply(lambda x: x['format'] if x['status'] == 'Legal' else None, axis=1)\nresult = result[['name', 'format']].drop_duplicates()"}
{"question_id": 397, "response": "result = pd.merge(cards, legalities, on='uuid', how='inner')\\\n    .query(\"artist == 'Allen Williams'\")\\\n    .assign(conditional_name=lambda x: np.where(x['status'] == 'Banned', x['name'], 'NO'))\\\n    [['frameVersion', 'name', 'conditional_name']]\\\n    .drop_duplicates()"}
{"question_id": 399, "response": "result = atom.merge(bond, on='molecule_id').query(\"bond_type == '-' and element == 'o'\").groupby('molecule_id')['element'].count().mean()"}
{"question_id": 401, "response": "result = bond.merge(molecule, on='molecule_id', how='inner')[\n    (bond['bond_type'] == '#') & \n    (molecule['label'] == '+')\n]['molecule_id'].drop_duplicates()"}
{"question_id": 402, "response": "result = (atom.merge(bond, on='molecule_id')\n         .loc[lambda x: x['bond_type'] == '=']\n         .assign(carbon_count=lambda x: x['element'] == 'c')\n         .agg(result=lambda x: 100 * x['carbon_count'].nunique() / x['atom_id'].nunique())\n         .iloc[0])"}
{"question_id": 403, "response": "result = atom.merge(connected, on='atom_id', how='inner').loc[lambda x: x['bond_id'] == 'TR004_8_9', ['element']].drop_duplicates()"}
{"question_id": 406, "response": "result = atom.merge(molecule, on='molecule_id').query(\"label == '-'\").groupby('element')['molecule_id'].nunique().sort_values().head(1).index[0]"}
{"question_id": 407, "response": "result = bond.merge(connected, on='bond_id', how='inner').query(\"(atom_id == 'TR004_8' and atom_id2 == 'TR004_20') or (atom_id2 == 'TR004_8' and atom_id == 'TR004_20')\")['bond_type']"}
{"question_id": 409, "response": "result = 100 - (atom[atom['element'] == 'f'].merge(molecule[molecule['label'] == '+'], on='molecule_id')['molecule_id'].nunique() * 100.0 / molecule[molecule['label'] == '+']['molecule_id'].nunique())"}
{"question_id": 410, "response": "result = (\n    atom.merge(molecule, on='molecule_id')\n    .merge(bond, on='molecule_id')\n    .query(\"bond_type == '#'\")\n    .groupby('molecule_id')\n    .agg({'label': 'first'})\n    .reset_index()\n    .assign(is_positive=lambda x: x['label'] == '+')\n    .agg(lambda x: x['is_positive'].sum() * 100 / len(x) if len(x) > 0 else 0)\n    .iloc[0]\n)"}
{"question_id": 413, "response": "result = round((molecule[molecule['label'] == '+']['molecule_id'].count() / molecule['molecule_id'].count() * 100), 3)"}
{"question_id": 414, "response": "result = round((atom[atom['molecule_id'] == 'TR206']['element'].eq('h').sum() * 100 / atom[atom['molecule_id'] == 'TR206'].shape[0]), 4)"}
{"question_id": 415, "response": "result = pd.merge(atom, molecule, on='molecule_id', how='inner')[['element', 'label']].loc[lambda x: x['molecule_id'] == 'TR060'].drop_duplicates()"}
{"question_id": 416, "response": "result = bond[bond['molecule_id'] == 'TR010'].groupby('bond_type').size().sort_values(ascending=False).head(1).index[0]"}
{"question_id": 417, "response": "result = bond.merge(molecule, on='molecule_id', how='inner').query(\"bond_type == '-' and label == '-'\")['molecule_id'].drop_duplicates().nsmallest(3)"}
{"question_id": 419, "response": "result = pd.merge(bond, connected, on='bond_id', how='inner').query(\"bond_id == 'TR001_6_9'\")[['bond_type', 'atom_id', 'atom_id2']]"}
{"question_id": 420, "response": "result = len(connected[connected['atom_id'].str[-2:] == '19']['bond_id'])"}
{"question_id": 423, "response": "result = connected.merge(atom, on='atom_id', how='inner').loc[lambda x: (x['bond_id'].isin(connected.merge(atom, on='atom_id', how='inner').loc[lambda y: y['element'] == 'p', 'bond_id'])) & (x['element'] == 'n'), 'bond_id']"}
{"question_id": 424, "response": "result = molecule.merge(\n    bond.loc[bond['bond_type'] == '=']\n    .groupby('molecule_id')\n    .size()\n    .reset_index(name='count')\n    .sort_values('count', ascending=False, na_position='last')\n    .head(1),\n    on='molecule_id'\n)[['label']]"}
{"question_id": 426, "response": "result = atom[~atom['atom_id'].isin(connected['atom_id'])]['element'].drop_duplicates()"}
{"question_id": 427, "response": "result = pd.merge(atom, connected, on='atom_id').merge(bond, on='bond_id').query(\"bond_type == '#' and molecule_id == 'TR041'\")[['atom_id', 'atom_id2']]"}
{"question_id": 428, "response": "result = pd.merge(connected, atom, on='atom_id').query(\"bond_id == 'TR144_8_19'\")[['element']]"}
{"question_id": 429, "response": "result = pd.merge(bond, connected, on='bond_id').merge(atom, left_on='atom_id', right_on='atom_id').query(\"bond_type == '#'\")[['element']].drop_duplicates()"}
{"question_id": 432, "response": "result = (atom[atom['molecule_id'].isin(molecule[molecule['molecule_id'].isin(bond[bond['bond_type'] == '-']['molecule_id'].unique())]['molecule_id'])]['element'].eq('cl').sum() * 100.0 / atom[atom['molecule_id'].isin(molecule[molecule['molecule_id'].isin(bond[bond['bond_type'] == '-']['molecule_id'].unique())]['molecule_id'])].shape[0])"}
{"question_id": 433, "response": "result = pd.merge(connected, atom, on='atom_id').query(\"bond_id == 'TR001_10_11'\")[['element']]"}
{"question_id": 435, "response": "result = atom.merge(molecule, on='molecule_id', how='inner')[\n    (molecule['label'] == '+') & \n    (atom['atom_id'].str[-1] == '4') & \n    (atom['atom_id'].str.len() == 7)\n]['element'].drop_duplicates()"}
{"question_id": 437, "response": "result = molecule[molecule['label'] == '-'].merge(atom, on='molecule_id').groupby('molecule_id').size().reset_index(name='count').query('count > 5')[['molecule_id']]"}
{"question_id": 439, "response": "result = pd.merge(schools, frpm, on='CDSCode', how='inner').loc[lambda x: x['Enrollment (K-12)'] + x['Enrollment (Ages 5-17)'] > 500, ['CDSCode']]"}
{"question_id": 440, "response": "result = pd.merge(frpm, satscores, left_on='CDSCode', right_on='cds').loc[satscores['NumGE1500'].astype(float) / satscores['NumTstTakr'].replace(0, np.nan) > 0.3]['Free Meal Count (Ages 5-17)'].astype(float).div(frpm['Enrollment (Ages 5-17)'].replace(0, np.nan)).max()"}
{"question_id": 441, "response": "result = pd.merge(schools, satscores, left_on='CDSCode', right_on='cds')[['CharterNum', 'AvgScrWrite']][\n    (satscores['AvgScrWrite'] > 499) & \n    (schools['CharterNum'].notna())\n].assign(WritingScoreRank=lambda x: x['AvgScrWrite'].rank(method='min', ascending=False))"}
{"question_id": 442, "response": "result = pd.merge(schools, frpm, on='CDSCode', how='inner')[['School', 'Street']].loc[frpm['Enrollment (K-12)'] - frpm['Enrollment (Ages 5-17)'] > 30]"}
{"question_id": 443, "response": "result = pd.merge(satscores, frpm, left_on='cds', right_on='CDSCode', how='inner').query('`Free Meal Count (K-12)` / `Enrollment (K-12)` > 0.1 and NumGE1500 > 0')[['School Name']]"}
{"question_id": 445, "response": "result = pd.merge(frpm, schools, on='CDSCode', how='inner')[['School Name', 'Street', 'City', 'State', 'Zip']].loc[(schools['County'] == 'Monterey') & (frpm['FRPM Count (Ages 5-17)'] > 800) & (frpm['School Type'] == 'High Schools (Public)')]"}
{"question_id": 447, "response": "avg_diff = pd.merge(frpm, schools, on='CDSCode').query(\"FundingType == 'Locally funded'\")['Enrollment (K-12)'].sub(pd.merge(frpm, schools, on='CDSCode').query(\"FundingType == 'Locally funded'\")['Enrollment (Ages 5-17)']).mean()\n\nresult = pd.merge(frpm, schools, on='CDSCode').query(\"FundingType == 'Locally funded' and (`Enrollment (K-12)` - `Enrollment (Ages 5-17)`) > @avg_diff\")[['School', 'DOCType']]"}
{"question_id": 449, "response": "result = pd.merge(frpm, schools, on='CDSCode', how='inner').query('SOC == \"66\"')[['FRPM Count (K-12)', 'Enrollment (K-12)']].assign(ratio=lambda x: x['FRPM Count (K-12)'] / x['Enrollment (K-12)'].replace(0, np.nan))['ratio'].nlargest(5)"}
{"question_id": 452, "response": "result = satscores.merge(schools, left_on='cds', right_on='CDSCode').loc[lambda x: (pd.to_datetime(x['OpenDate']).dt.strftime('%Y') == '1980') & (x['County'] == 'Fresno'), 'NumTstTakr'].mean()"}
{"question_id": 453, "response": "result = pd.merge(satscores, schools, left_on='cds', right_on='CDSCode').query(\"District == 'Fresno Unified' and AvgScrRead.notna()\").sort_values('AvgScrRead').iloc[0:1][['Phone']]"}
{"question_id": 455, "response": "result = pd.merge(satscores, schools, left_on='cds', right_on='CDSCode').query(\"AdmFName1 == 'Ricci' and AdmLName1 == 'Ulrich'\")[['School', 'AvgScrWrite']]"}
{"question_id": 456, "response": "result = pd.merge(frpm, schools, on='CDSCode', how='inner').loc[lambda x: x['DOC'] == '31'].nlargest(1, 'Enrollment (K-12)')['School']"}
{"question_id": 457, "response": "result = len(schools[(schools['DOC'] == '52') & (schools['County'] == 'Alameda') & (pd.to_datetime(schools['OpenDate']).dt.year == 1980)]) / 12.0"}
{"question_id": 458, "response": "result = float(len(schools[(schools['StatusType'] == 'Merged') & (schools['County'] == 'Orange') & (schools['DOC'].astype(int) == 54)]) / len(schools[(schools['StatusType'] == 'Merged') & (schools['County'] == 'Orange') & (schools['DOC'].astype(int) == 52)]) if len(schools[(schools['StatusType'] == 'Merged') & (schools['County'] == 'Orange') & (schools['DOC'].astype(int) == 52)]) != 0 else None)"}
{"question_id": 459, "response": "result = pd.merge(satscores, schools, left_on='cds', right_on='CDSCode')[['MailStreet', 'School']].iloc[6:7].sort_values('AvgScrMath', ascending=False)"}
{"question_id": 463, "response": "result = schools[((schools['County'] == 'San Diego') | (schools['County'] == 'Santa Barbara')) & (schools['Virtual'] == 'F')].groupby('County')['Virtual'].count().sort_values(ascending=False).head(1)"}
{"question_id": 464, "response": "result = schools.loc[schools['Longitude'].abs().sort_values(ascending=False, na_position='last').index[0], 'GSoffered']"}
{"question_id": 465, "response": "result = (frpm.merge(schools, on='CDSCode', how='inner')\n         .query('Magnet == 1 and GSoffered == \"K-8\" and `NSLP Provision Status` == \"Multiple Provision Types\"')\n         .groupby('City')\n         .agg({'CDSCode': 'count'})\n         .reset_index())"}
{"question_id": 467, "response": "result = pd.merge(frpm, schools, on='CDSCode', how='inner')[['AdmEmail1', 'AdmEmail2']].loc[\n    (schools['County'] == 'San Bernardino') & \n    (schools['City'] == 'San Bernardino') & \n    (schools['DOC'].astype(int) == 54) & \n    (pd.to_datetime(schools['OpenDate']).dt.year.between(2009, 2010)) & \n    (schools['SOC'].astype(int) == 62)\n]"}
{"question_id": 468, "response": "result = len(district.merge(account, on='district_id').query(\"A3 == 'east Bohemia' and frequency == 'POPLATEK PO OBRATU'\"))"}
{"question_id": 469, "response": "result = len(client[client['gender'] == 'F'].merge(district, on='district_id', how='inner').query('6000 <= A11 <= 10000')['district_id'].unique())"}
{"question_id": 471, "response": "district_id_subquery = client[client['gender'] == 'F'].sort_values('birth_date', ascending=True, na_position='first').iloc[0]['district_id']\n\na11_range = district['A11'].max() - district['A11'].min()\n\nresult = (account.merge(district, on='district_id')\n         .merge(disp, on='account_id')\n         .merge(client, on='client_id')\n         .loc[lambda x: x['district_id'] == district_id_subquery]\n         .sort_values('A11', ascending=False, na_position='last')\n         .head(1)[['account_id']]\n         .assign(range=a11_range))"}
{"question_id": 474, "response": "result = loan.merge(account, on='account_id', suffixes=('', '_account')).query('duration > 12 and date.str[:4] == \"1993\"').nlargest(1, 'amount')[['account_id']]"}
{"question_id": 475, "response": "result = len(district.merge(client, on='district_id', how='inner').query(\"gender == 'F' and birth_date.str[:4] < '1950' and A2 == 'Sokolov'\"))"}
{"question_id": 477, "response": "result = (client.merge(district, on='district_id')\n         .query(\"A3 == 'south Bohemia'\")\n         .groupby('A4')\n         .agg(male_percent=('gender', lambda x: (x == 'M').sum() * 100 / len(x)))\n         .sort_values('A4', key=lambda x: x.astype(int), ascending=False)\n         .head(1)\n         .iloc[0])"}
{"question_id": 478, "response": "result = (\n    loan.merge(account, on='account_id')\n    .merge(trans, on='account_id')\n    .loc[loan['date'] == '1993-07-05']\n    .assign(\n        balance_1998 = lambda x: x['balance'].where(x['date_y'] == '1998-12-27', 0),\n        balance_1993 = lambda x: x['balance'].where(x['date_y'] == '1993-03-22', 0)\n    )\n    .agg({\n        'balance_1998': 'sum',\n        'balance_1993': 'sum'\n    })\n    .pipe(lambda x: (x['balance_1998'] - x['balance_1993']) * 100 / x['balance_1993'] if x['balance_1993'] != 0 else float('nan'))\n)"}
{"question_id": 479, "response": "result = (loan[loan['status'] == 'A']['amount'].sum() * 100.0) / loan['amount'].sum()"}
{"question_id": 480, "response": "result = (loan[loan['amount'] < 100000].assign(status_c=lambda x: x['status'] == 'C')['status_c'].sum() * 100.0 / loan[loan['amount'] < 100000]['account_id'].count())"}
{"question_id": 485, "response": "result = len(client.merge(district, on='district_id', how='inner').query('gender == \"M\" and A15 == @district.nlargest(2, \"A15\").iloc[1][\"A15\"]'))"}
{"question_id": 488, "response": "result = pd.merge(district, account, on='district_id').merge(disp, on='account_id').query(\"type == 'OWNER' and A2 == 'Tabor'\")[['account_id']]"}
{"question_id": 490, "response": "result = pd.merge(district, account, on='district_id')\\\n           .merge(disp, on='account_id')\\\n           .query(\"type != 'OWNER' and A11 >= 8000 and A11 <= 9000\")\\\n           [['type']]"}
{"question_id": 493, "response": "result = (district.merge(client, on='district_id')\n         .query('A11 > 10000')\n         .assign(female_count=lambda x: (x['gender'] == 'F').astype(int))\n         .agg(percentage=('female_count', lambda x: x.sum() * 100 / len(x)))\n         .iloc[0])"}
{"question_id": 497, "response": "result = (client.merge(district, on='district_id')\n         .merge(account, on='district_id')\n         .merge(disp, on=['client_id', 'account_id'])\n         .query('gender == \"F\"')\n         .sort_values(['birth_date', 'A11'], na_position='first')\n         .head(1)[['account_id']])"}
{"question_id": 498, "response": "result = pd.merge(account, loan, on='account_id').loc[lambda x: (x['status'].isin(['C', 'D'])) & (x['frequency'] == 'POPLATEK PO OBRATU')]['amount'].mean()"}
{"question_id": 499, "response": "result = disp.merge(card, on='disp_id').merge(client, on='client_id').query(\"card.type == 'gold' and disp.type == 'OWNER'\")[['client_id']].assign(age=lambda x: pd.Timestamp.now().year - pd.to_datetime(client.birth_date).dt.year)"}
