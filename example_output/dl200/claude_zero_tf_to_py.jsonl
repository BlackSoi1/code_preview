{"question_id": 0, "response": "result = torch.from_numpy(np_array)"}
{"question_id": 1, "response": "result = torch.tensor(3.43)"}
{"question_id": 2, "response": "result = torch.arange(1, 10).reshape(3, 3)"}
{"question_id": 3, "response": "result = torch.diag(torch.arange(1, 6))"}
{"question_id": 4, "response": "result = torch.eye(4)"}
{"question_id": 5, "response": "result = tensor1 + tensor2"}
{"question_id": 6, "response": "result = tensor1 - tensor2"}
{"question_id": 7, "response": "result = torch.matmul(tensor1, tensor2)"}
{"question_id": 8, "response": "result = torch.zeros(shape)"}
{"question_id": 9, "response": "result = tensor.shape"}
{"question_id": 10, "response": "result = tensor.dim()"}
{"question_id": 11, "response": "result = tensor[1:, 1:]"}
{"question_id": 12, "response": "result = tensor.numpy()"}
{"question_id": 13, "response": "result = tensor.reshape(2, 6)"}
{"question_id": 14, "response": "result = torch.randn(4, 6, requires_grad=True)"}
{"question_id": 15, "response": "y = x ** 2\ny.backward()\nresult = x.grad"}
{"question_id": 16, "response": "y = (x - 5) ** 2\ny.backward()\nwith torch.no_grad():\n    x.data -= 0.1 * x.grad\nresult = x"}
{"question_id": 17, "response": "Q = 3 * a**3 - b**2\nQ.backward(torch.ones_like(Q))\nresult = [a.grad, b.grad]"}
{"question_id": 18, "response": "(train_dataset, _) = mnist.load_data()"}
{"question_id": 19, "response": "result = torch.nn.functional.relu(tensor)"}
{"question_id": 20, "response": "result = torch.clamp(torch.nn.functional.relu(tensor), min=0, max=6)"}
{"question_id": 21, "response": "result = torch.sigmoid(tensor)"}
{"question_id": 22, "response": "class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork()"}
{"question_id": 23, "response": "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"}
{"question_id": 24, "response": "loss_fn = nn.CrossEntropyLoss()"}
{"question_id": 25, "response": "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"}
{"question_id": 26, "response": "result = tensor.max().item()"}
{"question_id": 27, "response": "result = torch.mean(tensor).item()"}
{"question_id": 28, "response": "result = torch.prod(tensor).item()"}
{"question_id": 29, "response": "result = torch.unique(tensor)"}
{"question_id": 30, "response": "result = torch.bitwise_xor(a, b)"}
{"question_id": 31, "response": "sines = torch.sin(angles)"}
{"question_id": 32, "response": "class TinyModel(torch.nn.Module):\n    def __init__(self):\n        super(TinyModel, self).__init__()\n        self.layer1 = torch.nn.Linear(100, 200)\n        self.relu = torch.nn.ReLU()\n        self.layer2 = torch.nn.Linear(200, 10)\n        self.softmax = torch.nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.relu(x)\n        x = self.layer2(x)\n        x = self.softmax(x)\n        return x"}
{"question_id": 33, "response": "total_params = sum(p.numel() for p in model.parameters())"}
{"question_id": 34, "response": "first_layer_params = sum(p.numel() for p in model.linear1.parameters())"}
{"question_id": 35, "response": "output = torch.nn.functional.max_pool2d(tensor, kernel_size=2, stride=2, padding=0)"}
{"question_id": 36, "response": "bn_layer = nn.BatchNorm1d(5)\noutput = bn_layer(tensor)"}
{"question_id": 37, "response": "dropout = torch.nn.Dropout(p=0.5)\noutput = dropout(tensor)"}
{"question_id": 38, "response": "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"}
{"question_id": 39, "response": "loss = torch.nn.MSELoss()(target_tensor, input_tensor)"}
{"question_id": 40, "response": "optimizer.zero_grad()\noutputs = model(input_tensor)\nloss = criterion(outputs, target)\nloss.backward()\noptimizer.step()"}
{"question_id": 41, "response": "loss = torch.nn.L1Loss()(target_tensor, input_tensor)"}
{"question_id": 42, "response": "hinge_loss = torch.nn.HingeEmbeddingLoss()\nloss = hinge_loss(input_tensor, target_tensor)"}
{"question_id": 43, "response": "huber_loss = torch.nn.HuberLoss()\nloss = huber_loss(input_tensor, target_tensor)"}
{"question_id": 44, "response": "model = nn.Sequential(\n    nn.Conv2d(1, 20, kernel_size=5, padding=0),\n    nn.ReLU(),\n    nn.Conv2d(20, 64, kernel_size=5, padding=0),\n    nn.ReLU()\n)"}
{"question_id": 45, "response": "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = TinyModel()\nmodel = model.to(device)"}
{"question_id": 46, "response": "torch.save(model.state_dict(), 'seq_model.pth')"}
{"question_id": 47, "response": "loaded_model = torch.load('seq_model.pth')"}
{"question_id": 48, "response": "model = nn.Sequential(\n    nn.Embedding(num_embeddings=1000, embedding_dim=64),\n    nn.LSTM(input_size=64, hidden_size=128, batch_first=True),\n    Lambda(lambda x: x[0][:, -1, :]),\n    nn.Linear(128, 10)\n)"}
{"question_id": 49, "response": "class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.lstm1 = nn.LSTM(10, 64, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(128, 32, bidirectional=True, batch_first=True)\n        self.dense = nn.Linear(64, 10)\n        \n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x = self.dense(x[:, -1, :])\n        return x\n\nmodel = Model()"}
{"question_id": 50, "response": "cosine_similarity = -torch.nn.functional.cosine_similarity(tensor1.unsqueeze(0), tensor2.unsqueeze(0))"}
{"question_id": 51, "response": "euclidean_distance = torch.norm(tensor1 - tensor2)"}
{"question_id": 52, "response": "hello_embed = embeds(lookup_tensor)"}
{"question_id": 53, "response": "def scaled_dot_product_attention(Q, K, V, mask=None):\n    matmul_qk = torch.matmul(Q, K.transpose(-2, -1))\n    depth = K.size(-1)\n    logits = matmul_qk / torch.sqrt(torch.tensor(depth, dtype=torch.float32))\n    if mask is not None:\n        mask = mask.float()\n        logits += (mask * -1e9)\n    attention_weights = torch.softmax(logits, dim=-1)\n    output = torch.matmul(attention_weights, V)\n    return output"}
{"question_id": 54, "response": "def split_heads(num_heads, d_k, x):\n    batch_size, seq_length, d_model = x.shape\n    x = x.reshape(batch_size, seq_length, num_heads, d_k)\n    return x.permute(0, 2, 1, 3)"}
{"question_id": 55, "response": "def combine_heads(d_model, x):\n    batch_size, num_heads, seq_length, depth = x.size()\n    x = x.permute(0, 2, 1, 3)\n    return x.reshape(batch_size, seq_length, d_model)"}
{"question_id": 56, "response": "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n\n    def split_heads(self, x):\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V"}
{"question_id": 57, "response": "class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)"}
{"question_id": 58, "response": "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super().__init__()\n        position = torch.arange(0, max_seq_length, dtype=torch.float32).unsqueeze(-1)\n        i = torch.arange(0, d_model, 2, dtype=torch.float32)\n        div_term = torch.exp(i * -(math.log(10000.0) / d_model))\n        pe = torch.cat([torch.sin(position * div_term), torch.cos(position * div_term)], dim=-1)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :]"}
{"question_id": 59, "response": "class SelfAttention(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.query = nn.Linear(input_dim, input_dim)\n        self.key = nn.Linear(input_dim, input_dim)\n        self.value = nn.Linear(input_dim, input_dim)\n        \n    def forward(self, x):\n        queries = self.query(x)\n        keys = self.key(x)\n        values = self.value(x)\n        \n        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.input_dim ** 0.5)\n        attention = torch.softmax(scores, dim=-1)\n        weighted = torch.matmul(attention, values)\n        return weighted"}
{"question_id": 60, "response": "def softmax(x):\n    exp_x = torch.exp(x - torch.max(x))\n    return exp_x / torch.sum(exp_x, dim=-1, keepdim=True)"}
{"question_id": 61, "response": "x_values = [i for i in range(11)]\nx_train = np.array(x_values, dtype=np.float32)\nx_train = x_train.reshape(-1, 1)\nx_train = torch.from_numpy(x_train)\ny_values = [2*i + 1 for i in x_values]\ny_train = np.array(y_values, dtype=np.float32)\ny_train = y_train.reshape(-1, 1)\ny_train = torch.from_numpy(y_train)"}
{"question_id": 62, "response": "class LinearRegression(torch.nn.Module):\n    def __init__(self, inputSize, outputSize):\n        super().__init__()\n        self.linear = torch.nn.Linear(inputSize, outputSize)\n    \n    def forward(self, x):\n        return self.linear(x)\n\n# model = LinearRegression(inputSize, outputSize)"}
{"question_id": 63, "response": "learningRate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"}
{"question_id": 64, "response": "for epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = model(x_train)\n    loss = torch.nn.MSELoss()(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())"}
{"question_id": 65, "response": "x_train = torch.tensor(x_values, dtype=torch.float32).reshape(-1, 1)\ny_train = torch.tensor(y_values, dtype=torch.float32).reshape(-1, 1)"}
{"question_id": 66, "response": "class LogisticRegression(nn.Module):\n    def __init__(self, inputSize, outputSize):\n        super().__init__()\n        self.linear = nn.Linear(inputSize, outputSize)\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        return self.sigmoid(self.linear(x))"}
{"question_id": 67, "response": "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"}
{"question_id": 68, "response": "for epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = model(x_train)\n    loss = nn.BCELoss()(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss.item())"}
{"question_id": 69, "response": "atanh_values = torch.atanh(values)"}
{"question_id": 70, "response": "class RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size, hidden_size)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.h2o = nn.Linear(hidden_size, output_size)\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input, hidden):\n        hidden = self.i2h(input) + self.h2h(hidden)\n        hidden = self.tanh(hidden)\n        output = self.h2o(hidden)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)"}
{"question_id": 71, "response": "def accuracy(y_pred, y_true):\n    correct = torch.sum(y_pred == y_true).float()\n    total = torch.tensor(y_true.size(0)).float()\n    return correct / total"}
{"question_id": 72, "response": "def precision(y_pred, y_true):\n    true_positive = torch.sum((y_pred == 1) & (y_true == 1)).float()\n    predicted_positive = torch.sum(y_pred == 1).float()\n    return true_positive / predicted_positive"}
{"question_id": 73, "response": "def recall(y_pred, y_true):\n    true_positive = torch.sum((y_pred == 1) & (y_true == 1)).float()\n    actual_positive = torch.sum(y_true == 1).float()\n    return true_positive / actual_positive"}
{"question_id": 74, "response": "def f1_score(y_pred, y_true):\n    true_positive = torch.sum((y_pred == 1) & (y_true == 1)).float()\n    predicted_positive = torch.sum(y_pred == 1).float()\n    actual_positive = torch.sum(y_true == 1).float()\n    precision = true_positive / predicted_positive if predicted_positive != 0 else 0\n    recall = true_positive / actual_positive if actual_positive != 0 else 0\n    return 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0"}
{"question_id": 75, "response": "tensor_squeezed = tensor.squeeze()"}
{"question_id": 76, "response": "matrix.fill_diagonal_(1)\nupper_indices = torch.triu_indices(4, 4, offset=1)\nmatrix[upper_indices[0], upper_indices[1]] = 2"}
{"question_id": 77, "response": "transposed_tensor = tensor.transpose(0, 1)"}
{"question_id": 78, "response": "concatenated_tensor = torch.cat([tensor_a, tensor_b], dim=1)"}
{"question_id": 79, "response": "tensor_a, tensor_b = torch.split(tensor, split_size_or_sections=3, dim=1)"}
{"question_id": 80, "response": "result_tensor = torch.matmul(tensor_a, tensor_b)"}
{"question_id": 81, "response": "result_tensor = tensor_a + tensor_b"}
{"question_id": 82, "response": "mean_tensor = torch.mean(tensor, dim=0)"}
{"question_id": 83, "response": "linspace_tensor = torch.linspace(1.0, 10.0, 10)"}
{"question_id": 84, "response": "tensors_unbound = torch.unbind(tensor, dim=0)"}
{"question_id": 85, "response": "bernoulli_tensor = (torch.rand(2, 3) < prob_tensor).int()"}
{"question_id": 86, "response": "top_values, top_indices = torch.topk(tensor, k=3, dim=1, largest=True, sorted=True)"}
{"question_id": 87, "response": "full_tensor = torch.full((3, 3), 7)"}
{"question_id": 88, "response": "trace_value = torch.trace(matrix)"}
{"question_id": 89, "response": "total_elements = tensor.numel()"}
{"question_id": 90, "response": "complex_tensor = torch.complex(real, imag)"}
{"question_id": 91, "response": "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\ntest_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\nx_train = train_set.data.reshape(-1, 1, 28, 28).float() / 255\nx_test = test_set.data.reshape(-1, 1, 28, 28).float() / 255\ny_train = F.one_hot(train_set.targets, 10).float()\ny_test = F.one_hot(test_set.targets, 10).float()"}
{"question_id": 92, "response": "class CNNModel(nn.Module):\n    def __init__(self, input_shape):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_shape[2], 32, kernel_size=3)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3) \n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(0.5)\n        \n        # Calculate input features for final dense layer\n        h = (((input_shape[0] - 2) // 2) - 2) // 2\n        w = (((input_shape[1] - 2) // 2) - 2) // 2\n        self.dense = nn.Linear(64 * h * w, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.pool1(self.relu1(self.conv1(x)))\n        x = self.pool2(self.relu2(self.conv2(x)))\n        x = self.flatten(x)\n        x = self.dropout(x)\n        x = self.sigmoid(self.dense(x))\n        return x"}
{"question_id": 93, "response": "class Polynomial3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = nn.Parameter(torch.randn(1))\n        self.b = nn.Parameter(torch.randn(1)) \n        self.c = nn.Parameter(torch.randn(1))\n        self.d = nn.Parameter(torch.randn(1))\n\n    def forward(self, x):\n        return self.a + self.b * x + self.c * x.pow(2) + self.d * x.pow(3)\n\nmodel = Polynomial3()"}
{"question_id": 94, "response": "class SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(3, 2)\n        self.layer2 = nn.Linear(2, 3)\n        self.layer3 = nn.Linear(3, 4)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.layer3(x)\n        return x"}
{"question_id": 95, "response": "class LinearRegression(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.layer1 = nn.Linear(input_size, 2000)\n        self.dropout = nn.Dropout(0.3)\n        self.layer2 = nn.Linear(2000, output_size)\n        self.leaky_relu = nn.LeakyReLU()\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.leaky_relu(self.layer1(x))\n        x = self.dropout(x)\n        x = self.sigmoid(self.layer2(x))\n        return x"}
{"question_id": 96, "response": "class LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights), freeze=True)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=0.3)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embeds = self.embedding(x)\n        lstm_out, _ = self.lstm(embeds)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        out = self.sigmoid(out)\n        return out\n        \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                 weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        return hidden\n\nmodel = LSTMClassifier(1000, 10, 300, 256, 2, torch.randn(1000, 300))"}
{"question_id": 97, "response": "class LeNet5(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n        self.bn1 = nn.BatchNorm2d(6)\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.bn2 = nn.BatchNorm2d(16)\n        self.pool2 = nn.MaxPool2d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.pool1(x)\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.softmax(self.fc3(x))\n        return x"}
{"question_id": 98, "response": "class ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.downsample = downsample\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out"}
{"question_id": 99, "response": "class AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4),\n            nn.BatchNorm2d(96),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(),\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU(),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return nn.functional.softmax(x, dim=1)"}
{"question_id": 100, "response": "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"}
{"question_id": 101, "response": "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n\n# Example usage:\n# criterion = nn.CrossEntropyLoss()\n# optimizer.step()"}
{"question_id": 102, "response": "class LeNet5(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n            nn.BatchNorm2d(6),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc = nn.Linear(400, 120)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(120, 84)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(84, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = x.view(-1, 400)\n        x = self.fc(x)\n        x = self.relu(x)\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n\nmodel = LeNet5(num_classes=10)\noptimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"}
{"question_id": 103, "response": "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.002)"}
{"question_id": 104, "response": "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.003)"}
{"question_id": 105, "response": "nll_loss = nn.CrossEntropyLoss()\nloss = nll_loss(input_tensor, target_tensor)"}
{"question_id": 106, "response": "margin = 1.0\nloss = torch.mean(torch.clamp(-target_tensor * (input1 - input2) + margin, min=0.0))"}
{"question_id": 107, "response": "positive_dist = torch.sum((anchor - positive) ** 2)\nnegative_dist = torch.sum((anchor - negative) ** 2)\nloss = torch.clamp(positive_dist - negative_dist + margin, min=0.0)"}
{"question_id": 108, "response": "kldiv_loss = nn.KLDivLoss(reduction='batchmean')\nloss = kldiv_loss(F.log_softmax(input_tensor, dim=0), F.softmax(target_tensor, dim=0))"}
{"question_id": 109, "response": "def my_custom_loss(my_outputs, my_labels):\n    my_batch_size = my_outputs.size(0)\n    my_outputs = torch.nn.functional.log_softmax(my_outputs, dim=1)\n    my_outputs = my_outputs[torch.arange(my_batch_size), my_labels]\n    return -torch.sum(my_outputs) / my_batch_size"}
{"question_id": 110, "response": "class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True, smooth=1):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, y_pred, y_true):\n        y_pred = torch.sigmoid(y_pred)\n        y_true = y_true.view(-1)\n        y_pred = y_pred.view(-1)\n        intersection = (y_true * y_pred).sum()\n        dice = (2.0 * intersection + self.smooth) / (y_true.sum() + y_pred.sum() + self.smooth)\n        return 1 - dice"}
{"question_id": 111, "response": "pairwise_distance = torch.norm(tensor1 - tensor2, p=2)"}
{"question_id": 112, "response": "loss = torch.nn.PoissonNLLLoss()(target_tensor, input_tensor)"}
{"question_id": 113, "response": "gaussian_nll_loss = torch.mean(torch.distributions.Normal(loc=input_tensor, scale=torch.sqrt(var_tensor)).log_prob(target_tensor))\nloss = -gaussian_nll_loss"}
{"question_id": 114, "response": "bce_loss = nn.BCELoss()\nloss = bce_loss(input_tensor, target_tensor)"}
{"question_id": 115, "response": "output_tensor = torch.nn.functional.dropout(input_tensor, p=0.3, training=True)"}
{"question_id": 116, "response": "class DynamicNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = nn.Parameter(torch.randn(()))\n        self.b = nn.Parameter(torch.randn(()))\n        self.c = nn.Parameter(torch.randn(()))\n        self.d = nn.Parameter(torch.randn(()))\n        self.e = nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        y = self.a + self.b * x + self.c * x**2 + self.d * x**3\n        for exp in range(4, random.randint(4, 6)):\n            y = y + self.e * x**exp\n        return y\n\nmodel = DynamicNet()"}
{"question_id": 117, "response": "class CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features, custom_param):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(in_features, out_features))\n        nn.init.kaiming_uniform_(self.weight)\n        self.custom_param = custom_param\n        \n    def forward(self, inputs):\n        return torch.matmul(inputs, self.weight)\n\nmodel = CustomLayer(100, 200, custom_param=0.5)"}
{"question_id": 118, "response": "long_tensor = input_tensor.to(torch.int64)"}
{"question_id": 119, "response": "class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.max_pool = nn.MaxPool2d(2)\n        self.dropout1 = nn.Dropout(0.25)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(9216, 128)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.max_pool(x)\n        x = self.dropout1(x)\n        x = self.flatten(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        return F.softmax(self.fc2(x), dim=1)\n\nmodel = Net()"}
{"question_id": 120, "response": "total_params = sum(p.numel() for p in model.parameters())"}
{"question_id": 121, "response": "class VAE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 400)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n        \n    def encode(self, x):\n        h1 = F.relu(self.fc1(x))\n        return self.fc21(h1), self.fc22(h1)\n        \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n        \n    def decode(self, z):\n        h3 = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h3))\n        \n    def forward(self, x):\n        x = x.view(-1, 784)\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\nmodel = VAE()"}
{"question_id": 122, "response": "class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super().__init__()\n        nz = 100\n        ngf = 64\n        nc = 3\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nmodel = Generator(ngpu=1)"}
{"question_id": 123, "response": "class BoWClassifier(nn.Module):\n    def __init__(self, num_labels, vocab_size):\n        super().__init__()\n        self.linear = nn.Linear(vocab_size, num_labels)\n\n    def forward(self, bow_vec):\n        return F.log_softmax(self.linear(bow_vec), dim=1)\n\nmodel = BoWClassifier(num_labels=2, vocab_size=1000)"}
{"question_id": 124, "response": "class LSTMTagger(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n\n    def forward(self, sentence):\n        if len(sentence.shape) == 1:\n            sentence = sentence.unsqueeze(0)\n        embeds = self.embedding(sentence)\n        lstm_out, _ = self.lstm(embeds)\n        tag_space = self.hidden2tag(lstm_out)\n        tag_scores = F.log_softmax(tag_space, dim=-1)\n        return tag_scores\n\nmodel = LSTMTagger(embedding_dim=6, hidden_dim=6, vocab_size=10, tagset_size=3)"}
{"question_id": 125, "response": "class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.gru(embedded)\n        return output, hidden\n\nmodel = EncoderRNN(input_size=10, hidden_size=20, dropout_p=0.1)\n\ninput_sequence = torch.tensor([[1, 2, 3], [4, 5, 6]])\noutput, hidden = model(input_sequence)"}
{"question_id": 126, "response": "class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.SOS_token = 0\n        self.MAX_LENGTH = 5\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.full((batch_size, 1), self.SOS_token, device=encoder_outputs.device)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n\n        for i in range(self.MAX_LENGTH):\n            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n            if target_tensor is not None:\n                decoder_input = target_tensor[:, i].unsqueeze(1)\n            else:\n                decoder_input = decoder_output.argmax(dim=-1)\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        return decoder_outputs, decoder_hidden, None\n\n    def forward_step(self, input, hidden):\n        output = self.embedding(input)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.out(output)\n        return output, hidden\n\nmodel = DecoderRNN(hidden_size=20, output_size=10)"}
{"question_id": 127, "response": "class BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze(-1)\n        weights = F.softmax(scores, dim=-1)\n        weights = weights.unsqueeze(1)\n        context = torch.matmul(weights, keys)\n        return context, weights"}
{"question_id": 128, "response": "class SimpleMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        return self.fc3(x)"}
{"question_id": 129, "response": "class TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n        self.fc = nn.Linear(embed_dim, num_class)\n\n    def forward(self, text, offsets):\n        x = self.embedding(text)\n        x = torch.mean(x, dim=1)\n        return self.fc(x)"}
{"question_id": 130, "response": "class Net(nn.Module):\n    def __init__(self, upscale_factor):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, padding=2)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1) \n        self.conv3 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.upscale_factor = upscale_factor\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.conv4(x)\n        x = nn.PixelShuffle(self.upscale_factor)(x)\n        x = x.permute(0, 2, 3, 1)\n        return x\n\nmodel = Net(upscale_factor=2)"}
{"question_id": 131, "response": "class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=28, hidden_size=64, batch_first=True)\n        self.batchnorm = nn.BatchNorm1d(64)\n        self.dropout1 = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(64, 32)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(32, 10)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = x.squeeze(1)\n        x, _ = self.lstm(x)\n        x = x[:, -1, :]\n        x = self.batchnorm(x)\n        x = self.dropout1(x)\n        x = self.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.softmax(self.fc2(x))\n        return x\n\nmodel = Net()"}
{"question_id": 132, "response": "class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) \n        self.max_pool = nn.MaxPool2d(kernel_size=2)\n        self.dropout = nn.Dropout(0.5)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.max_pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.max_pool(x)\n        x = self.dropout(x)\n        x = self.flatten(x)\n        x = F.relu(self.fc1(x))\n        return F.softmax(self.fc2(x), dim=1)\n\nmodel = Net()"}
{"question_id": 133, "response": "def loss_function(recon_x, x, mu, logvar):\n    BCE = torch.nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD"}
{"question_id": 134, "response": "class Policy(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1)"}
{"question_id": 135, "response": "model = BertModel.from_pretrained('bert-base-cased')"}
{"question_id": 136, "response": "class DeepNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32 * 8 * 8, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x"}
{"question_id": 137, "response": "class LightNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 8 * 8, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = x.permute(0, 3, 1, 2)\n        x = self.features(x)\n        x = self.classifier(x)\n        return x"}
{"question_id": 138, "response": "class ModifiedDeepNNCosine(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32 * 8 * 8, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_classes)\n        )\n        self.avg_pool = nn.AvgPool1d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.features(x)\n        flattened_conv_output = torch.flatten(x, 1)\n        output = self.classifier(flattened_conv_output)\n        pooled_output = self.avg_pool(flattened_conv_output.unsqueeze(-1)).squeeze(-1)\n        return output, pooled_output"}
{"question_id": 139, "response": "class ModifiedLightNNCosine(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 8 * 8, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        flattened_conv_output = torch.flatten(x, 1)\n        output = self.classifier(flattened_conv_output)\n        return output, flattened_conv_output"}
{"question_id": 140, "response": "class ModifiedDeepNNRegressor(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32 * 16 * 16, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        conv_feature_map = self.features(x)\n        output = self.classifier(conv_feature_map)\n        return output, conv_feature_map"}
{"question_id": 141, "response": "class ModifiedLightNNRegressor(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.ReLU(), \n            nn.MaxPool2d(2, 2)\n        )\n        self.regressor = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(16 * 16 * 16, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        regressor_output = self.regressor(x)\n        x = torch.flatten(x, 1)\n        output = self.classifier(x)\n        return output, regressor_output"}
{"question_id": 142, "response": "class BiRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=0.1)\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out"}
{"question_id": 143, "response": "class ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n            nn.ReLU(), \n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc = nn.Linear(32 * 8 * 8, num_classes)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = x.reshape(x.size(0), -1)\n        x = self.fc(x)\n        return x"}
{"question_id": 144, "response": "class AutoEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.Tanh(),\n            nn.Linear(128, 64),\n            nn.Tanh(),\n            nn.Linear(64, 12),\n            nn.Tanh(),\n            nn.Linear(12, 3)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(3, 12),\n            nn.Tanh(),\n            nn.Linear(12, 64),\n            nn.Tanh(),\n            nn.Linear(64, 128),\n            nn.Tanh(),\n            nn.Linear(128, 28*28),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return encoded, decoded"}
{"question_id": 145, "response": "class DenseLayer(nn.Module):\n    def __init__(self, in_channels):\n        super(DenseLayer, self).__init__()\n        k = 12\n        self.BN1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels, 4*k, kernel_size=1, stride=1, padding=0, bias=False)\n        self.BN2 = nn.BatchNorm2d(4*k)\n        self.conv2 = nn.Conv2d(4*k, k, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        xin = x\n        x = self.BN1(x)\n        x = self.relu(x)\n        x = self.conv1(x)\n        x = self.BN2(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = torch.cat([xin, x], dim=1)\n        return x"}
{"question_id": 146, "response": "class TransitionLayer(nn.Module):\n    def __init__(self, in_channels, compression_factor):\n        super().__init__()\n        self.BN = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels, int(in_channels * compression_factor), kernel_size=1, stride=1, padding=0, bias=False)\n        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.BN(x)\n        x = self.conv1(x)\n        x = self.avgpool(x)\n        return x"}
{"question_id": 147, "response": "class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super().__init__()\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n        self.batchnorm2d = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.relu(self.batchnorm2d(self.conv2d(x)))"}
{"question_id": 148, "response": "class TwoLayerNet(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super().__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(H, D_out)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        y_pred = self.linear2(x)\n        return y_pred"}
{"question_id": 149, "response": "eigvals = torch.linalg.eigvals(square_matrix)"}
{"question_id": 150, "response": "torch.nn.init.xavier_uniform_(conv1.weight)"}
{"question_id": 151, "response": "y = x.detach().clone()"}
{"question_id": 152, "response": "y = x.permute(1, 2, 0)"}
{"question_id": 153, "response": "x = torch.normal(mean=2, std=4, size=(100,))"}
{"question_id": 154, "response": "one_hot = F.one_hot(indices, num_classes=n)"}
{"question_id": 155, "response": "mask = torch.arange(max_len).expand(len(lens), max_len) < lens.unsqueeze(1)"}
{"question_id": 156, "response": "t_integer = t.to(torch.int64)"}
{"question_id": 157, "response": "noise = torch.normal(mean=mean, std=std, size=t.shape)\nnoisy_t = t + noise"}
{"question_id": 158, "response": "y = x.unsqueeze(0).repeat(6, 1, 1)"}
{"question_id": 159, "response": "padded_tensor = torch.nn.utils.rnn.pad_sequence([a, b, c], batch_first=True, padding_value=0)"}
{"question_id": 160, "response": "mask = (t > 3) & (t < 7)"}
{"question_id": 161, "response": "sampler = RandomSampler(ds, replacement=True, num_samples=M)\ndl = DataLoader(ds, batch_size=5, sampler=sampler)"}
{"question_id": 162, "response": "class StringLookup(torch.nn.Module):\n    def __init__(self, vocabulary):\n        super().__init__()\n        self.lookup_table = {word: idx+1 for idx, word in enumerate(vocabulary)}\n        \n    def forward(self, x):\n        if isinstance(x, list):\n            return [[self.lookup_table.get(word, 0) for word in seq] for seq in x]\n        return self.lookup_table.get(x, 0)\n\nlayer = StringLookup(vocab)\noutput = layer(data)"}
{"question_id": 163, "response": "valid_cols = torch.where(torch.any(A != 0, dim=0))[0]\nA = A[:, valid_cols]"}
{"question_id": 164, "response": "k = round(percent_ones * d)\nbool_tensor = rand_mat <= k_th_quant\ndesired_tensor = bool_tensor.int()"}
{"question_id": 165, "response": "idx = ids.repeat(1, 2)\nresult = torch.gather(x, 1, idx)"}
{"question_id": 166, "response": "num_equal = torch.sum(A == B)"}
{"question_id": 167, "response": "A.index_add_(0, B, C)"}
{"question_id": 168, "response": "y, indices = torch.sort(x, dim=-1)\nmask = torch.cat([torch.ones_like(y[:, :1]), (y[:, 1:] != y[:, :-1]).long()], dim=-1)\ny = y * mask\nresult = torch.gather(y, -1, indices.argsort(dim=-1))"}
{"question_id": 169, "response": "old_new_dict = dict(old_new_value.numpy())\nresult = old_values.clone()\nfor old_val, new_val in old_new_dict.items():\n    result = torch.where(old_values == old_val, torch.tensor(new_val, dtype=torch.int32), result)"}
{"question_id": 170, "response": "conv2 = nn.Conv2d(1, 18, kernel_size=3, stride=1, padding='same')\nimage_d = torch.tensor(random_image_arr, dtype=torch.float32).reshape(1, 1, 27, 35)\nfc = conv2(image_d)"}
{"question_id": 171, "response": "norm_2 = torch.norm(t, p=2, dim=1)"}
{"question_id": 172, "response": "comparison_matrix = (B.unsqueeze(1) == A.unsqueeze(0))\nmatches_per_element = comparison_matrix.sum(dim=1).int()\n\nexists = (matches_per_element > 0).sum().item()\nnot_exist = len(B) - exists"}
{"question_id": 173, "response": "output = tensor[torch.arange(len(B)), torch.tensor(B)]"}
{"question_id": 174, "response": "def get_unique_elements_first_idx(tensor):\n    sorted_tensor, indices = torch.sort(tensor)\n    unique_mask = torch.cat((torch.tensor([True]), sorted_tensor[1:] != sorted_tensor[:-1]))\n    return indices[unique_mask]\n\nunique_indices = get_unique_elements_first_idx(v2_without_v1)"}
{"question_id": 175, "response": "class ValidatedArray:\n    def __init__(self, array: torch.Tensor):\n        self._array = array\n        self._validate_array()\n        \n    def _validate_array(self):\n        assert torch.all(torch.abs(torch.sum(self._array, dim=-1) - 1.0) < 1e-6), f'The last dim represents a categorical distribution. It must sum to one.'\n\n    @property\n    def array(self):\n        self._validate_array()\n        return self._array"}
{"question_id": 176, "response": "# Create a 5D tensor\ntensor = torch.rand(1, 3, 10, 40, 1)\n\n# Specify the dimension and size for splitting\ndimension = 3\nsize = 10\n\n# Split the tensor into smaller tensors\nsplit_tensors = torch.split(tensor, size, dim=dimension)\nsplit_tensors = torch.stack(split_tensors)"}
{"question_id": 177, "response": "# Determine maximum height and width\nmax_height = max([img.shape[1] for img in image_batch])\nmax_width = max([img.shape[2] for img in image_batch])\n\n# Pad the image batch\nimage_batch = [torch.nn.functional.pad(img, (0, max_width - img.shape[2], 0, max_height - img.shape[1])) for img in image_batch]\n\n# Pad the mask batch\nmask_batch = [torch.nn.functional.pad(mask, (0, max_width - mask.shape[1], 0, max_height - mask.shape[0])) for mask in mask_batch]"}
{"question_id": 178, "response": "a_masked = torch.any(a.unsqueeze(1) == b, dim=1) | torch.any(a.unsqueeze(1) == c, dim=1)"}
{"question_id": 179, "response": "B, C, H, W = x.size()\nz = torch.zeros_like(x)\n\nx_norm = torch.norm(x, dim=(2, 3))\ny_norm = torch.norm(y, dim=(2, 3))\n\ncondition = x_norm >= y_norm\n\nz = torch.where(condition.unsqueeze(2).unsqueeze(3), x, y)"}
{"question_id": 180, "response": "out = a[torch.arange(a.size(0)), b]"}
{"question_id": 181, "response": "t1 = torch.gather(t2, 1, indexes)"}
{"question_id": 182, "response": "dist = torch.sqrt(torch.sum((tensor1 - tensor2)**2, dim=3))"}
{"question_id": 183, "response": "v.index_add_(0, index, w)"}
{"question_id": 184, "response": "ind = torch.argsort(a[:, -1])\nsorted_a = a[ind]"}
{"question_id": 185, "response": "class LR(torch.nn.Module):\n    def __init__(self, n_features):\n        super().__init__()\n        self.lr = torch.nn.Sequential(\n            torch.nn.Linear(n_features, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 36, bias=False),\n            torch.nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        return self.lr(x)"}
{"question_id": 186, "response": "class Combined_model(nn.Module):\n    def __init__(self, modelA, modelB):\n        super(Combined_model, self).__init__()\n        self.modelA = modelA\n        self.modelB = modelB\n        self.classifier = nn.Linear(4, 2)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x1, x2):\n        x1 = self.modelA(x1)\n        x2 = self.modelB(x2)\n        x = torch.cat((x1, x2), dim=1)\n        x = self.classifier(x)\n        x = self.relu(x)\n        return x"}
{"question_id": 187, "response": "loss_fn = nn.CrossEntropyLoss(reduction='none')\nbatch_loss = loss_fn(source, target)\nweighted_loss = multiplier * batch_loss\nloss = torch.sum(weighted_loss) / torch.sum(multiplier)"}
{"question_id": 188, "response": "B = A.unsqueeze(1).repeat(1, K, 1)"}
{"question_id": 189, "response": "def sum_by_index(labels, source):\n    unique_labels, labels_inds, labels_counts = torch.flatten(labels).unique(return_inverse=True, return_counts=True)\n    out = torch.zeros((unique_labels.size(0), source.size(-1)), dtype=source.dtype, device=source.device)\n    out.index_add_(0, labels_inds, source.reshape(-1, source.size(-1)))\n    return out\n\nlabels = torch.tensor([[0, 1], [1, 2]])\nsource = torch.tensor(\n          [[[0, 1], \n           [1, 2]], \n\n           [[2, 3],\n            [3, 4]]], \n        dtype=torch.float, \n        requires_grad=True,\n)\nresult = sum_by_index(labels, source)"}
{"question_id": 190, "response": "x = torch.cat([original[:row_exclude], original[row_exclude+1:]], dim=0)"}
{"question_id": 191, "response": "class TinyVGG(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n        super().__init__()\n        \n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        self.dropout = nn.Dropout(p=0.4)\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=hidden_units * 28 * 28, out_features=output_shape)\n        )\n        \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block1(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x"}
{"question_id": 192, "response": "one_hot = torch.nn.functional.one_hot(torch.argmax(a), num_classes=len(a))"}
{"question_id": 193, "response": "trans_comp = T.Compose([\n    T.Resize([224, 224]),\n    T.ToTensor(),\n    T.Lambda(lambda x: x / 255.0)\n])"}
{"question_id": 194, "response": "d = torch.cat((a, b, c), dim=1)\ne = d.reshape(8, 3, -1)"}
{"question_id": 195, "response": "result = torch.cat((a, torch.zeros(n - 1, a.shape[1], a.shape[2])), dim=0)"}
{"question_id": 196, "response": "indices = torch.stack([starts, ends], dim=1)\nresult = torch.stack([data[start:end] for start, end in indices])"}
{"question_id": 197, "response": "result = x[:, None] - x[None, :]"}
{"question_id": 198, "response": "b = torch.argsort(a[:, :, sorted_column], dim=sorted_dim)\nsorted_a = torch.stack([a[i][b[i]] for i in range(a.size(0))])"}
{"question_id": 199, "response": "train_examples = []\nfor item in in_dict['train']:\n    in_tensor = torch.tensor(item['input'], dtype=torch.float32)\n    out_tensor = torch.tensor(item['output'], dtype=torch.float32)\n    train_examples.append([in_tensor, out_tensor])\n\nfor item in train_examples:\n    item[0] = torch.nn.functional.pad(item[0], (1, 1, 1, 1))\n    item[1] = torch.nn.functional.pad(item[1], (1, 1, 1, 1))\n\nout_dict = {'train': []}\nfor item in train_examples:\n    out_dict['train'].append({\n        'input': item[0].tolist(),\n        'output': item[1].tolist()\n    })"}
