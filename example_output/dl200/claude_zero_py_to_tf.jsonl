{"question_id": 0, "response": "result = tf.convert_to_tensor(np_array)"}
{"question_id": 1, "response": "result = tf.constant(3.43)"}
{"question_id": 2, "response": "result = tf.reshape(tf.range(1, 10), (3, 3))"}
{"question_id": 3, "response": "result = tf.linalg.diag(tf.range(1, 6))"}
{"question_id": 4, "response": "result = tf.eye(4)"}
{"question_id": 5, "response": "result = tensor1 + tensor2"}
{"question_id": 6, "response": "result = tf.subtract(tensor1, tensor2)"}
{"question_id": 7, "response": "result = tf.matmul(tensor1, tensor2)"}
{"question_id": 8, "response": "result = tf.zeros(shape)"}
{"question_id": 9, "response": "result = tf.shape(tensor)"}
{"question_id": 10, "response": "result = tf.rank(tensor)"}
{"question_id": 11, "response": "result = tensor[1:, 1:]"}
{"question_id": 12, "response": "result = tensor.numpy()"}
{"question_id": 13, "response": "result = tf.reshape(tensor, [2, 6])"}
{"question_id": 14, "response": "result = tf.Variable(tf.random.normal(shape), trainable=True)"}
{"question_id": 15, "response": "with tf.GradientTape() as tape:\n    y = x ** 2\nresult = tape.gradient(y, x)"}
{"question_id": 16, "response": "with tf.GradientTape() as tape:\n    y = tf.pow(x - 5, 2)\ngrad = tape.gradient(y, x)\nx.assign(x - 0.1 * grad)\nresult = x"}
{"question_id": 17, "response": "with tf.GradientTape() as tape:\n    Q = 3 * tf.pow(a, 3) - tf.pow(b, 2)\ngradients = tape.gradient(Q, [a, b])\nresult = gradients"}
{"question_id": 18, "response": "train_dataset = tf.keras.datasets.mnist.load_data()[0]"}
{"question_id": 19, "response": "result = tf.nn.relu(tensor)"}
{"question_id": 20, "response": "result = tf.nn.relu6(tensor)"}
{"question_id": 21, "response": "result = tf.sigmoid(tensor)"}
{"question_id": 22, "response": "model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(10)\n])"}
{"question_id": 23, "response": "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"}
{"question_id": 24, "response": "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"}
{"question_id": 25, "response": "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"}
{"question_id": 26, "response": "result = tf.reduce_max(tensor).numpy()"}
{"question_id": 27, "response": "result = tf.reduce_mean(tensor)"}
{"question_id": 28, "response": "result = tf.reduce_prod(tensor)"}
{"question_id": 29, "response": "result = tf.unique(tensor)[0]"}
{"question_id": 30, "response": "result = tf.bitwise.bitwise_xor(a, b)"}
{"question_id": 31, "response": "sines = tf.sin(angles)"}
{"question_id": 32, "response": "class TinyModel(tf.keras.Model):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.linear1 = tf.keras.layers.Dense(200)\n        self.activation = tf.keras.layers.ReLU()\n        self.linear2 = tf.keras.layers.Dense(10)\n        self.softmax = tf.keras.layers.Softmax()\n        \n    def call(self, x):\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.linear2(x)\n        x = self.softmax(x)\n        return x\n\nmodel = TinyModel()"}
{"question_id": 33, "response": "total_params = model.count_params()"}
{"question_id": 34, "response": "first_layer_params = model.layers[0].count_params()"}
{"question_id": 35, "response": "output = tf.nn.max_pool2d(tensor, ksize=2, strides=2, padding='VALID')"}
{"question_id": 36, "response": "bn_layer = tf.keras.layers.BatchNormalization()\noutput = bn_layer(tensor)"}
{"question_id": 37, "response": "dropout = tf.keras.layers.Dropout(0.5, seed=0)\noutput = dropout(tensor, training=True)"}
{"question_id": 38, "response": "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)"}
{"question_id": 39, "response": "loss = tf.keras.losses.MSE(target_tensor, input_tensor)"}
{"question_id": 40, "response": "with tf.GradientTape() as tape:\n    output = model(input_tensor)\n    loss = tf.keras.losses.mean_squared_error(target, output)\ngradients = tape.gradient(loss, model.trainable_variables)\nmodel.optimizer.apply_gradients(zip(gradients, model.trainable_variables))"}
{"question_id": 41, "response": "loss = tf.keras.losses.MeanAbsoluteError()(input_tensor, target_tensor)"}
{"question_id": 42, "response": "loss = tf.reduce_mean(tf.maximum(0.0, 1.0 - input_tensor * target_tensor))"}
{"question_id": 43, "response": "huber_loss = tf.keras.losses.Huber()\nloss = huber_loss(target_tensor, input_tensor)"}
{"question_id": 44, "response": "model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(20, (5, 5), input_shape=(None, None, 1), padding='valid'),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Conv2D(64, (5, 5), padding='valid'),\n    tf.keras.layers.ReLU()\n])"}
{"question_id": 45, "response": "if tf.test.is_built_with_cuda():\n    physical_devices = tf.config.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    device = '/GPU:0'\nelse:\n    device = '/CPU:0'\n\nwith tf.device(device):\n    model = model"}
{"question_id": 46, "response": "model.save('seq_model.keras')"}
{"question_id": 47, "response": "loaded_model = tf.keras.models.load_model('seq_model.keras')"}
{"question_id": 48, "response": "model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=1000, output_dim=64),\n    tf.keras.layers.LSTM(128, return_sequences=False),\n    tf.keras.layers.Dense(10)\n])"}
{"question_id": 49, "response": "model = tf.keras.Sequential([\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(10, 10)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False)),\n    tf.keras.layers.Dense(10)\n])"}
{"question_id": 50, "response": "cosine_similarity = tf.keras.losses.cosine_similarity(tensor1, tensor2, axis=0) * -1"}
{"question_id": 51, "response": "euclidean_distance = tf.norm(tensor1 - tensor2)"}
{"question_id": 52, "response": "hello_embed = embeds(lookup_tensor)"}
{"question_id": 53, "response": "def scaled_dot_product_attention(Q, K, V, mask=None):\n    attn_scores = tf.matmul(Q, tf.transpose(K, perm=[0, 2, 1])) / tf.math.sqrt(tf.cast(tf.shape(Q)[-1], tf.float32))\n    if mask is not None:\n        attn_scores = tf.where(mask == 0, -1e9, attn_scores)\n    attn_probs = tf.nn.softmax(attn_scores, axis=-1)\n    output = tf.matmul(attn_probs, V)\n    return output"}
{"question_id": 54, "response": "def split_heads(num_heads, d_k, x):\n    batch_size = tf.shape(x)[0]\n    seq_length = tf.shape(x)[1]\n    x = tf.reshape(x, (batch_size, seq_length, num_heads, d_k))\n    return tf.transpose(x, perm=[0, 2, 1, 3])"}
{"question_id": 55, "response": "def combine_heads(d_model, x):\n    batch_size = tf.shape(x)[0]\n    seq_length = tf.shape(x)[2]\n    x = tf.transpose(x, perm=[0, 2, 1, 3])\n    return tf.reshape(x, [batch_size, seq_length, d_model])"}
{"question_id": 56, "response": "class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.d_k = d_model // num_heads\n        \n        self.W_q = tf.keras.layers.Dense(d_model)\n        self.W_k = tf.keras.layers.Dense(d_model)\n        self.W_v = tf.keras.layers.Dense(d_model)\n        self.W_o = tf.keras.layers.Dense(d_model)\n    \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        matmul_qk = tf.matmul(Q, K, transpose_b=True)\n        depth = tf.cast(self.d_k, tf.float32)\n        logits = matmul_qk / tf.math.sqrt(depth)\n        if mask is not None:\n            logits += (mask * -1e9)\n        attention_weights = tf.nn.softmax(logits, axis=-1)\n        output = tf.matmul(attention_weights, V)\n        return output\n    \n    def split_heads(self, x):\n        batch_size = tf.shape(x)[0]\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def combine_heads(self, x):\n        batch_size = tf.shape(x)[0]\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        return tf.reshape(x, (batch_size, -1, self.d_model))\n    \n    def call(self, Q,"}
{"question_id": 57, "response": "class PositionWiseFeedForward(tf.keras.layers.Layer):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n        self.fc2 = tf.keras.layers.Dense(d_model)\n        \n    def call(self, x):\n        return self.fc2(self.fc1(x))\n\nmodel = PositionWiseFeedForward(d_model, d_ff)"}
{"question_id": 58, "response": "class PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self, d_model, max_seq_length):\n        super().__init__()\n        pe = tf.zeros([max_seq_length, d_model])\n        position = tf.expand_dims(tf.cast(tf.range(max_seq_length), dtype=tf.float32), 1)\n        div_term = tf.exp(tf.cast(tf.range(0, d_model, 2), dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n        \n        pe_sin = tf.sin(position * div_term)\n        pe_cos = tf.cos(position * div_term)\n        \n        pe = tf.concat([pe_sin, pe_cos], axis=1)[:, :d_model]\n        self.pe = tf.expand_dims(pe, 0)\n        \n    def call(self, x):\n        return x + self.pe[:, :tf.shape(x)[1]]"}
{"question_id": 59, "response": "class SelfAttention(tf.keras.layers.Layer):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.query = tf.keras.layers.Dense(input_dim)\n        self.key = tf.keras.layers.Dense(input_dim)\n        self.value = tf.keras.layers.Dense(input_dim)\n        \n    def call(self, x):\n        queries = self.query(x)\n        keys = self.key(x)\n        values = self.value(x)\n        \n        scores = tf.matmul(queries, keys, transpose_b=True)/tf.math.sqrt(tf.cast(self.input_dim, tf.float32))\n        attention = tf.nn.softmax(scores, axis=-1)\n        weighted = tf.matmul(attention, values)\n        return weighted"}
{"question_id": 60, "response": "def softmax(x):\n    exp_x = tf.exp(x - tf.reduce_max(x))\n    return exp_x / tf.reduce_sum(exp_x, axis=-1, keepdims=True)"}
{"question_id": 61, "response": "x_values = [i for i in range(11)]\nx_train = np.array(x_values, dtype=np.float32)\nx_train = x_train.reshape(-1, 1)\nx_train = tf.convert_to_tensor(x_train)\ny_values = [2*i + 1 for i in x_values]\ny_train = np.array(y_values, dtype=np.float32)\ny_train = y_train.reshape(-1, 1)\ny_train = tf.convert_to_tensor(y_train)"}
{"question_id": 62, "response": "class LinearRegression(tf.keras.Model):\n    def __init__(self, inputSize, outputSize):\n        super().__init__()\n        self.linear = tf.keras.layers.Dense(outputSize, input_shape=(inputSize,))\n\n    def call(self, x):\n        out = self.linear(x)\n        return out\n\n# model = LinearRegression(inputSize, outputSize)"}
{"question_id": 63, "response": "learningRate = 0.01\noptimizer = tf.keras.optimizers.SGD(learning_rate=learningRate)"}
{"question_id": 64, "response": "history = model.fit(x_train, y_train, epochs=epochs, verbose=0)\nlosses = history.history['loss']"}
{"question_id": 65, "response": "x_train = tf.cast(tf.reshape(x_values, (-1, 1)), dtype=tf.float32)\ny_train = tf.cast(tf.reshape(y_values, (-1, 1)), dtype=tf.float32)"}
{"question_id": 66, "response": "class LogisticRegression(tf.keras.Model):\n    def __init__(self, inputSize, outputSize):\n        super().__init__()\n        self.linear = tf.keras.layers.Dense(outputSize, input_shape=(inputSize,))\n        self.activation = tf.keras.layers.Activation('sigmoid')\n    \n    def call(self, x):\n        return self.activation(self.linear(x))"}
{"question_id": 67, "response": "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"}
{"question_id": 68, "response": "history = model.fit(x_train, y_train, epochs=epochs, verbose=0)\nlosses = history.history['loss']"}
{"question_id": 69, "response": "atanh_values = tf.math.atanh(values)"}
{"question_id": 70, "response": "class RNN(tf.keras.Model):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        \n        self.i2h = tf.keras.layers.Dense(hidden_size)\n        self.h2h = tf.keras.layers.Dense(hidden_size)\n        self.h2o = tf.keras.layers.Dense(output_size)\n        \n    def call(self, inputs, hidden):\n        hidden = tf.nn.tanh(self.i2h(inputs) + self.h2h(hidden))\n        output = self.h2o(hidden)\n        output = tf.nn.log_softmax(output)\n        return output, hidden\n        \n    def initHidden(self):\n        return tf.zeros((1, self.hidden_size))\n\n# n_hidden = 128\n# rnn = RNN(input_size, n_hidden, output_size)"}
{"question_id": 71, "response": "def accuracy(y_pred, y_true):\n    correct = tf.reduce_sum(tf.cast(y_pred == y_true, tf.float32))\n    total = tf.cast(tf.shape(y_true)[0], tf.float32)\n    return correct / total"}
{"question_id": 72, "response": "def precision(y_pred, y_true):\n    true_positive = tf.reduce_sum(tf.cast((y_pred == 1) & (y_true == 1), tf.float32))\n    predicted_positive = tf.reduce_sum(tf.cast(y_pred == 1, tf.float32))\n    return true_positive / predicted_positive"}
{"question_id": 73, "response": "def recall(y_pred, y_true):\n    true_positive = tf.reduce_sum(tf.cast((y_pred == 1) & (y_true == 1), tf.float32))\n    actual_positive = tf.reduce_sum(tf.cast(y_true == 1, tf.float32))\n    return true_positive / actual_positive"}
{"question_id": 74, "response": "def f1_score(y_pred, y_true):\n    true_positive = tf.reduce_sum(tf.cast((y_pred == 1) & (y_true == 1), tf.float32))\n    predicted_positive = tf.reduce_sum(tf.cast(y_pred == 1, tf.float32))\n    actual_positive = tf.reduce_sum(tf.cast(y_true == 1, tf.float32))\n    precision = tf.where(predicted_positive > 0, true_positive / predicted_positive, 0.0)\n    recall = tf.where(actual_positive > 0, true_positive / actual_positive, 0.0)\n    return tf.where(precision + recall > 0, 2 * (precision * recall) / (precision + recall), 0.0)"}
{"question_id": 75, "response": "tensor_squeezed = tf.squeeze(tensor)"}
{"question_id": 76, "response": "matrix = tf.linalg.set_diag(matrix, tf.ones(4))\nupper_tri = tf.linalg.band_part(tf.ones((4, 4)) * 2, 0, -1)\nmatrix = matrix + upper_tri - tf.linalg.band_part(upper_tri, 0, 0)"}
{"question_id": 77, "response": "transposed_tensor = tf.transpose(tensor)"}
{"question_id": 78, "response": "concatenated_tensor = tf.concat([tensor_a, tensor_b], axis=1)"}
{"question_id": 79, "response": "tensor_a, tensor_b = tf.split(tensor, [3, 3], axis=1)"}
{"question_id": 80, "response": "result_tensor = tf.matmul(tensor_a, tensor_b)"}
{"question_id": 81, "response": "result_tensor = tensor_a + tensor_b"}
{"question_id": 82, "response": "mean_tensor = tf.reduce_mean(tensor, axis=0)"}
{"question_id": 83, "response": "linspace_tensor = tf.linspace(1, 10, 10)"}
{"question_id": 84, "response": "tensors_unbound = tf.unstack(tensor, axis=0)"}
{"question_id": 85, "response": "bernoulli_tensor = tf.random.stateless_binomial(shape=tf.shape(prob_tensor), seed=[1, 2], counts=1, probs=prob_tensor) / 1.0"}
{"question_id": 86, "response": "values, indices = tf.nn.top_k(tensor, k=3)"}
{"question_id": 87, "response": "full_tensor = tf.fill([3, 3], 7)"}
{"question_id": 88, "response": "trace_value = tf.linalg.trace(matrix)"}
{"question_id": 89, "response": "total_elements = tf.size(tensor)"}
{"question_id": 90, "response": "complex_tensor = tf.complex(real, imag)"}
{"question_id": 91, "response": "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n\nx_train = (x_train - 0.5) / 0.5\nx_test = (x_test - 0.5) / 0.5\n\nx_train = x_train.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1, 28, 28, 1)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))"}
{"question_id": 92, "response": "model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n    tf.keras.layers.MaxPooling2D(pool_size=2),\n    tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])"}
{"question_id": 93, "response": "class Polynomial3(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        self.a = self.add_weight(shape=(), initializer='random_normal', trainable=True)\n        self.b = self.add_weight(shape=(), initializer='random_normal', trainable=True)\n        self.c = self.add_weight(shape=(), initializer='random_normal', trainable=True)\n        self.d = self.add_weight(shape=(), initializer='random_normal', trainable=True)\n\n    def call(self, x):\n        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n\nmodel = Polynomial3()"}
{"question_id": 94, "response": "model = keras.Sequential([\n    layers.Dense(2, activation='relu', input_shape=(3,)),\n    layers.Dense(3, activation='relu'),\n    layers.Dense(4)\n])"}
{"question_id": 95, "response": "model = tf.keras.Sequential([\n    tf.keras.layers.Dense(2000, input_shape=(input_size,)),\n    tf.keras.layers.LeakyReLU(alpha=0.01),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(output_size, activation='sigmoid')\n])"}
{"question_id": 96, "response": "class LSTMClassifier(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, output_size, weights):\n        super().__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim,\n            embeddings_initializer=tf.keras.initializers.Constant(weights),\n            trainable=False\n        )\n        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n        self.lstm = tf.keras.layers.LSTM(\n            hidden_dim,\n            return_sequences=True,\n            return_state=True,\n            dropout=0.3,\n            stateful=False,\n            units=hidden_dim,\n            num_layers=n_layers\n        )\n        self.dropout_2 = tf.keras.layers.Dropout(0.3)\n        self.label_layer = tf.keras.layers.Dense(output_size)\n        self.act = tf.keras.layers.Activation('sigmoid')\n\n    def init_hidden(self, batch_size):\n        return [tf.zeros((self.n_layers, batch_size, self.hidden_dim)), \n                tf.zeros((self.n_layers, batch_size, self.hidden_dim))]\n\n    def call(self, x, hidden):\n        x = self.word_embeddings(x)\n        x = self.dropout_1(x)\n        lstm_out, state_h, state_c = self.lstm(x, initial_state=hidden)\n        lstm_out = tf.reshape(lstm_out, [-1, self.hidden_dim])\n        out = self.dropout_2(lstm_out)\n        out = self.label_layer(out)\n        out = tf.reshape(out, [-1, self.output_size])\n        out = self.act(out)\n        return out, [state_h, state_c]"}
{"question_id": 97, "response": "model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(6, kernel_size=5, strides=1, padding='valid', input_shape=input_shape),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n    \n    tf.keras.layers.Conv2D(16, kernel_size=5, strides=1, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(120),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(84),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dense(num_classes)\n])"}
{"question_id": 98, "response": "class ResidualBlock(tf.keras.layers.Layer):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = tf.keras.Sequential([\n            tf.keras.layers.Conv2D(out_channels, kernel_size=3, strides=stride, padding='same'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU()\n        ])\n        self.conv2 = tf.keras.Sequential([\n            tf.keras.layers.Conv2D(out_channels, kernel_size=3, strides=1, padding='same'),\n            tf.keras.layers.BatchNormalization()\n        ])\n        self.downsample = downsample\n        self.relu = tf.keras.layers.ReLU()\n\n    def call(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out = out + residual\n        out = self.relu(out)\n        return out"}
{"question_id": 99, "response": "model = Sequential([\n    Conv2D(96, kernel_size=11, strides=4, padding='valid', input_shape=(224, 224, 3)),\n    BatchNormalization(),\n    ReLU(),\n    MaxPooling2D(pool_size=3, strides=2),\n    \n    Conv2D(256, kernel_size=5, strides=1, padding='same'),\n    BatchNormalization(),\n    ReLU(),\n    MaxPooling2D(pool_size=3, strides=2),\n    \n    Conv2D(384, kernel_size=3, strides=1, padding='same'),\n    BatchNormalization(),\n    ReLU(),\n    \n    Conv2D(384, kernel_size=3, strides=1, padding='same'),\n    BatchNormalization(),\n    ReLU(),\n    \n    Conv2D(256, kernel_size=3, strides=1, padding='same'),\n    BatchNormalization(),\n    ReLU(),\n    MaxPooling2D(pool_size=3, strides=2),\n    \n    Flatten(),\n    \n    Dropout(0.5),\n    Dense(4096),\n    ReLU(),\n    \n    Dropout(0.5),\n    Dense(4096),\n    ReLU(),\n    \n    Dense(10)\n])"}
{"question_id": 100, "response": "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"}
{"question_id": 101, "response": "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001)\n\n# Example usage:\n# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n# model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)"}
{"question_id": 102, "response": "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n\nmodel.compile(optimizer=optimizer,\n             loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n             metrics=['accuracy'])\n\n# Example usage:\n# model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)"}
{"question_id": 103, "response": "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.002)"}
{"question_id": 104, "response": "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)"}
{"question_id": 105, "response": "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_tensor, logits=input_tensor))"}
{"question_id": 106, "response": "margin = 1.0\nloss = tf.reduce_mean(tf.maximum(0.0, -target_tensor * (input1 - input2) + margin))"}
{"question_id": 107, "response": "loss = tf.reduce_mean(tf.maximum(tf.reduce_sum(tf.square(anchor - positive), axis=-1) - tf.reduce_sum(tf.square(anchor - negative), axis=-1) + margin, 0.0))"}
{"question_id": 108, "response": "loss = tf.reduce_mean(tf.keras.losses.kl_divergence(tf.nn.softmax(target_tensor), tf.nn.softmax(input_tensor)))"}
{"question_id": 109, "response": "def my_custom_loss(my_outputs, my_labels):\n    my_batch_size = tf.shape(my_outputs)[0]\n    my_outputs = tf.nn.log_softmax(my_outputs, axis=1)\n    my_outputs = tf.gather_nd(my_outputs, tf.stack([tf.range(my_batch_size), my_labels], axis=1))\n    return -tf.reduce_sum(my_outputs) / tf.cast(my_batch_size, tf.float32)"}
{"question_id": 110, "response": "class DiceLoss(tf.keras.losses.Loss):\n    def __init__(self, smooth=1):\n        super().__init__()\n        self.smooth = smooth\n        \n    def call(self, y_true, y_pred):\n        y_pred = tf.sigmoid(y_pred)\n        y_pred = tf.reshape(y_pred, [-1])\n        y_true = tf.reshape(y_true, [-1])\n        intersection = tf.reduce_sum(y_pred * y_true)\n        dice = (2.0 * intersection + self.smooth) / (tf.reduce_sum(y_pred) + tf.reduce_sum(y_true) + self.smooth)\n        return 1 - dice"}
{"question_id": 111, "response": "pairwise_distance = tf.norm(tf.expand_dims(tensor1, 0) - tf.expand_dims(tensor2, 0), ord=2, axis=-1)"}
{"question_id": 112, "response": "loss = tf.reduce_mean(tf.exp(input_tensor) - target_tensor * input_tensor)"}
{"question_id": 113, "response": "loss = 0.5 * (tf.math.log(2 * np.pi * var_tensor) + tf.square(input_tensor - target_tensor) / var_tensor)\nloss = tf.reduce_mean(loss)"}
{"question_id": 114, "response": "bce_loss = tf.keras.losses.BinaryCrossentropy()\nloss = bce_loss(target_tensor, input_tensor)"}
{"question_id": 115, "response": "output_tensor = tf.nn.dropout(input_tensor, rate=0.3)"}
{"question_id": 116, "response": "class DynamicNet(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        self.a = tf.Variable(tf.random.normal([]))\n        self.b = tf.Variable(tf.random.normal([]))\n        self.c = tf.Variable(tf.random.normal([]))\n        self.d = tf.Variable(tf.random.normal([]))\n        self.e = tf.Variable(tf.random.normal([]))\n\n    def call(self, x):\n        y = self.a + self.b * x + self.c * tf.pow(x, 2) + self.d * tf.pow(x, 3)\n        for exp in range(4, random.randint(4, 6)):\n            y = y + self.e * tf.pow(x, exp)\n        return y\n\nmodel = DynamicNet()"}
{"question_id": 117, "response": "class CustomLayer(layers.Layer):\n    def __init__(self, in_features, out_features, custom_param):\n        super(CustomLayer, self).__init__()\n        self.out_features = out_features\n        self.in_features = in_features\n        self.custom_param = custom_param\n        \n    def build(self, input_shape):\n        initializer = tf.keras.initializers.HeUniform()\n        self.weight = self.add_weight(\n            shape=(self.in_features, self.out_features),\n            initializer=initializer,\n            trainable=True,\n            name='weight'\n        )\n        self.bias = None\n        \n    def call(self, inputs):\n        return tf.matmul(inputs, self.weight)\n\nmodel = CustomLayer(2, 2, custom_param=0.5)"}
{"question_id": 118, "response": "long_tensor = tf.cast(input_tensor, dtype=tf.int64)"}
{"question_id": 119, "response": "class Net(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(32, 3, 1, padding='valid')\n        self.conv2 = tf.keras.layers.Conv2D(64, 3, 1, padding='valid')\n        self.dropout1 = tf.keras.layers.Dropout(0.25)\n        self.dropout2 = tf.keras.layers.Dropout(0.5)\n        self.flatten = tf.keras.layers.Flatten()\n        self.fc1 = tf.keras.layers.Dense(128)\n        self.fc2 = tf.keras.layers.Dense(10)\n\n    def call(self, x, training=False):\n        x = self.conv1(x)\n        x = tf.nn.relu(x)\n        x = self.conv2(x)\n        x = tf.nn.relu(x)\n        x = tf.nn.max_pool2d(x, 2, 2, padding='VALID')\n        x = self.dropout1(x, training=training)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = tf.nn.relu(x)\n        x = self.dropout2(x, training=training)\n        x = self.fc2(x)\n        output = tf.nn.log_softmax(x, axis=1)\n        return output\n\nmodel = Net()"}
{"question_id": 120, "response": "total_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_variables])"}
{"question_id": 121, "response": "class VAE(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = tf.keras.layers.Dense(400, activation='relu')\n        self.fc21 = tf.keras.layers.Dense(20)\n        self.fc22 = tf.keras.layers.Dense(20)\n        self.fc3 = tf.keras.layers.Dense(400, activation='relu')\n        self.fc4 = tf.keras.layers.Dense(784, activation='sigmoid')\n\n    def encode(self, x):\n        h1 = self.fc1(x)\n        return self.fc21(h1), self.fc22(h1)\n\n    def reparameterize(self, mu, logvar):\n        std = tf.exp(0.5 * logvar)\n        eps = tf.random.normal(shape=std.shape)\n        return mu + eps * std\n\n    def decode(self, z):\n        h3 = self.fc3(z)\n        return self.fc4(h3)\n\n    def call(self, x):\n        x = tf.reshape(x, [-1, 784])\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\nmodel = VAE()"}
{"question_id": 122, "response": "class Generator(Model):\n    def __init__(self):\n        super().__init__()\n        nz = 100\n        ngf = 64\n        nc = 3\n        \n        self.model = Sequential([\n            Conv2DTranspose(ngf * 8, kernel_size=4, strides=1, padding='valid', use_bias=False),\n            BatchNormalization(),\n            ReLU(),\n            Conv2DTranspose(ngf * 4, kernel_size=4, strides=2, padding='same', use_bias=False),\n            BatchNormalization(),\n            ReLU(),\n            Conv2DTranspose(ngf * 2, kernel_size=4, strides=2, padding='same', use_bias=False),\n            BatchNormalization(),\n            ReLU(),\n            Conv2DTranspose(ngf, kernel_size=4, strides=2, padding='same', use_bias=False),\n            BatchNormalization(),\n            ReLU(),\n            Conv2DTranspose(nc, kernel_size=4, strides=2, padding='same', use_bias=False),\n            Activation('tanh')\n        ])\n\n    def call(self, x):\n        return self.model(x)\n\nmodel = Generator()"}
{"question_id": 123, "response": "class BoWClassifier(tf.keras.Model):\n    \n    def __init__(self, num_labels, vocab_size):\n        super().__init__()\n        self.linear = tf.keras.layers.Dense(num_labels)\n        \n    def call(self, bow_vec):\n        return tf.nn.log_softmax(self.linear(bow_vec), axis=1)\n\nmodel = BoWClassifier(num_labels=2, vocab_size=1000)"}
{"question_id": 124, "response": "class LSTMTagger(tf.keras.Model):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.word_embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.lstm = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)\n        self.hidden2tag = tf.keras.layers.Dense(tagset_size)\n\n    def call(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        embeds = tf.expand_dims(embeds, axis=0)\n        lstm_out = self.lstm(embeds)\n        tag_space = self.hidden2tag(lstm_out)\n        tag_scores = tf.nn.log_softmax(tag_space, axis=-1)\n        return tag_scores\n\nmodel = LSTMTagger(embedding_dim=6, hidden_dim=6, vocab_size=10, tagset_size=3)"}
{"question_id": 125, "response": "class EncoderRNN(Model):\n    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embedding = layers.Embedding(input_size, hidden_size)\n        self.gru = layers.GRU(hidden_size, return_sequences=True, return_state=True)\n        self.dropout = layers.Dropout(dropout_p)\n\n    def call(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.gru(embedded)\n        return output, hidden\n\nmodel = EncoderRNN(input_size=10, hidden_size=20, dropout_p=0.1)"}
{"question_id": 126, "response": "class DecoderRNN(Model):\n    def __init__(self, hidden_size, output_size):\n        super().__init__()\n        self.embedding = layers.Embedding(output_size, hidden_size)\n        self.gru = layers.GRU(hidden_size, return_sequences=True, return_state=True)\n        self.out = layers.Dense(output_size)\n        self.SOS_token = 0\n        self.MAX_LENGTH = 5\n\n    def call(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = tf.shape(encoder_outputs)[0]\n        decoder_input = tf.fill([batch_size, 1], self.SOS_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n\n        for i in range(self.MAX_LENGTH):\n            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n            if target_tensor is not None:\n                decoder_input = tf.expand_dims(target_tensor[:, i], 1)\n            else:\n                decoder_input = tf.argmax(decoder_output, axis=-1)\n\n        decoder_outputs = tf.concat(decoder_outputs, axis=1)\n        decoder_outputs = tf.nn.log_softmax(decoder_outputs)\n        return decoder_outputs, decoder_hidden, None\n\n    def forward_step(self, input, hidden):\n        output = self.embedding(input)\n        output = tf.nn.relu(output)\n        output, hidden = self.gru(output, initial_state=hidden)\n        output = self.out(output)\n        return output, hidden\n\nmodel = DecoderRNN(hidden_size=20, output_size=10)"}
{"question_id": 127, "response": "class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.Wa = layers.Dense(hidden_size)\n        self.Ua = layers.Dense(hidden_size)\n        self.Va = layers.Dense(1)\n\n    def call(self, query, keys):\n        scores = self.Va(tf.nn.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = tf.squeeze(scores, axis=2)\n        scores = tf.expand_dims(scores, axis=1)\n        weights = tf.nn.softmax(scores, axis=-1)\n        context = tf.matmul(weights, keys)\n        return context, weights"}
{"question_id": 128, "response": "class SimpleMLP(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = tf.keras.layers.Dense(128)\n        self.fc2 = tf.keras.layers.Dense(128)\n        self.fc3 = tf.keras.layers.Dense(10)\n        self.flatten = tf.keras.layers.Flatten()\n\n    def call(self, x):\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = tf.nn.relu(x)\n        x = self.fc2(x)\n        x = tf.nn.relu(x)\n        x = self.fc3(x)\n        return x"}
{"question_id": 129, "response": "class TextClassificationModel(tf.keras.Model):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n        self.fc = tf.keras.layers.Dense(num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        initial_embedding_weights = tf.random.uniform(\n            shape=(self.embedding.input_dim, self.embedding.output_dim),\n            minval=-initrange,\n            maxval=initrange\n        )\n        initial_fc_weights = tf.random.uniform(\n            shape=(self.embedding.output_dim, self.fc.units),\n            minval=-initrange,\n            maxval=initrange\n        )\n        self.embedding.set_weights([initial_embedding_weights])\n        self.fc.set_weights([initial_fc_weights, tf.zeros(self.fc.units)])\n\n    def call(self, inputs):\n        x = self.embedding(inputs)\n        x = tf.reduce_mean(x, axis=1)\n        return self.fc(x)"}
{"question_id": 130, "response": "class Net(Model):\n    def __init__(self, upscale_factor):\n        super().__init__()\n        self.upscale_factor = upscale_factor\n        self.relu = layers.ReLU()\n        self.conv1 = layers.Conv2D(64, (5, 5), padding='same')\n        self.conv2 = layers.Conv2D(64, (3, 3), padding='same')\n        self.conv3 = layers.Conv2D(32, (3, 3), padding='same')\n        self.conv4 = layers.Conv2D(upscale_factor ** 2, (3, 3), padding='same')\n        self._initialize_weights()\n\n    def call(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.conv4(x)\n        batch_size = tf.shape(x)[0]\n        h = tf.shape(x)[1]\n        w = tf.shape(x)[2]\n        x = tf.reshape(x, [batch_size, h, w, self.upscale_factor, self.upscale_factor])\n        x = tf.transpose(x, [0, 1, 3, 2, 4])\n        x = tf.reshape(x, [batch_size, h * self.upscale_factor, w * self.upscale_factor, 1])\n        return x\n\n    def _initialize_weights(self):\n        for layer in [self.conv1, self.conv2, self.conv3]:\n            weights = tf.keras.initializers.Orthogonal(gain=tf.math.sqrt(2.0))(shape=layer.kernel.shape)\n            layer.kernel.assign(weights)\n        weights = tf.keras.initializers.Orthogonal()(shape=self.conv4.kernel.shape)\n        self.conv4.kernel.assign(weights)\n\nmodel = Net(upscale_factor=2)"}
{"question_id": 131, "response": "class Net(Model):\n    def __init__(self):\n        super().__init__()\n        self.rnn = tf.keras.layers.LSTM(64, input_shape=(28, 28), return_sequences=True)\n        self.batchnorm = tf.keras.layers.BatchNormalization()\n        self.dropout1 = tf.keras.layers.Dropout(0.25)\n        self.dropout2 = tf.keras.layers.Dropout(0.5)\n        self.fc1 = tf.keras.layers.Dense(32)\n        self.fc2 = tf.keras.layers.Dense(10)\n\n    def call(self, inputs, training=False):\n        x = tf.reshape(inputs, [-1, 28, 28])\n        x = self.rnn(x)\n        x = x[:, -1, :]\n        x = self.batchnorm(x, training=training)\n        x = self.dropout1(x, training=training)\n        x = self.fc1(x)\n        x = tf.nn.relu(x)\n        x = self.dropout2(x, training=training)\n        x = self.fc2(x)\n        x = tf.nn.log_softmax(x, axis=1)\n        return x\n\nmodel = Net()"}
{"question_id": 132, "response": "class Net(Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv2D(10, kernel_size=5, activation='relu', input_shape=(28, 28, 1))\n        self.conv2 = Conv2D(20, kernel_size=5, activation='relu')\n        self.dropout2d = Dropout(0.25)\n        self.flatten = Flatten()\n        self.fc1 = Dense(50, activation='relu')\n        self.dropout = Dropout(0.5)\n        self.fc2 = Dense(10)\n        \n    def call(self, x, training=False):\n        x = self.conv1(x)\n        x = MaxPool2D(2)(x)\n        x = self.conv2(x)\n        x = self.dropout2d(x, training=training)\n        x = MaxPool2D(2)(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.dropout(x, training=training)\n        x = self.fc2(x)\n        return tf.nn.log_softmax(x, axis=1)\n\nmodel = Net()"}
{"question_id": 133, "response": "def loss_function(recon_x, x, mu, logvar):\n    BCE = tf.reduce_sum(tf.keras.losses.binary_crossentropy(tf.reshape(x, [-1, 784]), recon_x))\n    KLD = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar))\n    return BCE + KLD"}
{"question_id": 134, "response": "class Policy(Model):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = layers.Dense(128, activation=None)\n        self.dropout = layers.Dropout(0.6)\n        self.fc2 = layers.Dense(2)\n        \n        self.saved_log_probs = []\n        self.rewards = []\n    \n    def call(self, x):\n        x = self.fc1(x)\n        x = self.dropout(x)\n        x = tf.nn.relu(x)\n        action_scores = self.fc2(x)\n        return tf.nn.softmax(action_scores)"}
{"question_id": 135, "response": "model = TFBertModel.from_pretrained('bert-base-cased')"}
{"question_id": 136, "response": "class DeepNN(Model):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = Sequential([\n            Conv2D(128, kernel_size=3, padding='same', input_shape=(32, 32, 3)),\n            ReLU(),\n            Conv2D(64, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2),\n            Conv2D(64, kernel_size=3, padding='same'),\n            ReLU(),\n            Conv2D(32, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        self.classifier = Sequential([\n            Flatten(),\n            Dense(512),\n            ReLU(),\n            Dropout(0.1),\n            Dense(num_classes)\n        ])\n\n    def call(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x"}
{"question_id": 137, "response": "class LightNN(Model):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = Sequential([\n            Conv2D(16, kernel_size=3, padding='same', input_shape=(32, 32, 3)),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2),\n            Conv2D(16, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        self.classifier = Sequential([\n            Flatten(),\n            Dense(256),\n            ReLU(),\n            Dropout(0.1),\n            Dense(num_classes)\n        ])\n\n    def call(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x"}
{"question_id": 138, "response": "class ModifiedDeepNNCosine(Model):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = Sequential([\n            Conv2D(128, kernel_size=3, padding='same', input_shape=(32, 32, 3)),\n            ReLU(),\n            Conv2D(64, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2),\n            Conv2D(64, kernel_size=3, padding='same'),\n            ReLU(),\n            Conv2D(32, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        \n        self.classifier = Sequential([\n            Flatten(),\n            Dense(512),\n            ReLU(),\n            Dropout(0.1),\n            Dense(num_classes)\n        ])\n        \n    def call(self, x):\n        x = tf.transpose(x, [0, 2, 3, 1])\n        features = self.features(x)\n        flattened_conv_output = tf.reshape(features, [tf.shape(features)[0], -1])\n        x = self.classifier(features)\n        flattened_conv_output_after_pooling = tf.nn.avg_pool1d(\n            tf.expand_dims(flattened_conv_output, 1),\n            ksize=2,\n            strides=2,\n            padding='VALID'\n        )\n        flattened_conv_output_after_pooling = tf.squeeze(flattened_conv_output_after_pooling, 1)\n        return x, flattened_conv_output_after_pooling"}
{"question_id": 139, "response": "class ModifiedLightNNCosine(Model):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = Sequential([\n            Conv2D(16, kernel_size=3, padding='same', input_shape=(32, 32, 3)),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2),\n            Conv2D(16, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        self.classifier = Sequential([\n            Dense(256),\n            ReLU(),\n            Dropout(0.1),\n            Dense(num_classes)\n        ])\n        self.flatten = Flatten()\n\n    def call(self, x):\n        x = self.features(x)\n        flattened_conv_output = self.flatten(x)\n        x = self.classifier(flattened_conv_output)\n        return x, flattened_conv_output"}
{"question_id": 140, "response": "class ModifiedDeepNNRegressor(Model):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = Sequential([\n            Conv2D(128, kernel_size=3, padding='same', input_shape=(64, 64, 3)),\n            ReLU(),\n            Conv2D(64, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2),\n            Conv2D(64, kernel_size=3, padding='same'),\n            ReLU(),\n            Conv2D(32, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        self.classifier = Sequential([\n            Flatten(),\n            Dense(512),\n            ReLU(),\n            Dropout(0.1),\n            Dense(num_classes)\n        ])\n\n    def call(self, x):\n        x = self.features(x)\n        conv_feature_map = x\n        x = self.classifier(x)\n        return x, conv_feature_map"}
{"question_id": 141, "response": "class ModifiedLightNNRegressor(Model):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = Sequential([\n            Conv2D(16, kernel_size=3, padding='same', input_shape=(64, 64, 3)),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2),\n            Conv2D(16, kernel_size=3, padding='same'),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        self.regressor = Sequential([\n            Conv2D(32, kernel_size=3, padding='same')\n        ])\n        self.classifier = Sequential([\n            Flatten(),\n            Dense(256),\n            ReLU(),\n            Dropout(0.1),\n            Dense(num_classes)\n        ])\n\n    def call(self, x):\n        x = self.features(x)\n        regressor_output = self.regressor(x)\n        x = self.classifier(x)\n        return x, regressor_output"}
{"question_id": 142, "response": "class BiRNN(Model):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = [Bidirectional(LSTM(hidden_size, return_sequences=True)) for _ in range(num_layers)]\n        self.fc = Dense(num_classes)\n\n    def call(self, x):\n        for lstm_layer in self.lstm:\n            x = lstm_layer(x)\n        out = self.fc(x[:, -1, :])\n        return out"}
{"question_id": 143, "response": "class ConvNet(Model):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.layer1 = Sequential([\n            Conv2D(16, kernel_size=5, strides=1, padding='same', input_shape=(None, None, 1)),\n            BatchNormalization(),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        self.layer2 = Sequential([\n            Conv2D(32, kernel_size=5, strides=1, padding='same'),\n            BatchNormalization(),\n            ReLU(),\n            MaxPool2D(pool_size=2, strides=2)\n        ])\n        self.fc = Dense(num_classes)\n\n    def call(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = tf.reshape(out, [tf.shape(out)[0], -1])\n        out = self.fc(out)\n        return out"}
{"question_id": 144, "response": "class AutoEncoder(Model):\n    def __init__(self):\n        super().__init__()\n        \n        self.encoder = Sequential([\n            Dense(128, activation='tanh', input_shape=(28*28,)),\n            Dense(64, activation='tanh'),\n            Dense(12, activation='tanh'),\n            Dense(3)\n        ])\n        \n        self.decoder = Sequential([\n            Dense(12, activation='tanh'),\n            Dense(64, activation='tanh'),\n            Dense(128, activation='tanh'),\n            Dense(28*28, activation='sigmoid')\n        ])\n\n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return encoded, decoded"}
{"question_id": 145, "response": "class DenseLayer(Model):\n    def __init__(self, in_channels):\n        super().__init__()\n        k = 12\n        self.BN1 = layers.BatchNormalization()\n        self.conv1 = layers.Conv2D(filters=4*k, kernel_size=1, strides=1, padding='valid', use_bias=False)\n        self.BN2 = layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(filters=k, kernel_size=3, strides=1, padding='same', use_bias=False)\n        self.relu = layers.ReLU()\n\n    def call(self, x):\n        xin = x\n        x = self.BN1(x)\n        x = self.relu(x)\n        x = self.conv1(x)\n        x = self.BN2(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = tf.concat([xin, x], axis=3)\n        return x"}
{"question_id": 146, "response": "class TransitionLayer(Model):\n    def __init__(self, in_channels, compression_factor):\n        super().__init__()\n        self.BN = layers.BatchNormalization()\n        self.conv1 = layers.Conv2D(filters=int(in_channels * compression_factor), kernel_size=1, strides=1, padding='valid', use_bias=False)\n        self.avgpool = layers.AveragePooling2D(pool_size=2, strides=2)\n\n    def call(self, x):\n        x = self.BN(x)\n        x = self.conv1(x)\n        x = self.avgpool(x)\n        return x"}
{"question_id": 147, "response": "class ConvBlock(Model):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super().__init__()\n        self.conv2d = Conv2D(filters=out_channels, kernel_size=kernel_size, strides=stride, padding='same' if padding else 'valid', use_bias=bias)\n        self.batchnorm2d = BatchNormalization()\n        self.relu = ReLU()\n\n    def call(self, x):\n        x = self.conv2d(x)\n        x = self.batchnorm2d(x)\n        return self.relu(x)"}
{"question_id": 148, "response": "class TwoLayerNet(Model):\n    def __init__(self, D_in, H, D_out):\n        super().__init__()\n        self.linear1 = Dense(H, input_shape=(D_in,))\n        self.linear2 = Dense(D_out)\n\n    def call(self, x):\n        h_relu = tf.nn.relu(self.linear1(x))\n        y_pred = self.linear2(h_relu)\n        return y_pred"}
{"question_id": 149, "response": "eigvals = tf.linalg.eigvals(square_matrix)"}
{"question_id": 150, "response": "initializer = tf.keras.initializers.GlorotUniform()\nconv1.kernel_initializer = initializer"}
{"question_id": 151, "response": "y = tf.identity(x)"}
{"question_id": 152, "response": "y = tf.transpose(x, perm=[1, 2, 0])"}
{"question_id": 153, "response": "x = tf.random.normal(shape=(100,), mean=2, stddev=4)"}
{"question_id": 154, "response": "one_hot = tf.one_hot(indices, n)"}
{"question_id": 155, "response": "mask = tf.cast(tf.expand_dims(lens, 1) > tf.range(max_len), tf.int64)"}
{"question_id": 156, "response": "t_integer = tf.cast(t, tf.int64)"}
{"question_id": 157, "response": "noisy_t = t + tf.random.normal(shape=t.shape) * std + mean"}
{"question_id": 158, "response": "y = tf.tile(tf.expand_dims(x, axis=0), [6, 1, 1])"}
{"question_id": 159, "response": "padded_tensor = tf.keras.preprocessing.sequence.pad_sequences([a, b, c], padding='post')"}
{"question_id": 160, "response": "mask = tf.logical_and(tf.greater(t, 3), tf.less(t, 7))"}
{"question_id": 161, "response": "ds_sampled = ds.shuffle(N, reshuffle_each_iteration=True).take(M).batch(batch_size)"}
{"question_id": 162, "response": "layer = tf.keras.layers.StringLookup(vocabulary=vocab)\noutput = layer(data)"}
{"question_id": 163, "response": "valid_cols = []\nfor col_idx in range(A.shape[1]):\n    if not tf.reduce_all(tf.equal(A[:, col_idx], 0)):\n        valid_cols.append(col_idx)\nA = tf.gather(tf.transpose(A), valid_cols, axis=0)\nA = tf.transpose(A)"}
{"question_id": 164, "response": "k = round(percent_ones * d)\nk_th_quant = tf.sort(rand_mat, axis=1)[:, k-1:k]\nbool_tensor = rand_mat <= k_th_quant\ndesired_tensor = tf.where(bool_tensor, tf.ones_like(rand_mat), tf.zeros_like(rand_mat))"}
{"question_id": 165, "response": "idx = tf.tile(ids, [1, 2])\nidx = tf.reshape(idx, [4, 1, 2])\nresult = tf.gather(x, idx, batch_dims=1)"}
{"question_id": 166, "response": "num_equal = tf.reduce_sum(tf.cast(tf.equal(A, B), tf.int32))"}
{"question_id": 167, "response": "A = tf.tensor_scatter_nd_add(A, tf.expand_dims(B, 1), C)"}
{"question_id": 168, "response": "y = tf.sort(x, axis=-1)\nindices = tf.argsort(x, axis=-1)\nmask = tf.cast(y[:, 1:] - y[:, :-1] != 0, tf.int64)\ny_modified = tf.concat([y[:, :1], y[:, 1:] * mask], axis=1)\ninverse_indices = tf.argsort(indices, axis=-1)\nresult = tf.gather(y_modified, inverse_indices, batch_dims=1)"}
{"question_id": 169, "response": "old_new_dict = {k.numpy(): v.numpy() for k, v in zip(old_new_value[:, 0], old_new_value[:, 1])}\nresult = tf.identity(old_values)\nfor old_val, new_val in old_new_dict.items():\n    mask = tf.equal(old_values, old_val)\n    result = tf.where(mask, new_val, result)"}
{"question_id": 170, "response": "conv2 = tf.keras.layers.Conv2D(18, kernel_size=3, strides=1, padding='same')\nimage_d = tf.reshape(random_image_arr, (1, 27, 35, 1))\nfc = conv2(image_d)"}
{"question_id": 171, "response": "norm_2 = tf.norm(t, axis=1, ord=2)"}
{"question_id": 172, "response": "comparison_matrix = tf.equal(tf.expand_dims(B, 1), tf.expand_dims(A, 0))\nmatches_per_element = tf.reduce_sum(tf.cast(comparison_matrix, tf.int32), axis=1)\n\nexists = tf.reduce_sum(tf.cast(matches_per_element > 0, tf.int32)).numpy()\nnot_exist = tf.size(B).numpy() - exists"}
{"question_id": 173, "response": "output = tf.gather_nd(tensor, tf.stack([tf.range(len(B)), B], axis=1))"}
{"question_id": 174, "response": "def get_unique_elements_first_idx(tensor):\n    sorted_tensor, indices = tf.nn.top_k(-tensor, k=tf.size(tensor))\n    sorted_tensor = -sorted_tensor\n    unique_mask = tf.concat([tf.constant([True]), tf.not_equal(sorted_tensor[1:], sorted_tensor[:-1])], axis=0)\n    return tf.gather(indices, tf.where(unique_mask)[:, 0])\n\nunique_indices = get_unique_elements_first_idx(v2_without_v1)"}
{"question_id": 175, "response": "class ValidatedArray:\n    def __init__(self, array: tf.Tensor):\n        self._array = array\n        self._validate_array()\n        \n    def _validate_array(self):\n        sums = tf.reduce_sum(self._array, axis=-1)\n        ones = tf.ones(self._array.shape[:-1])\n        assert tf.reduce_all(tf.abs(sums - ones) < 1e-6), f'The last dim represents a categorical distribution. It must sum to one.'\n\n    @property\n    def array(self):\n        self._validate_array()\n        return self._array"}
{"question_id": 176, "response": "split_tensors = tf.split(tensor, num_or_size_splits=size, axis=dimension)\nsplit_tensors = tf.stack(split_tensors)"}
{"question_id": 177, "response": "# Determine maximum height and width\nmax_height = max([img.shape[0] for img in image_batch])\nmax_width = max([img.shape[1] for img in image_batch])\n\n# Pad the image batch\nimage_batch = [tf.pad(img, [[0, max_height - img.shape[0]], [0, max_width - img.shape[1]], [0, 0]]) for img in image_batch]\n\n# Pad the mask batch\nmask_batch = [tf.pad(mask, [[0, max_height - mask.shape[0]], [0, max_width - mask.shape[1]]]) for mask in mask_batch]"}
{"question_id": 178, "response": "a_masked = tf.cast(tf.reduce_sum(tf.cast(tf.equal(a[:, tf.newaxis], b), tf.int32), axis=1), tf.bool) | tf.cast(tf.reduce_sum(tf.cast(tf.equal(a[:, tf.newaxis], c), tf.int32), axis=1), tf.bool)"}
{"question_id": 179, "response": "B, C, H, W = x.shape\nz = tf.zeros_like(x)\n\nx_norm = tf.norm(x, axis=[2, 3])\ny_norm = tf.norm(y, axis=[2, 3])\n\ncondition = tf.greater_equal(x_norm, y_norm)\ncondition = tf.expand_dims(tf.expand_dims(condition, -1), -1)\ncondition = tf.broadcast_to(condition, x.shape)\n\nz = tf.where(condition, x, y)"}
{"question_id": 180, "response": "out = tf.gather_nd(a, tf.stack([tf.range(tf.shape(a)[0]), b], axis=1))"}
{"question_id": 181, "response": "t1 = tf.tensor_scatter_nd_update(t1, tf.stack([tf.range(tf.shape(indexes)[0])[:, None], indexes], axis=-1), t2)"}
{"question_id": 182, "response": "dist = tf.sqrt(tf.reduce_sum(tf.square(tensor1 - tensor2), axis=3))"}
{"question_id": 183, "response": "tf.tensor_scatter_nd_add(v, tf.expand_dims(index, 1), w)"}
{"question_id": 184, "response": "ind = tf.argsort(a[:, -1])\nsorted_a = tf.gather(a, ind)"}
{"question_id": 185, "response": "model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, input_shape=(n_features,), activation=\"relu\"),\n    tf.keras.layers.Dense(36, activation=\"softmax\", use_bias=False)\n])"}
{"question_id": 186, "response": "class Combined_model(tf.keras.Model):\n    def __init__(self, modelA, modelB):\n        super(Combined_model, self).__init__()\n        self.modelA = modelA\n        self.modelB = modelB\n        self.classifier = tf.keras.layers.Dense(2)\n        \n    def call(self, x1, x2):\n        x1 = self.modelA(x1)\n        x2 = self.modelB(x2)\n        x = tf.concat([x1, x2], axis=1)\n        x = tf.nn.relu(x)\n        x = self.classifier(x)\n        return x"}
{"question_id": 187, "response": "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\nbatch_loss = loss_fn(target, source)\nweighted_loss = multiplier * batch_loss\nloss = tf.reduce_sum(weighted_loss) / tf.reduce_sum(multiplier)"}
{"question_id": 188, "response": "B = tf.repeat(tf.expand_dims(A, axis=1), repeats=K, axis=1)"}
{"question_id": 189, "response": "def sum_by_index(labels, source):\n    unique_labels, _, labels_counts = tf.unique_with_counts(tf.reshape(labels, [-1]))\n    out = tf.zeros((tf.shape(unique_labels)[0], tf.shape(source)[-1]), dtype=source.dtype)\n    flattened_source = tf.reshape(source, [-1, tf.shape(source)[-1]])\n    indices = tf.expand_dims(labels_inds, 1)\n    updates = flattened_source\n    out = tf.tensor_scatter_nd_add(out, indices, updates)\n    return out\n\nlabels = tf.constant([[0, 1], [1, 2]])\nsource = tf.constant(\n          [[[0, 1], \n           [1, 2]], \n\n           [[2, 3],\n            [3, 4]]], \n        dtype=tf.float32\n)\nresult = sum_by_index(labels, source)"}
{"question_id": 190, "response": "x = tf.concat([original[:row_exclude], original[row_exclude+1:]], axis=0)"}
{"question_id": 191, "response": "class TinyVGG(tf.keras.Model):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n        super().__init__()\n        \n        self.conv_block1 = tf.keras.Sequential([\n            tf.keras.layers.Conv2D(filters=hidden_units, kernel_size=3, strides=1, padding='same', input_shape=(224, 224, input_shape)),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n            tf.keras.layers.Conv2D(filters=hidden_units, kernel_size=3, strides=1, padding='same'),\n            tf.keras.layers.ReLU(), \n            tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n            tf.keras.layers.Conv2D(filters=hidden_units, kernel_size=3, strides=1, padding='same'),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.MaxPool2D(pool_size=2, strides=2)\n        ])\n        \n        self.dropout = tf.keras.layers.Dropout(0.4)\n        \n        self.classifier = tf.keras.Sequential([\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(output_shape)\n        ])\n        \n    def call(self, x: tf.Tensor):\n        x = self.conv_block1(x)\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x"}
{"question_id": 192, "response": "one_hot = tf.one_hot(tf.argmax(a), depth=tf.size(a))"}
{"question_id": 193, "response": "def resize(image):\n    return tf.image.resize(image, [224, 224])\n\ndef normalize(image):\n    return image / 255.0\n\ndef transform(image, label):\n    image = resize(image)\n    image = normalize(image)\n    return image, label\n\ntransformed_dataset = dataset.map(transform)"}
{"question_id": 194, "response": "d = tf.concat([a, b, c], axis=1)\ne = tf.reshape(d, (8, 3, -1))"}
{"question_id": 195, "response": "result = tf.concat([a, tf.zeros((n - 1, a.shape[1], a.shape[2]))], axis=0)"}
{"question_id": 196, "response": "indices = tf.stack([starts, ends], axis=1)\nresult = tf.stack([data[idx[0]:idx[1]] for idx in indices])"}
{"question_id": 197, "response": "result = x[:, tf.newaxis] - x[tf.newaxis, :]"}
{"question_id": 198, "response": "indices = tf.argsort(a[:, :, sorted_column], axis=sorted_dim)\nbatch_indices = tf.range(tf.shape(a)[0])[:, tf.newaxis]\nbatch_indices = tf.tile(batch_indices, [1, tf.shape(a)[1]])\ngather_indices = tf.stack([batch_indices, indices], axis=-1)\nsorted_a = tf.gather_nd(a, gather_indices)"}
{"question_id": 199, "response": "train_examples = []\nfor item in in_dict['train']:\n    in_tensor = tf.convert_to_tensor(item['input'], dtype=tf.float32)\n    out_tensor = tf.convert_to_tensor(item['output'], dtype=tf.float32)\n    train_examples.append([in_tensor, out_tensor])\n\nfor item in train_examples:\n    item[0] = tf.pad(item[0], [[1, 1], [1, 1]])\n    item[1] = tf.pad(item[1], [[1, 1], [1, 1]])\n\nout_dict = {'train': []}\nfor item in train_examples:\n    out_dict['train'].append({\n        'input': item[0].numpy().tolist(),\n        'output': item[1].numpy().tolist()\n    })"}
